{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d483674f-429c-4679-bca0-eeaf59a184c4",
   "metadata": {},
   "source": [
    "# Small models - July 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b729d-a1a0-4e09-8c62-116d3e4da7e9",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebfaa9-103a-4c9b-9780-346313cd7d12",
   "metadata": {},
   "source": [
    "**Important :** this will only work if **cuda 12.4.0** and Pytorch 2.4.0 are installed in the underlying conda environment.\n",
    "\n",
    "```bash\n",
    "conda install -y cuda -c nvidia/label/cuda-12.4.0\n",
    "\n",
    "conda install -y pytorch=2.4.0 torchvision=0.19.0 torchaudio=2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia/label/cuda-12.4.0\n",
    "```\n",
    "\n",
    "The FP8 kernels are not supported in cuda 12.1.0 required by pytorch 2.3.1.\n",
    "\n",
    "Starting with version 0.5.4, VLLM requires pytorch 2.4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389414a-363c-4bf4-8b7e-5b37576ffbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea41d4-0891-4472-9099-887e67993121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6839c4-f252-43bf-934e-06138254d7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:49:18.446939Z",
     "iopub.status.busy": "2024-08-11T13:49:18.446226Z",
     "iopub.status.idle": "2024-08-11T13:49:18.452181Z",
     "shell.execute_reply": "2024-08-11T13:49:18.451289Z",
     "shell.execute_reply.started": "2024-08-11T13:49:18.446900Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4fd2026-16b5-414c-909d-58df9cad9995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:49:18.963935Z",
     "iopub.status.busy": "2024-08-11T13:49:18.962974Z",
     "iopub.status.idle": "2024-08-11T13:49:18.972867Z",
     "shell.execute_reply": "2024-08-11T13:49:18.972583Z",
     "shell.execute_reply.started": "2024-08-11T13:49:18.963904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5cd0082-9bd8-48d7-a136-671ecbd1a3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:49:20.092011Z",
     "iopub.status.busy": "2024-08-11T13:49:20.091683Z",
     "iopub.status.idle": "2024-08-11T13:49:20.100309Z",
     "shell.execute_reply": "2024-08-11T13:49:20.099661Z",
     "shell.execute_reply.started": "2024-08-11T13:49:20.091988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.44.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedc9c78-8559-4aec-b455-32003407281a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:49:21.219030Z",
     "iopub.status.busy": "2024-08-11T13:49:21.218691Z",
     "iopub.status.idle": "2024-08-11T13:49:21.226930Z",
     "shell.execute_reply": "2024-08-11T13:49:21.226223Z",
     "shell.execute_reply.started": "2024-08-11T13:49:21.219009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131852f7-39be-483f-b0fe-55e65294c8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:49:21.673033Z",
     "iopub.status.busy": "2024-08-11T13:49:21.672754Z",
     "iopub.status.idle": "2024-08-11T13:49:21.679396Z",
     "shell.execute_reply": "2024-08-11T13:49:21.678959Z",
     "shell.execute_reply.started": "2024-08-11T13:49:21.673012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm-flash-attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91c4fb-dba2-44c6-b598-74c39d26ad21",
   "metadata": {},
   "source": [
    "## Mistral 7b v0.3\n",
    "\n",
    "RTX 4090 speed (FP8): **90 tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **57 tokens/sec**\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "\n",
    "https://huggingface.co/neuralmagic/Mistral-7B-Instruct-v0.3-FP8\n",
    "\n",
    "**LICENSE :** Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fc6ac3-be3e-48e3-b774-e41faa03cf79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T20:15:02.852583Z",
     "iopub.status.busy": "2024-08-11T20:15:02.852202Z",
     "iopub.status.idle": "2024-08-11T20:29:30.847366Z",
     "shell.execute_reply": "2024-08-11T20:29:30.846937Z",
     "shell.execute_reply.started": "2024-08-11T20:15:02.852558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d10223f3d54264841204c38e413386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa4bfe1b9d940b6b7cb536be6ad8a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f679be177f9c4d77bc20aaae36a9ce46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2352b82009402f9503028eca0607e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a14415112f4ddcbae02d8170e1319a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 22:15:05 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/Mistral-7B-Instruct-v0.3-FP8', speculative_config=None, tokenizer='neuralmagic/Mistral-7B-Instruct-v0.3-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Mistral-7B-Instruct-v0.3-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150fe83bf83b41338cb90225b46f5ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 22:15:06 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 22:15:06 model_runner.py:720] Starting to load model neuralmagic/Mistral-7B-Instruct-v0.3-FP8...\n",
      "WARNING 08-11 22:15:06 fp8.py:43] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 08-11 22:15:06 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac2e774472483f8203797f04859d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb02f9df3420484ea9e375246668c165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618ebf67100648179f68946ed30d05f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a63cef22854d83bea85b65f7b37cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 22:29:20 model_runner.py:732] Loading model weights took 7.0084 GB\n",
      "INFO 08-11 22:29:21 gpu_executor.py:102] # GPU blocks: 7125, # CPU blocks: 2048\n",
      "INFO 08-11 22:29:21 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 22:29:21 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 22:29:30 model_runner.py:1225] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Mistral-7B-Instruct-v0.3-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4814defe-62db-4407-a50f-19aeea33d3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T20:50:20.483929Z",
     "iopub.status.busy": "2024-08-11T20:50:20.483600Z",
     "iopub.status.idle": "2024-08-11T20:50:27.322456Z",
     "shell.execute_reply": "2024-08-11T20:50:27.322060Z",
     "shell.execute_reply.started": "2024-08-11T20:50:20.483907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:06<00:00,  6.83s/it, est. speed input: 6.30 toks/s, output: 91.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mon cher ami, je suis un simple assistant de conversation et ne suis pas un philosophe vivant. Cependant, je peux vous présenter une interprétation résumée de la philosophie de Voltaire, un des grands philosophes français du siècle des Lumières, sur le sens de la vie.\n",
      "\n",
      "1. La vie est une aventure et une quête permanente de connaissances. Voltaire a écrit : \"La connaissance est la seule chose qui donne de la valeur à la vie.\" Il croit que la vie est une aventure de découverte, de connaissances et de progrès intellectuel. Il encourage les gens à chercher la vérité, à s'éloigner de l'ignorance et à se consacrer à l'étude et à la raison.\n",
      "\n",
      "2. La raison est le guide de la vie. Voltaire croit que la raison est le seul moyen de discerner la vérité et de vivre une vie heureuse. Il est convaincu que la raison permet de faire la différence entre le bien et le mal, et de vivre une vie morale et éthique. Il encourage les gens à utiliser leur raison pour faire le bien et à s'éloigner de la superstition et de l'intolérance.\n",
      "\n",
      "3. La tolérance est la clé de la paix et de la prospérité. Voltaire est connu pour son combat pour la tolérance religieuse et politique. Il croit que la tolérance est la clé de la paix et de la prospérité, et qu'il est important de respecter les différences entre les personnes et de vivre en harmonie.\n",
      "\n",
      "4. La liberté est essentielle pour la prospérité et la paix. Voltaire est un défenseur de la liberté d'expression et de la liberté religieuse. Il croit que la liberté est essentielle pour la prospérité et la paix, et qu'il est important de protéger les droits des individus et de respecter leur liberté.\n",
      "\n",
      "5. La vie est courte et fragile, il est important de la faire compte. Voltaire a écrit : \"La vie est trop courte pour lire toutes les bonnes livres et trop longue pour ne pas en lire aucune.\" Il croit que la vie est courte et fragile, et qu'il est important de la faire compte en cherchant la connaissance, en vivant moralement et en respectant les autres. Il encourage les gens à vivre une vie heureuse et à profiter de chaque journée.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbca7af-3da5-4597-acdf-db71bf6b96b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T20:52:40.998848Z",
     "iopub.status.busy": "2024-08-11T20:52:40.998506Z",
     "iopub.status.idle": "2024-08-11T20:52:41.012690Z",
     "shell.execute_reply": "2024-08-11T20:52:41.012210Z",
     "shell.execute_reply.started": "2024-08-11T20:52:40.998823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f1cf3f-1521-441b-bc8d-cd4ab640d768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T20:52:41.840655Z",
     "iopub.status.busy": "2024-08-11T20:52:41.840298Z",
     "iopub.status.idle": "2024-08-11T21:37:53.031895Z",
     "shell.execute_reply": "2024-08-11T21:37:53.031214Z",
     "shell.execute_reply.started": "2024-08-11T20:52:41.840628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d032a1d2e30441eb1fd3a735a28337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd8f614508d42eeaf2e6f5c6f0faff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585442bce3854fe5b7b021217d368d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad691e43d5e8497487d1b1f0f8fc384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a7446677564af6a014be5ef630d027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 22:52:44 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8cd538b0a4437792b2fa41655cd069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 22:52:44 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 22:52:44 model_runner.py:720] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "INFO 08-11 22:52:45 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c422d644ac947df891510cea9730ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d64fea1aa64de2b4532d36ababdd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed76256c06b844989ff67e87ce464a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f617f9e893492385e3c880183d66e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1281d5cddb84daf87bf5707b0873d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ae3e9b696f4831a5eeca107307cabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 23:37:35 model_runner.py:732] Loading model weights took 13.5083 GB\n",
      "INFO 08-11 23:37:36 gpu_executor.py:102] # GPU blocks: 3835, # CPU blocks: 2048\n",
      "INFO 08-11 23:37:42 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 23:37:42 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 23:37:53 model_runner.py:1225] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f203787-fe8b-4fc8-a997-986a382c3e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T21:54:52.097867Z",
     "iopub.status.busy": "2024-08-11T21:54:52.097532Z",
     "iopub.status.idle": "2024-08-11T21:55:03.173578Z",
     "shell.execute_reply": "2024-08-11T21:55:03.173162Z",
     "shell.execute_reply.started": "2024-08-11T21:54:52.097842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:11<00:00, 11.07s/it, est. speed input: 3.89 toks/s, output: 57.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bienvenue ! Je suis un assistant de conversation basé sur l'IA et je ne suis pas un philosophe vivant, mais je peux tenter de répondre à votre question en vous offrant une synthèse de la pensée de Voltaire, un philosophe français du siècle des Lumières, sur le sens de la vie.\n",
      "\n",
      "1. La vie est un don précieux et fragile : Voltaire considérait la vie comme un don précieux et fragile, qui ne devrait pas être perdu sans raison. Il pensait que la vie devrait être vécue avec respect et gratitude, car elle est rare et précieuse.\n",
      "2. La raison est le guide de la vie : Voltaire pensait que la raison était le guide de la vie. Il pensait que la raison devait être utilisée pour comprendre le monde et pour guider les actions humaines. Il pensait que la raison devait être utilisée pour éliminer les superstitions et les préjugés, et pour promouvoir la tolérance et la compréhension.\n",
      "3. La liberté est la base de la vie : Voltaire pensait que la liberté était la base de la vie. Il pensait que la liberté était nécessaire pour permettre aux individus de vivre en harmonie avec eux-mêmes et avec les autres. Il pensait que la liberté devait être protégée et défendue, car elle est essentielle pour la croissance et le progrès de l'humanité.\n",
      "4. La tolérance est la clé de la paix : Voltaire pensait que la tolérance était la clé de la paix. Il pensait que la tolérance devait être pratiquée envers les autres, même envers ceux qui avaient des croyances différentes. Il pensait que la tolérance devait être pratiquée envers les autres, même envers ceux qui avaient des croyances différentes. Il pensait que la tolérance devait être pratiquée envers les autres, même envers ceux qui avaient des croyances différentes.\n",
      "5. La vie est courte et inconnue : Voltaire pensait que la vie était courte et inconnue. Il pensait que la vie devait être vécue avec passion et zèle, car elle est courte et inconnue. Il pensait que la vie devait être vécue avec passion et zèle, car elle est courte et inconnue. Il pensait que la vie devrait être vécue avec passion et zèle, car elle est courte et inconnue. Il pensait que la vie devrait être vécue avec passion et zèle, car elle est courte et inconnue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2f39-aa24-49e9-ab66-8e40d1a742e3",
   "metadata": {},
   "source": [
    "## Mistral Nemo 12b\n",
    "\n",
    "RTX 4090 speed (FP8): **58 tokens/sec**\n",
    "\n",
    "https://mistral.ai/fr/news/mistral-nemo/\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\n",
    "\n",
    "https://huggingface.co/neuralmagic/Mistral-Nemo-Instruct-2407-FP8\n",
    "\n",
    "**LICENSE :** Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b777ddf-0707-47b9-a1e7-edd82ba83b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:52:28.203907Z",
     "iopub.status.busy": "2024-08-11T13:52:28.203735Z",
     "iopub.status.idle": "2024-08-11T13:52:50.715209Z",
     "shell.execute_reply": "2024-08-11T13:52:50.714635Z",
     "shell.execute_reply.started": "2024-08-11T13:52:28.203898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 15:52:31 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/Mistral-Nemo-Instruct-2407-FP8', speculative_config=None, tokenizer='neuralmagic/Mistral-Nemo-Instruct-2407-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Mistral-Nemo-Instruct-2407-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-11 15:52:31 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 15:52:31 model_runner.py:720] Starting to load model neuralmagic/Mistral-Nemo-Instruct-2407-FP8...\n",
      "WARNING 08-11 15:52:31 fp8.py:43] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 08-11 15:52:32 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc449b88fe54c30b34aa5dc9543b888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 15:52:40 model_runner.py:732] Loading model weights took 12.9013 GB\n",
      "INFO 08-11 15:52:41 gpu_executor.py:102] # GPU blocks: 2922, # CPU blocks: 1638\n",
      "INFO 08-11 15:52:41 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 15:52:41 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 15:52:50 model_runner.py:1225] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Mistral-Nemo-Instruct-2407-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b1050c-bd5f-4d8f-9f8a-519411cd3d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T13:56:56.450627Z",
     "iopub.status.busy": "2024-08-11T13:56:56.450260Z",
     "iopub.status.idle": "2024-08-11T13:57:05.697230Z",
     "shell.execute_reply": "2024-08-11T13:57:05.696873Z",
     "shell.execute_reply.started": "2024-08-11T13:56:56.450598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:09<00:00,  9.24s/it, est. speed input: 3.68 toks/s, output: 58.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis désolé, mais je ne suis pas un grand philosophe français du siècle des lumières. Je suis un modèle de langage développé grâce à l'intelligence artificielle. Cependant, je peux vous donner mon point de vue sur le sens de la vie en cinq paragraphes argumentés.\n",
      "\n",
      "Le sens de la vie est une question qui a été posée depuis des siècles et qui continue de préoccuper les philosophes, les religieux et les individus dans leur vie quotidienne. Il n'y a pas de réponse unique à cette question, car elle dépend de la perspective de chacun. Cependant, il est possible de proposer plusieurs pistes de réflexion pour y répondre.\n",
      "\n",
      "Tout d'abord, on peut considérer que le sens de la vie est lié à la recherche du bonheur et du bien-être. Selon cette perspective, la vie a un sens dans la mesure où elle nous permet de réaliser nos désirs et nos aspirations les plus profondes. Cependant, il est important de noter que la recherche du bonheur ne doit pas être confondue avec l'égoïsme ou l'individualisme excessif. Au contraire, elle doit être tempérée par une préoccupation pour le bien-être des autres et par une prise de responsabilité envers la société dans laquelle nous vivons.\n",
      "\n",
      "Ensuite, on peut considérer que le sens de la vie est lié à la réalisation de soi et à l'accomplissement de ses potentialités. Selon cette perspective, la vie a un sens dans la mesure où elle nous permet de développer nos compétences et nos talents, de nous réaliser en tant qu'individus et de laisser une trace dans le monde. Cela peut prendre différentes formes, telles que la création artistique, la recherche scientifique, l'engagement politique ou l'action humanitaire.\n",
      "\n",
      "Enfin, on peut considérer que le sens de la vie est lié à la quête de sens en soi. Selon cette perspective, la vie a un sens dans la mesure où elle nous permet de poser des questions sur le monde qui nous entoure et sur notre place dans l'univers. Cette quête de sens peut prendre différentes formes, telles que la pratique d'une religion, la méditation, la lecture de livres philosophiques ou la contemplation de la nature.\n",
      "\n",
      "En conclusion, le sens de la vie est une question complexe qui dépend de la perspective de chacun. Cependant, il est possible de proposer plusieurs pistes de réflexion pour y répondre, telles que la recherche du bonheur, la réalisation de soi, l'accomplissement de ses potentialités et la quête de sens en soi. Il est important de noter que ces différentes perspectives ne sont pas exclusives les unes des autres et qu'il est possible de les combiner pour donner un sens à sa vie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf59fbe-27b6-4402-bca6-91979bdf33e8",
   "metadata": {},
   "source": [
    "## Llama 3.1 8b\n",
    "\n",
    "RTX 4090 speed (FP8): **79 tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **54 tokens/sec**\n",
    "\n",
    "https://llama.meta.com/\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\n",
    "\n",
    "**LICENSE :** https://llama.meta.com/llama3_1/license/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f0db3b-262a-4211-a7fa-188e112b681e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:01:18.228859Z",
     "iopub.status.busy": "2024-08-11T14:01:18.228774Z",
     "iopub.status.idle": "2024-08-11T14:01:36.642445Z",
     "shell.execute_reply": "2024-08-11T14:01:36.642154Z",
     "shell.execute_reply.started": "2024-08-11T14:01:18.228852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 16:01:21 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-11 16:01:21 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 16:01:21 model_runner.py:720] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic...\n",
      "INFO 08-11 16:01:22 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98b06a86a24d4d9d5f2ca2fba51dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 16:01:27 model_runner.py:732] Loading model weights took 8.4939 GB\n",
      "INFO 08-11 16:01:27 gpu_executor.py:102] # GPU blocks: 5758, # CPU blocks: 2048\n",
      "INFO 08-11 16:01:28 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 16:01:28 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 16:01:36 model_runner.py:1225] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29beac5f-86ec-4d58-9858-0133b67466ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:06:08.243724Z",
     "iopub.status.busy": "2024-08-11T14:06:08.243347Z",
     "iopub.status.idle": "2024-08-11T14:06:16.066280Z",
     "shell.execute_reply": "2024-08-11T14:06:16.065922Z",
     "shell.execute_reply.started": "2024-08-11T14:06:08.243698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:07<00:00,  7.82s/it, est. speed input: 6.27 toks/s, output: 79.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenue ! Je suis Voltaire, philosophe français du XVIIIe siècle. Je vais vous exposer ma vision sur le sens de la vie, basée sur mes réflexions et mes écrits.\n",
      "\n",
      "Le sens de la vie, c'est-à-dire la raison d'être de l'existence humaine, est un problème éternel qui a préoccupé les penseurs depuis l'Antiquité. Pour moi, le sens de la vie réside dans la recherche du bonheur et de la sagesse. Le bonheur, c'est-à-dire la satisfaction et le contentement, est le but ultime de la vie humaine. Mais, ajouterais-je, il ne s'agit pas d'un bonheur égoïste ou égo-centré, mais bien d'un bonheur qui se trouve dans la recherche de la connaissance, de l'amitié et de la beauté.\n",
      "\n",
      "La recherche de la connaissance est une des voies les plus sûres pour atteindre le bonheur. En effet, la connaissance nous permet de comprendre le monde qui nous entoure, de reconnaître les lois qui régissent la nature et de développer nos capacités intellectuelles. C'est pourquoi je défends l'importance de l'éducation et de la raison, qui sont les outils les plus efficaces pour atteindre la connaissance.\n",
      "\n",
      "Mais le bonheur ne se trouve pas uniquement dans la connaissance. Il est également lié à l'amitié et à la beauté. L'amitié, c'est-à-dire la relation sincère et sincère entre les individus, est une source de bonheur inépuisable. La beauté, qui peut prendre la forme d'une œuvre d'art, d'une nature splendide ou d'une pensée profonde, est également un élément essentiel pour atteindre le bonheur.\n",
      "\n",
      "Cependant, il est important de noter que le bonheur n'est pas toujours facile à atteindre. Les obstacles, les épreuves et les souffrances sont inévitables dans la vie humaine. C'est pourquoi il est essentiel de développer une sagesse qui nous permet de les surmonter. La sagesse, qui consiste à avoir une vision claire de la réalité et à savoir prendre les décisions justes, est la clé pour atteindre le bonheur.\n",
      "\n",
      "En conclusion, le sens de la vie est une quête de bonheur, de connaissance, d'amitié et de beauté. C'est une quête qui nécessite de la raison, de la sagesse et de la volonté de surmonter les obstacles. Mais, ajouterais-je, c'est une quête qui vaut la peine d'être entreprise, car elle nous permet de vivre une vie pleine, riche et significative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2338d2-7a21-4d60-adfe-d41f5bb92fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:53:57.252895Z",
     "iopub.status.busy": "2024-08-11T14:53:57.252527Z",
     "iopub.status.idle": "2024-08-11T14:53:57.259097Z",
     "shell.execute_reply": "2024-08-11T14:53:57.258628Z",
     "shell.execute_reply.started": "2024-08-11T14:53:57.252867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c74effa-e7df-44d6-a9d6-f215b9dd46ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:53:59.831008Z",
     "iopub.status.busy": "2024-08-11T14:53:59.830651Z",
     "iopub.status.idle": "2024-08-11T15:15:40.169607Z",
     "shell.execute_reply": "2024-08-11T15:15:40.169187Z",
     "shell.execute_reply.started": "2024-08-11T14:53:59.830986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b44cf113cf4528b232c8ee9a79c0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 16:54:00 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332f55f073d14527b75e735f4603ace6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:54:00 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 16:54:01 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 08-11 16:54:01 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14a060d7f0d49cba2025378fbdc3514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c3198733b841ebbd7f9daecc70e331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420115d26e134ff0a1c852cc0dca0f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a316584ddb41719b0ac8617614870c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d1949e966440c897939726258a320c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4930be4ee14a318a0838217c305fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 17:15:28 model_runner.py:732] Loading model weights took 14.9888 GB\n",
      "INFO 08-11 17:15:29 gpu_executor.py:102] # GPU blocks: 2666, # CPU blocks: 2048\n",
      "INFO 08-11 17:15:29 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 17:15:29 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 17:15:40 model_runner.py:1225] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BF16\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=myhftoken)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435b8347-9652-4c49-9c8c-bb7fd1e2127b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T16:52:17.754243Z",
     "iopub.status.busy": "2024-08-11T16:52:17.753905Z",
     "iopub.status.idle": "2024-08-11T16:52:30.581555Z",
     "shell.execute_reply": "2024-08-11T16:52:30.581098Z",
     "shell.execute_reply.started": "2024-08-11T16:52:17.754220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:12<00:00, 12.82s/it, est. speed input: 5.15 toks/s, output: 54.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "La question de la signification de la vie est un thème éternel qui a préoccupé les philosophes et les penseurs à travers les siècles. Pour répondre à cette question, je vais explorer cinq perspectives différentes, chacune issue de ma propre réflexion, inspirée des idées des grands philosophes français du Siècle des Lumières.\n",
      "\n",
      "Tout d'abord, la vie est-elle un simple hasard ou un destin déterminé ? Selon les idées de Pierre-Simon Laplace, la vie est un processus naturel, soumis aux lois de la physique et de la biologie. Les événements qui se déroulent dans l'univers sont le résultat d'une chaîne causale, où chaque événement est la conséquence de l'un précédent. Dans ce sens, la vie est une suite de hasards aléatoires, sans direction ni sens particulier.\n",
      "\n",
      "Cependant, d'autres philosophes, comme Jean-Jacques Rousseau, défendent l'idée que la vie a un sens, mais que ce sens est déterminé par la nature humaine. Selon Rousseau, l'homme est un être doué de raison et de conscience, capable de réfléchir et de prendre des décisions. La vie a un sens parce que nous avons la capacité de choisir et de créer notre propre destin. Nous sommes libres de nous conformer aux lois de la nature ou de les remettre en question, de suivre nos désirs ou de les contrôler.\n",
      "\n",
      "Mais qu'est-ce qui donne un sens à notre vie ? Selon les idées de Denis Diderot, la vie est un processus de découverte et d'exploration. Nous sommes des êtres curieux, toujours à la recherche de connaissances et de compréhension. La vie a un sens parce que nous pouvons apprendre, nous adapter et nous améliorer. Nous sommes capables de créer, d'inventer et de faire progresser la société.\n",
      "\n",
      "Cependant, cette perspective soulève une question : qu'est-ce qui est important dans la vie ? Selon les idées de Voltaire, la vie a un sens parce que nous sommes capables de nous améliorer nous-mêmes et de faire du bien aux autres. La vie a un sens lorsqu'on se consacre à la vertu, à la justice et à la charité. Nous devons nous efforcer de devenir meilleurs, de nous élever au-dessus de nos défauts et de faire du monde un endroit meilleur.\n",
      "\n",
      "Enfin, la vie a un sens parce que nous sommes des êtres finis et mortels. Selon les idées de Jean-Jacques Rousseau, la vie a un sens parce que nous pouvons vivre pleinement et apprécier les joies de l'existence. Nous devons profiter de chaque instant, apprécier la beauté de la nature et les liens que nous nouons avec les autres. La vie a un sens parce que nous pouvons vivre pleinement, dans l'ici et maintenant, et laisser derrière nous un héritage de bonheur et de sagesse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70226fa-f54c-4dbf-a42d-fd6091db1b73",
   "metadata": {},
   "source": [
    "## Gemma 2 2b\n",
    "\n",
    "RTX 4090 speed (FP8): **135 tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **108 tokens/sec**\n",
    "\n",
    "https://ai.google.dev/gemma/docs/model_card_2\n",
    "\n",
    "https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "https://huggingface.co/neuralmagic/gemma-2-2b-it-FP8\n",
    "\n",
    "**LICENSE :** https://ai.google.dev/gemma/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a10c76-8da6-4923-a039-e70e87884f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T18:52:29.562079Z",
     "iopub.status.busy": "2024-08-11T18:52:29.561692Z",
     "iopub.status.idle": "2024-08-11T18:58:34.241466Z",
     "shell.execute_reply": "2024-08-11T18:58:34.240916Z",
     "shell.execute_reply.started": "2024-08-11T18:52:29.562056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cf30b816c348e19ae3a9207d1fae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014400ee773a44fa993403fd4694208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95930b2ba20f426cb1ae31a1931a71b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb210e3a6a7f40cb9b7aa95cd69ad1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e66b6da43a140b29ebc527960456049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 20:52:35 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 08-11 20:52:36 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/gemma-2-2b-it-FP8', speculative_config=None, tokenizer='neuralmagic/gemma-2-2b-it-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/gemma-2-2b-it-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205ed705622349e3a5379914f233c036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 20:52:36 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 20:52:36 model_runner.py:720] Starting to load model neuralmagic/gemma-2-2b-it-FP8...\n",
      "INFO 08-11 20:52:37 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1e22729b844e30ba66bfa38de53dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 20:58:23 weight_utils.py:269] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0477610f8c7475ab0c4bcf5899a7716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 20:58:23 gemma2.py:399] Some weights are not initialized from checkpoints: {'model.layers.17.self_attn.attn.v_scale', 'model.layers.11.self_attn.attn.k_scale', 'model.layers.6.self_attn.attn.k_scale', 'model.layers.14.self_attn.attn.v_scale', 'model.layers.16.self_attn.attn.v_scale', 'model.layers.12.self_attn.attn.v_scale', 'model.layers.22.self_attn.attn.v_scale', 'model.layers.23.self_attn.attn.k_scale', 'model.layers.15.self_attn.attn.v_scale', 'model.layers.1.self_attn.attn.k_scale', 'model.layers.20.self_attn.attn.k_scale', 'model.layers.19.self_attn.attn.k_scale', 'model.layers.19.self_attn.attn.v_scale', 'model.layers.6.self_attn.attn.v_scale', 'model.layers.5.self_attn.attn.k_scale', 'model.layers.24.self_attn.attn.k_scale', 'model.layers.8.self_attn.attn.v_scale', 'model.layers.10.self_attn.attn.v_scale', 'model.layers.18.self_attn.attn.k_scale', 'model.layers.17.self_attn.attn.k_scale', 'model.layers.2.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.k_scale', 'model.layers.23.self_attn.attn.v_scale', 'model.layers.0.self_attn.attn.k_scale', 'model.layers.22.self_attn.attn.k_scale', 'model.layers.24.self_attn.attn.v_scale', 'model.layers.5.self_attn.attn.v_scale', 'model.layers.1.self_attn.attn.v_scale', 'model.layers.21.self_attn.attn.k_scale', 'model.layers.0.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.v_scale', 'model.layers.10.self_attn.attn.k_scale', 'model.layers.8.self_attn.attn.k_scale', 'model.layers.14.self_attn.attn.k_scale', 'model.layers.21.self_attn.attn.v_scale', 'model.layers.25.self_attn.attn.v_scale', 'model.layers.13.self_attn.attn.k_scale', 'model.layers.9.self_attn.attn.k_scale', 'model.layers.18.self_attn.attn.v_scale', 'model.layers.2.self_attn.attn.k_scale', 'model.layers.11.self_attn.attn.v_scale', 'model.layers.4.self_attn.attn.k_scale', 'model.layers.16.self_attn.attn.k_scale', 'model.layers.25.self_attn.attn.k_scale', 'model.layers.12.self_attn.attn.k_scale', 'model.layers.13.self_attn.attn.v_scale', 'model.layers.9.self_attn.attn.v_scale', 'model.layers.15.self_attn.attn.k_scale', 'model.layers.4.self_attn.attn.v_scale', 'model.layers.20.self_attn.attn.v_scale', 'model.layers.3.self_attn.attn.k_scale', 'model.layers.3.self_attn.attn.v_scale'}\n",
      "INFO 08-11 20:58:23 model_runner.py:732] Loading model weights took 3.0870 GB\n",
      "INFO 08-11 20:58:24 gpu_executor.py:102] # GPU blocks: 10152, # CPU blocks: 2520\n",
      "INFO 08-11 20:58:24 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 20:58:24 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 20:58:34 model_runner.py:1225] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/gemma-2-2b-it-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54420015-94ff-45c5-af1d-225e8edb112a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T18:59:16.200385Z",
     "iopub.status.busy": "2024-08-11T18:59:16.200054Z",
     "iopub.status.idle": "2024-08-11T18:59:23.060552Z",
     "shell.execute_reply": "2024-08-11T18:59:23.060187Z",
     "shell.execute_reply.started": "2024-08-11T18:59:16.200362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:06<00:00,  6.85s/it, est. speed input: 5.25 toks/s, output: 134.27 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* **Le bonheur** : Le bonheur est le but ultime de la vie, une recherche constante et inconditionnelle. \n",
      "* **Le progrès** : Le progrès est l'essence même de l'existence, la voie vers un monde meilleur.\n",
      "* **L'émancipation** : L'homme doit se libérer des contraintes traditionnelles et s'affranchir de ses passions.\n",
      "* **La raison** : La raison est la seule source de vérité et de connaissance.\n",
      "* **L'individualisme** : Chaque individu est unique et a la liberté de choisir son propre destin.\n",
      "\n",
      "**Comment ces cinq idées s'articulent-elles et comment les justifiez-vous ?**\n",
      "\n",
      "Ces cinq idées sont intrinsèquement liées et s'articulent autour de la notion de progrès et d'émancipation. Le bonheur est un produit direct de la réalisation de ce progrès et de l'émancipation. Le progrès, en soi, est un moteur de bonheur et d'émancipation, car il permet à l'homme de se libérer des contraintes traditionnelles et de s'affranchir de ses passions. L'émancipation, en s'appuyant sur la raison, permet à l'homme de saisir la vérité et de s'orienter vers le progrès. La raison, en étant la seule source de vérité, guide l'homme vers un monde meilleur. L'individualisme, qui est la conséquence de l'émancipation, permet à l'homme de choisir son propre destin et de vivre un bonheur unique et authentique.\n",
      "\n",
      "\n",
      "**Argumentation:**\n",
      "\n",
      "1. **Le bonheur est le but ultime de la vie.**  \n",
      "    * Le bonheur est un sentiment universel et inconditionnel, une recherche constante et inconditionnelle. \n",
      "    * Il est l'objectif principal de la vie, car il permet de vivre pleinement et de réaliser son potentiel. \n",
      "    * Le bonheur n'est pas un état de tranquillité, mais une action constante de vivre pleinement et de s'épanouir.\n",
      "\n",
      "2. **Le progrès est l'essence même de l'existence.** \n",
      "    * L'humanité a toujours cherché à s'améliorer, à dépasser ses limites et à atteindre un état supérieur. \n",
      "    * Le progrès est la manifestation de cette aspiration, de l'ambition de l'homme de devenir plus intelligent, plus fort, plus juste et plus libre. \n",
      "    * Le progrès est une force motrice de l'humanité, un moteur de l'évolution et de la transformation.\n",
      "\n",
      "3. **L'émancipation est un acte de liberté.**\n",
      "    * L'homme est capable de dépasser les contraintes traditionnelles et les dogmes qui le limitent. \n",
      "    * Il est capable de se libérer des passions et des instincts primitifs qui le retiennent. \n",
      "    * L'émancipation est une quête de liberté intérieure et une quête de la vérité.\n",
      "\n",
      "4. **La raison est la seule source de vérité et de connaissance.**\n",
      "    * La raison est la seule force qui peut nous permettre de comprendre le monde et de nous en détacher. \n",
      "    * Elle nous permet de discerner la vérité de la fausse, de distinguer l'illusion de la réalité. \n",
      "    * La raison est la seule source de connaissance fiable et objective.\n",
      "\n",
      "5. **L'individualisme est la conséquence de l'émancipation.**\n",
      "    * L'émancipation permet à l'homme de se libérer de la domination des autres et de ses passions. \n",
      "    * Il est alors possible de choisir son propre destin et de vivre une vie qui correspond à ses aspirations et à ses valeurs. \n",
      "    * L'individualisme est la conséquence de la liberté et de la responsabilité personnelle.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "En conclusion, ces cinq idées constituent une vision optimiste et progressiste de la vie. Elles s'articulent autour de l'idée que l'homme est capable de s'émanciper de ses contraintes traditionnelles et de devenir un être libre, capable de vivre pleinement et de réaliser son potentiel. \n",
      "\n",
      "\n",
      "**Note:** \n",
      "* Cette réponse est une synthèse de l'influence de Montesquieu, Voltaire et Rousseau sur le sujet.\n",
      "* Il est important de noter que ces idées sont souvent critiquées aujourd'hui.\n",
      "\n",
      "**J'espère que cette réponse répond à votre question.** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6263c91e-d35d-46c4-96f8-f0b66f0c5e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T19:00:15.349809Z",
     "iopub.status.busy": "2024-08-11T19:00:15.349428Z",
     "iopub.status.idle": "2024-08-11T19:00:15.367009Z",
     "shell.execute_reply": "2024-08-11T19:00:15.366090Z",
     "shell.execute_reply.started": "2024-08-11T19:00:15.349785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802bea90-7f32-4720-ad89-c87e06103c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T19:00:15.756840Z",
     "iopub.status.busy": "2024-08-11T19:00:15.756039Z",
     "iopub.status.idle": "2024-08-11T19:07:20.449127Z",
     "shell.execute_reply": "2024-08-11T19:07:20.448775Z",
     "shell.execute_reply.started": "2024-08-11T19:00:15.756809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d30b0429944c1af3877af50b1dc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ca62570ca74de5afb6c748409be942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5b3e6bd9114273b7eee2cd01562888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45bc709404f414b95d59ab8d7f041dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1f05b224c346a78db479c58f025bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 21:00:19 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 08-11 21:00:19 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-2b-it, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0ecc67fe84e07b03fd2eab68f0a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 21:00:19 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 21:00:20 model_runner.py:720] Starting to load model google/gemma-2-2b-it...\n",
      "INFO 08-11 21:00:20 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5794571c5e4595a9eafc6999bf25d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791a37e149024e99b8e611ca41eb006e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66e42987b58457ea381e24fda923cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71e4012ab87469db9439550e167ee5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:07:09 model_runner.py:732] Loading model weights took 4.9975 GB\n",
      "INFO 08-11 21:07:10 gpu_executor.py:102] # GPU blocks: 8981, # CPU blocks: 2520\n",
      "INFO 08-11 21:07:10 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 21:07:10 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 21:07:20 model_runner.py:1225] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BF16\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1af966e-fd0c-40b3-bf49-652c93edd502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T19:09:45.880881Z",
     "iopub.status.busy": "2024-08-11T19:09:45.880535Z",
     "iopub.status.idle": "2024-08-11T19:09:55.220469Z",
     "shell.execute_reply": "2024-08-11T19:09:55.220075Z",
     "shell.execute_reply.started": "2024-08-11T19:09:45.880858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:09<00:00,  9.33s/it, est. speed input: 3.86 toks/s, output: 108.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* **Le sens de la vie n'est pas une question à laquelle on peut répondre de façon absolue.** Il n'y a pas de vérité absolue, et chaque individu doit trouver son propre chemin.\n",
      "* **Le bonheur est un élément clé pour vivre une vie pleine et significative.** Le bonheur n'est pas une fin en soi, mais plutôt un moyen pour atteindre des objectifs et des valeurs.\n",
      "* **L'éducation est un instrument puissant pour la construction d'une vie réussie et épanouie.** Elle permet de développer des compétences, des connaissances et des valeurs qui nous permettent de vivre une vie plus juste et plus enrichissante.\n",
      "* **La raison et la réflexion sont des outils précieux pour comprendre le monde et notre place dans celui-ci.**  La réflexion nous permet d'accéder à la vérité et à la connaissance, et de prendre des décisions éclairées.\n",
      "* **L'amour est un élément fondamental pour le bonheur et la vie sociale.** L'amour est une force puissante qui nous relie aux autres et nous permet de vivre une vie plus riche et plus significative.\n",
      "\n",
      "\n",
      "**Conclusion:** \n",
      "\n",
      "En conclusion, le sens de la vie n'est pas une question à laquelle il y a une réponse unique et universelle.  Il est plutôt une construction personnelle, un cheminement qui se construit au fil des expériences et des réflexions. Le bonheur, l'éducation, la raison et l'amour sont des éléments essentiels pour vivre une vie épanouie et significative.\n",
      "\n",
      "\n",
      "**Votre réponse:**\n",
      "\n",
      "La vie, tel un fleuve sinueux, ne se résume pas à une réponse unique et définitive. Elle est une exploration, un voyage sans destination finale, et c'est à chacun d'entre nous de naviguer à travers ses propres eaux, de trouver son propre courant et de laisser sa trace sur le fleuve. Le bonheur, tel un phare dans la nuit, guide nos pas, mais il ne s'agit pas d'une destination à atteindre, mais d'un chemin à parcourir pour atteindre un état d'épanouissement personnel.  L'éducation, comme un outil précieux, nous permet de nous familiariser avec les connaissances, les compétences et les valeurs qui nous permettent de naviguer avec intelligence et discernement dans ce fleuve de vie. La raison, comme un miroir qui nous reflète la vérité, nous éclaire sur notre place dans l'univers et nous permet de prendre des décisions éclairées. Enfin, l'amour, comme un lien indissoluble entre les êtres humains, nous permet de vivre une vie sociale enrichissante et significative. \n",
      "\n",
      "Le sens de la vie, c'est donc l'accumulation de ces éléments, la construction d'une existence qui est à la fois personnelle et universelle. Il ne s'agit pas d'une quête d'une vérité absolue, mais d'une exploration constante, une quête de sens qui se nourrit de la réflexion, de l'expérience et de l'amour. \n",
      "\n",
      "\n",
      "**Note:**\n",
      "\n",
      "Ce texte est une réponse inspirée par les idées de la philosophie des Lumières, en particulier celles de Rousseau, Voltaire et Montesquieu.\n",
      "\n",
      "\n",
      "**Analyse:**\n",
      "\n",
      "Ce texte utilise des arguments et des exemples tirés de la philosophie des Lumières pour soutenir ses points. Il met en avant l'importance de la liberté individuelle, de l'éducation, de la raison et de l'amour dans la construction d'une vie épanouie et significative. \n",
      "\n",
      "**Cependant, il manque quelques éléments pour être considéré comme un texte philosophique complet.** \n",
      "\n",
      "* **Il manque une critique des idées de l'époque.** Le texte ne mentionne pas les critiques des Lumières envers les dogmes et les superstitions. \n",
      "* **Il ne mentionne pas les différentes formes de bonheur.** Le texte se concentre sur le bonheur comme un état d'épanouissement personnel, mais il pourrait être enrichi en abordant les différentes formes de bonheur que les philosophes des Lumières ont considérées.\n",
      "* **Il manque une réflexion sur le rôle de l'éthique.** Le texte mentionne l'éducation et l'amour, mais il ne traite pas le rôle de l'éthique dans la construction d'une vie réussie et épanouie.\n",
      "\n",
      "\n",
      "**Pour améliorer ce texte, il serait utile de:**\n",
      "\n",
      "* **Incorporer des citations de philosophes des Lumières pour étayer les arguments.** \n",
      "* **Développer les arguments sur les différentes formes de bonheur.**\n",
      "* **Explorer le rôle de l'éthique dans la construction d'une vie épanouie.**\n",
      "* **Discuter des défis de la société de l'époque et comment ils influencent la recherche du sens de la vie.**\n",
      "\n",
      "\n",
      "En conclusion, ce texte offre une vision philosophique intéressante de la recherche du sens de la vie, mais il manque de profondeur et de nuances pour être considéré comme un texte philosophique complet. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fc4e6-2f76-4b36-9b0e-00b7854415a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T21:56:28.347023Z",
     "iopub.status.busy": "2024-07-30T21:56:28.346440Z",
     "iopub.status.idle": "2024-07-30T21:56:28.350612Z",
     "shell.execute_reply": "2024-07-30T21:56:28.350074Z",
     "shell.execute_reply.started": "2024-07-30T21:56:28.347003Z"
    }
   },
   "source": [
    "## Gemma 2 9b\n",
    "\n",
    "VLLM 0.5.4 : WARNING 08-11 16:17:33 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
    "\n",
    "RTX 4090 speed (FP8): **58 tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **40 tokens/sec**\n",
    "\n",
    "https://ai.google.dev/gemma/docs/model_card_2\n",
    "\n",
    "https://huggingface.co/google/gemma-2-9b-it\n",
    "\n",
    "https://huggingface.co/neuralmagic/gemma-2-9b-it-FP8\n",
    "\n",
    "**LICENSE :** https://ai.google.dev/gemma/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bfbe97-ccca-4291-9424-6c16aee26f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:17:29.752171Z",
     "iopub.status.busy": "2024-08-11T14:17:29.751793Z",
     "iopub.status.idle": "2024-08-11T14:31:10.605485Z",
     "shell.execute_reply": "2024-08-11T14:31:10.605129Z",
     "shell.execute_reply.started": "2024-08-11T14:17:29.752144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f962e4f2aa864ede98e6f9eb88e1861c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4481ff2e91034d0583c9e0731bcd84c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35f45b636fb43d7a8c20370730e14a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0543ef3fef1d4e319208d9a856d73e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e433c25e04056bdee5dcbc43ef741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:17:33 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 08-11 16:17:33 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/gemma-2-9b-it-FP8', speculative_config=None, tokenizer='neuralmagic/gemma-2-9b-it-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/gemma-2-9b-it-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef45c34af10746b48796c98541c2310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:17:34 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 16:17:34 model_runner.py:720] Starting to load model neuralmagic/gemma-2-9b-it-FP8...\n",
      "WARNING 08-11 16:17:34 fp8.py:43] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 08-11 16:17:34 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb8cda4af6e42c6a376384c5dd26e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e2a0157f4b457eaacbd28defc7b3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/250M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54258469cb4e4e0c8abbad46b398b8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76da1d836588415aaea5a707f602a281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360f4cff4e384576833e6bd24a5ff0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:30:58 gemma2.py:399] Some weights are not initialized from checkpoints: {'model.layers.30.self_attn.attn.k_scale', 'model.layers.23.self_attn.attn.k_scale', 'model.layers.41.self_attn.attn.k_scale', 'model.layers.20.self_attn.attn.k_scale', 'model.layers.21.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.k_scale', 'model.layers.12.self_attn.attn.k_scale', 'model.layers.28.self_attn.attn.k_scale', 'model.layers.3.self_attn.attn.k_scale', 'model.layers.0.self_attn.attn.v_scale', 'model.layers.17.self_attn.attn.v_scale', 'model.layers.34.self_attn.attn.v_scale', 'model.layers.24.self_attn.attn.k_scale', 'model.layers.6.self_attn.attn.v_scale', 'model.layers.23.self_attn.attn.v_scale', 'model.layers.37.self_attn.attn.k_scale', 'model.layers.0.self_attn.attn.k_scale', 'model.layers.16.self_attn.attn.k_scale', 'model.layers.10.self_attn.attn.v_scale', 'model.layers.37.self_attn.attn.v_scale', 'model.layers.25.self_attn.attn.v_scale', 'model.layers.3.self_attn.attn.v_scale', 'model.layers.39.self_attn.attn.v_scale', 'model.layers.27.self_attn.attn.v_scale', 'model.layers.5.self_attn.attn.v_scale', 'model.layers.18.self_attn.attn.k_scale', 'model.layers.13.self_attn.attn.v_scale', 'model.layers.9.self_attn.attn.k_scale', 'model.layers.35.self_attn.attn.k_scale', 'model.layers.38.self_attn.attn.k_scale', 'model.layers.34.self_attn.attn.k_scale', 'model.layers.40.self_attn.attn.v_scale', 'model.layers.15.self_attn.attn.k_scale', 'model.layers.38.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.v_scale', 'model.layers.4.self_attn.attn.v_scale', 'model.layers.27.self_attn.attn.k_scale', 'model.layers.11.self_attn.attn.v_scale', 'model.layers.16.self_attn.attn.v_scale', 'model.layers.24.self_attn.attn.v_scale', 'model.layers.20.self_attn.attn.v_scale', 'model.layers.33.self_attn.attn.v_scale', 'model.layers.22.self_attn.attn.k_scale', 'model.layers.19.self_attn.attn.v_scale', 'model.layers.14.self_attn.attn.v_scale', 'model.layers.2.self_attn.attn.k_scale', 'model.layers.12.self_attn.attn.v_scale', 'model.layers.35.self_attn.attn.v_scale', 'model.layers.25.self_attn.attn.k_scale', 'model.layers.8.self_attn.attn.v_scale', 'model.layers.13.self_attn.attn.k_scale', 'model.layers.1.self_attn.attn.k_scale', 'model.layers.6.self_attn.attn.k_scale', 'model.layers.4.self_attn.attn.k_scale', 'model.layers.29.self_attn.attn.k_scale', 'model.layers.11.self_attn.attn.k_scale', 'model.layers.17.self_attn.attn.k_scale', 'model.layers.31.self_attn.attn.k_scale', 'model.layers.26.self_attn.attn.k_scale', 'model.layers.31.self_attn.attn.v_scale', 'model.layers.21.self_attn.attn.k_scale', 'model.layers.26.self_attn.attn.v_scale', 'model.layers.9.self_attn.attn.v_scale', 'model.layers.36.self_attn.attn.k_scale', 'model.layers.1.self_attn.attn.v_scale', 'model.layers.2.self_attn.attn.v_scale', 'model.layers.32.self_attn.attn.k_scale', 'model.layers.14.self_attn.attn.k_scale', 'model.layers.33.self_attn.attn.k_scale', 'model.layers.30.self_attn.attn.v_scale', 'model.layers.32.self_attn.attn.v_scale', 'model.layers.15.self_attn.attn.v_scale', 'model.layers.36.self_attn.attn.v_scale', 'model.layers.39.self_attn.attn.k_scale', 'model.layers.22.self_attn.attn.v_scale', 'model.layers.41.self_attn.attn.v_scale', 'model.layers.19.self_attn.attn.k_scale', 'model.layers.28.self_attn.attn.v_scale', 'model.layers.8.self_attn.attn.k_scale', 'model.layers.10.self_attn.attn.k_scale', 'model.layers.18.self_attn.attn.v_scale', 'model.layers.5.self_attn.attn.k_scale', 'model.layers.40.self_attn.attn.k_scale', 'model.layers.29.self_attn.attn.v_scale'}\n",
      "INFO 08-11 16:30:58 model_runner.py:732] Loading model weights took 9.6673 GB\n",
      "INFO 08-11 16:30:59 gpu_executor.py:102] # GPU blocks: 1827, # CPU blocks: 780\n",
      "INFO 08-11 16:30:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 16:30:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 16:31:10 model_runner.py:1225] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/gemma-2-9b-it-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c7b5c5-89ce-4de7-80b4-90b25674dbc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T14:35:10.558830Z",
     "iopub.status.busy": "2024-08-11T14:35:10.558522Z",
     "iopub.status.idle": "2024-08-11T14:35:18.489714Z",
     "shell.execute_reply": "2024-08-11T14:35:18.489362Z",
     "shell.execute_reply.started": "2024-08-11T14:35:10.558808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:07<00:00,  7.92s/it, est. speed input: 4.54 toks/s, output: 58.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "\n",
      "L'homme, dans son essence même, est un être rationnel. Il est doté d'une capacité d'analyse et de réflexion unique, lui permettant de discerner le vrai du faux, le bien du mal, le juste de l'injuste. C'est par cette raison que la vie humaine revêt une importance particulière, car elle offre l'opportunité unique de cultiver son esprit, de s'élever au-dessus des instincts primitifs et de participer à la construction d'une société plus juste et plus éclairée.\n",
      "\n",
      "Le sens de la vie, donc, ne réside pas dans la recherche d'un bonheur immédiat ou d'une satisfaction sensorielle. Il se trouve plutôt dans l'effort constant de l'amélioration personnelle et du progrès social. En cultivant ses connaissances, en développant son raisonnement et en agissant de manière morale, l'homme réalise son plein potentiel et contribue à l'avancement de l'humanité.\n",
      "\n",
      "La raison, guide infaillible de l'homme, nous permet de démêler les vérités universelles et de nous libérer des chaînes de l'ignorance et de la superstition. En appliquant la logique et la critique à tous les aspects de la vie, nous pouvons construire un monde fondé sur la justice, l'égalité et la liberté. C'est par la diffusion de la connaissance et de la pensée éclairée que nous pouvons atteindre la véritable civilisation et le bonheur collectif.\n",
      "\n",
      "Cependant, il est important de noter que la quête du progrès ne doit jamais se faire au détriment de l'éthique et de la compassion. L'homme, malgré sa raison, reste un être social et sensible. Il est donc essentiel de cultiver les vertus morales telles que la justice, la tolérance et l'empathie. Un monde éclairé ne peut exister sans une société juste et solidaire.\n",
      "\n",
      "En somme, le sens de la vie réside dans l'utilisation éclairée de notre raison pour améliorer notre propre existence et celle de notre communauté. C'est par l'éducation, la réflexion et l'action morale que nous pouvons atteindre la véritable grandeur de l'homme et bâtir un monde meilleur pour tous.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236d83e0-875a-4c26-8aeb-74b9473b0977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T17:09:48.720100Z",
     "iopub.status.busy": "2024-08-11T17:09:48.719753Z",
     "iopub.status.idle": "2024-08-11T17:09:48.725776Z",
     "shell.execute_reply": "2024-08-11T17:09:48.725270Z",
     "shell.execute_reply.started": "2024-08-11T17:09:48.720089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bd2c44-f1f7-49a2-8e7a-f43363431f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T17:09:50.182088Z",
     "iopub.status.busy": "2024-08-11T17:09:50.181765Z",
     "iopub.status.idle": "2024-08-11T17:28:13.504453Z",
     "shell.execute_reply": "2024-08-11T17:28:13.504030Z",
     "shell.execute_reply.started": "2024-08-11T17:09:50.182074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 19:09:53 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 08-11 19:09:53 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=google/gemma-2-9b-it, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-11 19:09:54 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 19:09:54 model_runner.py:720] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 08-11 19:09:54 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bf5726d9ce479a8597aa726ab32426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  35%|###5      | 1.72G/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd78dee06d5f40a5989ae9213e9e8804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:  30%|###       | 1.11G/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b811ad394dac4df5a3ab12cbfb269944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:  22%|##2       | 1.10G/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfac7611673445818c7fdd5774a26a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:  21%|##        | 1.03G/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1972544deb044df89171c522a710623e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d148cf612cc74731bca6c0d8e2bac0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 19:28:01 model_runner.py:732] Loading model weights took 17.3781 GB\n",
      "INFO 08-11 19:28:02 gpu_executor.py:102] # GPU blocks: 340, # CPU blocks: 780\n",
      "INFO 08-11 19:28:03 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 19:28:03 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 19:28:13 model_runner.py:1225] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BF16\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b680770-d2e1-4153-af7f-ef2ac8270a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T17:43:51.168563Z",
     "iopub.status.busy": "2024-08-11T17:43:51.167757Z",
     "iopub.status.idle": "2024-08-11T17:44:02.427796Z",
     "shell.execute_reply": "2024-08-11T17:44:02.427290Z",
     "shell.execute_reply.started": "2024-08-11T17:43:51.168529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:11<00:00, 11.25s/it, est. speed input: 3.20 toks/s, output: 39.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mes chers amis, la quête du sens de la vie est un voyage initiatique que chaque individu entreprend. C'est un questionnement fondamental, qui résonne au cœur de l'âme humaine depuis la nuit des temps. En tant que philosophe éclairé, je vous propose aujourd'hui de réfléchir ensemble à cette question essentielle, en nous appuyant sur les principes de la raison et de la liberté.\n",
      "\n",
      "La vie, en elle-même, n'est pas intrinsèquement dotée de sens. C'est à l'homme, par son libre arbitre, de lui donner un but, une direction. Nous sommes des êtres rationnels, capables de réfléchir, d'analyser et de choisir. Cette faculté nous distingue des autres créatures et nous confère la responsabilité de façonner notre propre destinée.\n",
      "\n",
      "Le sens de la vie réside donc dans l'exercice de cette liberté, dans la poursuite du bonheur et du bien-être, à la fois pour soi et pour autrui. Le bonheur, il est vrai, est un concept complexe, mais il peut être défini comme un état de satisfaction et de réalisation personnelle. Il découle de l'utilisation éclairée de nos capacités intellectuelles et morales, de la recherche de la connaissance, de l'amour, de la justice et de la fraternité.\n",
      "\n",
      "La société, elle aussi, joue un rôle crucial dans la quête du sens. En tant que membres d'une communauté, nous sommes appelés à contribuer au bien commun, à promouvoir l'égalité, la liberté et la fraternité. La coopération, la solidarité et le respect mutuel sont des valeurs fondamentales qui nous permettent de construire une société plus juste et plus harmonieuse.\n",
      "\n",
      "Ainsi, le sens de la vie n'est pas une vérité absolue, mais un idéal à poursuivre, un cheminement personnel et collectif. Il est inscrit dans l'exercice de notre liberté, dans la recherche du bonheur et du bien-être, et dans la construction d'une société plus juste et plus éclairée. N'oublions jamais que la vie est un cadeau précieux, et que nous avons le devoir de l'utiliser de manière responsable et éthique.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab7abc-c38e-4415-9ab1-c780d69ad9b0",
   "metadata": {},
   "source": [
    "## Gemma 2 27b\n",
    "\n",
    "RTX 4090 speed (FP8): **?? tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **?? tokens/sec**\n",
    "\n",
    "https://ai.google.dev/gemma/docs/model_card_2\n",
    "\n",
    "https://huggingface.co/google/gemma-2-27b-it\n",
    "\n",
    "As of August 11 2024 : 4 bits version available is bitsandbytes from Unsloth.\n",
    "- https://huggingface.co/unsloth/gemma-2-27b-it-bnb-4bit\n",
    "- VLLM v0.5.4 : \"BitAndBytes with enforce_eager = False is not supported yet.\" => add param enforce_eager = True.\n",
    "- => KeyError: 'model.layers.0.mlp.down_proj.weight'\n",
    "\n",
    "As of August 22 2024 : 4 bits GPTQ version below\n",
    "\n",
    "https://huggingface.co/ModelCloud/gemma-2-27b-it-gptq-4bit\n",
    "\n",
    "**LICENSE :** https://ai.google.dev/gemma/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604212b3-0aad-4832-80d8-c0d13ec9c7ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T21:41:08.165600Z",
     "iopub.status.busy": "2024-08-22T21:41:08.165247Z",
     "iopub.status.idle": "2024-08-22T21:41:08.170858Z",
     "shell.execute_reply": "2024-08-22T21:41:08.170107Z",
     "shell.execute_reply.started": "2024-08-22T21:41:08.165570Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549cfbf-236e-466f-ae08-06a091a2c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gptqmodel --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902ad6fd-af8a-4c2c-b7d0-74644a821d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T22:38:30.257577Z",
     "iopub.status.busy": "2024-08-22T22:38:30.256340Z",
     "iopub.status.idle": "2024-08-22T22:46:44.722866Z",
     "shell.execute_reply": "2024-08-22T22:46:44.722353Z",
     "shell.execute_reply.started": "2024-08-22T22:38:30.257552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/gptqmodel/nn_modules/triton_utils/dequant.py:122: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/gptqmodel/nn_modules/triton_utils/dequant.py:130: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-23 00:38:35 utils.py:578] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).\n",
      "INFO 08-23 00:38:35 gptq_marlin.py:98] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 08-23 00:38:35 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='ModelCloud/gemma-2-27b-it-gptq-4bit', speculative_config=None, tokenizer='ModelCloud/gemma-2-27b-it-gptq-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=ModelCloud/gemma-2-27b-it-gptq-4bit, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-23 00:38:36 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-23 00:38:36 selector.py:80] Using Flashinfer backend.\n",
      "INFO 08-23 00:38:36 model_runner.py:720] Starting to load model ModelCloud/gemma-2-27b-it-gptq-4bit...\n",
      "INFO 08-23 00:38:36 selector.py:80] Using Flashinfer backend.\n",
      "INFO 08-23 00:38:37 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4987638ecf8c485384a3188ae6e2a5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  56%|#####6    | 10.3G/18.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 00:46:15 weight_utils.py:269] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb2a9f886745648226bd73975cd2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 00:46:24 model_runner.py:732] Loading model weights took 14.8627 GB\n",
      "INFO 08-23 00:46:26 gpu_executor.py:102] # GPU blocks: 618, # CPU blocks: 712\n",
      "INFO 08-23 00:46:27 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-23 00:46:27 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-23 00:46:42 model_runner.py:1225] Graph capturing finished in 15 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:02<00:00,  2.51s/it, est. speed input: 15.52 toks/s, output: 41.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expirations are known.\n",
      "\n",
      "The following table shows the expected intervals for various types of events based on the estimated decay rate.\n",
      "\n",
      "| Event         | Expected Interval (years)|\n",
      "|--------------|----------------------------|\n",
      "| Major earthquake | 10-100                     |\n",
      "| Supernova    | 50-100                      |\n",
      "| Mass extinction | millions                      |\n",
      "\n",
      " You shouldn’t expect to see these events happen within your lifetime.\n",
      "\n",
      "  See Guardian for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Gemma-2 use Flashinfer backend for models with logits_soft_cap. Otherwise, the output might be wrong.\n",
    "os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASHINFER'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from gptqmodel import BACKEND, GPTQModel\n",
    "\n",
    "model_name = \"ModelCloud/gemma-2-27b-it-gptq-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPTQModel.from_quantized(\n",
    "            model_name,\n",
    "            backend=BACKEND.VLLM,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4372e667-34f7-4af8-89d9-0d654190952c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T22:50:24.831930Z",
     "iopub.status.busy": "2024-08-22T22:50:24.831006Z",
     "iopub.status.idle": "2024-08-22T22:50:28.073976Z",
     "shell.execute_reply": "2024-08-22T22:50:28.073601Z",
     "shell.execute_reply.started": "2024-08-22T22:50:24.831896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:03<00:00,  3.23s/it, est. speed input: 12.07 toks/s, output: 39.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firewall  to this website.\n",
      "\n",
      "'\n",
      "`\n",
      "\n",
      "The issue, it seems, is that the original author's intention was to prevent any automated access to the content.\n",
      "\n",
      "It's important to remember that the web is meant to be accessible to everyone, and including a  \n",
      " ```\n",
      " ```\n",
      "    \n",
      " ```\n",
      "  \n",
      "```\n",
      " ```\n",
      " blocking certain media types (like images or videos) hinders the experience for users who rely on these media for understanding the content.\n",
      "\n",
      "So, while I understand the intention, I don't think the original approach was ideal.\n",
      "\n",
      "Let me know if you have any other questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"user\", \"content\": \"Vous êtes un grand philosophe français du siècle des lumières. Quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"}]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "print(inputs)\n",
    "outputs = model.generate(prompts=inputs, temperature=0.95, max_length=128)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d006928-0bb3-4f01-9447-f4699992e533",
   "metadata": {},
   "source": [
    "## Phi-3.5 mini 128k 3.8b\n",
    "\n",
    "RTX 4090 speed (FP8): **130 tokens/sec**\n",
    "\n",
    "RTX 4090 speed (BF16): **87 tokens/sec**\n",
    "\n",
    "https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Phi-3-mini-128k-instruct-FP8\n",
    "\n",
    "**LICENSE :** MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d73530-90f1-416e-bac3-6d8ac5d92af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/Phi-3-mini-128k-instruct-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03c4651-59a4-406b-9e3f-8cb0b9e22498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T18:37:40.259008Z",
     "iopub.status.busy": "2024-08-11T18:37:40.258719Z",
     "iopub.status.idle": "2024-08-11T18:37:46.441419Z",
     "shell.execute_reply": "2024-08-11T18:37:46.441060Z",
     "shell.execute_reply.started": "2024-08-11T18:37:40.258941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:06<00:00,  6.18s/it, est. speed input: 6.15 toks/s, output: 130.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La vie, selon la perspective des philosophes du siècle des Lumières, est un concept riche et complexe qui a été exploré à travers diverses perspectives. Voici un résumé en cinq paragraphes argumentés sur le sens de la vie à travers cette période intellectuelle.\n",
      "\n",
      "\n",
      "Premièrement, le siècle des Lumières a été marqué par une quête de raison et de connaissance. Les philosophes, tels que Voltaire et Diderot, ont cherché à comprendre la nature humaine et la société par l'application de la raison. Pour eux, le sens de la vie résidait dans la poursuite de la connaissance et le développement de l'esprit. Ils croyaient que par la compréhension et l'éducation, les individus pouvaient améliorer leur propre existence et contribuer à la progression de la société.\n",
      "\n",
      "\n",
      "Deuxièmement, l'optimisme était un thème récurrent parmi les philosophes des Lumières. Des penseurs comme Leibniz et Rousseau ont soutenu que la vie était intrinsèquement bonne et que la nature humaine était fondamentalement noble. Pour Rousseau, le sens de la vie résidait dans la simplicité, l'authenticité et la liberté. Il soutenait que la société et la civilisation ont détourné les humains de leur état naturel de bien-être.\n",
      "\n",
      "\n",
      "Troisièmement, l'importance de la raison et de l'individualisme est apparue comme un élément clé dans la compréhension du sens de la vie. Les philosophes tels que Kant et Montesquieu ont plaidé pour l'importance de l'autonomie de l'individu et la nécessité d'un gouvernement basé sur la justice et la liberté. Ils croyaient que le sens de la vie se trouvait dans la capacité de l'homme à s'auto-gouverner et à contribuer à un ordre moral et social juste.\n",
      "\n",
      "\n",
      "Quatrièmement, la valeur de la sagesse et de la vertu a été mise en avant comme un élément essentiel dans la quête du sens de la vie. Des philosophes comme Montaigne et Rousseau ont insisté sur l'importance de la sagesse morale, de la bonté et de la vertu dans la conduite d'une vie significative. Ils soutenaient que le sens de la vie résidait dans la cultivation d'une bonne moralité et le respect des autres.\n",
      "\n",
      "\n",
      "Enfin, la philosophie des Lumières a souligné l'importance de la réforme sociale et politique. Les philosophes croyaient que le sens de la vie se trouvait dans la lutte pour une société juste et égalitaire. Ils ont prôné la tolérance, l'égalité des sexes et la fin des privilèges arbitraires. Leurs idées ont inspiré les révolutions américaine et française et ont contribué à façonner les fondements des sociétés modernes.\n",
      "\n",
      "\n",
      "En conclusion, le sens de la vie, selon les philosophes du siècle des Lumières, réside dans la poursuite de la connaissance, l'optimisme, l'individualisme, la sagesse morale et la réforme sociale et politique. Ces concepts ont été cruciaux pour façonner les idées politiques et sociales qui ont conduit à l'émergence des sociétés modernes et à la progression continue de l'humanité.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce39195f-9423-4968-beef-43cc56a2f561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T20:24:45.806283Z",
     "iopub.status.busy": "2024-08-22T20:24:45.805397Z",
     "iopub.status.idle": "2024-08-22T20:35:24.847490Z",
     "shell.execute_reply": "2024-08-22T20:35:24.847091Z",
     "shell.execute_reply.started": "2024-08-22T20:24:45.806236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf84360214a4b3fab539d1c40f29242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2738185ac11c40f7a5d7eae9e2e1c225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e4d803df014d72a365a444dd913b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b828bdfe5c4e97ab7f32b346e06ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67b1c0afeba4180b4d4ce27bbf50731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2d4f7897e64738b8e56b898e6561ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 22:24:48 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752ae360fb754e6c86d293ad9d6c5064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-22 22:24:48 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-22 22:24:48 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-22 22:24:48 selector.py:54] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 22:24:51 model_runner.py:720] Starting to load model microsoft/Phi-3.5-mini-instruct...\n",
      "INFO 08-22 22:24:52 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-22 22:24:52 selector.py:54] Using XFormers backend.\n",
      "INFO 08-22 22:24:52 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364163c00c784d96a03fc12e194b7271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fface6194f45cf98092ae81ee51ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165e86e86b334034aa842f8bfd58bfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a683e2e9de44fc5913c70e0704b5846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-22 22:35:11 model_runner.py:732] Loading model weights took 7.1659 GB\n",
      "INFO 08-22 22:35:12 gpu_executor.py:102] # GPU blocks: 2367, # CPU blocks: 682\n",
      "INFO 08-22 22:35:13 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-22 22:35:13 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-22 22:35:24 model_runner.py:1225] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BF16\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34f6e77-f1bf-4ef9-8253-fe004f1f6d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T20:46:58.172462Z",
     "iopub.status.busy": "2024-08-22T20:46:58.171125Z",
     "iopub.status.idle": "2024-08-22T20:47:08.541986Z",
     "shell.execute_reply": "2024-08-22T20:47:08.541623Z",
     "shell.execute_reply.started": "2024-08-22T20:46:58.172418Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:10<00:00, 10.33s/it, est. speed input: 3.68 toks/s, output: 87.10 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La quête du sens de la vie est une préoccupation philosophique profonde qui a fasciné les hommes depuis les temps anciens. En tant que philosophe moderne, je peux présenter un argumentation synthétisée en cinq paragraphes pour tenter de saisir ce concept complexe :\n",
      "\n",
      "1. **Diversité des Interprétations Historiques :** Historiquement, le sens de la vie a été interprété de diverses manières selon les cultures et les époques. Du mythe de Prométhée symbolisant la quête d'ignorance à l'éthique de l'utilitarisme visant le plus grand bonheur pour le plus grand nombre, les perspectives ont varié. Les philosophes du siècle des Lumières, tels que Voltaire, Rousseau, et Kant, ont contribué à cette diversité en mettant l'accent sur la raison, la liberté et l'autonomie individuelle.\n",
      "\n",
      "2. **Réflexion sur la Liberté et l'Autonomie :** Une perspective moderne pourrait s'appuyer sur les principes des Lumières, qui suggèrent que le sens de la vie réside dans l'autonomie et la liberté de l'individu. Selon cette vision, chaque personne possède la capacité de définir sa propre valeur et de poursuivre ses objectifs personnels. Cela implique un engagement constant envers la réflexion et l'action éclairée, permettant à chacun de créer une vie significative à sa manière.\n",
      "\n",
      "3. **Contribution à la Société et au Progrès :** Un autre aspect pourrait être l'implication active dans la société et le progrès. Le sens de la vie peut être trouvé dans l'effort de contribuer positivement à la condition humaine, en favorisant le bien-être collectif. La pensée des Lumières, avec son accent sur l'utilitarisme et la raison, nous invite à considérer la vie non seulement comme une quête personnelle, mais aussi comme un acte de participation au développement durable et à l'amélioration de la condition humaine.\n",
      "\n",
      "4. **Quête de la Sagesse et de la Compréhension :** La quête de la sagesse et de la compréhension peut être vue comme un pilier du sens de la vie. En s'engageant dans la philosophie, la science, l'art et la littérature, on peut enrichir son expérience de vie et celle des autres. Cela reflète l'esprit des Lumières, qui valorisait l'éducation et la connaissance comme moyens de révéler la vérité et de promouvoir la progrès intellectuel et moral.\n",
      "\n",
      "5. **Équilibre entre l'Existence et l'Esprit :** Enfin, le sens de la vie peut être perçu comme un équilibre entre l'existence matérielle et l'esprit. Cela ne signifie pas une opposition entre les deux, mais plutôt une reconnaissance de leur interdépendance. En prenant soin de sa santé physique et de son bien-être émotionnel, tout en stimulant sa curiosité intellectuelle et son créativité, on peut atteindre une vie harmonieuse. Ce principe, qui trouve son origine dans la philosophie des Lumières, reconnaît la complexité de l'être humain et la nécessité d'une vie bien équilibrée.\n",
      "\n",
      "En résumé, bien que le sens de la vie puisse être sujet à des interprétations variées, une perspective moderne pourrait reconnaître l'importance de l'autonomie, de la contribution sociale, de la quête de la sagesse, et de l'équilibre entre l'existence et l'esprit. Ces principes, inspirés par les Lumières, offrent une piste pour comprendre et vivre une existence significative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e713d-e00f-454c-8dad-488af75fabd2",
   "metadata": {},
   "source": [
    "## Phi-3 medium 128k 14b\n",
    "\n",
    "RTX 4090 speed (FP8): **50 tokens/sec**\n",
    "\n",
    "https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3-medium-128k-instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Phi-3-medium-128k-instruct-FP8\n",
    "\n",
    "**LICENSE :** MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe79ed72-2cbb-4cfc-9414-e462d8dedc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T19:49:21.569480Z",
     "iopub.status.busy": "2024-08-11T19:49:21.568996Z",
     "iopub.status.idle": "2024-08-11T20:11:34.003772Z",
     "shell.execute_reply": "2024-08-11T20:11:34.003369Z",
     "shell.execute_reply.started": "2024-08-11T19:49:21.569455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9270d27210584d84a1929ea7cc81d4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b81bcc21eb4713ba292bb5c99c7075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4788184c26cc4c04b45a63de832981cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7140d49ef24da28f5ae33c61e589e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcb06fb1a994c759a8a76a45e1485a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86979e4fa934c8fb9f13d68461ce013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:49:24 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='neuralmagic/Phi-3-medium-128k-instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Phi-3-medium-128k-instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Phi-3-medium-128k-instruct-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d69af6e69042d182c295ff1cae4451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 21:49:24 utils.py:578] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-11 21:49:24 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-11 21:49:24 selector.py:54] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 21:49:28 model_runner.py:720] Starting to load model neuralmagic/Phi-3-medium-128k-instruct-FP8...\n",
      "WARNING 08-11 21:49:28 fp8.py:43] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 08-11 21:49:28 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-11 21:49:28 selector.py:54] Using XFormers backend.\n",
      "INFO 08-11 21:49:29 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fe9d29e2ef4428a88600078cec12b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92c493259c2494aa808d3ecaea794c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2833ef70086045b6a7a890689fc751b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7771294ecc4a05ba51f55d29b27a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/48.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e174fcce00494f999e6b52c1e699b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 22:11:20 model_runner.py:732] Loading model weights took 13.4873 GB\n",
      "INFO 08-11 22:11:21 gpu_executor.py:102] # GPU blocks: 2354, # CPU blocks: 1310\n",
      "INFO 08-11 22:11:21 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-11 22:11:21 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-11 22:11:33 model_runner.py:1225] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/Phi-3-medium-128k-instruct-FP8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25f7f03-c134-4847-8068-153be36a51de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T20:13:07.276079Z",
     "iopub.status.busy": "2024-08-11T20:13:07.275547Z",
     "iopub.status.idle": "2024-08-11T20:13:20.965770Z",
     "shell.execute_reply": "2024-08-11T20:13:20.965396Z",
     "shell.execute_reply.started": "2024-08-11T20:13:07.276045Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:13<00:00, 13.68s/it, est. speed input: 1.61 toks/s, output: 49.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La question du sens de la vie est une interrogation philosophique et existentielle qui a intrigué les humains depuis des siècles. Elle cherche à comprendre la raison d'être de l'existence humaine et la finalité de notre vie sur Terre. Cette question n'a pas de réponse unique et universelle, car elle dépend souvent des croyances, des valeurs et des expériences personnelles de chacun.\n",
      "\n",
      "Pour certains, le sens de la vie est intrinsèquement lié à la spiritualité ou à la religion. Dans de nombreuses traditions religieuses, le but de la vie est de vivre selon les principes divins, de chercher la vérité, l'amour et la paix, et de se préparer à une vie après la mort. Cette perspective offre une structure et une orientation morale, en encourageant les individus à vivre une vie de bonté, de compassion et de dévouement.\n",
      "\n",
      "D'autres considèrent le sens de la vie comme étant lié à la recherche de la connaissance et de la compréhension. Cette approche est souvent associée au rationalisme et à l'humanisme, où l'objectif est de comprendre le monde naturel, l'univers et soi-même. Cette quête de connaissance peut se manifester par la recherche scientifique, la philosophie, l'art ou tout autre domaine qui contribue à l'avancement de la compréhension humaine.\n",
      "\n",
      "Une autre perspective est que le sens de la vie est lié à la poursuite du bonheur et de la satisfaction personnelle. Cette vision est souvent associée au concept d'épicurisme, qui prône la recherche du plaisir et du bonheur dans la vie quotidienne. Selon cette approche, le but de la vie est de cultiver des relations significatives, de poursuivre des activités gratifiantes et de vivre dans un état de bien-être émotionnel et mental.\n",
      "\n",
      "Enfin, certains considèrent que le sens de la vie réside dans la contribution à la société et à l'humanité. Cette vision est souvent associée au concept de philanthropie, où l'objectif est d'utiliser ses compétences, ses ressources et son temps pour aider les autres et améliorer le monde. Cette approche souligne l'importance de l'empathie, de la compassion et de l'altruisme, et encourage les individus à agir pour le bien commun.\n",
      "\n",
      "En conclusion, la question du sens de la vie est complexe et multidimensionnelle, et sa réponse dépend souvent des croyances, des valeurs et des expériences personnelles de chacun. Quelle que soit la perspective adoptée, il est essentiel de reconnaître que le sens de la vie est subjectif et personnel, et que chacun a le droit de définir sa propre vision du bonheur et de la finalité de l'existence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22e39a-2313-442d-b91d-97e4dedf2ad1",
   "metadata": {},
   "source": [
    "## Deepseek coder v2 Lite 16b.\n",
    "\n",
    "https://arxiv.org/abs/2406.11931\n",
    "\n",
    "https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8\n",
    "- **Conversion from/to f8e4m3nv is only supported on compute capability >= 90**\n",
    "- => Kernel dies ..\n",
    "\n",
    "https://huggingface.co/TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ\n",
    "- => AssertionError\n",
    "\n",
    "**LICENSE :** https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94ea27-73c1-4b2d-9fef-4e85850851ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8664f-642d-4c42-8a64-ab58467a50b7",
   "metadata": {
    "execution": {
     "execution_failed": "2024-08-25T18:29:29.263Z",
     "iopub.execute_input": "2024-08-25T18:28:56.209987Z",
     "iopub.status.busy": "2024-08-25T18:28:56.209239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-25 20:28:56 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-25 20:28:57 utils.py:721] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 08-25 20:28:57 model_runner.py:879] Starting to load model neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8...\n",
      "WARNING 08-25 20:28:57 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "Cache shape torch.Size([163840, 64])\n",
      "INFO 08-25 20:29:01 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77562a792c9455989112f31688fccf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-25 20:29:14 utils.py:721] Found input_scales that are not equal for fp8 MoE layer. Using the maximum across experts for each layer. \n",
      "INFO 08-25 20:29:24 model_runner.py:890] Loading model weights took 15.0870 GB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FP8\n",
    "model_id = \"neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8\"\n",
    "# INT4\n",
    "#model_id = \"TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, trust_remote_code=True, max_model_len=1024)\n",
    "#llm = LLM(model=model_id, trust_remote_code=True, quantization=\"awq\", max_model_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca1406-06bd-4561-954a-546f9d03da1e",
   "metadata": {
    "execution": {
     "execution_failed": "2024-08-11T22:20:57.842Z"
    }
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=1024)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq paragraphes argumentés ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e84b4-0ef2-4cba-a91f-aa422acf2f81",
   "metadata": {},
   "source": [
    "## Phi-3.5-vision\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3.5-vision-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d34554-20c7-4128-9775-a58810ab48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BF16\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, trust_remote_code=True, max_model_len=4096)\n",
    "\n",
    "==> ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08fd2e-79f5-4988-9af2-4c052145a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312fe2b-fc79-476c-8f19-e2556c764831",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae32248b-dbf1-4877-ba7e-2509cccef962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T21:12:32.678149Z",
     "iopub.status.busy": "2024-08-22T21:12:32.677977Z",
     "iopub.status.idle": "2024-08-22T21:13:14.389421Z",
     "shell.execute_reply": "2024-08-22T21:13:14.389048Z",
     "shell.execute_reply.started": "2024-08-22T21:12:32.678138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd12ff0b31486ca2b9468781d9c632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af201213acc24216af34f63f6538afc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To summarize, the slides are composed of these sections:\n",
      "\n",
      "- Introduction to Azure: \n",
      "The presentation introduces Microsoft Azure, a cloud computing platform that offers a continuously expanding set of cloud services to help organizations meet their current and future business challenges. It provides the freedom to build, manage, and deploy applications on a massive global network using favorite tools and frameworks.\n",
      "\n",
      "- Cloud Computing Models: \n",
      "The presentation explains three types of cloud computing models: Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service (SaaS). Each model is represented by a unique icon and a brief description.\n",
      "\n",
      "- Cloud Services: \n",
      "The presentation illustrates the concept of cloud services using a metaphor of a pizza delivery process. It shows how different services are managed at different stages of the process, from the customer ordering the pizza to the vendor delivering it.\n",
      "\n",
      "- Why the Cloud: \n",
      "The presentation highlights the benefits of cloud computing, including rapid setup environments, scalability, and cost efficiency. It uses symbols to represent speed, scale, and economics.\n",
      "\n",
      "- Azure Services: \n",
      "The presentation provides an overview of Azure services, categorized into different sections such as Security & Management, Platform Services, and Infrastructure Services. Each section contains icons and text describing the services offered.\n",
      "\n",
      "- Azure Data Centers: \n",
      "The presentation provides a global overview of Microsoft Azure data centers, showing the number of data centers in different regions around the world.\n",
      "\n",
      "- Azure Compute Options: \n",
      "The presentation details the different compute options available in Azure, including virtual machines, containers, Azure App Service, and serverless computing.\n",
      "\n",
      "- Azure Pricing Calculator: \n",
      "The presentation introduces the Azure pricing calculator, a tool that allows users to configure and estimate the costs for Azure products.\n",
      "\n",
      "- Azure Stack and Cognitive Services: \n",
      "The presentation explains the Azure Stack, a portfolio of products that extend Azure services and capabilities to the user's environment of choice. It also introduces Azure Cognitive Services, a comprehensive family of AI services and cognitive APIs.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import requests \n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor \n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") \n",
    "\n",
    "images = []\n",
    "placeholder = \"\"\n",
    "\n",
    "# Note: if OOM, you might consider reduce number of frames in this example.\n",
    "for i in range(1,20):\n",
    "    url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\" \n",
    "    images.append(Image.open(requests.get(url, stream=True).raw))\n",
    "    placeholder += f\"<|image_{i}|>\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "  messages, \n",
    "  tokenize=False, \n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 1000, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "generate_ids = model.generate(**inputs, \n",
    "  eos_token_id=processor.tokenizer.eos_token_id, \n",
    "  **generation_args\n",
    ")\n",
    "\n",
    "# remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, \n",
    "  skip_special_tokens=True, \n",
    "  clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
