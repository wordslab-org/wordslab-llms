{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5fa121-c6ac-4a69-a8a2-7e1f3a3964e9",
   "metadata": {},
   "source": [
    "# Exllama v2 performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf000ad0-ee0a-44bc-b3a1-9fd82d454939",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "https://github.com/turboderp/exllamav2\n",
    "\n",
    "https://github.com/theroyallab/tabbyAPI\n",
    "\n",
    "https://github.com/turboderp/exui\n",
    "\n",
    "Vidéo and example:\n",
    "\n",
    "https://www.youtube.com/watch?v=N5lDUZRI8sc\n",
    "\n",
    "https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/exllamaV2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c567cc-3e99-46c3-9359-02b3e2f186a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T14:13:50.092931Z",
     "iopub.status.busy": "2024-04-07T14:13:50.089730Z",
     "iopub.status.idle": "2024-04-07T14:13:50.148652Z",
     "shell.execute_reply": "2024-04-07T14:13:50.145676Z",
     "shell.execute_reply.started": "2024-04-07T14:13:50.092731Z"
    },
    "tags": []
   },
   "source": [
    "## Quantized models repositories\n",
    "\n",
    "https://huggingface.co/turboderp\n",
    "\n",
    "https://huggingface.co/LoneStriker\n",
    "\n",
    "https://huggingface.co/bartowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08510c-208e-4dff-82c9-3cc6aad5b4da",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924999c-ae09-4e8d-8c8d-8c0d8c2c4500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade exllamav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0425c2b-52d2-40b9-88c1-411716d59938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:09:55.734899Z",
     "iopub.status.busy": "2024-04-06T21:09:55.733865Z",
     "iopub.status.idle": "2024-04-06T21:09:55.752281Z",
     "shell.execute_reply": "2024-04-06T21:09:55.751840Z",
     "shell.execute_reply.started": "2024-04-06T21:09:55.734861Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0.dev20231217+cu121'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307d4cbc-4d82-4300-8a17-9fdc7b484277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:10:31.705238Z",
     "iopub.status.busy": "2024-04-06T21:10:31.704556Z",
     "iopub.status.idle": "2024-04-06T21:10:31.712674Z",
     "shell.execute_reply": "2024-04-06T21:10:31.712038Z",
     "shell.execute_reply.started": "2024-04-06T21:10:31.705210Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.17'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"exllamav2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58806929-1974-465f-9577-e0cd327f7766",
   "metadata": {},
   "source": [
    "Open a terminal:\n",
    "\n",
    "```bash\n",
    "source ../test-pytorch/.venv/bin/activate\n",
    "git clone https://github.com/turboderp/exllamav2\n",
    "cd exllamav2\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4e6e2-c70f-4e5c-8696-bd0b821d1c9c",
   "metadata": {},
   "source": [
    "## Download a model and test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eeca56-bd51-4d54-8779-ebdcdf81550c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T14:17:41.096110Z",
     "iopub.status.busy": "2024-04-07T14:17:41.074157Z",
     "iopub.status.idle": "2024-04-07T14:17:41.127066Z",
     "shell.execute_reply": "2024-04-07T14:17:41.122618Z",
     "shell.execute_reply.started": "2024-04-07T14:17:41.095961Z"
    },
    "tags": []
   },
   "source": [
    "Open a terminal:\n",
    "\n",
    "1. Download an exl2 model:    \n",
    "    \n",
    "```bash\n",
    "mkdir /models/huggingface/exllama2\n",
    "cd /models/huggingface/exllama2\n",
    "git clone --branch 4.0bpw --single-branch --depth 1 https://huggingface.co/turboderp/command-r-v01-35B-exl2\n",
    "rm -rf command-r-v01-35B-exl2/.git\n",
    "```\n",
    "\n",
    "2. Test inference:\n",
    "\n",
    "```bash\n",
    "cd /workspace/wordslab-llms/exllamav2/\n",
    "python test_inference.py -m \"/models/huggingface/exllama2/command-r-v01-35B-exl2\" -p \"Les avantages du Crédit Mutuel sont \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b41b9e-a524-487d-91fa-007287aedf09",
   "metadata": {},
   "source": [
    "## Usage in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d92a2-bab2-4d31-8da0-e06443233e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/content/exllamav2')\n",
    "\n",
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "# Input prompts\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "prompts = \\\n",
    "[\n",
    "    \"How do I open a can of beans?\",\n",
    "    \"How do I open a can of soup?\",\n",
    "    \"How do I open a can of strawberry jam?\",\n",
    "    \"How do I open a can of raspberry jam?\",\n",
    "    \"What's the tallest building in Paris?\",\n",
    "    \"What's the most populous nation on Earth?\",\n",
    "    \"What's the most populous nation on Mars?\",\n",
    "    \"What do the Mole People actually want and how can we best appease them?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"Where is Waldo?\",\n",
    "    \"Who is Waldo?\",\n",
    "    \"Why is Waldo?\",\n",
    "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
    "    \"Is it legal to base jump into a volcano?\",\n",
    "    \"Why are cats better than dogs?\",\n",
    "    \"Why is the Hulk so angry all the time?\",\n",
    "    \"How do I build a time machine?\",\n",
    "    \"Is it legal to grow your own catnip?\"\n",
    "]\n",
    "\n",
    "# Sort by length to minimize padding\n",
    "\n",
    "s_prompts = sorted(prompts, key = len)\n",
    "\n",
    "# Apply prompt format\n",
    "\n",
    "def format_prompt(sp, p):\n",
    "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
    "\n",
    "system_prompt = \"Answer the question to the best of your ability.\"\n",
    "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
    "\n",
    "# Split into batches\n",
    "\n",
    "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "# Initialize model and cache\n",
    "\n",
    "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
    "\n",
    "config = ExLlamaV2Config()\n",
    "config.model_dir = model_directory\n",
    "config.prepare()\n",
    "\n",
    "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
    "\n",
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + model_directory)\n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
    "model.load_autosplit(cache)\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Sampling settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.05\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
    "\n",
    "# Generate for each batch\n",
    "\n",
    "collected_outputs = []\n",
    "for b, batch in enumerate(batches):\n",
    "\n",
    "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
    "\n",
    "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
    "\n",
    "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
    "    collected_outputs += trimmed_outputs\n",
    "\n",
    "# Print the results\n",
    "\n",
    "for q, a in zip(s_prompts, collected_outputs):\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Q: \" + q)\n",
    "    print(\"A: \" + a)\n",
    "\n",
    "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde84a4b-e9a0-4e81-800c-12e04a827625",
   "metadata": {},
   "source": [
    "## Performance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcddcd-5341-4822-ad03-28e4cfc1a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2StreamingGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.top_a = 0.0\n",
    "settings.token_repetition_penalty = 1.05\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "prompt_tokens = input_ids.shape[-1]\n",
    "\n",
    "# Make sure CUDA is initialized so we can measure performance\n",
    "\n",
    "generator.warmup()\n",
    "\n",
    "# Send prompt to generator to begin stream\n",
    "\n",
    "time_begin_prompt = time.time()\n",
    "\n",
    "print (prompt, end = \"\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "generator.set_stop_conditions([])\n",
    "generator.begin_stream(input_ids, settings)\n",
    "\n",
    "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
    "# consoles won't update partial lines without it.\n",
    "\n",
    "time_begin_stream = time.time()\n",
    "generated_tokens = 0\n",
    "\n",
    "while True:\n",
    "    chunk, eos, _ = generator.stream()\n",
    "    generated_tokens += 1\n",
    "    print (chunk, end = \"\")\n",
    "    sys.stdout.flush()\n",
    "    if eos or generated_tokens == max_new_tokens: break\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "time_prompt = time_begin_stream - time_begin_prompt\n",
    "time_tokens = time_end - time_begin_stream\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
    "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f549e7-5f05-41aa-af0f-72a91373e94c",
   "metadata": {},
   "source": [
    "- git clone --branch 3.75bpw --single-branch --depth 1 https://huggingface.co/turboderp/command-r-v01-35B-exl2\n",
    "- python ./test_inference.py -m \"/models/huggingface/exllama2/command-r-v01-35B-exl2\" -p \"Les avantages du Crédit Mutuel sont \" --tokens 1024\n",
    "\n",
    " -- Response generated in 171.37 seconds, 1024 tokens, 5.98 tokens/second (includes prompt eval.)\n",
    "\n",
    "```\n",
    "Les avantages du Crédit Mutuel sont nombreux et attrayants. En effet, le Crédit Mutuel est une banque coopérative qui offre à ses clients des services adaptés en fonction de leurs besoins. Il propose également des tarifs très intéressants et des opérations financières variées. Le Crédit Mutuel est aussi une banque de proximité qui permet de rester proche de ses clients et de leur offrir des conseils personnalisés.\n",
    "Le Crédit Mutuel est une banque coopérative. C'est-à-dire que les clients sont considérés comme des sociétaires, ils ont ainsi voix au chapitre lors des assemblées générales et peuvent voter pour élire leurs représentants. Les sociétaires sont aussi les propriétaires de la banque puisqu'ils sont actionnaires. Le Crédit Mutuel est donc une banque dont les bénéfices sont réinvestis dans la banque elle-même au lieu d'être distribués aux actionnaires. Le Crédit Mutuel est aussi une banque mutualiste, c'est-à-dire qu'elle est régie par des valeurs d'entraide et de solidarité.\n",
    "\n",
    "Le Crédit Mutuel : une banque de proximité\n",
    "\n",
    "Le Crédit Mutuel est une banque régionale qui a été créée dans le but de promouvoir l'épargne et le crédit auprès des populations locales. Ainsi, chaque région possède son propre Crédit Mutuel et les clients peuvent bénéficier de conseils personnalisés. Le Crédit Mutuel est aussi connu pour sa proximité avec ses clients et pour le dialogue qu'il entretient avec eux. Il est possible de trouver un Crédit Mutuel dans toutes les régions de France et de bénéficier des conseils d'un conseiller bancaire compétent, présent au quotidien.\n",
    "Le Crédit Mutuel propose également des offres adaptées aux besoins de ses clients. La banque dispose de plusieurs formules de comptes courants et de comptes sur livret. Elle offre aussi la possibilité d'ouvrir un compte épargne logement et de souscrire à des produits d'assurance et de prêt divers et variés. Les clients peuvent aussi profiter d'un large éventail de produits de défiscalisation ou encore de crédits immobiliers ou à la consommation.\n",
    "\n",
    "Une banque au service de ses clients\n",
    "\n",
    "Le Crédit Mutuel met un point d'honneur à être une banque au service de ses clients. Ainsi, la banque propose des tarifs très intéressants et des offres très attractives. Ces dernières sont d'ailleurs régulièrement mises à jour afin de toujours proposer des produits innovants et adaptés aux besoins des clients. Le Crédit Mutuel est aussi une banque très souple qui permet à ses clients de gérer leurs comptes en ligne, mais aussi par téléphone ou en agence. Les clients du Crédit Mutuel peuvent aussi bénéficier du service de paiement sans contact grâce à la carte Blue Cash et de la carte Blue Visa Premier.\n",
    "Les avantages du Crédit Mutuel sont nombreux et la banque offre à ses clients des produits innovants et adaptés à leurs besoins. Le Crédit Mutuel est aussi une banque qui se préoccupe du bien-être de ses clients et les accompagne tout au long de leur vie.\n",
    "\n",
    "Par Jean Louis le 12/09/2016 à 15:40 bonjour;\n",
    "j'ai un compte chez crédit mutuel et ma carte bleue n'a pas été renouvelée, je viens de recevoir un courrier m'indiquant que ma carte blue cash n'est plus disponible et que la nouvelle carte blue visa est payante; je trouve cela abusif et honteux, je suis client depuis plus de 18 ans , il me semble normal que le renouvellement de ma carte soit gratuit.\n",
    "Que puis-je faire, j'ai l'impression que les banques nous prennent pour des pigeons.\n",
    "Merci de votre réponse et bonne journée. Répondre Répondre au commentaire de Jean Louis Annonces GooglePar Jean-Louis le 17/09/2016 à 13:40\n",
    "En effet, il est parfois difficile de comprendre pourquoi une banque décide de changer la gratuité en option payante pour certains services.\n",
    "Le Credit Mutuel est une banque régionale et les conditions peuvent varier d'une région à l'autre.\n",
    "Vous pouvez toujours tenter de négocier la gratuité de la carte et peut-être qu'ils accepteront.\n",
    "Vous pouvez aussi regarder du côté des banques en ligne qui proposent parfois des offres très intéressantes.Par Marie le 21/09/2016 à 12:19\n",
    "J'ai appris récemment que la carte blue cash était supprimée et remplacée par une carte visa premier. J'ai contacté mon agence et on m'a proposé de prendre la carte blue visa premier avec une offre de bienvenue sur les frais de tenue de compte pendant 1 an. Ce qui est très bien. Mais par contre, la carte est payante. C'est un peu abusif quand même. Surtout que les clients ne sont pas vraiment informés.\n",
    "Est-ce que\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0a71e-6195-4345-b1af-aef51082c149",
   "metadata": {},
   "source": [
    "exllama2/Qwen1.5-32B-Chat-4.0bpw-h6-exl2\n",
    "\n",
    "18.84 tokens/second\n",
    "\n",
    "exllama2/Qwen1.5-14B-Chat-4.0bpw-h6-exl2\n",
    "\n",
    "15.64 tokens/second"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-pytorch",
   "language": "python",
   "name": "test-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
