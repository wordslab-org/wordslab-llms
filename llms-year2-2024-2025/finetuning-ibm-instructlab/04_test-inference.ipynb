{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79e0989-feba-4f18-a754-856427aca303",
   "metadata": {},
   "source": [
    "# Instructlab local - Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cec0a20-5a4b-4b59-b1cb-794470b6e223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T17:12:32.462159Z",
     "iopub.status.busy": "2024-06-02T17:12:32.461787Z",
     "iopub.status.idle": "2024-06-02T17:12:32.469578Z",
     "shell.execute_reply": "2024-06-02T17:12:32.468724Z",
     "shell.execute_reply.started": "2024-06-02T17:12:32.462125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"mistral\":\"mistralai/Mistral-7B-v0.3\",\n",
    "    \"mistral-instruct\":\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"llama3\":\"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"llama3-instruct\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"phi3-mini\":\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"phi3-small\":\"microsoft/Phi-3-small-8k-instruct\",\n",
    "    \n",
    "    \"mistral-instruct-gptq-marlin\":\"neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit\",\n",
    "    \"llama3-instruct-gptq\":\"astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit\",\n",
    "    \"llama3-instruct-fp8\":\"neuralmagic/Meta-Llama-3-8B-Instruct-FP8\",\n",
    "    \"phi3-medium-gptq\":\"Phi-3-medium-4k-instruct-gptq-4bit\",\n",
    "    \n",
    "    # Big ones\n",
    "    \"qwen-awq\":\"Qwen/Qwen1.5-32B-Chat-AWQ\",\n",
    "    # Too SLOW\n",
    "    \"mixtral-q3\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\",\n",
    "    \"mixtral-q2\":\"mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29a6b4-09aa-4217-8222-8779195f456c",
   "metadata": {},
   "source": [
    "**IMPORTANT: always set the local download cache directory explicitly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8eb5493-b4a5-4ef0-b989-530bae9df9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T17:12:33.275139Z",
     "iopub.status.busy": "2024-06-02T17:12:33.273858Z",
     "iopub.status.idle": "2024-06-02T17:12:33.281608Z",
     "shell.execute_reply": "2024-06-02T17:12:33.280716Z",
     "shell.execute_reply.started": "2024-06-02T17:12:33.275093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download cache dir: /models/huggingface/transformers OK\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_CACHE_DIR = \"/models/huggingface/transformers\"\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Download cache dir: {DOWNLOAD_CACHE_DIR} {('OK' if Path(DOWNLOAD_CACHE_DIR).exists() else 'KO')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f29e0-f67d-4920-9510-7646222082fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. VLLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fc852-9777-46e4-87bb-b1c7e2512cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T16:10:43.923375Z",
     "iopub.status.busy": "2024-05-25T16:10:43.920040Z",
     "iopub.status.idle": "2024-05-25T16:10:43.974118Z",
     "shell.execute_reply": "2024-05-25T16:10:43.973099Z",
     "shell.execute_reply.started": "2024-05-25T16:10:43.922598Z"
    },
    "tags": []
   },
   "source": [
    "### 1.1 Mistral 7B instruct v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c932f200-a455-4180-9055-ca972b699b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:46.349702Z",
     "iopub.status.busy": "2024-06-01T21:46:46.348866Z",
     "iopub.status.idle": "2024-06-01T21:46:59.960329Z",
     "shell.execute_reply": "2024-06-01T21:46:59.959917Z",
     "shell.execute_reply.started": "2024-06-01T21:46:46.349665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model mistralai/Mistral-7B-Instruct-v0.3\n",
      "INFO 06-01 21:46:47 config.py:390] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n",
      "INFO 06-01 21:46:47 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/models/huggingface/transformers', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/instructlab-local/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-01 21:46:48 utils.py:451] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 06-01 21:46:48 selector.py:130] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 06-01 21:46:48 selector.py:51] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-01 21:46:49 selector.py:130] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 06-01 21:46:49 selector.py:51] Using XFormers backend.\n",
      "INFO 06-01 21:46:49 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-01 21:46:52 model_runner.py:146] Loading model weights took 13.5083 GB\n",
      "INFO 06-01 21:46:55 gpu_executor.py:83] # GPU blocks: 6082, # CPU blocks: 4096\n",
      "INFO 06-01 21:46:55 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-01 21:46:55 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-01 21:46:59 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = models[\"mistral-instruct\"]\n",
    "\n",
    "print(f\"Loading model {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model_name, kv_cache_dtype=\"fp8\", gpu_memory_utilization=0.99, download_dir=DOWNLOAD_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdb123a-ca7c-491f-9ffd-5231b7a8f0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:46:59.961282Z",
     "iopub.status.busy": "2024-06-01T21:46:59.961096Z",
     "iopub.status.idle": "2024-06-01T21:47:00.082021Z",
     "shell.execute_reply": "2024-06-01T21:47:00.081487Z",
     "shell.execute_reply.started": "2024-06-01T21:46:59.961271Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/models/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/83e9aa141f2e28c82232fea5325f54edf17c43de/tokenizer.model.v3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils.hub import cached_file \n",
    "\n",
    "tokenizer_model_file = cached_file(model_name, \"tokenizer.model.v3\", cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "tokenizer_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df31fb71-7374-4f22-a17a-52983ba3bc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:04.581044Z",
     "iopub.status.busy": "2024-06-01T21:47:04.579924Z",
     "iopub.status.idle": "2024-06-01T21:47:04.631835Z",
     "shell.execute_reply": "2024-06-01T21:47:04.631338Z",
     "shell.execute_reply.started": "2024-06-01T21:47:04.580993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(tokenizer_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84a83a5-2bcf-43c2-a4b6-29479ecfbe73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:04.999597Z",
     "iopub.status.busy": "2024-06-01T21:47:04.999033Z",
     "iopub.status.idle": "2024-06-01T21:47:05.016864Z",
     "shell.execute_reply": "2024-06-01T21:47:05.016387Z",
     "shell.execute_reply.started": "2024-06-01T21:47:04.999563Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]▁quelles▁sont▁les▁principales▁ouvertures▁des▁échecs▁?[/INST]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mistral_common.protocol.instruct.messages import SystemMessage, UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"quelles sont les principales ouvertures des échecs ?\")])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request)\n",
    "tokens.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c0b757-c114-434c-876d-0b2d8cd0d856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:05.519864Z",
     "iopub.status.busy": "2024-06-01T21:47:05.519044Z",
     "iopub.status.idle": "2024-06-01T21:47:05.528911Z",
     "shell.execute_reply": "2024-06-01T21:47:05.528473Z",
     "shell.execute_reply.started": "2024-06-01T21:47:05.519835Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "def vllm_stream_output(\n",
    "    llm: LLM,\n",
    "    prompt: str,\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    lora_request: Optional[LoRARequest] = None\n",
    "    ):\n",
    "\n",
    "    if sampling_params is None:\n",
    "        # Use default sampling params.\n",
    "        sampling_params = SamplingParams()\n",
    "\n",
    "    llm._validate_and_add_requests(\n",
    "        inputs=prompt,\n",
    "        params=sampling_params,\n",
    "        lora_request=lora_request,\n",
    "    )\n",
    "\n",
    "    while llm.llm_engine.has_unfinished_requests():\n",
    "        step_outputs = llm.llm_engine.step()\n",
    "        output = step_outputs[0]\n",
    "        yield output.outputs[0].text\n",
    "        if output.finished:\n",
    "            yield output\n",
    "\n",
    "def vllm_display_output(\n",
    "    llm: LLM,\n",
    "    prompt: str,\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    lora_request: Optional[LoRARequest] = None\n",
    "    ):\n",
    "          \n",
    "    last_length = 0\n",
    "    for result in vllm_stream_output(llm, prompt, sampling_params=sampling_params, lora_request=lora_request):\n",
    "        if isinstance(result,str):\n",
    "            print(result[last_length:], end=\"\", flush=True)\n",
    "            last_length = len(result)\n",
    "        else:\n",
    "            print()\n",
    "            print(\"--------------------------\")\n",
    "            input_tokens = len(result.prompt_token_ids)\n",
    "            output_tokens = len(result.outputs[0].token_ids)\n",
    "            total_time = result.metrics.finished_time-result.metrics.arrival_time\n",
    "            time_to_first_token = result.metrics.first_token_time-result.metrics.arrival_time\n",
    "            tokens_per_sec = (output_tokens-1)/(total_time-time_to_first_token)\n",
    "            print(f\"{output_tokens} tokens generated in {total_time:.2f} sec | prompt: {input_tokens:.2f} tokens | time to first token: {time_to_first_token:.2f} sec | throughput: {tokens_per_sec:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22823e51-7379-42fc-a8ce-a70b41f313a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:08.096536Z",
     "iopub.status.busy": "2024-06-01T21:47:08.095296Z",
     "iopub.status.idle": "2024-06-01T21:47:15.854989Z",
     "shell.execute_reply": "2024-06-01T21:47:15.854543Z",
     "shell.execute_reply.started": "2024-06-01T21:47:08.096476Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Les principales ouvertures d'échecs, classées par ordre de popularité, sont :\n",
      "\n",
      "1. Ouverture française (ECO C00) : 1.e4 e6 2.d4 d5\n",
      "2. Défense sicilienne (ECO B00-B99) : 1.e4 c5\n",
      "3. Ouverture du roi (ECO A00-A99) : 1.d4\n",
      "4. Défense indienne (ECO E00-E99) : 1.d4 Cf6\n",
      "5. Défense gréco (ECO D00-D99) : 1.e4 e5\n",
      "6. Défense caro-kann (ECO B10-B99) : 1.e4 c6\n",
      "7. Ouverture scandinave (ECO A10-A99) : 1.e4 d5 2.exd5 Cf6\n",
      "8. Ouverture espagnole (ECO C80-C99) : 1.e4 e5 2.Cf3 Cc6\n",
      "9. Défense sicilienne variante Najdorf (ECO B97) : 1.e4 c5 2.d4 cxd4 3.Cc3 a6 4.Cf3 Cf6 5.dxc5\n",
      "10. Défense sicilienne variante Scheveningen (ECO B84) : 1.e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6 6.f3.\n",
      "--------------------------\n",
      "403 tokens generated in 7.75 sec | prompt: 18.00 tokens | time to first token: 0.27 sec | throughput: 53.72 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1024\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=max_tokens)\n",
    "\n",
    "vllm_display_output(llm, tokens.text,sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09c1a46-9d3f-4c25-813c-7c9b45025f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T20:08:12.724697Z",
     "iopub.status.busy": "2024-06-01T20:08:12.724229Z",
     "iopub.status.idle": "2024-06-01T20:08:31.427637Z",
     "shell.execute_reply": "2024-06-01T20:08:31.427149Z",
     "shell.execute_reply.started": "2024-06-01T20:08:12.724670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.70s/it, Generation Speed: 54.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Les principales ouvertures d'échecs sont les suivantes :\n",
      "\n",
      "1. Ouverture française (ECO C00-C99) : 1. e4 e6\n",
      "2. Ouverture anglaise (ECO A00-A99) : 1. c4\n",
      "3. Défense sicilienne (ECO B00-B99) : 1. e4 c5\n",
      "4. Défense indienne (ECO E00-E99) : 1. d4\n",
      "5. Défense sicilienne, variante Najdorf (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Cf6 5. Nc3 a6 6. Rd1\n",
      "6. Défense sicilienne, variante Scheveningen (ECO B84) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2\n",
      "7. Ouverture du roi (ECO D00-D99) : 1. d4 d5\n",
      "8. Défense russe (ECO E10-E99) : 1. d4 d5 2. c4 c6\n",
      "9. Défense gréco (ECO D05-D09) : 1. d4 d5 2. c4 c6 3. Cf3\n",
      "10. Défense caro-kann (ECO B10-B99) : 1. e4 c6 2. d4 d5\n",
      "11. Défense benoni (ECO A60-A69) : 1. d4 c5 2. d5 e6 3. c4\n",
      "12. Défense scandinave (ECO B03) : 1. e4 d5 2. exd5 Nf6 3. Nc3\n",
      "13. Défense danoise (ECO B04) : 1. e4 e5 2. d4 exd4 3. Qxd4 Nc6\n",
      "14. Défense sicilienne, variante Taimanov (ECO B88) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bd3\n",
      "15. Défense sicilienne, variante Scheveningen, variante Richter-Rauzer (ECO B95) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bg5 h6 7. Bxf6\n",
      "16. Défense sicilienne, variante Dragon (ECO B09) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 g6\n",
      "17. Défense sicilienne, variante Najdorf, variante 6. Be2 (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2\n",
      "18. Défense sicilienne, variante Najdorf, variante 6. Bd3 (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Bd3\n",
      "19. Défense sicilienne, variante Najdorf, variante 6. Be2, variante Bb4+ (ECO B97) : 1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 b5 7. Bb4+ c6 8. d5\n",
      "20. Défense sicilienne, variante Najdorf, variante\n",
      "-------------------------\n",
      "1024 tokens generated in 18.70 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "requests_results = llm.generate(tokens.text, sampling_params=sampling_params)\n",
    "result = requests_results[0]\n",
    "output = result.outputs[0]\n",
    "print(output.text)\n",
    "print(\"-------------------------\")\n",
    "print(f\"{len(output.token_ids)} tokens generated in {result.metrics.finished_time-result.metrics.arrival_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0a8156-4951-4a28-aaab-9170adfcad6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:23.959367Z",
     "iopub.status.busy": "2024-06-01T21:47:23.958566Z",
     "iopub.status.idle": "2024-06-01T21:47:23.966048Z",
     "shell.execute_reply": "2024-06-01T21:47:23.965571Z",
     "shell.execute_reply.started": "2024-06-01T21:47:23.959333Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This port used by gradio apps in the container: 7860\n",
      "Will be exposed at the following url on the virtual machine: /notebooks/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/notebooks/gradio/\" target=\"_blank\">Click here to access the gradio app</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gradio_server_port = int(os.environ.get('GRADIO_PORT'))\n",
    "gradio_root_path = os.environ.get('GRADIO_BASE_URL')\n",
    "\n",
    "print(f\"This port used by gradio apps in the container: {gradio_server_port}\")\n",
    "print(f\"Will be exposed at the following url on the virtual machine: {gradio_root_path}\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f'<a href=\"{gradio_root_path}/\" target=\"_blank\">Click here to access the gradio app</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0122029-aa1c-475b-9791-86da2e092062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:27.337652Z",
     "iopub.status.busy": "2024-06-01T21:47:27.336721Z",
     "iopub.status.idle": "2024-06-01T21:47:28.452177Z",
     "shell.execute_reply": "2024-06-01T21:47:28.451590Z",
     "shell.execute_reply.started": "2024-06-01T21:47:27.337598Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "69 tokens generated in 1.58 sec | prompt: 49.00 tokens | time to first token: 0.34 sec | throughput: 54.76 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "121 tokens generated in 2.47 sec | prompt: 129.00 tokens | time to first token: 0.31 sec | throughput: 55.61 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "195 tokens generated in 3.74 sec | prompt: 267.00 tokens | time to first token: 0.23 sec | throughput: 55.21 tokens/sec\n",
      "\n",
      "--------------------------\n",
      "195 tokens generated in 3.95 sec | prompt: 277.00 tokens | time to first token: 0.33 sec | throughput: 53.73 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "system_prompt = \"Tu es MonIA, un assistant expert qui essaie toujours d'être concis, exact, utile et poli.\"\n",
    "\n",
    "def apply_chat_template(system_prompt, history, message):\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    for turn in history:\n",
    "        messages.append(UserMessage(content=turn[0]))\n",
    "        messages.append(AssistantMessage(content=turn[1]))\n",
    "    messages.append(UserMessage(content=message))\n",
    "    completion_request = ChatCompletionRequest(messages=messages)\n",
    "    tokens = tokenizer.encode_chat_completion(completion_request)\n",
    "    return tokens.text\n",
    "\n",
    "def process_message(message, history, system_prompt):\n",
    "    prompt = apply_chat_template(system_prompt, history, message)\n",
    "    for result in vllm_stream_output(llm, prompt, sampling_params=sampling_params):\n",
    "        if isinstance(result,str):\n",
    "            yield result\n",
    "        else:\n",
    "            print()\n",
    "            print(\"--------------------------\")\n",
    "            input_tokens = len(result.prompt_token_ids)\n",
    "            output_tokens = len(result.outputs[0].token_ids)\n",
    "            total_time = result.metrics.finished_time-result.metrics.arrival_time\n",
    "            time_to_first_token = result.metrics.first_token_time-result.metrics.arrival_time\n",
    "            tokens_per_sec = (output_tokens-1)/(total_time-time_to_first_token)\n",
    "            print(f\"{output_tokens} tokens generated in {total_time:.2f} sec | prompt: {input_tokens:.2f} tokens | time to first token: {time_to_first_token:.2f} sec | throughput: {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(process_message, title=model_name, additional_inputs=[\n",
    "        gr.Textbox(system_prompt, label=\"System Prompt\")\n",
    "    ], additional_inputs_accordion=\"Configuration\").queue()\n",
    "demo.launch(inline=False, share=False, server_name=\"0.0.0.0\", server_port=gradio_server_port, root_path=gradio_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "147bb51e-36e3-4a7a-aa92-9913bfd90ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:44:07.712133Z",
     "iopub.status.busy": "2024-06-01T21:44:07.710608Z",
     "iopub.status.idle": "2024-06-01T21:44:07.720868Z",
     "shell.execute_reply": "2024-06-01T21:44:07.720453Z",
     "shell.execute_reply.started": "2024-06-01T21:44:07.712074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63b755-6929-4239-9caa-7d3feeccb292",
   "metadata": {},
   "source": [
    "## 2. Huggingface inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35a893-4165-4b11-b8bd-fecd7fd0070a",
   "metadata": {},
   "source": [
    "### 2.1 Mistral 7B instruct v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018f3764-b797-46d5-a817-3fbd041a08da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T17:18:45.955217Z",
     "iopub.status.busy": "2024-06-02T17:18:45.952296Z",
     "iopub.status.idle": "2024-06-02T17:18:45.961081Z",
     "shell.execute_reply": "2024-06-02T17:18:45.960484Z",
     "shell.execute_reply.started": "2024-06-02T17:18:45.955170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "def hf_display_output(model, tokenizer, prompt, max_new_tokens=1024):\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_params = dict(\n",
    "        tokenizer(prompt, return_tensors=\"pt\").to('cuda'),\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.90,\n",
    "        top_k=50,\n",
    "        temperature= 0.6,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_params)\n",
    "    t.start()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tokens_count = 0\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        tokens_count += 1\n",
    "        outputs.append(text)\n",
    "        if tokens_count%50 == 0:\n",
    "            current_time = time.time()\n",
    "            tokens_per_sec = tokens_count/(current_time - start_time)\n",
    "            print(f\" --> {tokens_per_sec:.2f} tokens/sec\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    return ''.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3838054-9ef9-41e7-b911-2e2507868190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T17:12:44.120807Z",
     "iopub.status.busy": "2024-06-02T17:12:44.119866Z",
     "iopub.status.idle": "2024-06-02T17:13:09.833080Z",
     "shell.execute_reply": "2024-06-02T17:13:09.832016Z",
     "shell.execute_reply.started": "2024-06-02T17:12:44.120772Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c9349e21c94245a444a0b4ead91252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "model_name = models[\"mistral-instruct\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR, use_safetensors=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2066d1bc-b485-4398-9fbe-075de7a05e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T17:18:52.089056Z",
     "iopub.status.busy": "2024-06-02T17:18:52.088261Z",
     "iopub.status.idle": "2024-06-02T17:19:17.503303Z",
     "shell.execute_reply": "2024-06-02T17:19:17.502844Z",
     "shell.execute_reply.started": "2024-06-02T17:18:52.089022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les principales ouvertures d'échecs, classées par ordre de popularité croissante et en commençant par la plus connue, sont :\n",
      "\n",
      "1. Ouverture du roi  --> 22.77 tokens/sec\n",
      "(1. e4)\n",
      "  - Variantes :\n",
      "    - Ouverture française (1.e4 e6 2.d4 d5)\n",
      "      - Défense sicilienne (1.e4  --> 24.46 tokens/sec\n",
      "c5)\n",
      "        - Variantes telles que la défense Najdorf, la variante Scheveningen, la variante Taimanov, etc.\n",
      "    - Ouverture italienne (1.e4 e5)\n",
      " --> 25.07 tokens/sec\n",
      "      - Gambit de Dame (1.e4 e5 2.Fc4 Fc5)\n",
      "        - Gambit de Lopez (3.d4 exd4  --> 24.75 tokens/sec\n",
      "4.Cf3)\n",
      "    - Ouverture anglaise (1.e4 c4)\n",
      "      - Défense scandinave (1.e4 e5 2.Fc4 Ec5)\n",
      "        - Variantes  --> 24.52 tokens/sec\n",
      "comme le gambit Evans (3.c3) ou le gambit Albin (3.d4)\n",
      "\n",
      "2. Ouverture du Damier (1.d4)\n",
      "  - Variantes :\n",
      "     --> 24.41 tokens/sec\n",
      "- Défense indienne (1.d4 Cf6 2.c4 e6)\n",
      "      - Partie espagnole (1.d4 Cf6 2.c4 e6  --> 24.38 tokens/sec\n",
      "3.Cc3 d5)\n",
      "    - Ouverture du Damier flamande (1.d4 d5)\n",
      "      - Défense Scotch (1.d4 d5 2.c4 c6)\n",
      "         --> 24.24 tokens/sec\n",
      "- Variante Winawer de la partie Espagnole (1.d4 d5 2.c4 c6 3.Cf3 dxc4 4.a4)\n",
      "    - Défense  --> 24.18 tokens/sec\n",
      "tarrasque (1.d4 d5 2.c4 c6 3.Cc3 a6)\n",
      "\n",
      "3. Ouverture du Pirouette (1.c4)\n",
      "  - Variantes :\n",
      "    --> 24.12 tokens/sec\n",
      " - Ouverture du Damier anglais (1.c4 c5)\n",
      "      - Défense sicilienne transposée (1.c4 c5 2.Cc3 e6)\n",
      "     --> 24.10 tokens/sec\n",
      "- Ouverture du Damier italien (1.c4 e5)\n",
      "      - Gambit de Dame (1.c4 e5 2.Fc4 Fc5)\n",
      "        - Gambit de  --> 24.06 tokens/sec\n",
      "Dama de Danseby (3.d4)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Les principales ouvertures d'échecs, classées par ordre de popularité croissante et en commençant par la plus connue, sont :\\n\\n1. Ouverture du roi (1. e4)\\n  - Variantes :\\n    - Ouverture française (1.e4 e6 2.d4 d5)\\n      - Défense sicilienne (1.e4 c5)\\n        - Variantes telles que la défense Najdorf, la variante Scheveningen, la variante Taimanov, etc.\\n    - Ouverture italienne (1.e4 e5)\\n      - Gambit de Dame (1.e4 e5 2.Fc4 Fc5)\\n        - Gambit de Lopez (3.d4 exd4 4.Cf3)\\n    - Ouverture anglaise (1.e4 c4)\\n      - Défense scandinave (1.e4 e5 2.Fc4 Ec5)\\n        - Variantes comme le gambit Evans (3.c3) ou le gambit Albin (3.d4)\\n\\n2. Ouverture du Damier (1.d4)\\n  - Variantes :\\n    - Défense indienne (1.d4 Cf6 2.c4 e6)\\n      - Partie espagnole (1.d4 Cf6 2.c4 e6 3.Cc3 d5)\\n    - Ouverture du Damier flamande (1.d4 d5)\\n      - Défense Scotch (1.d4 d5 2.c4 c6)\\n        - Variante Winawer de la partie Espagnole (1.d4 d5 2.c4 c6 3.Cf3 dxc4 4.a4)\\n    - Défense tarrasque (1.d4 d5 2.c4 c6 3.Cc3 a6)\\n\\n3. Ouverture du Pirouette (1.c4)\\n  - Variantes :\\n    - Ouverture du Damier anglais (1.c4 c5)\\n      - Défense sicilienne transposée (1.c4 c5 2.Cc3 e6)\\n    - Ouverture du Damier italien (1.c4 e5)\\n      - Gambit de Dame (1.c4 e5 2.Fc4 Fc5)\\n        - Gambit de Dama de Danseby (3.d4)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_display_output(model, tokenizer, '<s>[INST]▁quelles▁sont▁les▁principales▁ouvertures▁des▁échecs▁?[/INST]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a18908-2e84-4aa9-8413-5da3849cff21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T06:24:18.188887Z",
     "iopub.status.busy": "2024-06-02T06:24:18.187943Z",
     "iopub.status.idle": "2024-06-02T06:24:18.194513Z",
     "shell.execute_reply": "2024-06-02T06:24:18.193407Z",
     "shell.execute_reply.started": "2024-06-02T06:24:18.188847Z"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Mixtral 8x7B Instruct v0.1 HQQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ad3cc-0e33-4add-bef2-f2c45aeed3e3",
   "metadata": {},
   "source": [
    "**NOTE: HQQ quantization has the best precision for Mixtral on 24 GB, but is not yet supported in VLLM.**\n",
    "\n",
    "https://github.com/vllm-project/vllm/issues/2871\n",
    "\n",
    "https://github.com/mobiusml/hqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b422d2-5aaa-4ef5-9956-1b494f5dfd99",
   "metadata": {},
   "source": [
    "*Installing HQQ ATEN backend*\n",
    "\n",
    "```bash\n",
    "cd /workspace/instructlab-local/\n",
    "source .venv/bin/activate\n",
    "\n",
    "git clone https://github.com/mobiusml/hqq/\n",
    "\n",
    "cd hqq/hqq/kernels\n",
    "python setup_cuda.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeac753d-9691-44e6-928f-d12471f91dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T15:59:09.166632Z",
     "iopub.status.busy": "2024-06-02T15:59:09.166135Z",
     "iopub.status.idle": "2024-06-02T15:59:09.189703Z",
     "shell.execute_reply": "2024-06-02T15:59:09.189179Z",
     "shell.execute_reply.started": "2024-06-02T15:59:09.166602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libc10.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6828/3290198042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hqq-aten'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhqq_aten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: libc10.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.getenv(\"LD_LIBRARY_PATH\") + \":/workspace/instructlab-local/.venv/lib/python3.10/site-packages/torch/lib\"\n",
    "\n",
    "from importlib.metadata import version\n",
    "print(version('hqq-aten'))\n",
    "\n",
    "import hqq_aten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d6f5e-fae2-488f-9c7a-e1131b6082f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer as HQQAutoTokenizer\n",
    "from hqq.core.quantize import HQQLinear, HQQBackend\n",
    "from hqq.utils.patching import prepare_for_inference\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "model_name = models[\"mixtral-q2\"]\n",
    "\n",
    "HQQLinear.set_backend(HQQBackend.ATEN)\n",
    "\n",
    "tokenizer = HQQAutoTokenizer.from_pretrained(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name, cache_dir=DOWNLOAD_CACHE_DIR)\n",
    "\n",
    "# prepare_for_inference(model, backend=\"marlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b1ac72-cc69-45e5-9f78-80a3494a6d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T09:41:29.408185Z",
     "iopub.status.busy": "2024-06-02T09:41:29.408048Z",
     "iopub.status.idle": "2024-06-02T09:41:29.412473Z",
     "shell.execute_reply": "2024-06-02T09:41:29.412049Z",
     "shell.execute_reply.started": "2024-06-02T09:41:29.408175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HQQBackend.PYTORCH_COMPILE: 'forward_pytorch_backprop_compile'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37675adc-10c5-4c22-9bcf-d2efe3f7e36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T09:41:29.413266Z",
     "iopub.status.busy": "2024-06-02T09:41:29.413074Z",
     "iopub.status.idle": "2024-06-02T09:41:29.420198Z",
     "shell.execute_reply": "2024-06-02T09:41:29.419795Z",
     "shell.execute_reply.started": "2024-06-02T09:41:29.413254Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nbits': 4, 'group_size': 64, 'axis': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"nbits\":model.model.layers[0].self_attn.q_proj.meta['nbits'], \"group_size\":model.model.layers[0].self_attn.q_proj.meta['group_size'], \"axis\":model.model.layers[0].self_attn.q_proj.meta['axis']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d91f52-6071-4733-a867-15d199ebeed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T09:41:29.421194Z",
     "iopub.status.busy": "2024-06-02T09:41:29.421019Z",
     "iopub.status.idle": "2024-06-02T09:41:30.794535Z",
     "shell.execute_reply": "2024-06-02T09:41:30.794077Z",
     "shell.execute_reply.started": "2024-06-02T09:41:29.421185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Warmup model: !!! IMPORTANT when using Pytorch compile !!!\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "dummy_input = tokenizer.encode(\"This is a dummy input\", return_tensors='pt').to(model.device)\n",
    "with torch.no_grad(): \n",
    "    outputs = model(dummy_input, use_cache=False, output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71906e-fec8-428e-9d18-fb590fd910c6",
   "metadata": {},
   "source": [
    "**Performance tests**\n",
    "\n",
    "mixtral-q2\n",
    "- PYTORCH backend : 1.18 tokens/sec\n",
    "- PYTORCH_COMPILE backend : 1.41 tokens/sec\n",
    "- \"marlin\" backend : 1.30 tokens/sec\n",
    "- ATEN backend : 3 tokens /sec (from previous test)\n",
    "\n",
    "prepare_for_inference(model, backend=\"torchao_int4\") => does nothing, all layers skipped (because axis=0)\n",
    "\n",
    "=> TOO SLOW for local usage, we will test GPTQ versions of smaller models instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa4e712-c77c-4038-84ae-abe6c1d09158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T09:43:35.458598Z",
     "iopub.status.busy": "2024-06-02T09:43:35.457621Z",
     "iopub.status.idle": "2024-06-02T09:44:50.544700Z",
     "shell.execute_reply": "2024-06-02T09:44:50.544266Z",
     "shell.execute_reply.started": "2024-06-02T09:43:35.458564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Ouverture du  --> 1.27 tokens/sec\n",
      "pion de roi (e4) :\n",
      "-  --> 1.31 tokens/sec\n",
      "La partie espagnole (Cf3,  --> 1.32 tokens/sec\n",
      "d4) ;\n",
      "- La partie italienne  --> 1.32 tokens/sec\n",
      "(Cf3, e4) ;\n",
      " --> 1.33 tokens/sec\n",
      "- Le gambit des deux épées  --> 1.33 tokens/sec\n",
      "(Cf3, Cf3,  --> 1.33 tokens/sec\n",
      "d4) ;\n",
      "- L'attaque  --> 1.33 tokens/sec\n",
      "viennoise (Cf3, d3).\n",
      " --> 1.33 tokens/sec\n",
      "2. Ouverture du pion  --> 1.33 tokens/sec\n",
      "de d"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def chat_processor(message, max_new_tokens=1024, do_sample=True):\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_params = dict(\n",
    "        tokenizer(\"<s> [INST] \" + message + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=0.90,\n",
    "        top_k=50,\n",
    "        temperature= 0.6,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_params)\n",
    "    t.start()\n",
    "    start_time = time.time()\n",
    "    tokens_count = 0\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        tokens_count += 1\n",
    "        outputs.append(text)\n",
    "        if tokens_count%10 == 0:\n",
    "            current_time = time.time()\n",
    "            tokens_per_sec = tokens_count/(current_time - start_time)\n",
    "            print(f\" --> {tokens_per_sec:.2f} tokens/sec\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Generation\n",
    "outputs = chat_processor(\"Quelles sont les principales ouvertures aux échecs ?\", max_new_tokens=100, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructlab-local",
   "language": "python",
   "name": "instructlab-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
