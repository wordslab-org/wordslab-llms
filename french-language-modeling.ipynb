{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:14:03.286861Z",
     "iopub.status.busy": "2023-11-17T20:14:03.286304Z",
     "iopub.status.idle": "2023-11-17T20:14:03.804335Z",
     "shell.execute_reply": "2023-11-17T20:14:03.803869Z",
     "shell.execute_reply.started": "2023-11-17T20:14:03.286844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T23:38:05.973188Z",
     "iopub.status.busy": "2023-11-16T23:38:05.972336Z",
     "iopub.status.idle": "2023-11-16T23:38:05.977439Z",
     "shell.execute_reply": "2023-11-16T23:38:05.976555Z",
     "shell.execute_reply.started": "2023-11-16T23:38:05.973159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:22:18.988139Z",
     "iopub.status.busy": "2023-11-15T06:22:18.987564Z",
     "iopub.status.idle": "2023-11-15T06:22:25.849906Z",
     "shell.execute_reply": "2023-11-15T06:22:25.849209Z",
     "shell.execute_reply.started": "2023-11-15T06:22:18.988115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-6B.0b745f4b791d153df9fbd0062e81c45728868b56.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.f7bc352f27bb1c02ee371a4576942a7d96c8bb97.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.d08b22ae22ee52b7fd762bf6e44711d90cc96794.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-34B.3fac38b00c086fb2420a731d53517582d9303843.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:25:16.477085Z",
     "iopub.status.busy": "2023-11-15T06:25:16.476437Z",
     "iopub.status.idle": "2023-11-15T20:18:05.373331Z",
     "shell.execute_reply": "2023-11-15T20:18:05.361097Z",
     "shell.execute_reply.started": "2023-11-15T06:25:16.477066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466e7dfdc62a4073bf32fd1beb3b099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fad18c8c64ca2bc8f09635d0d7d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26e5a4d1f744dcb6933fe85293ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6edff8851f4abdb00d256bd38919f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fdd7631cc4e2a89580fabf990cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1636c2841b24452b9dd3a0b609300911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-webscraper/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b9087acf2b42248cf5af8a981024df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3407c0b6940c09a7f929cab3bc22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e252a7ffff74a4d909a8bf7b5d4aef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffde3291ccd43aebed0045a6bc157e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efbb66c793e4f02bfa39c35e3d6a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c569195cc44a782e98dbd953ccb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6aefa8c53c42a69a223d33daa7e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cc7b0e0e94dfd9080fc4a7faf9848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff776e00b6cf42e58bd0caeff74a8a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c009ec2f1df34c0c915013bdc3c3ffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f3b8d94964fb18fbc66d9745d21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d622690413b4bd8a9257d48b7a34f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd40479c0b4e4a8bdf4d2d0e850932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d9471d7744e16b18245d0bf57d7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad3b977791546c985708da95bde3fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710cca87f5c34722a8e52d6f0a0f3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c334be2f31e4d34ac6d1c5de47f91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15c1cd12cf4e73b48aa58c95609a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b096d674c8e429ea2383ee3e4d322d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07b3f0c-fa88-49c0-a2a9-74da94b0181d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:36.772930Z",
     "iopub.status.busy": "2023-11-15T22:28:36.772616Z",
     "iopub.status.idle": "2023-11-15T22:28:36.782697Z",
     "shell.execute_reply": "2023-11-15T22:28:36.781762Z",
     "shell.execute_reply.started": "2023-11-15T22:28:36.772887Z"
    }
   },
   "outputs": [],
   "source": [
    "other_models = { \n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b093a3-5c98-4506-8ff6-fd4d3fcf4dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:54.337937Z",
     "iopub.status.busy": "2023-11-15T22:28:54.337640Z",
     "iopub.status.idle": "2023-11-15T22:47:11.856419Z",
     "shell.execute_reply": "2023-11-15T22:47:11.855564Z",
     "shell.execute_reply.started": "2023-11-15T22:28:54.337905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95363c4b2ecb4817aefe35f5efd8e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff40d5c48894470a78b4613e83e09a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670c247f14504a7499e23e49e7e90f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5ed54abf3424580c210b51b182742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "for model in other_models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef2623-1568-44c9-b0e1-22d434c8d673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:11.007140Z",
     "iopub.status.busy": "2023-11-18T12:50:11.006435Z",
     "iopub.status.idle": "2023-11-18T12:50:11.012078Z",
     "shell.execute_reply": "2023-11-18T12:50:11.011446Z",
     "shell.execute_reply.started": "2023-11-18T12:50:11.007116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ea611a-0c97-45f6-aa47-4fe8474954b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:16.865613Z",
     "iopub.status.busy": "2023-11-18T12:50:16.864880Z",
     "iopub.status.idle": "2023-11-18T12:50:16.870681Z",
     "shell.execute_reply": "2023-11-18T12:50:16.869546Z",
     "shell.execute_reply.started": "2023-11-18T12:50:16.865578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = models[\"redpajama_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f28d43-e44b-49b2-a29f-20b103d1d1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:51.109646Z",
     "iopub.status.busy": "2023-11-17T23:49:51.109274Z",
     "iopub.status.idle": "2023-11-17T23:49:56.330791Z",
     "shell.execute_reply": "2023-11-17T23:49:56.330376Z",
     "shell.execute_reply.started": "2023-11-17T23:49:51.109630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc4ac7-3333-4310-bb4d-bc555b41ebf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:56.331914Z",
     "iopub.status.busy": "2023-11-17T23:49:56.331453Z",
     "iopub.status.idle": "2023-11-17T23:50:06.822516Z",
     "shell.execute_reply": "2023-11-17T23:50:06.821911Z",
     "shell.execute_reply.started": "2023-11-17T23:49:56.331902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb64419b9e54f1d9271e6a7034fe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8817b2eeb74d83a7ac4fc16bd30f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1290ddeb18645fa93e0fd5058c7949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a3a7bc743445fbe9feaef5900fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda35c0f10d4d15a9a0fbb3aa408b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7eb0a738c40e991f2de95404a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"frenchtext/banque-fr-2311\"\n",
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dbb014-c670-410e-88a6-219a3466770f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.823168Z",
     "iopub.status.busy": "2023-11-17T23:50:06.822994Z",
     "iopub.status.idle": "2023-11-17T23:50:06.826497Z",
     "shell.execute_reply": "2023-11-17T23:50:06.825982Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.823160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = dataset[0][\"Text\"]\n",
    "len(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896de44a-2e1c-45d3-87ce-527e6b3f38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.827305Z",
     "iopub.status.busy": "2023-11-17T23:50:06.827192Z",
     "iopub.status.idle": "2023-11-17T23:50:06.848030Z",
     "shell.execute_reply": "2023-11-17T23:50:06.847618Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.827297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text_example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fa350c-36ef-4615-9c2b-23b845880794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.848944Z",
     "iopub.status.busy": "2023-11-17T23:50:06.848554Z",
     "iopub.status.idle": "2023-11-17T23:50:08.534591Z",
     "shell.execute_reply": "2023-11-17T23:50:08.534182Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.848929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 23:50:07.766211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-17 23:50:07.786800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.375988006591797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenize_input = tokenizer.encode(text_example[:2048], return_tensors=\"pt\")\n",
    "    loss = model(tokenize_input, labels=tokenize_input)[0]\n",
    "torch.exp(loss.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c21247-3ea4-450f-b810-3699c1f9e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.535351Z",
     "iopub.status.busy": "2023-11-17T23:50:08.535080Z",
     "iopub.status.idle": "2023-11-17T23:50:08.541585Z",
     "shell.execute_reply": "2023-11-17T23:50:08.541199Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.535337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375\n",
      "3689\n",
      "3804\n",
      "2104\n",
      "7936\n",
      "3802\n",
      "10669\n",
      "2171\n",
      "3568\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(dataset[i][\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ad0bf0-41ec-4a90-98d7-db8ab7150b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.542513Z",
     "iopub.status.busy": "2023-11-17T23:50:08.542112Z",
     "iopub.status.idle": "2023-11-17T23:50:08.557331Z",
     "shell.execute_reply": "2023-11-17T23:50:08.556940Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.542501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c375df-43ab-44f7-a3e0-4ec139734e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.558032Z",
     "iopub.status.busy": "2023-11-17T23:50:08.557899Z",
     "iopub.status.idle": "2023-11-17T23:50:08.573642Z",
     "shell.execute_reply": "2023-11-17T23:50:08.573227Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.558022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text = dataset[0:10][\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73874f-5726-4e82-941f-ccf68c109291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.574256Z",
     "iopub.status.busy": "2023-11-17T23:50:08.574143Z",
     "iopub.status.idle": "2023-11-17T23:50:08.580661Z",
     "shell.execute_reply": "2023-11-17T23:50:08.580268Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.574248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048 1630\n",
      "1 2048 1160\n",
      "2 2048 1162\n",
      "3 2048 658\n",
      "4 2048 2048\n",
      "4 2048 1741\n",
      "5 2048 1175\n",
      "6 2048 2048\n",
      "6 2048 2048\n",
      "6 2048 1234\n",
      "7 2048 648\n",
      "8 2048 1072\n",
      "9 2048 585\n"
     ]
    }
   ],
   "source": [
    "for sample_mapping,input_ids,attention_mask in zip(encodings[\"overflow_to_sample_mapping\"],encodings[\"input_ids\"],encodings[\"attention_mask\"]):\n",
    "    print(sample_mapping,len(input_ids),sum(attention_mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b66afe1-835e-4f37-b749-9066805d35ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.581766Z",
     "iopub.status.busy": "2023-11-17T23:50:08.581571Z",
     "iopub.status.idle": "2023-11-17T23:50:08.588527Z",
     "shell.execute_reply": "2023-11-17T23:50:08.588112Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.581757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dataset = dataset.sort(\"Words\").filter(lambda example: example[\"Words\"]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f520b9-2e06-4158-ab39-476563ed75fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.589061Z",
     "iopub.status.busy": "2023-11-17T23:50:08.588968Z",
     "iopub.status.idle": "2023-11-17T23:50:08.591277Z",
     "shell.execute_reply": "2023-11-17T23:50:08.590863Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.589053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(batch_size=32):\n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294cc1d6-bb4a-4253-a249-3af30b0ee43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.591941Z",
     "iopub.status.busy": "2023-11-17T23:50:08.591730Z",
     "iopub.status.idle": "2023-11-17T23:50:08.594172Z",
     "shell.execute_reply": "2023-11-17T23:50:08.593798Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.591928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(dataset_batch):\n",
    "    return tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16,\n",
    "                      return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6f843d-e0ab-41db-b6ef-15334d943e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.594732Z",
     "iopub.status.busy": "2023-11-17T23:50:08.594630Z",
     "iopub.status.idle": "2023-11-17T23:50:08.614329Z",
     "shell.execute_reply": "2023-11-17T23:50:08.613824Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.594724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4,  6,  5,  7, 23,  7,  7,  7,  7,  7, 10,  7,  7, 13,  6,  4,  6,\n",
      "         6, 15,  4,  4,  4, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 32])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12,  6, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 16])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         6, 12, 12, 12,  7, 12, 12, 12, 12, 12, 12, 12, 12, 47])\n",
      "torch.Size([32, 48])\n",
      "tensor([45, 45, 44, 48, 55, 39, 51, 46, 43, 42, 40, 47, 41, 44, 47, 42, 40, 62,\n",
      "        43, 55, 34, 44, 42, 53, 53, 30, 56, 52, 51, 53, 37, 41])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for idx,dataset_batch in enumerate(get_dataset_batches()):\n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "    print(encodings[\"attention_mask\"].sum(axis=1))\n",
    "    print(encodings[\"input_ids\"].size())\n",
    "    if idx == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe85da1-182d-4d7b-8cac-452fb26bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nlls = []\n",
    "for idx,dataset_batch in enumerate(get_dataset_batches(batch_size)):\n",
    "    \n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    \n",
    "    if idx%10==0: print(f\"{(idx+1)*batch_size} / {len(dataset)}: {neg_log_likelihood}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74e49-e5f3-48c4-86ba-7f84fbdb6cfb",
   "metadata": {},
   "source": [
    "15 min\n",
    "\n",
    "32 / 85229: 7.71484375\n",
    "352 / 85229: 7.09765625\n",
    "672 / 85229: 8.1015625\n",
    "992 / 85229: 5.12109375\n",
    "1312 / 85229: 5.78125\n",
    "1632 / 85229: 5.51953125\n",
    "1952 / 85229: 6.12109375\n",
    "2272 / 85229: 3.36328125\n",
    "2592 / 85229: 5.26171875\n",
    "2912 / 85229: 5.0703125\n",
    "3232 / 85229: 4.87109375\n",
    "3552 / 85229: 4.74609375\n",
    "3872 / 85229: 4.046875\n",
    "4192 / 85229: 4.3046875\n",
    "4512 / 85229: 4.11328125\n",
    "4832 / 85229: 4.56640625\n",
    "5152 / 85229: 4.51171875\n",
    "5472 / 85229: 4.359375\n",
    "5792 / 85229: 4.32421875\n",
    "6112 / 85229: 4.68359375\n",
    "6432 / 85229: 4.6015625\n",
    "6752 / 85229: 3.658203125\n",
    "7072 / 85229: 4.7734375\n",
    "7392 / 85229: 6.6328125\n",
    "7712 / 85229: 3.869140625\n",
    "8032 / 85229: 4.9375\n",
    "8352 / 85229: 4.359375\n",
    "8672 / 85229: 5.0078125\n",
    "8992 / 85229: 4.09375\n",
    "9312 / 85229: 3.513671875\n",
    "9632 / 85229: 3.578125\n",
    "9952 / 85229: 3.609375\n",
    "10272 / 85229: 3.505859375\n",
    "10592 / 85229: 2.99609375\n",
    "10912 / 85229: 3.5859375\n",
    "11232 / 85229: 3.18359375\n",
    "11552 / 85229: 3.48046875\n",
    "11872 / 85229: 3.11328125\n",
    "12192 / 85229: 3.40234375\n",
    "12512 / 85229: 3.189453125\n",
    "12832 / 85229: 4.3515625\n",
    "13152 / 85229: 3.306640625\n",
    "13472 / 85229: 3.40234375\n",
    "13792 / 85229: 3.162109375\n",
    "14112 / 85229: 3.451171875\n",
    "14432 / 85229: 2.845703125\n",
    "14752 / 85229: 4.3515625\n",
    "15072 / 85229: 4.2890625\n",
    "15392 / 85229: 2.91015625\n",
    "15712 / 85229: 3.1328125\n",
    "16032 / 85229: 4.02734375\n",
    "16352 / 85229: 3.126953125\n",
    "16672 / 85229: 3.478515625\n",
    "16992 / 85229: 3.193359375\n",
    "17312 / 85229: 3.3203125\n",
    "17632 / 85229: 3.0078125\n",
    "\n",
    "OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6177ee4-eb93-44c3-a6ad-52784cc55742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T06:49:07.814995Z",
     "iopub.status.busy": "2023-11-18T06:49:07.814819Z",
     "iopub.status.idle": "2023-11-18T06:49:07.841774Z",
     "shell.execute_reply": "2023-11-18T06:49:07.841206Z",
     "shell.execute_reply.started": "2023-11-18T06:49:07.814984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.5841)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611711-1e58-46d7-84bf-8c4263ed78d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Memory study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5319-aca9-41ac-816e-1b65065ac3e5",
   "metadata": {},
   "source": [
    "## CUDA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585551-ae01-4dd9-a0d8-7bb4946bb196",
   "metadata": {},
   "source": [
    "[CUDA semantics / Memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management)\n",
    "\n",
    "[Understanding CUDA Memory Usage](https://pytorch.org/docs/stable/torch_cuda_memory.html#torch-cuda-memory)\n",
    "\n",
    "[CUDA memory management API](https://pytorch.org/docs/stable/cuda.html#cuda-memory-management-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d2a5f9-40a1-4fce-a106-c3f16cf58484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:55:37.303562Z",
     "iopub.status.busy": "2023-11-24T21:55:37.302740Z",
     "iopub.status.idle": "2023-11-24T21:55:38.016517Z",
     "shell.execute_reply": "2023-11-24T21:55:38.016087Z",
     "shell.execute_reply.started": "2023-11-24T21:55:37.303527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved :      0.0 MB -   0 %\n",
      "Free     : 23,008.0 MB -  93 %\n",
      "------------------------------\n",
      "Used     :      0.0 MB -   0 %\n",
      "Max used :      0.0 MB -   0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "    \n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeea17f5-409d-4230-bab9-4010f3b6cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:55:15.162845Z",
     "iopub.status.busy": "2023-11-18T12:55:15.162447Z",
     "iopub.status.idle": "2023-11-18T12:55:15.167405Z",
     "shell.execute_reply": "2023-11-18T12:55:15.166833Z",
     "shell.execute_reply.started": "2023-11-18T12:55:15.162820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41785e96-8956-4aee-97d5-d0479c89eb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:56:49.681121Z",
     "iopub.status.busy": "2023-11-18T12:56:49.680290Z",
     "iopub.status.idle": "2023-11-18T12:56:55.117481Z",
     "shell.execute_reply": "2023-11-18T12:56:55.117059Z",
     "shell.execute_reply.started": "2023-11-18T12:56:49.681088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a128d7-8070-4de1-bf66-b6683881921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:00.726679Z",
     "iopub.status.busy": "2023-11-18T13:05:00.725839Z",
     "iopub.status.idle": "2023-11-18T13:05:00.739971Z",
     "shell.execute_reply": "2023-11-18T13:05:00.739570Z",
     "shell.execute_reply.started": "2023-11-18T13:05:00.726644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231118_130500.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22441-8d1d-43c8-90eb-abb78cd3fa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:06.460703Z",
     "iopub.status.busy": "2023-11-18T13:05:06.459810Z",
     "iopub.status.idle": "2023-11-18T13:05:06.468484Z",
     "shell.execute_reply": "2023-11-18T13:05:06.467915Z",
     "shell.execute_reply.started": "2023-11-18T13:05:06.460670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0d6c-289c-4e79-9c6b-c95c0d033581",
   "metadata": {},
   "source": [
    "## Pytorch models exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85be5910-7850-4787-b36c-203bdf93ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:40:02.824012Z",
     "iopub.status.busy": "2023-11-19T16:40:02.823531Z",
     "iopub.status.idle": "2023-11-19T16:40:02.826926Z",
     "shell.execute_reply": "2023-11-19T16:40:02.826475Z",
     "shell.execute_reply.started": "2023-11-19T16:40:02.823996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def find_attribute_origin(obj, attr_name):\n",
    "    for cls in obj.__class__.__mro__:\n",
    "        if attr_name in dir(cls):\n",
    "            return cls.__name__\n",
    "    return obj.__class__.__name__\n",
    "\n",
    "def display_members(obj):\n",
    "    obj_attributes = {}\n",
    "    for member_name in dir(obj):\n",
    "        if member_name[0:1]!=\"_\":\n",
    "            obj_attributes[getattr(obj,member_name).__qualname__ if hasattr(getattr(obj,member_name),\"__qualname__\") else f\"{find_attribute_origin(obj,member_name)}.{member_name}\"] = str(type(getattr(obj,member_name)))\n",
    "    obj_attributes = {k: obj_attributes[k] for k in sorted(obj_attributes)}\n",
    "    for member_name in obj_attributes.keys():\n",
    "        print(member_name, obj_attributes[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "663d0ad3-8085-4699-a6b4-c176eb76aca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T16:24:40.156700Z",
     "iopub.status.busy": "2023-11-18T16:24:40.156401Z",
     "iopub.status.idle": "2023-11-18T16:24:40.186241Z",
     "shell.execute_reply": "2023-11-18T16:24:40.185806Z",
     "shell.execute_reply.started": "2023-11-18T16:24:40.156689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXConfig <class 'type'>\n",
      "GPTNeoXForCausalLM.T_destination <class 'typing.TypeVar'>\n",
      "GPTNeoXForCausalLM.base_model <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.base_model_prefix <class 'str'>\n",
      "GPTNeoXForCausalLM.call_super_init <class 'bool'>\n",
      "GPTNeoXForCausalLM.config <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>\n",
      "GPTNeoXForCausalLM.device <class 'torch.device'>\n",
      "GPTNeoXForCausalLM.dtype <class 'torch.dtype'>\n",
      "GPTNeoXForCausalLM.dummy_inputs <class 'dict'>\n",
      "GPTNeoXForCausalLM.dump_patches <class 'bool'>\n",
      "GPTNeoXForCausalLM.embed_out <class 'torch.nn.modules.linear.Linear'>\n",
      "GPTNeoXForCausalLM.forward <class 'functools.partial'>\n",
      "GPTNeoXForCausalLM.framework <class 'str'>\n",
      "GPTNeoXForCausalLM.generation_config <class 'transformers.generation.configuration_utils.GenerationConfig'>\n",
      "GPTNeoXForCausalLM.get_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.gpt_neox <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.hf_device_map <class 'dict'>\n",
      "GPTNeoXForCausalLM.is_8bit_serializable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_4bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_8bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_parallelizable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_quantized <class 'bool'>\n",
      "GPTNeoXForCausalLM.main_input_name <class 'str'>\n",
      "GPTNeoXForCausalLM.name_or_path <class 'str'>\n",
      "GPTNeoXForCausalLM.prepare_inputs_for_generation <class 'method'>\n",
      "GPTNeoXForCausalLM.quantization_method <enum 'QuantizationMethod'>\n",
      "GPTNeoXForCausalLM.set_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.supports_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.training <class 'bool'>\n",
      "GPTNeoXForCausalLM.warnings_issued <class 'dict'>\n",
      "GenerationMixin.assisted_decoding <class 'method'>\n",
      "GenerationMixin.beam_sample <class 'method'>\n",
      "GenerationMixin.beam_search <class 'method'>\n",
      "GenerationMixin.compute_transition_scores <class 'method'>\n",
      "GenerationMixin.constrained_beam_search <class 'method'>\n",
      "GenerationMixin.contrastive_search <class 'method'>\n",
      "GenerationMixin.generate <class 'method'>\n",
      "GenerationMixin.greedy_search <class 'method'>\n",
      "GenerationMixin.group_beam_search <class 'method'>\n",
      "GenerationMixin.sample <class 'method'>\n",
      "Module.add_module <class 'method'>\n",
      "Module.apply <class 'method'>\n",
      "Module.bfloat16 <class 'method'>\n",
      "Module.buffers <class 'method'>\n",
      "Module.children <class 'method'>\n",
      "Module.cpu <class 'method'>\n",
      "Module.cuda <class 'function'>\n",
      "Module.double <class 'method'>\n",
      "Module.eval <class 'method'>\n",
      "Module.extra_repr <class 'method'>\n",
      "Module.get_buffer <class 'method'>\n",
      "Module.get_extra_state <class 'method'>\n",
      "Module.get_parameter <class 'method'>\n",
      "Module.get_submodule <class 'method'>\n",
      "Module.ipu <class 'method'>\n",
      "Module.load_state_dict <class 'method'>\n",
      "Module.modules <class 'method'>\n",
      "Module.named_buffers <class 'method'>\n",
      "Module.named_children <class 'method'>\n",
      "Module.named_modules <class 'method'>\n",
      "Module.named_parameters <class 'method'>\n",
      "Module.parameters <class 'method'>\n",
      "Module.register_backward_hook <class 'method'>\n",
      "Module.register_buffer <class 'method'>\n",
      "Module.register_forward_hook <class 'method'>\n",
      "Module.register_forward_pre_hook <class 'method'>\n",
      "Module.register_full_backward_hook <class 'method'>\n",
      "Module.register_full_backward_pre_hook <class 'method'>\n",
      "Module.register_load_state_dict_post_hook <class 'method'>\n",
      "Module.register_module <class 'method'>\n",
      "Module.register_parameter <class 'method'>\n",
      "Module.register_state_dict_pre_hook <class 'method'>\n",
      "Module.requires_grad_ <class 'method'>\n",
      "Module.set_extra_state <class 'method'>\n",
      "Module.share_memory <class 'method'>\n",
      "Module.state_dict <class 'method'>\n",
      "Module.to <class 'function'>\n",
      "Module.to_empty <class 'method'>\n",
      "Module.train <class 'method'>\n",
      "Module.type <class 'method'>\n",
      "Module.xpu <class 'method'>\n",
      "Module.zero_grad <class 'method'>\n",
      "ModuleUtilsMixin.add_memory_hooks <class 'method'>\n",
      "ModuleUtilsMixin.create_extended_attention_mask_for_decoder <class 'function'>\n",
      "ModuleUtilsMixin.estimate_tokens <class 'method'>\n",
      "ModuleUtilsMixin.floating_point_ops <class 'method'>\n",
      "ModuleUtilsMixin.get_extended_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.get_head_mask <class 'method'>\n",
      "ModuleUtilsMixin.invert_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.num_parameters <class 'method'>\n",
      "ModuleUtilsMixin.reset_memory_hooks_state <class 'method'>\n",
      "PeftAdapterMixin.active_adapter <class 'method'>\n",
      "PeftAdapterMixin.active_adapters <class 'method'>\n",
      "PeftAdapterMixin.add_adapter <class 'method'>\n",
      "PeftAdapterMixin.disable_adapters <class 'method'>\n",
      "PeftAdapterMixin.enable_adapters <class 'method'>\n",
      "PeftAdapterMixin.get_adapter_state_dict <class 'method'>\n",
      "PeftAdapterMixin.load_adapter <class 'method'>\n",
      "PeftAdapterMixin.set_adapter <class 'method'>\n",
      "PreTrainedModel.can_generate <class 'method'>\n",
      "PreTrainedModel.disable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.enable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.float <class 'method'>\n",
      "PreTrainedModel.from_pretrained <class 'method'>\n",
      "PreTrainedModel.get_input_embeddings <class 'method'>\n",
      "PreTrainedModel.get_memory_footprint <class 'method'>\n",
      "PreTrainedModel.get_position_embeddings <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_disable <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_enable <class 'method'>\n",
      "PreTrainedModel.half <class 'method'>\n",
      "PreTrainedModel.init_weights <class 'method'>\n",
      "PreTrainedModel.post_init <class 'method'>\n",
      "PreTrainedModel.prune_heads <class 'method'>\n",
      "PreTrainedModel.register_for_auto_class <class 'method'>\n",
      "PreTrainedModel.resize_position_embeddings <class 'method'>\n",
      "PreTrainedModel.resize_token_embeddings <class 'method'>\n",
      "PreTrainedModel.retrieve_modules_from_names <class 'method'>\n",
      "PreTrainedModel.reverse_bettertransformer <class 'method'>\n",
      "PreTrainedModel.save_pretrained <class 'method'>\n",
      "PreTrainedModel.set_input_embeddings <class 'method'>\n",
      "PreTrainedModel.tie_weights <class 'method'>\n",
      "PreTrainedModel.to_bettertransformer <class 'method'>\n",
      "PreTrainedModel.warn_if_padding_and_no_attention_mask <class 'method'>\n",
      "PushToHubMixin.push_to_hub <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "display_members(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd82911-00ea-48a4-80d2-709a0797818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:17:20.621530Z",
     "iopub.status.busy": "2023-11-27T21:17:20.621108Z",
     "iopub.status.idle": "2023-11-27T21:17:21.722775Z",
     "shell.execute_reply": "2023-11-27T21:17:21.722342Z",
     "shell.execute_reply.started": "2023-11-27T21:17:20.621499Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "56d76159-52cd-4a11-acae-b3461ce566b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:31:35.570614Z",
     "iopub.status.busy": "2023-11-19T07:31:35.569668Z",
     "iopub.status.idle": "2023-11-19T07:31:35.592081Z",
     "shell.execute_reply": "2023-11-19T07:31:35.591622Z",
     "shell.execute_reply.started": "2023-11-19T07:31:35.570571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "GPTNeoXForCausalLM\n",
      "> submodules\n",
      "- gpt_neox: GPTNeoXModel\n",
      "- embed_out: Linear\n",
      "  ---------------------\n",
      "  gpt_neox#GPTNeoXModel\n",
      "  > submodules\n",
      "  - embed_in: Embedding\n",
      "  - emb_dropout: Dropout\n",
      "  - layers: ModuleList\n",
      "  - final_layer_norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_in#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [50432, 2560] (246.2 MB)\n",
      "    ---------------------\n",
      "    emb_dropout#Dropout\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X GPTNeoXLayer\n",
      "      ---------------------\n",
      "      0..31#GPTNeoXLayer\n",
      "      > submodules\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "      - post_attention_dropout: Dropout\n",
      "      - post_mlp_dropout: Dropout\n",
      "      - attention: GPTNeoXAttention\n",
      "      - mlp: GPTNeoXMLP\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        post_mlp_dropout#Dropout\n",
      "        ---------------------\n",
      "        attention#GPTNeoXAttention\n",
      "        > buffers\n",
      "        - bias: bool [1, 1, 2048, 2048] (4.0 MB)\n",
      "        - masked_bias: float16 [] (0.0 MB)\n",
      "        > submodules\n",
      "        - rotary_emb: GPTNeoXRotaryEmbedding\n",
      "        - query_key_value: Linear8bitLt\n",
      "        - dense: Linear8bitLt\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          rotary_emb#GPTNeoXRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [40] (0.0 MB)\n",
      "          - cos_cached: float16 [2048, 80] (0.3 MB)\n",
      "          - sin_cached: float16 [2048, 80] (0.3 MB)\n",
      "          ---------------------\n",
      "          query_key_value#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [7680, 2560] (18.8 MB)\n",
      "          - bias: float16 [7680] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 2560] (6.2 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#GPTNeoXMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: Linear8bitLt\n",
      "        - dense_4h_to_h: Linear8bitLt\n",
      "        - act: GELUActivation\n",
      "          ---------------------\n",
      "          dense_h_to_4h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [10240, 2560] (25.0 MB)\n",
      "          - bias: float16 [10240] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense_4h_to_h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 10240] (25.0 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          act#GELUActivation\n",
      "    ---------------------\n",
      "    final_layer_norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: float16 [2560] (0.0 MB)\n",
      "    - bias: float16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  embed_out#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [50432, 2560] (246.2 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "GPTNeoXForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "            only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "            `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "        >>> import torch\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config.is_decoder = True\n",
      "        >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "        >>> outputs = model(**inputs)\n",
      "\n",
      "        >>> prediction_logits = outputs.logits\n",
      "        ```\"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.gpt_neox(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            past_key_values=past_key_values,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        lm_logits = self.embed_out(hidden_states)\n",
      "\n",
      "        lm_loss = None\n",
      "        if labels is not None:\n",
      "            # move labels to correct device to enable model parallelism\n",
      "            labels = labels.to(lm_logits.device)\n",
      "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
      "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
      "            labels = labels[:, 1:].contiguous()\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + outputs[1:]\n",
      "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=lm_loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "GPTNeoXModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPast,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \"\"\"\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "            input_shape = input_ids.size()\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        batch_size, seq_length = input_shape\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_length = 0\n",
      "            past_key_values = tuple([None] * self.config.num_hidden_layers)\n",
      "        else:\n",
      "            past_length = past_key_values[0][0].size(-2)\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        # Attention mask.\n",
      "        if attention_mask is not None:\n",
      "            assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "            attention_mask = attention_mask.view(batch_size, -1)\n",
      "            # We create a 3D attention mask from a 2D tensor mask.\n",
      "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
      "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
      "            # this attention mask is more simple than the triangular masking of causal attention\n",
      "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
      "            attention_mask = attention_mask[:, None, None, :]\n",
      "\n",
      "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "            # masked positions, this operation will create a tensor which is 0.0 for\n",
      "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
      "            # Since we are adding it to the raw scores before the softmax, this is\n",
      "            # effectively the same as removing these entirely.\n",
      "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
      "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
      "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_in(input_ids)\n",
      "\n",
      "        hidden_states = self.emb_dropout(inputs_embeds)\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        presents = () if use_cache else None\n",
      "        all_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    use_cache,\n",
      "                    None,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    layer_past=layer_past,\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                )\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "            if output_attentions:\n",
      "                all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        hidden_states = self.final_layer_norm(hidden_states)\n",
      "        # Add last hidden state\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        attention_layer_outputs = self.attention(\n",
      "            self.input_layernorm(hidden_states),\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            layer_past=layer_past,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "        )\n",
      "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
      "        attn_output = self.post_attention_dropout(attn_output)\n",
      "        outputs = attention_layer_outputs[1:]\n",
      "\n",
      "        if self.use_parallel_residual:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output + hidden_states\n",
      "        else:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x))\n",
      "            # x = x + mlp(ln2(x))\n",
      "            attn_output = attn_output + hidden_states\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
      "        else:\n",
      "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        has_layer_past = layer_past is not None\n",
      "\n",
      "        # Compute QKV\n",
      "        # Attention heads [batch, seq_len, hidden_size]\n",
      "        #   --> [batch, seq_len, (np * 3 * head_size)]\n",
      "        qkv = self.query_key_value(hidden_states)\n",
      "\n",
      "        # [batch, seq_len, (num_heads * 3 * head_size)]\n",
      "        #   --> [batch, seq_len, num_heads, 3 * head_size]\n",
      "        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
      "        qkv = qkv.view(*new_qkv_shape)\n",
      "\n",
      "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
      "        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
      "        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
      "        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
      "\n",
      "        # Compute rotary embeddings on rotary_ndims\n",
      "        query_rot = query[..., : self.rotary_ndims]\n",
      "        query_pass = query[..., self.rotary_ndims :]\n",
      "        key_rot = key[..., : self.rotary_ndims]\n",
      "        key_pass = key[..., self.rotary_ndims :]\n",
      "\n",
      "        # Compute token offset for rotary embeddings (when decoding)\n",
      "        seq_len = key.shape[-2]\n",
      "        if has_layer_past:\n",
      "            seq_len += layer_past[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
      "        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "        query = torch.cat((query, query_pass), dim=-1)\n",
      "        key = torch.cat((key, key_pass), dim=-1)\n",
      "\n",
      "        # Cache QKV values\n",
      "        if has_layer_past:\n",
      "            past_key = layer_past[0]\n",
      "            past_value = layer_past[1]\n",
      "            key = torch.cat((past_key, key), dim=-2)\n",
      "            value = torch.cat((past_value, value), dim=-2)\n",
      "        present = (key, value) if use_cache else None\n",
      "\n",
      "        # Compute attention\n",
      "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "\n",
      "        # Reshape outputs\n",
      "        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n",
      "        attn_output = self.dense(attn_output)\n",
      "\n",
      "        outputs = (attn_output, present)\n",
      "        if output_attentions:\n",
      "            outputs += (attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "GPTNeoXRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Linear8bitLt.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor):\n",
      "        self.state.is_training = self.training\n",
      "        if self.weight.CB is not None:\n",
      "            self.init_8bit_state()\n",
      "\n",
      "        # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
      "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
      "            self.bias.data = self.bias.data.to(x.dtype)\n",
      "\n",
      "        out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "\n",
      "        if not self.state.has_fp16_weights:\n",
      "            if self.state.CB is not None and self.state.CxB is not None:\n",
      "                # we converted 8-bit row major to turing/ampere format in the first inference pass\n",
      "                # we no longer need the row-major weight\n",
      "                del self.state.CB\n",
      "                self.weight.data = self.state.CxB\n",
      "        return out\n",
      "\n",
      "---------------------\n",
      "GPTNeoXMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "---------------------\n",
      "GELUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self.act(input)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72c72-5447-48ac-a895-4fda1ae5d6fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Huggingface language models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c480c-21f4-4e01-a4ad-4e2808fbd5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T22:14:40.901070Z",
     "iopub.status.busy": "2023-11-29T22:14:40.895922Z",
     "iopub.status.idle": "2023-11-29T22:14:40.964250Z",
     "shell.execute_reply": "2023-11-29T22:14:40.960213Z",
     "shell.execute_reply.started": "2023-11-29T22:14:40.900411Z"
    },
    "tags": []
   },
   "source": [
    "### Install flash attention\n",
    "\n",
    "https://pypi.org/project/flash-attn/\n",
    "\n",
    "See supported combinations of Pytorch / Python / Cuda versions for prebuilt wheels at:\n",
    "\n",
    "https://github.com/Dao-AILab/flash-attention/releases\n",
    "\n",
    "Run this commands as root to upgrade Pytorch 2.0 to Pytorch 2.1\n",
    "\n",
    "> pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5555e-7bb7-4502-863d-a2cbbe5f3db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d191215-bda3-4727-abb6-4218efd96e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60d14e0-b415-49f2-9066-303a18b7f8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:17:43.378089Z",
     "iopub.status.busy": "2023-12-01T22:17:43.377975Z",
     "iopub.status.idle": "2023-12-01T22:17:43.993456Z",
     "shell.execute_reply": "2023-12-01T22:17:43.992954Z",
     "shell.execute_reply.started": "2023-12-01T22:17:43.378080Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e511f42-0b16-4317-8882-cd38b2a9708e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:17:45.956174Z",
     "iopub.status.busy": "2023-12-01T22:17:45.955370Z",
     "iopub.status.idle": "2023-12-01T22:17:45.965456Z",
     "shell.execute_reply": "2023-12-01T22:17:45.965051Z",
     "shell.execute_reply.started": "2023-12-01T22:17:45.956137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flash_attn\n",
    "flash_attn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c3cb8-66ec-4acb-b67a-674a58448856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526914e-3ed9-454a-8005-aa3bf5b85594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T14:24:29.139926Z",
     "iopub.status.busy": "2023-11-19T14:24:29.139496Z",
     "iopub.status.idle": "2023-11-19T14:24:30.216800Z",
     "shell.execute_reply": "2023-11-19T14:24:30.214848Z",
     "shell.execute_reply.started": "2023-11-19T14:24:29.139898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bbef6d-f92b-44e3-9a41-b8db521c49a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:15:59.131118Z",
     "iopub.status.busy": "2023-11-25T11:15:59.130020Z",
     "iopub.status.idle": "2023-11-25T11:16:00.206536Z",
     "shell.execute_reply": "2023-11-25T11:16:00.206079Z",
     "shell.execute_reply.started": "2023-11-25T11:15:59.131081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.modules.linear.Linear.forward(self, input: torch.Tensor) -> torch.Tensor>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.modules.Linear.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8967c84-c3ff-4d1d-ac12-e053462ab2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:39:48.287488Z",
     "iopub.status.busy": "2023-11-25T11:39:48.286757Z",
     "iopub.status.idle": "2023-11-25T11:39:48.292507Z",
     "shell.execute_reply": "2023-11-25T11:39:48.291894Z",
     "shell.execute_reply.started": "2023-11-25T11:39:48.287456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import textwrap\n",
    "\n",
    "def find_called_functions(func):\n",
    "    source = inspect.getsource(func)\n",
    "    source = textwrap.dedent(source)\n",
    "    tree = ast.parse(source)\n",
    "    \n",
    "    called_functions = []\n",
    "\n",
    "    class FunctionCallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            called_functions.append(node)\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                called_functions.append(node.func.id)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    FunctionCallVisitor().visit(tree)\n",
    "    return called_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b21b728-de1f-47d2-884c-9dcfe780361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:22:59.203254Z",
     "iopub.status.busy": "2023-11-25T13:22:59.201912Z",
     "iopub.status.idle": "2023-11-25T13:22:59.260171Z",
     "shell.execute_reply": "2023-11-25T13:22:59.259714Z",
     "shell.execute_reply.started": "2023-11-25T13:22:59.203213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Script: '/usr/lib/python3/dist-packages/torch/nn/modules/linear.py' <SameEnvironment: 3.10.12 in /workspace/wordslab-llms/.venv>>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import jedi\n",
    "\n",
    "classobj = torch.nn.modules.Linear\n",
    "script = jedi.Script(path=inspect.getfile(classobj))\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a87b7371-faa8-45cd-9212-cfe1db3e713b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:42:30.005791Z",
     "iopub.status.busy": "2023-11-25T13:42:30.005055Z",
     "iopub.status.idle": "2023-11-25T13:42:30.017508Z",
     "shell.execute_reply": "2023-11-25T13:42:30.017186Z",
     "shell.execute_reply.started": "2023-11-25T13:42:30.005760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Name full_name='torch.nn.modules.linear.Linear.forward.linear', description='linear'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall = None\n",
    "\n",
    "for name in script.get_names(all_scopes=True, definitions=True, references=True):\n",
    "    if name.full_name == \"torch.nn.modules.linear.Linear.forward.linear\":\n",
    "        myfunctioncall = name\n",
    "        break\n",
    "\n",
    "myfunctioncall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0501354c-ee1d-44cc-a191-47a03f6b7af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:48:57.920380Z",
     "iopub.status.busy": "2023-11-25T13:48:57.919635Z",
     "iopub.status.idle": "2023-11-25T13:48:57.927712Z",
     "shell.execute_reply": "2023-11-25T13:48:57.926688Z",
     "shell.execute_reply.started": "2023-11-25T13:48:57.920331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name full_name='torch.nn.functional.linear', description='linear = _add_docstr( torch._C._nn.linear, r\"\"\" linear(input, weight, bias=None) -> Tensor Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. This operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>` {sparse_beta_warning} This operator supports :ref:`TensorFloat32<tf32_on_ampere>`. Shape: - Input: :math:`(*, in\\\\_features)` where `*` means any number of additional dimensions, including none - Weight: :math:`(out\\\\_features, in\\\\_features)` or :math:`(in\\\\_features)` - Bias: :math:`(out\\\\_features)` or :math:`()` - Output: :math:`(*, out\\\\_features)` or :math:`(*)`, based on the shape of the weight \"\"\".format(**sparse_support_notes))'>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall.goto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "432af689-ab81-406a-b4ec-d6bf5c860b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:43:31.131568Z",
     "iopub.status.busy": "2023-11-25T13:43:31.130642Z",
     "iopub.status.idle": "2023-11-25T13:43:31.140520Z",
     "shell.execute_reply": "2023-11-25T13:43:31.139446Z",
     "shell.execute_reply.started": "2023-11-25T13:43:31.131514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attr for attr in dir(myfunctioncall) if not callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunctioncall) if callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "634c5e15-19dc-4cd4-9d98-273bc123aef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:54:02.333093Z",
     "iopub.status.busy": "2023-11-25T13:54:02.332278Z",
     "iopub.status.idle": "2023-11-25T13:54:02.343608Z",
     "shell.execute_reply": "2023-11-25T13:54:02.343184Z",
     "shell.execute_reply.started": "2023-11-25T13:54:02.333056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function torch._C._nn.linear>, True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "called_function = eval(script.search(\"F.linear\")[0].full_name)\n",
    "called_function, isinstance(called_function, types.BuiltinFunctionType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01f57ae2-097b-4ccf-9c53-122b97363826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:09.977323Z",
     "iopub.status.busy": "2023-11-25T13:17:09.976180Z",
     "iopub.status.idle": "2023-11-25T13:17:09.986413Z",
     "shell.execute_reply": "2023-11-25T13:17:09.985735Z",
     "shell.execute_reply.started": "2023-11-25T13:17:09.977274Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://jedi.readthedocs.io/en/latest/docs/api-classes.html#name\n",
    "\n",
    "myfunction = None\n",
    "\n",
    "for function in script.get_names(all_scopes=True):\n",
    "    if function.name == \"forward\" and function.type == 'function':\n",
    "        myfunction = function\n",
    "        break\n",
    "        \n",
    "[attr for attr in dir(myfunction) if not callable(getattr(myfunction, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunction) if callable(getattr(myfunction, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "916f8095-b82e-4f35-9a70-adf9afb71aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:21:10.665309Z",
     "iopub.status.busy": "2023-11-25T13:21:10.664447Z",
     "iopub.status.idle": "2023-11-25T13:21:10.673674Z",
     "shell.execute_reply": "2023-11-25T13:21:10.672333Z",
     "shell.execute_reply.started": "2023-11-25T13:21:10.665259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name name='self', description='param self'>,\n",
       " <Name name='input', description='param input: Tensor'>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e226688-03ec-4a9b-8e5b-7d5383215253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:29.900165Z",
     "iopub.status.busy": "2023-11-25T13:17:29.899925Z",
     "iopub.status.idle": "2023-11-25T13:17:29.903541Z",
     "shell.execute_reply": "2023-11-25T13:17:29.903082Z",
     "shell.execute_reply.started": "2023-11-25T13:17:29.900152Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Call at 0x7f23381ad4e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_call = called_functions[0]\n",
    "ast_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "374e43f5-7461-49e9-9f9e-ce48991df618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:41:06.795032Z",
     "iopub.status.busy": "2023-11-25T12:41:06.794448Z",
     "iopub.status.idle": "2023-11-25T12:41:06.800139Z",
     "shell.execute_reply": "2023-11-25T12:41:06.799738Z",
     "shell.execute_reply.started": "2023-11-25T12:41:06.795005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F.linear'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inspect.getsource(torch.nn.modules.Linear.forward)\n",
    "source = textwrap.dedent(source)\n",
    "function_called = \"\"\n",
    "for idx,line in enumerate(source.splitlines()):\n",
    "    if idx+1 == ast_func.lineno:\n",
    "        if ast_func.end_lineno > ast_func.lineno:\n",
    "            function_called += line[ast_func.col_offset:].strip()\n",
    "        else:\n",
    "            function_called += line[ast_func.col_offset:ast_func.end_col_offset] \n",
    "    elif idx+1 > ast_func.lineno and idx+1 < ast_func.end_lineno:\n",
    "        function_called += \" \" + line.strip()\n",
    "    elif ast_func.end_lineno > ast_func.lineno and idx+1 == ast_func.end_lineno:\n",
    "        function_calledµ += \" \" + line[:ast_func.end_col_offset].strip()\n",
    "    elif idx+1 > ast_func.end_lineno:\n",
    "        break\n",
    "        \n",
    "function_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9287d570-50bf-4676-bd3b-18992ae4b7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:45:56.280459Z",
     "iopub.status.busy": "2023-11-25T11:45:56.280108Z",
     "iopub.status.idle": "2023-11-25T11:45:56.285847Z",
     "shell.execute_reply": "2023-11-25T11:45:56.285195Z",
     "shell.execute_reply.started": "2023-11-25T11:45:56.280437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ast.Attribute at 0x7f23381ad5d0>,\n",
       " [<ast.Name at 0x7f23381aec20>,\n",
       "  <ast.Attribute at 0x7f23381ae4d0>,\n",
       "  <ast.Attribute at 0x7f23381ae290>],\n",
       " [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func = ast_call.func\n",
    "ast_args = ast_call.args\n",
    "ast_keywords = ast_call.keywords\n",
    "(ast_func,ast_args,ast_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3637b890-a087-4b96-8042-9f9e2d47f96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:14:06.116817Z",
     "iopub.status.busy": "2023-11-25T12:14:06.116420Z",
     "iopub.status.idle": "2023-11-25T12:14:06.123582Z",
     "shell.execute_reply": "2023-11-25T12:14:06.123077Z",
     "shell.execute_reply.started": "2023-11-25T12:14:06.116790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F', 'linear')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func.value.id, ast_func.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3c05174-7530-4616-bad1-5cb1d757a0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:19:56.383491Z",
     "iopub.status.busy": "2023-11-25T12:19:56.382989Z",
     "iopub.status.idle": "2023-11-25T12:19:56.390702Z",
     "shell.execute_reply": "2023-11-25T12:19:56.390280Z",
     "shell.execute_reply.started": "2023-11-25T12:19:56.383469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input', 'self', 'weight', 'self', 'bias')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_args[0].id,  ast_args[1].value.id, ast_args[1].attr, ast_args[2].value.id, ast_args[2].attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e86d8aa-616c-4b74-9959-444e3a46933d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:20:09.664297Z",
     "iopub.status.busy": "2023-11-25T12:20:09.663902Z",
     "iopub.status.idle": "2023-11-25T12:20:09.666740Z",
     "shell.execute_reply": "2023-11-25T12:20:09.666369Z",
     "shell.execute_reply.started": "2023-11-25T12:20:09.664282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9847189a-0abb-41d7-9c50-648b313104b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:31:04.364054Z",
     "iopub.status.busy": "2023-11-25T12:31:04.363488Z",
     "iopub.status.idle": "2023-11-25T12:31:04.368369Z",
     "shell.execute_reply": "2023-11-25T12:31:04.367844Z",
     "shell.execute_reply.started": "2023-11-25T12:31:04.364015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_attributes',\n",
       " '_fields',\n",
       " 'attr',\n",
       " 'col_offset',\n",
       " 'ctx',\n",
       " 'end_col_offset',\n",
       " 'end_lineno',\n",
       " 'lineno',\n",
       " 'value']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = ast_func\n",
    "[attr for attr in dir(obj) if not callable(getattr(obj, attr)) and not attr.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238de63e-d6f1-4d6e-8a49-f25dc14fd621",
   "metadata": {},
   "source": [
    "### Ipyexperiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38f749-d8fe-46ad-8efe-1d08d696ebe0",
   "metadata": {},
   "source": [
    "https://github.com/stas00/ipyexperiments/blob/master/README.md\n",
    "\n",
    "> pip install ipyexperiments\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "exp1 = IPyExperimentsPytorch()\n",
    "...\n",
    "exp1.keep_var_names('var1', 'var2')\n",
    "\n",
    "# optional\n",
    "data = exp1.finish()\n",
    "cpu_data_final = data.cpu\n",
    "gpu_data_final = data.gpu\n",
    "\n",
    "del exp1\n",
    "```\n",
    "\n",
    "Detailed syntax:\n",
    "\n",
    "```python\n",
    "exp = IPyExperimentsPytorch(cl_enable=True, cl_compact=False, cl_gc_collect=True, cl_set_seed=0)\n",
    "```\n",
    "\n",
    "- cl_enable - enable the subsystem\n",
    "- cl_compact - use compact one line printouts\n",
    "- cl_gc_collect - get correct memory usage reports. Don't use when tracking memory leaks (objects with circular reference).\n",
    "- cl_set_seed - set RNG seed before each cell is run to the provided seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56717d6-49ea-4e04-8f68-59dc6b92fac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab18c80d-f840-40b2-b17e-de2167fa9a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:36:27.224564Z",
     "iopub.status.busy": "2023-12-01T23:36:27.224019Z",
     "iopub.status.idle": "2023-12-01T23:36:28.769260Z",
     "shell.execute_reply": "2023-12-01T23:36:28.768797Z",
     "shell.execute_reply.started": "2023-12-01T23:36:27.224534Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757796c0-57bd-419f-828e-2d845e430877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:36:28.770082Z",
     "iopub.status.busy": "2023-12-01T23:36:28.769931Z",
     "iopub.status.idle": "2023-12-01T23:36:28.985159Z",
     "shell.execute_reply": "2023-12-01T23:36:28.984712Z",
     "shell.execute_reply.started": "2023-12-01T23:36:28.770073Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "# Need to test the line below with Pytorch 2.1\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.8,expandable_segments:True\"\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            total_size += os.path.getsize(full_entry_path)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_used_cpu_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_memory = process.memory_info().rss\n",
    "    return process_memory\n",
    "\n",
    "def get_used_and_max_gpu_memory():\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    return used_memory,max_used_memory\n",
    "\n",
    "def reset_max_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c95b2c-f905-477a-aa52-f685e848ff9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:36:28.985944Z",
     "iopub.status.busy": "2023-12-01T23:36:28.985738Z",
     "iopub.status.idle": "2023-12-01T23:36:29.169004Z",
     "shell.execute_reply": "2023-12-01T23:36:29.168597Z",
     "shell.execute_reply.started": "2023-12-01T23:36:28.985933Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000\n",
    "\n",
    "def get_tensor_params_size_and_dim(param):\n",
    "    if param is None:\n",
    "        return 0,\"\"\n",
    "    elif isinstance(param, torch.Tensor):\n",
    "        psize = param.numel() * param.element_size()\n",
    "        pdim = f\"{str(param.dtype)[6:]}{str(param.size())[11:-1]}\"\n",
    "        return psize,pdim\n",
    "    elif isinstance(param, dict):\n",
    "        size = 0\n",
    "        dim = \"\"\n",
    "        for value in param.values():            \n",
    "            psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "            size += psize\n",
    "            dim += pdim\n",
    "        return size, dim\n",
    "    else:\n",
    "        try:\n",
    "            iter(param)\n",
    "            size = 0\n",
    "            dim = \"\"\n",
    "            for value in param:            \n",
    "                psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "                size += psize\n",
    "                dim += pdim\n",
    "            return size, dim\n",
    "        except TypeError:\n",
    "            return 0,\"\"\n",
    "\n",
    "class ModulePerf:\n",
    "    \n",
    "    def __init__(self, module_name, module, is_leaf_module):\n",
    "        self.module_name = module_name\n",
    "        self.module = module\n",
    "        self.is_leaf_module = is_leaf_module\n",
    "        \n",
    "        self.before_forward_time_ns = 0\n",
    "        self.before_forward_used_memory = 0\n",
    "        self.forward_inputs_memory_size = 0 \n",
    "        self.forward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_forward_time_ns = 0\n",
    "        self.after_forward_used_memory = 0\n",
    "        self.forward_max_used_memory = 0        \n",
    "        self.forward_outputs_memory_size = 0\n",
    "        self.forward_outputs_memory_dim = \"\"\n",
    "        \n",
    "        self.before_backward_time_ns = 0\n",
    "        self.before_backward_used_memory = 0\n",
    "        self.backward_inputs_memory_size = 0\n",
    "        self.backward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_backward_time_ns = 0\n",
    "        self.after_backward_used_memory = 0\n",
    "        self.backward_max_used_memory = 0\n",
    "        self.backward_outputs_memory_size = 0\n",
    "        self.backward_outputs_memory_dim = \"\"\n",
    "        \n",
    "    def before_forward(self, module, args, kwargs):\n",
    "        self.before_forward_time_ns = perf_counter_ns()\n",
    "        self.before_forward_used_memory,_ = get_used_and_max_gpu_memory()   \n",
    "        args_size,args_dim = get_tensor_params_size_and_dim(args)\n",
    "        kwargs_size,kwargs_dim = get_tensor_params_size_and_dim(kwargs) \n",
    "        self.forward_inputs_memory_size = args_size + kwargs_size\n",
    "        self.forward_inputs_memory_dim = args_dim + kwargs_dim\n",
    "        if self.is_leaf_module: reset_max_gpu_memory()\n",
    "        \n",
    "    def after_forward(self, module, args, kwargs, output):\n",
    "        self.after_forward_time_ns = perf_counter_ns()\n",
    "        self.after_forward_used_memory, self.forward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.forward_outputs_memory_size, self.forward_outputs_memory_dim = get_tensor_params_size_and_dim(output) \n",
    "        \n",
    "    def before_backward(self, module, grad_output):\n",
    "        self.before_backward_time_ns = perf_counter_ns()\n",
    "        self.before_backward_used_memory,_ = get_used_and_max_gpu_memory()\n",
    "        self.backward_inputs_memory_size, self.backward_inputs_memory_dim = get_tensor_params_size_and_dim(grad_output) \n",
    "        \n",
    "    def after_backward(self, module, grad_input, grad_output):\n",
    "        self.after_backward_time_ns = perf_counter_ns()\n",
    "        self.after_backward_used_memory, self.backward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.backward_outputs_memory_size, self.backward_outputs_memory_dim = get_tensor_params_size_and_dim(grad_input) \n",
    "    \n",
    "    def get_stats_line(self, initial_used_memory):\n",
    "        return f\"{self.module_name};{self.is_leaf_module};;{(self.after_forward_time_ns-self.before_forward_time_ns)/time_unit_µs:.1f};;{self.forward_inputs_memory_dim};{self.forward_inputs_memory_size/memory_unit_mb:.1f};{(self.before_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.forward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.after_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.forward_outputs_memory_dim};{self.forward_outputs_memory_size/memory_unit_mb:.1f};;{(self.after_backward_time_ns-self.before_backward_time_ns)/time_unit_µs:.1f};;{self.backward_inputs_memory_dim};{(self.backward_inputs_memory_size-initial_used_memory)/memory_unit_mb:.1f};{(self.before_backward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.backward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.after_backward_used_memory/memory_unit_mb:.1f};{self.backward_outputs_memory_dim};{self.backward_outputs_memory_size/memory_unit_mb:.1f}\"\n",
    "    \n",
    "class ModelForCausalLMBenchmark:   \n",
    "    \n",
    "    @staticmethod\n",
    "    def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        path,size = get_model_path_and_size_on_disk(model)\n",
    "        print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"--> stored in directory: {path}\")\n",
    "        print()\n",
    "    \n",
    "    def __init__(self, pretrained_model_id):\n",
    "        self.pretrained_model_id = pretrained_model_id\n",
    "        self.tokenizer = None \n",
    "        self.model = None\n",
    "        \n",
    "        self.model_path = None\n",
    "        self.model_size_on_disk = 0\n",
    "        self.tokenizer_load_time_ns = 0\n",
    "        self.tokenizer_cpu_memory = 0\n",
    "        self.model_load_time_ns = 0\n",
    "        self.model_cpu_memory = 0\n",
    "        self.model_gpu_memory = 0\n",
    "        self.model_load_max_gpu_memory = 0\n",
    "        \n",
    "    def trace_load_from_cache(self, **kwargs):\n",
    "        cpu_memory_before = get_used_cpu_memory()\n",
    "        gpu_memory_before = get_used_and_max_gpu_memory()[0]\n",
    "        reset_max_gpu_memory()        \n",
    "        time_before = perf_counter_ns()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_tokenizer = get_used_cpu_memory()\n",
    "        time_tokenizer = perf_counter_ns()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_model = get_used_cpu_memory()\n",
    "        gpu_memory_model,max_gpu_memory_model = get_used_and_max_gpu_memory()     \n",
    "        time_model = perf_counter_ns()\n",
    "        \n",
    "        self.model_path,self.model_size_on_disk = get_model_path_and_size_on_disk(self.model)\n",
    "        self.tokenizer_load_time_ns = time_tokenizer-time_before\n",
    "        self.tokenizer_cpu_memory = cpu_memory_tokenizer-cpu_memory_before\n",
    "        self.model_load_time_ns = time_model-time_tokenizer\n",
    "        self.model_cpu_memory = cpu_memory_model-cpu_memory_tokenizer\n",
    "        self.model_gpu_memory = gpu_memory_model-gpu_memory_before\n",
    "        self.model_load_max_gpu_memory = max_gpu_memory_model\n",
    "        \n",
    "        self.display_load_results()            \n",
    "    \n",
    "    def display_load_results(self):\n",
    "        print(f\"Model files: {(self.model_size_on_disk/1024/1024/1024):.2f} GB on disk\")\n",
    "        print(\"\"f\"(cache path: {self.model_path})\")\n",
    "        print()\n",
    "        print(f\"Tokenizer load time : {(self.tokenizer_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Tokenizer CPU memory: {(self.tokenizer_cpu_memory/memory_unit_mb):.2f} MB\")\n",
    "        print()\n",
    "        print(f\"Model load time : {(self.model_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Model CPU memory: {(self.model_cpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Model GPU memory: {(self.model_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Max   GPU memory: {(self.model_load_max_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print()\n",
    "        \n",
    "    def trace_prefill(self, batch_size, seq_length):\n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "        \n",
    "        # measure perfs\n",
    "        moduleperfs = []\n",
    "        hookhandles = []\n",
    "        try:\n",
    "            for module_name,module in self.model.named_modules():\n",
    "                if module_name==\"\": module_name=\"<model>\"\n",
    "                mperf = ModulePerf(module_name, module, len(list(module.children())) == 0)\n",
    "                moduleperfs.append(mperf)                \n",
    "                hookhandles.append(module.register_forward_pre_hook(mperf.before_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_forward_hook(mperf.after_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_full_backward_pre_hook(mperf.before_backward))\n",
    "                hookhandles.append(module.register_full_backward_hook(mperf.after_backward))\n",
    "            \n",
    "            # perf test\n",
    "            input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "            attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "        finally:\n",
    "            for handle in hookhandles:\n",
    "                handle.remove()    \n",
    "                \n",
    "        # sort modules\n",
    "        sorted_moduleperfs = sorted(moduleperfs, key=lambda mp: mp.after_forward_time_ns)\n",
    "        first_mperf = None\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.before_forward_used_memory>0:\n",
    "                first_mperf = mperf\n",
    "                break\n",
    "        initial_used_memory = first_mperf.before_forward_used_memory - first_mperf.forward_inputs_memory_size\n",
    "        \n",
    "        # display results\n",
    "        print(f\"Prefill test for batch size {batch_size} and sequence length {seq_length}:\")\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.after_forward_time_ns>0:\n",
    "                print(mperf.get_stats_line(initial_used_memory))\n",
    "    \n",
    "    def check_prefill(self, max_batch_size):\n",
    "        seq_length = self.tokenizer.model_max_length\n",
    "        batch_size = 1\n",
    "        \n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "        # perf test\n",
    "        base_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "        seq_length = 128\n",
    "        while seq_length <= self.tokenizer.model_max_length:\n",
    "            for batch_size in range(1,max_batch_size):\n",
    "                #print(f\"--- {batch_size} x {seq_length} ---\")\n",
    "                reset_max_gpu_memory()\n",
    "                initial_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "                input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "                attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "                before_forward_time_ns = perf_counter_ns()\n",
    "                with torch.no_grad():\n",
    "                    self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "                after_forward_time_ns = perf_counter_ns()\n",
    "                before_release = perf_counter_ns()\n",
    "                # https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "                # see \"expandable_segments\": Pytorch allocator doesn't work when we gradually increase batch size !\n",
    "                # when inferencing with a constant batch size, this should not be needed\n",
    "                release_cached_memory()\n",
    "                after_release = perf_counter_ns()            \n",
    "                gpu_memory, max_gpu_memory = get_used_and_max_gpu_memory()\n",
    "                #print(f\"Forward pass  : {(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.1f} ms\")\n",
    "                #print(f\"Initial memory  : {((initial_gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Maximum memory: {((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Final memory  : {((gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"+ GPU cache release  : {(after_release-before_release)/time_unit_ms:.1f} ms\")\n",
    "                print(f\"{batch_size},{seq_length},{(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.2f},{((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f}\")\n",
    "            seq_length *= 2\n",
    "    \n",
    "    def trace_generate(self):\n",
    "        return\n",
    "    \n",
    "    def trace_train(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e2e674-839c-4675-8589-13f17e33b6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:36:29.169875Z",
     "iopub.status.busy": "2023-12-01T23:36:29.169690Z",
     "iopub.status.idle": "2023-12-01T23:36:29.174652Z",
     "shell.execute_reply": "2023-12-01T23:36:29.174211Z",
     "shell.execute_reply.started": "2023-12-01T23:36:29.169865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipyexperiments import IPyExperimentsPytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5635-beae-4071-9a4b-c9707160f30c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Redpajama-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faf697d-93d3-4712-88ff-b524e9e0ac42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:16:09.894748Z",
     "iopub.status.busy": "2023-11-25T18:16:09.894043Z",
     "iopub.status.idle": "2023-11-25T18:16:30.782608Z",
     "shell.execute_reply": "2023-11-25T18:16:30.781887Z",
     "shell.execute_reply.started": "2023-11-25T18:16:09.894725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     187  14,273  15,837 MB   1.18% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n",
      "Loading model togethercomputer/RedPajama-INCITE-Base-3B-v1 in local cache ...\n",
      "--> model files size   : 5.30 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:19 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      993     -111 MB (-11.25%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,293  13,175  15,837 MB   8.17% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f082f-d604-4a13-a8e8-198824f5723f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78895cd2-c609-4c2b-b772-26084c30bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd6f6f-2c07-4126-a332-919716b51166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7d86f-040d-4af2-8c6c-6decdb8a894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.trace_prefill(20, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8559f3-f244-41c6-9f88-bca7ff7ca1f6",
   "metadata": {},
   "source": [
    "For MPT-3B, the line of code which triggers the maximum memory is this one:\n",
    "\n",
    "transformers/models/gpt_neox/modeling_gpt_neox.py\n",
    "```\n",
    "def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "...\n",
    "    attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    " \n",
    "attn_weights is a very large matrix of size: float16[20, 32, 2048, 2048] => 5120 MB of memory.\n",
    "\n",
    "On this line we need to allocate it twice.\n",
    "An inplace softmax would divide the memory requirements by a factor of 2 !\n",
    "\n",
    "https://lernapparat.de/pytorch-inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48c2cb-ff83-43dc-8fdd-b69b7dd24b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5011ed-6662-4dd1-825a-20a0f4584e31",
   "metadata": {},
   "source": [
    "## 16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cc6fab-c238-4ecd-8cd2-fdfe20baf888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:20:43.832126Z",
     "iopub.status.busy": "2023-11-25T18:20:43.831302Z",
     "iopub.status.idle": "2023-11-25T18:20:43.885587Z",
     "shell.execute_reply": "2023-11-25T18:20:43.885119Z",
     "shell.execute_reply.started": "2023-11-25T18:20:43.832094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,303  13,105  15,837 MB   8.23% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85524c6f-dced-412b-8852-1cd78ca99260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:21:20.689065Z",
     "iopub.status.busy": "2023-11-25T18:21:20.687548Z",
     "iopub.status.idle": "2023-11-25T18:21:28.427278Z",
     "shell.execute_reply": "2023-11-25T18:21:28.426716Z",
     "shell.execute_reply.started": "2023-11-25T18:21:20.689015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 403.71 ms\n",
      "Tokenizer CPU memory: 24.60 MB\n",
      "\n",
      "Model load time : 7301.54 ms\n",
      "Model CPU memory: 0.48 GB\n",
      "Model GPU memory: 5.33 GB\n",
      "Max   GPU memory: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeae00e8-b62a-4307-9fe8-c98b5af4551d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:22:15.150822Z",
     "iopub.status.busy": "2023-11-25T18:22:15.150032Z",
     "iopub.status.idle": "2023-11-25T18:24:42.179098Z",
     "shell.execute_reply": "2023-11-25T18:24:42.178685Z",
     "shell.execute_reply.started": "2023-11-25T18:22:15.150789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,26.00,0.01\n",
      "2,128,21.64,0.03\n",
      "3,128,23.94,0.04\n",
      "4,128,26.08,0.05\n",
      "5,128,33.23,0.06\n",
      "6,128,37.86,0.08\n",
      "7,128,44.32,0.09\n",
      "8,128,44.16,0.10\n",
      "9,128,51.76,0.11\n",
      "10,128,57.60,0.13\n",
      "11,128,61.57,0.14\n",
      "12,128,78.21,0.15\n",
      "13,128,70.41,0.16\n",
      "14,128,75.92,0.18\n",
      "15,128,79.01,0.19\n",
      "16,128,83.47,0.20\n",
      "17,128,89.58,0.22\n",
      "18,128,95.11,0.23\n",
      "19,128,99.08,0.24\n",
      "20,128,104.85,0.25\n",
      "21,128,111.69,0.27\n",
      "22,128,130.38,0.28\n",
      "23,128,120.73,0.29\n",
      "24,128,124.92,0.30\n",
      "25,128,132.10,0.32\n",
      "26,128,140.93,0.33\n",
      "27,128,141.85,0.34\n",
      "28,128,148.08,0.35\n",
      "29,128,154.12,0.37\n",
      "1,256,31.73,0.03\n",
      "2,256,28.20,0.05\n",
      "3,256,39.70,0.08\n",
      "4,256,45.57,0.10\n",
      "5,256,57.65,0.13\n",
      "6,256,66.39,0.15\n",
      "7,256,90.81,0.18\n",
      "8,256,88.43,0.20\n",
      "9,256,102.81,0.23\n",
      "10,256,115.68,0.25\n",
      "11,256,127.42,0.28\n",
      "12,256,138.54,0.30\n",
      "13,256,152.49,0.33\n",
      "14,256,163.05,0.35\n",
      "15,256,174.82,0.38\n",
      "16,256,185.17,0.40\n",
      "17,256,200.74,0.43\n",
      "18,256,211.15,0.46\n",
      "19,256,224.28,0.48\n",
      "20,256,235.55,0.51\n",
      "21,256,249.25,0.53\n",
      "22,256,262.17,0.56\n",
      "23,256,279.91,0.58\n",
      "24,256,284.02,0.61\n",
      "25,256,298.43,0.63\n",
      "26,256,348.39,0.66\n",
      "27,256,331.03,0.68\n",
      "28,256,336.68,0.71\n",
      "29,256,349.37,0.73\n",
      "1,512,30.82,0.05\n",
      "2,512,49.98,0.11\n",
      "3,512,79.85,0.17\n",
      "4,512,108.43,0.22\n",
      "5,512,136.74,0.28\n",
      "6,512,162.38,0.34\n",
      "7,512,190.85,0.39\n",
      "8,512,215.60,0.45\n",
      "9,512,245.34,0.50\n",
      "10,512,271.71,0.56\n",
      "11,512,301.73,0.62\n",
      "12,512,328.14,0.67\n",
      "13,512,356.85,0.72\n",
      "14,512,390.02,0.79\n",
      "15,512,415.09,0.84\n",
      "16,512,440.04,0.89\n",
      "17,512,474.15,0.95\n",
      "18,512,502.06,1.01\n",
      "19,512,524.08,1.06\n",
      "20,512,552.86,1.11\n",
      "21,512,584.60,1.17\n",
      "22,512,607.31,1.23\n",
      "23,512,634.40,1.28\n",
      "24,512,661.45,1.34\n",
      "25,512,694.51,1.39\n",
      "26,512,719.78,1.46\n",
      "27,512,752.86,1.51\n",
      "28,512,769.67,1.56\n",
      "29,512,803.23,1.61\n",
      "1,1024,81.24,0.17\n",
      "2,1024,140.51,0.35\n",
      "3,1024,207.61,0.53\n",
      "4,1024,275.44,0.70\n",
      "5,1024,348.17,0.88\n",
      "6,1024,443.23,1.04\n",
      "7,1024,494.68,1.22\n",
      "8,1024,559.51,1.39\n",
      "9,1024,637.32,1.57\n",
      "10,1024,715.64,1.74\n",
      "11,1024,772.57,1.92\n",
      "12,1024,861.62,2.09\n",
      "13,1024,914.35,2.27\n",
      "14,1024,980.50,2.43\n",
      "15,1024,1050.63,2.62\n",
      "16,1024,1119.38,2.78\n",
      "17,1024,1191.98,2.96\n",
      "18,1024,1264.71,3.13\n",
      "19,1024,1329.68,3.31\n",
      "20,1024,1408.70,3.48\n",
      "21,1024,1510.00,3.66\n",
      "22,1024,1568.26,3.82\n",
      "23,1024,1635.51,4.01\n",
      "24,1024,1681.92,4.17\n",
      "25,1024,1753.95,4.35\n",
      "26,1024,1823.89,4.52\n",
      "27,1024,1894.38,4.70\n",
      "28,1024,1961.38,4.87\n",
      "29,1024,2033.24,5.05\n",
      "1,2048,198.67,0.59\n",
      "2,2048,404.08,1.20\n",
      "3,2048,596.81,1.79\n",
      "4,2048,794.71,2.39\n",
      "5,2048,997.44,2.99\n",
      "6,2048,1203.87,3.59\n",
      "7,2048,1397.73,4.18\n",
      "8,2048,1593.36,4.78\n",
      "9,2048,1793.75,5.38\n",
      "10,2048,1990.36,5.98\n",
      "11,2048,2190.12,6.57\n",
      "12,2048,2387.64,7.17\n",
      "13,2048,2587.98,7.77\n",
      "14,2048,2796.69,8.37\n",
      "15,2048,2987.08,8.96\n",
      "16,2048,3208.74,9.56\n",
      "17,2048,3395.49,10.16\n",
      "18,2048,3589.17,10.76\n",
      "19,2048,3809.31,11.36\n",
      "20,2048,4016.76,11.95\n",
      "21,2048,4213.65,12.55\n",
      "22,2048,4410.30,13.15\n",
      "23,2048,4610.12,13.75\n",
      "24,2048,4811.09,14.34\n",
      "25,2048,5012.10,14.94\n",
      "26,2048,5216.95,15.54\n",
      "27,2048,5413.27,16.14\n",
      "28,2048,5626.83,16.73\n",
      "29,2048,5895.19,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354f0cbc-031d-46ef-bfc3-fe77c1f48008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:24:58.160134Z",
     "iopub.status.busy": "2023-11-25T18:24:58.158780Z",
     "iopub.status.idle": "2023-11-25T18:24:58.256042Z",
     "shell.execute_reply": "2023-11-25T18:24:58.255606Z",
     "shell.execute_reply.started": "2023-11-25T18:24:58.160096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:04:14 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 2046 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      576        0 MB (  0.00%)\n",
      "GPU:    5,296    5,524 MB (104.29%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,604  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e94de-0941-4598-9d82-16fac43b8f28",
   "metadata": {},
   "source": [
    "## 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1462911-a827-4c0a-a996-bed393c311bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:25:57.106319Z",
     "iopub.status.busy": "2023-11-25T18:25:57.105737Z",
     "iopub.status.idle": "2023-11-25T18:25:57.160960Z",
     "shell.execute_reply": "2023-11-25T18:25:57.160534Z",
     "shell.execute_reply.started": "2023-11-25T18:25:57.106301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,601  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbcbf67-c0e1-4d19-9e75-6743faeb0dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:25.556750Z",
     "iopub.status.busy": "2023-11-25T18:26:25.555375Z",
     "iopub.status.idle": "2023-11-25T18:26:29.675190Z",
     "shell.execute_reply": "2023-11-25T18:26:29.674654Z",
     "shell.execute_reply.started": "2023-11-25T18:26:25.556698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 234.35 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 3867.51 ms\n",
      "Model CPU memory: 0.43 GB\n",
      "Model GPU memory: 3.06 GB\n",
      "Max   GPU memory: 3.07 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b05918-fd0f-4a58-b4d3-77dc494eff25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:59.213176Z",
     "iopub.status.busy": "2023-11-25T18:26:59.212633Z",
     "iopub.status.idle": "2023-11-25T18:36:48.697127Z",
     "shell.execute_reply": "2023-11-25T18:36:48.696617Z",
     "shell.execute_reply.started": "2023-11-25T18:26:59.213156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:26:59.999616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-25 18:27:00.165015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,139.23,0.01\n",
      "2,128,126.06,0.03\n",
      "3,128,150.38,0.04\n",
      "4,128,128.92,0.06\n",
      "5,128,148.06,0.07\n",
      "6,128,99.86,0.09\n",
      "7,128,110.90,0.10\n",
      "8,128,112.90,0.11\n",
      "9,128,146.82,0.13\n",
      "10,128,98.36,0.14\n",
      "11,128,118.09,0.15\n",
      "12,128,162.51,0.17\n",
      "13,128,125.73,0.18\n",
      "14,128,131.17,0.20\n",
      "15,128,181.03,0.21\n",
      "16,128,150.02,0.22\n",
      "17,128,168.77,0.23\n",
      "18,128,170.73,0.25\n",
      "19,128,159.60,0.26\n",
      "20,128,188.03,0.28\n",
      "21,128,192.24,0.29\n",
      "22,128,243.53,0.30\n",
      "23,128,175.19,0.32\n",
      "24,128,192.78,0.33\n",
      "25,128,215.48,0.35\n",
      "26,128,227.16,0.36\n",
      "27,128,235.21,0.37\n",
      "28,128,236.02,0.39\n",
      "29,128,240.61,0.40\n",
      "1,256,119.39,0.03\n",
      "2,256,144.90,0.06\n",
      "3,256,136.77,0.08\n",
      "4,256,112.83,0.11\n",
      "5,256,153.43,0.14\n",
      "6,256,144.35,0.16\n",
      "7,256,158.68,0.20\n",
      "8,256,160.82,0.22\n",
      "9,256,190.68,0.25\n",
      "10,256,199.55,0.28\n",
      "11,256,210.79,0.30\n",
      "12,256,223.55,0.33\n",
      "13,256,232.53,0.36\n",
      "14,256,258.61,0.39\n",
      "15,256,258.73,0.41\n",
      "16,256,283.96,0.44\n",
      "17,256,284.48,0.47\n",
      "18,256,277.84,0.50\n",
      "19,256,302.58,0.53\n",
      "20,256,322.53,0.55\n",
      "21,256,342.52,0.58\n",
      "22,256,335.89,0.61\n",
      "23,256,360.73,0.63\n",
      "24,256,382.35,0.66\n",
      "25,256,388.89,0.69\n",
      "26,256,396.49,0.72\n",
      "27,256,419.27,0.74\n",
      "28,256,433.78,0.77\n",
      "29,256,454.33,0.80\n",
      "1,512,126.69,0.05\n",
      "2,512,110.98,0.11\n",
      "3,512,134.16,0.17\n",
      "4,512,173.40,0.22\n",
      "5,512,194.66,0.28\n",
      "6,512,252.41,0.34\n",
      "7,512,282.69,0.39\n",
      "8,512,298.69,0.45\n",
      "9,512,330.79,0.50\n",
      "10,512,368.96,0.56\n",
      "11,512,387.23,0.61\n",
      "12,512,425.58,0.67\n",
      "13,512,458.24,0.72\n",
      "14,512,486.69,0.78\n",
      "15,512,521.96,0.84\n",
      "16,512,555.36,0.89\n",
      "17,512,595.87,0.95\n",
      "18,512,627.89,1.01\n",
      "19,512,653.96,1.06\n",
      "20,512,688.20,1.11\n",
      "21,512,739.88,1.17\n",
      "22,512,751.76,1.23\n",
      "23,512,783.23,1.28\n",
      "24,512,820.04,1.34\n",
      "25,512,848.24,1.39\n",
      "26,512,886.66,1.45\n",
      "27,512,922.13,1.50\n",
      "28,512,953.99,1.56\n",
      "29,512,989.09,1.62\n",
      "1,1024,143.60,0.17\n",
      "2,1024,211.74,0.35\n",
      "3,1024,296.30,0.53\n",
      "4,1024,363.21,0.70\n",
      "5,1024,427.58,0.87\n",
      "6,1024,512.56,1.04\n",
      "7,1024,594.01,1.22\n",
      "8,1024,672.58,1.39\n",
      "9,1024,754.09,1.57\n",
      "10,1024,831.65,1.74\n",
      "11,1024,911.53,1.92\n",
      "12,1024,994.36,2.09\n",
      "13,1024,1074.88,2.26\n",
      "14,1024,1157.32,2.43\n",
      "15,1024,1244.02,2.61\n",
      "16,1024,1325.28,2.78\n",
      "17,1024,1410.68,2.96\n",
      "18,1024,1508.79,3.13\n",
      "19,1024,1621.96,3.31\n",
      "20,1024,1738.23,3.48\n",
      "21,1024,1822.41,3.65\n",
      "22,1024,1929.39,3.82\n",
      "23,1024,2010.18,4.00\n",
      "24,1024,2113.01,4.17\n",
      "25,1024,2215.16,4.35\n",
      "26,1024,2323.07,4.52\n",
      "27,1024,2448.84,4.70\n",
      "28,1024,2589.33,4.87\n",
      "29,1024,2713.17,5.04\n",
      "1,2048,260.51,0.59\n",
      "2,2048,491.49,1.20\n",
      "3,2048,704.28,1.80\n",
      "4,2048,924.34,2.39\n",
      "5,2048,1152.22,2.99\n",
      "6,2048,1372.37,3.59\n",
      "7,2048,1613.14,4.19\n",
      "8,2048,1833.02,4.78\n",
      "9,2048,2079.61,5.38\n",
      "10,2048,2356.59,5.98\n",
      "11,2048,2605.31,6.58\n",
      "12,2048,2901.83,7.17\n",
      "13,2048,3178.02,7.77\n",
      "14,2048,3458.56,8.37\n",
      "15,2048,3787.02,8.97\n",
      "16,2048,4034.38,9.56\n",
      "17,2048,4298.46,10.16\n",
      "18,2048,4534.19,10.76\n",
      "19,2048,4773.00,11.36\n",
      "20,2048,5028.51,11.95\n",
      "21,2048,5325.63,12.55\n",
      "22,2048,5606.51,13.15\n",
      "23,2048,8651.16,13.75\n",
      "24,2048,12485.91,14.34\n",
      "25,2048,16329.58,14.94\n",
      "26,2048,20278.89,15.54\n",
      "27,2048,104959.78,16.14\n",
      "28,2048,128232.94,16.74\n",
      "29,2048,155784.38,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e078485-b8fc-4898-b121-ca3035410013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.698207Z",
     "iopub.status.busy": "2023-11-25T18:36:48.697921Z",
     "iopub.status.idle": "2023-11-25T18:36:48.867224Z",
     "shell.execute_reply": "2023-11-25T18:36:48.866826Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.698196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:51 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 11457 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      802        0 MB (  0.00%)\n",
      "GPU:    3,400    3,324 MB ( 97.76%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1af3b5-0c94-4981-aae7-ff5db560d54a",
   "metadata": {},
   "source": [
    "## 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5bfc6bf-1eae-49ae-a0d6-c6005f939637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.867829Z",
     "iopub.status.busy": "2023-11-25T18:36:48.867719Z",
     "iopub.status.idle": "2023-11-25T18:36:48.969935Z",
     "shell.execute_reply": "2023-11-25T18:36:48.969501Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.867820Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df1d8cdc-6bb6-4de7-84b2-db67b517f25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.970943Z",
     "iopub.status.busy": "2023-11-25T18:36:48.970653Z",
     "iopub.status.idle": "2023-11-25T18:36:53.462226Z",
     "shell.execute_reply": "2023-11-25T18:36:53.461768Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.970926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 244.97 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 4242.37 ms\n",
      "Model CPU memory: 0.40 GB\n",
      "Model GPU memory: 1.95 GB\n",
      "Max   GPU memory: 1.96 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86db86aa-38c6-46bf-8cad-6f2e80d6f9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:53.462979Z",
     "iopub.status.busy": "2023-11-25T18:36:53.462843Z",
     "iopub.status.idle": "2023-11-25T18:47:10.690379Z",
     "shell.execute_reply": "2023-11-25T18:47:10.689843Z",
     "shell.execute_reply.started": "2023-11-25T18:36:53.462970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,60.92,0.16\n",
      "2,128,71.53,0.17\n",
      "3,128,84.30,0.18\n",
      "4,128,97.68,0.19\n",
      "5,128,118.82,0.20\n",
      "6,128,129.78,0.21\n",
      "7,128,145.19,0.22\n",
      "8,128,155.80,0.23\n",
      "9,128,175.67,0.24\n",
      "10,128,191.66,0.25\n",
      "11,128,203.37,0.26\n",
      "12,128,221.43,0.27\n",
      "13,128,241.67,0.28\n",
      "14,128,254.04,0.29\n",
      "15,128,271.51,0.30\n",
      "16,128,282.05,0.31\n",
      "17,128,298.93,0.32\n",
      "18,128,320.51,0.33\n",
      "19,128,339.63,0.35\n",
      "20,128,353.81,0.36\n",
      "21,128,369.18,0.37\n",
      "22,128,383.35,0.38\n",
      "23,128,405.28,0.39\n",
      "24,128,412.20,0.40\n",
      "25,128,434.94,0.41\n",
      "26,128,453.60,0.42\n",
      "27,128,469.91,0.43\n",
      "28,128,527.36,0.44\n",
      "29,128,499.12,0.45\n",
      "1,256,70.25,0.17\n",
      "2,256,97.60,0.19\n",
      "3,256,133.43,0.21\n",
      "4,256,158.39,0.23\n",
      "5,256,192.77,0.25\n",
      "6,256,223.31,0.27\n",
      "7,256,255.22,0.29\n",
      "8,256,289.06,0.31\n",
      "9,256,324.01,0.33\n",
      "10,256,359.26,0.36\n",
      "11,256,397.50,0.38\n",
      "12,256,422.38,0.40\n",
      "13,256,465.23,0.42\n",
      "14,256,494.10,0.44\n",
      "15,256,524.89,0.46\n",
      "16,256,554.36,0.48\n",
      "17,256,594.87,0.50\n",
      "18,256,625.88,0.52\n",
      "19,256,657.29,0.54\n",
      "20,256,693.34,0.57\n",
      "21,256,734.68,0.59\n",
      "22,256,765.58,0.61\n",
      "23,256,798.60,0.63\n",
      "24,256,865.42,0.66\n",
      "25,256,869.00,0.68\n",
      "26,256,945.38,0.70\n",
      "27,256,938.99,0.73\n",
      "28,256,968.48,0.75\n",
      "29,256,1013.42,0.77\n",
      "1,512,101.13,0.19\n",
      "2,512,162.31,0.23\n",
      "3,512,231.70,0.27\n",
      "4,512,300.92,0.31\n",
      "5,512,378.70,0.36\n",
      "6,512,444.53,0.40\n",
      "7,512,514.73,0.44\n",
      "8,512,577.65,0.48\n",
      "9,512,652.16,0.52\n",
      "10,512,722.41,0.56\n",
      "11,512,801.97,0.61\n",
      "12,512,867.43,0.67\n",
      "13,512,951.10,0.73\n",
      "14,512,1016.43,0.78\n",
      "15,512,1118.21,0.84\n",
      "16,512,1161.92,0.89\n",
      "17,512,1262.12,0.95\n",
      "18,512,1301.05,1.00\n",
      "19,512,1374.66,1.06\n",
      "20,512,1446.17,1.11\n",
      "21,512,1516.33,1.17\n",
      "22,512,1621.12,1.23\n",
      "23,512,1668.60,1.28\n",
      "24,512,1741.53,1.34\n",
      "25,512,1824.29,1.39\n",
      "26,512,1876.20,1.45\n",
      "27,512,1957.67,1.51\n",
      "28,512,2018.63,1.56\n",
      "29,512,2111.34,1.62\n",
      "1,1024,177.56,0.23\n",
      "2,1024,325.82,0.35\n",
      "3,1024,481.74,0.52\n",
      "4,1024,626.40,0.70\n",
      "5,1024,783.28,0.87\n",
      "6,1024,947.58,1.04\n",
      "7,1024,1113.18,1.22\n",
      "8,1024,1252.86,1.39\n",
      "9,1024,1423.26,1.57\n",
      "10,1024,1582.51,1.74\n",
      "11,1024,1743.23,1.92\n",
      "12,1024,1876.38,2.09\n",
      "13,1024,2053.46,2.26\n",
      "14,1024,2214.09,2.43\n",
      "15,1024,2355.47,2.61\n",
      "16,1024,2551.10,2.78\n",
      "17,1024,2707.79,2.96\n",
      "18,1024,2840.95,3.13\n",
      "19,1024,3022.16,3.31\n",
      "20,1024,3155.51,3.48\n",
      "21,1024,3326.08,3.65\n",
      "22,1024,3477.24,3.82\n",
      "23,1024,3649.82,4.00\n",
      "24,1024,3793.07,4.17\n",
      "25,1024,3983.23,4.35\n",
      "26,1024,4103.07,4.52\n",
      "27,1024,4321.68,4.70\n",
      "28,1024,4432.53,4.87\n",
      "29,1024,4618.06,5.04\n",
      "1,2048,380.29,0.59\n",
      "2,2048,735.99,1.20\n",
      "3,2048,1109.80,1.79\n",
      "4,2048,1481.10,2.39\n",
      "5,2048,1857.48,2.99\n",
      "6,2048,2220.60,3.59\n",
      "7,2048,2597.17,4.18\n",
      "8,2048,3008.82,4.78\n",
      "9,2048,3350.06,5.38\n",
      "10,2048,3721.51,5.98\n",
      "11,2048,4100.85,6.57\n",
      "12,2048,4472.83,7.17\n",
      "13,2048,4874.75,7.77\n",
      "14,2048,5250.82,8.37\n",
      "15,2048,5607.23,8.96\n",
      "16,2048,6032.71,9.56\n",
      "17,2048,6416.30,10.16\n",
      "18,2048,6791.63,10.76\n",
      "19,2048,7186.32,11.36\n",
      "20,2048,7609.28,11.95\n",
      "21,2048,8022.64,12.55\n",
      "22,2048,8408.02,13.15\n",
      "23,2048,8836.85,13.75\n",
      "24,2048,13241.47,14.34\n",
      "25,2048,30777.34,14.94\n",
      "26,2048,44202.53,15.54\n",
      "27,2048,61871.65,16.14\n",
      "28,2048,78737.15,16.73\n",
      "29,2048,138651.96,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bf77fa-129e-45a1-b5f9-97bd8bd89eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:47:10.691037Z",
     "iopub.status.busy": "2023-11-25T18:47:10.690918Z",
     "iopub.status.idle": "2023-11-25T18:47:10.827226Z",
     "shell.execute_reply": "2023-11-25T18:47:10.826730Z",
     "shell.execute_reply.started": "2023-11-25T18:47:10.691028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:21 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 13086 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      406      416 MB (102.46%)\n",
      "GPU:    2,130    2,002 MB ( 93.99%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,673  12,018  15,837 MB  16.88% \n",
      "GPU:   1,204  23,359  24,564 MB   4.91% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d779f88-0477-4b7d-9111-3a3bcecc092e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# StableLM-3B\n",
    "\n",
    "stabilityai/stablelm-3b-4e1t\n",
    "\n",
    "https://huggingface.co/stabilityai/stablelm-3b-4e1t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae435303-cdac-49c6-89fc-f9a970127f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:24:29.894242Z",
     "iopub.status.busy": "2023-11-29T23:24:29.893754Z",
     "iopub.status.idle": "2023-11-29T23:24:29.896378Z",
     "shell.execute_reply": "2023-11-29T23:24:29.895930Z",
     "shell.execute_reply.started": "2023-11-29T23:24:29.894224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b18a20-0ca7-47e7-9279-e6ee4eb7e323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T20:49:27.416244Z",
     "iopub.status.busy": "2023-11-25T20:49:27.414961Z",
     "iopub.status.idle": "2023-11-25T20:49:41.696725Z",
     "shell.execute_reply": "2023-11-25T20:49:41.695754Z",
     "shell.execute_reply.started": "2023-11-25T20:49:27.415926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     230  14,298  15,837 MB   1.45% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n",
      "Loading model stabilityai/stablelm-3b-4e1t in local cache ...\n",
      "--> model files size   : 5.21 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:12 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -180     -110 MB ( 61.27%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     160  14,382  15,837 MB   1.01% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d606c66-5b9e-4329-8397-1ffebb7d77e9",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9084f350-715a-40b1-92a4-acdd6f002e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:42:58.895791Z",
     "iopub.status.busy": "2023-11-29T23:42:58.895248Z",
     "iopub.status.idle": "2023-11-29T23:42:58.952643Z",
     "shell.execute_reply": "2023-11-29T23:42:58.952154Z",
     "shell.execute_reply.started": "2023-11-29T23:42:58.895769Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     971  29,608  31,785 MB   3.06% \n",
      "GPU:   1,404  23,159  24,564 MB   5.72% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cba0519-c2fb-4945-9c0c-dbca1fc4a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:43:00.831760Z",
     "iopub.status.busy": "2023-11-29T23:43:00.831132Z",
     "iopub.status.idle": "2023-11-29T23:43:02.715793Z",
     "shell.execute_reply": "2023-11-29T23:43:02.715326Z",
     "shell.execute_reply.started": "2023-11-29T23:43:00.831734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.21 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813)\n",
      "\n",
      "Tokenizer load time : 256.93 ms\n",
      "Tokenizer CPU memory: 2.06 MB\n",
      "\n",
      "Model load time : 1622.29 ms\n",
      "Model CPU memory: 0.01 GB\n",
      "Model GPU memory: 5.24 GB\n",
      "Max   GPU memory: 5.25 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06aa726-38a3-4661-a6d3-d0d9e37f689d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:43:14.380805Z",
     "iopub.status.busy": "2023-11-29T23:43:14.380128Z",
     "iopub.status.idle": "2023-11-29T23:45:47.558962Z",
     "shell.execute_reply": "2023-11-29T23:45:47.557246Z",
     "shell.execute_reply.started": "2023-11-29T23:43:14.380776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 13.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4398/1717004278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_prefill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel_benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4398/1240821869.py\u001b[0m in \u001b[0;36mtrace_prefill\u001b[0;34m(self, batch_size, seq_length)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhookhandles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/models/huggingface/modules/transformers_modules/stabilityai/stablelm-3b-4e1t/c6554ba60f40a8252d2a43e38e55ee2e3a645813/modeling_stablelm_epoch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/models/huggingface/modules/transformers_modules/stabilityai/stablelm-3b-4e1t/c6554ba60f40a8252d2a43e38e55ee2e3a645813/modeling_stablelm_epoch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    499\u001b[0m                 )\n\u001b[1;32m    500\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    502\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/models/huggingface/modules/transformers_modules/stabilityai/stablelm-3b-4e1t/c6554ba60f40a8252d2a43e38e55ee2e3a645813/modeling_stablelm_epoch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/models/huggingface/modules/transformers_modules/stabilityai/stablelm-3b-4e1t/c6554ba60f40a8252d2a43e38e55ee2e3a645813/modeling_stablelm_epoch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 20.99 GiB is allocated by PyTorch, and 13.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(4, 4096)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0574275-ac61-4370-840f-8061454f1ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:39:49.355571Z",
     "iopub.status.busy": "2023-11-29T23:39:49.354598Z",
     "iopub.status.idle": "2023-11-29T23:39:49.632502Z",
     "shell.execute_reply": "2023-11-29T23:39:49.632061Z",
     "shell.execute_reply.started": "2023-11-29T23:39:49.355540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8519680, 11880278016)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6972b666-9b87-4d7f-a49f-32de8be0e306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:42:42.033544Z",
     "iopub.status.busy": "2023-11-29T23:42:42.032614Z",
     "iopub.status.idle": "2023-11-29T23:42:42.091806Z",
     "shell.execute_reply": "2023-11-29T23:42:42.091390Z",
     "shell.execute_reply.started": "2023-11-29T23:42:42.033508Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:05:26 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 551 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:       13        0 MB (  0.00%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     971  29,609  31,785 MB   3.06% \n",
      "GPU:   1,404  23,159  24,564 MB   5.72% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e583cd32-3b5e-47bc-a7be-0678447f84a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:22:19.181710Z",
     "iopub.status.busy": "2023-11-27T21:22:19.181264Z",
     "iopub.status.idle": "2023-11-27T21:22:19.206494Z",
     "shell.execute_reply": "2023-11-27T21:22:19.206079Z",
     "shell.execute_reply.started": "2023-11-27T21:22:19.181682Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "StableLMEpochForCausalLM\n",
      "> submodules\n",
      "- model: StableLMEpochModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#StableLMEpochModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [50304, 2560] (245.6 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X DecoderLayer\n",
      "      ---------------------\n",
      "      0..31#DecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: Attention\n",
      "      - mlp: MLP\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "        ---------------------\n",
      "        self_attn#Attention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: RotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#RotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [10] (0.0 MB)\n",
      "          - cos_cached: bfloat16 [1, 1, 4096, 20] (0.2 MB)\n",
      "          - sin_cached: bfloat16 [1, 1, 4096, 20] (0.2 MB)\n",
      "        ---------------------\n",
      "        mlp#MLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLU\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [6912, 2560] (33.8 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [6912, 2560] (33.8 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 6912] (33.8 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLU\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [2560] (0.0 MB)\n",
      "        - bias: bfloat16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [2560] (0.0 MB)\n",
      "        - bias: bfloat16 [2560] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [2560] (0.0 MB)\n",
      "    - bias: bfloat16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [50304, 2560] (245.6 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "StableLMEpochForCausalLM.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        output_attentions = (\n",
      "            output_attentions\n",
      "            if output_attentions is not None\n",
      "            else self.config.output_attentions\n",
      "        )\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states\n",
      "            if output_hidden_states is not None\n",
      "            else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = (\n",
      "            return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        )\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        logits = self.lm_head(hidden_states).float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "StableLMEpochModel.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # Retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\n",
      "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n",
      "            )\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n",
      "            )\n",
      "\n",
      "        seq_length_with_past = seq_length\n",
      "        past_key_values_length = 0\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length,\n",
      "                seq_length + past_key_values_length,\n",
      "                dtype=torch.long,\n",
      "                device=device,\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        else:\n",
      "            position_ids = position_ids.view(-1, seq_length).long()\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "        # Embed positions\n",
      "        if attention_mask is None:\n",
      "            attention_mask = torch.ones(\n",
      "                (batch_size, seq_length_with_past),\n",
      "                dtype=torch.bool,\n",
      "                device=inputs_embeds.device,\n",
      "            )\n",
      "        attention_mask = self._prepare_decoder_attention_mask(\n",
      "            attention_mask,\n",
      "            (batch_size, seq_length),\n",
      "            inputs_embeds,\n",
      "            past_key_values_length,\n",
      "        )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # Decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = (\n",
      "                past_key_values[idx] if past_key_values is not None else None\n",
      "            )\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "\n",
      "                def create_custom_forward(module):\n",
      "                    def custom_forward(*inputs):\n",
      "                        # None for past_key_value\n",
      "                        return module(*inputs, past_key_value, output_attentions)\n",
      "\n",
      "                    return custom_forward\n",
      "\n",
      "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
      "                    create_custom_forward(decoder_layer),\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # Add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(\n",
      "                v\n",
      "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n",
      "                if v is not None\n",
      "            )\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "DecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "Attention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        query_rot = query_states[..., : self.rotary_ndims]\n",
      "        query_pass = query_states[..., self.rotary_ndims :]\n",
      "        key_rot = key_states[..., : self.rotary_ndims]\n",
      "        key_pass = key_states[..., self.rotary_ndims :]\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "\n",
      "        # [batch_size, num_heads, seq_len, head_dim]\n",
      "        query_states = torch.cat((query_states, query_pass), dim=-1)\n",
      "        key_states = torch.cat((key_states, key_pass), dim=-1)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # Reuse k, v, self_attention\n",
      "            key_states = torch.cat((past_key_value[0], key_states), dim=2)\n",
      "            value_states = torch.cat((past_key_value[1], value_states), dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        # Repeat k/v heads if n_kv_heads < n_heads\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # Upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        # Merge heads\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        # Final linear projection\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "RotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None):\n",
      "        # x: [batch_size, num_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=torch.get_default_dtype())\n",
      "        return (\n",
      "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "---------------------\n",
      "SiLU.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa330f-5df8-4ab8-8c68-cbb5faf8b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bcae39-2ec1-4674-9872-bd07b944499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:28.932661Z",
     "iopub.status.busy": "2023-11-22T23:14:28.931840Z",
     "iopub.status.idle": "2023-11-22T23:14:28.935824Z",
     "shell.execute_reply": "2023-11-22T23:14:28.935080Z",
     "shell.execute_reply.started": "2023-11-22T23:14:28.932634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs['logits']\n",
    "past_key_values = outputs['past_key_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e6b133-7b78-48ac-ba06-938daf8a44f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:59.065051Z",
     "iopub.status.busy": "2023-11-22T23:14:59.064635Z",
     "iopub.status.idle": "2023-11-22T23:14:59.067809Z",
     "shell.execute_reply": "2023-11-22T23:14:59.067428Z",
     "shell.execute_reply.started": "2023-11-22T23:14:59.065034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 50432]), torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size(),logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915d0c6-9bae-47eb-9712-44a9de6892e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:12:26.134495Z",
     "iopub.status.busy": "2023-11-22T23:12:26.133905Z",
     "iopub.status.idle": "2023-11-22T23:12:26.141171Z",
     "shell.execute_reply": "2023-11-22T23:12:26.140688Z",
     "shell.execute_reply.started": "2023-11-22T23:12:26.134464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_key_values), past_key_values[0][0].size(), past_key_values[0][0].dtype, past_key_values[0][1].size(), past_key_values[0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d13c74a-e0c1-449e-b2fa-59943661c78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:56.688325Z",
     "iopub.status.busy": "2023-11-20T21:23:56.687590Z",
     "iopub.status.idle": "2023-11-20T21:23:57.046020Z",
     "shell.execute_reply": "2023-11-20T21:23:57.045602Z",
     "shell.execute_reply.started": "2023-11-20T21:23:56.688294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291d9889-5bb6-4635-8ec1-17dfd0439542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:57.100183Z",
     "iopub.status.busy": "2023-11-20T21:23:57.099421Z",
     "iopub.status.idle": "2023-11-20T21:23:57.109529Z",
     "shell.execute_reply": "2023-11-20T21:23:57.109088Z",
     "shell.execute_reply.started": "2023-11-20T21:23:57.100149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer([\"un test\",\"un deuxième test\"], padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf3dd39-1341-44a7-9119-85b4d1298d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:24:01.082171Z",
     "iopub.status.busy": "2023-11-20T21:24:01.081390Z",
     "iopub.status.idle": "2023-11-20T21:24:01.087945Z",
     "shell.execute_reply": "2023-11-20T21:24:01.087300Z",
     "shell.execute_reply.started": "2023-11-20T21:24:01.082139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  328,  1071,     0,     0,     0],\n",
       "        [  328, 23156,    74, 22722,  1071]]), 'attention_mask': tensor([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fe31a0-9b15-421f-b069-e67546d7ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:32:12.116422Z",
     "iopub.status.busy": "2023-11-20T21:32:12.116190Z",
     "iopub.status.idle": "2023-11-20T21:32:12.120878Z",
     "shell.execute_reply": "2023-11-20T21:32:12.120463Z",
     "shell.execute_reply.started": "2023-11-20T21:32:12.116408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"input_ids\"].size(), encodings[\"input_ids\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef91e00-7d3c-452f-9d93-ec2d707215d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:34:04.936740Z",
     "iopub.status.busy": "2023-11-20T21:34:04.936503Z",
     "iopub.status.idle": "2023-11-20T21:34:04.939242Z",
     "shell.execute_reply": "2023-11-20T21:34:04.938877Z",
     "shell.execute_reply.started": "2023-11-20T21:34:04.936731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"attention_mask\"].size(), encodings[\"attention_mask\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca42a0a-eabb-46a6-aa77-4e925029d168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:22.386686Z",
     "iopub.status.busy": "2023-11-20T21:41:22.385561Z",
     "iopub.status.idle": "2023-11-20T21:41:22.393072Z",
     "shell.execute_reply": "2023-11-20T21:41:22.392631Z",
     "shell.execute_reply.started": "2023-11-20T21:41:22.386648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 50432]), torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"logits\"].size(), output[\"logits\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e887edcc-6554-466d-970e-9a800e7193fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:02.274392Z",
     "iopub.status.busy": "2023-11-20T21:41:02.273974Z",
     "iopub.status.idle": "2023-11-20T21:41:02.281515Z",
     "shell.execute_reply": "2023-11-20T21:41:02.280358Z",
     "shell.execute_reply.started": "2023-11-20T21:41:02.274364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172ffae5-19b4-42e4-a072-57882f130641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:46:28.031823Z",
     "iopub.status.busy": "2023-11-20T21:46:28.031023Z",
     "iopub.status.idle": "2023-11-20T21:46:28.037420Z",
     "shell.execute_reply": "2023-11-20T21:46:28.036755Z",
     "shell.execute_reply.started": "2023-11-20T21:46:28.031790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, torch.Size([2, 32, 5, 80]), torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"past_key_values\"]),output[\"past_key_values\"][0][1].size(),output[\"past_key_values\"][0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67521338-e1f2-4ed2-bb0e-a7d4f77aef74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3025cf6-e7ed-4e21-929a-68809d3ba753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Falcon-7B\n",
    "\n",
    "tiiuae/falcon-7b\n",
    "\n",
    "https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25cd17d-c436-4668-a2c8-055718e0c9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:18:28.584083Z",
     "iopub.status.busy": "2023-12-01T22:18:28.583608Z",
     "iopub.status.idle": "2023-12-01T22:33:00.935728Z",
     "shell.execute_reply": "2023-12-01T22:33:00.934198Z",
     "shell.execute_reply.started": "2023-12-01T22:18:28.584050Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     556  29,651  31,785 MB   1.75% \n",
      "GPU:   1,395  23,168  24,564 MB   5.68% \n",
      "\n",
      "\n",
      "Loading model tiiuae/falcon-7b in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ba02dc34e9451e81173fb58ddae878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc31d3ebc3d4d3ba2ae78bd580e6437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcccc99c9a94fafaf8da7a5e1582035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec3c3a819e45e7bd3425455c744024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c847f873fe5446c1a112fe79b92a5267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7aa8a6f9d9448c81cda3d128cbff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 13.45 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:14:32 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 105 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -408        0 MB (  0.09%)\n",
      "GPU:      -16        0 MB ( -0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     148  30,360  31,785 MB   0.47% \n",
      "GPU:   1,379  23,184  24,564 MB   5.61% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"tiiuae/falcon-7b\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2d554-d0d7-4628-ad06-c752df5a4a9c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49eb7f2-bbcf-482d-b6d0-c68d9ca0bf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:29.283823Z",
     "iopub.status.busy": "2023-12-01T22:48:29.283224Z",
     "iopub.status.idle": "2023-12-01T22:48:29.349883Z",
     "shell.execute_reply": "2023-12-01T22:48:29.349297Z",
     "shell.execute_reply.started": "2023-12-01T22:48:29.283800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     969  29,606  31,785 MB   3.05% \n",
      "GPU:   1,906  22,657  24,564 MB   7.76% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d6f958-0b26-4386-9d07-905c2713e2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:30.261978Z",
     "iopub.status.busy": "2023-12-01T22:48:30.260834Z",
     "iopub.status.idle": "2023-12-01T22:48:35.367687Z",
     "shell.execute_reply": "2023-12-01T22:48:35.367270Z",
     "shell.execute_reply.started": "2023-12-01T22:48:30.261935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c5e5c2474d482f8243265d44dd91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 157.58 ms\n",
      "Tokenizer CPU memory: 28.09 MB\n",
      "\n",
      "Model load time : 4941.91 ms\n",
      "Model CPU memory: 0.01 GB\n",
      "Model GPU memory: 12.94 GB\n",
      "Max   GPU memory: 13.50 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bf5255-83c4-4bf8-8f6d-4cd52367b34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:47:00.107180Z",
     "iopub.status.busy": "2023-12-01T22:47:00.106701Z",
     "iopub.status.idle": "2023-12-01T22:47:14.265789Z",
     "shell.execute_reply": "2023-12-01T22:47:14.265044Z",
     "shell.execute_reply.started": "2023-12-01T22:47:00.107163Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 6 and sequence length 2048:\n",
      "transformer.word_embeddings;True;;222.1;;int64[6, 2048];0.1;0.1;106.6;106.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.input_layernorm;True;;178.6;;bfloat16[6, 2048, 4544];106.5;154.6;261.2;261.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.query_key_value;True;;206.6;;bfloat16[6, 2048, 4544];106.5;261.1;370.6;370.6;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.maybe_rotary;True;;407.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;480.1;1016.1;588.1;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.dense;True;;253.6;;bfloat16[6, 2048, 4544];106.5;586.6;693.1;693.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention;False;;99356.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;693.1;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.dense_h_to_4h;True;;129.1;;bfloat16[6, 2048, 4544];106.5;367.6;793.6;793.6;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.act;True;;114.9;;bfloat16[6, 2048, 18176];426.0;793.6;1219.6;1219.6;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.dense_4h_to_h;True;;110.5;;bfloat16[6, 2048, 18176];426.0;793.6;900.1;900.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp;False;;809.3;;bfloat16[6, 2048, 4544];106.5;367.6;900.1;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0;False;;100925.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;154.6;900.1;261.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.input_layernorm;True;;136.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.query_key_value;True;;111.2;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.maybe_rotary;True;;665.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.dense;True;;284.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention;False;;737624.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.dense_h_to_4h;True;;161.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.act;True;;168.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.dense_4h_to_h;True;;131.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp;False;;1130.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1;False;;739440.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.input_layernorm;True;;140.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.query_key_value;True;;272.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.maybe_rotary;True;;498.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.dense;True;;179.7;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention;False;;356670.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.dense_h_to_4h;True;;123.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.act;True;;119.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.dense_4h_to_h;True;;126.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp;False;;923.3;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2;False;;358189.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.input_layernorm;True;;134.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.query_key_value;True;;118.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.maybe_rotary;True;;688.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.dense;True;;167.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention;False;;358679.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.dense_h_to_4h;True;;121.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.act;True;;121.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.dense_4h_to_h;True;;125.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp;False;;1038.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3;False;;360299.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.input_layernorm;True;;135.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.query_key_value;True;;120.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.maybe_rotary;True;;674.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.dense;True;;449.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention;False;;358821.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.dense_h_to_4h;True;;163.2;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.act;True;;117.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.dense_4h_to_h;True;;118.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp;False;;766.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4;False;;360149.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.input_layernorm;True;;251.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.query_key_value;True;;235.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.maybe_rotary;True;;511.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.dense;True;;257.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention;False;;357363.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.dense_h_to_4h;True;;203.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.act;True;;169.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.dense_4h_to_h;True;;149.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp;False;;912.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5;False;;359411.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.input_layernorm;True;;311.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.query_key_value;True;;116.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.maybe_rotary;True;;573.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.dense;True;;167.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention;False;;358488.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.dense_h_to_4h;True;;358.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.act;True;;184.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.dense_4h_to_h;True;;164.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp;False;;1109.0;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6;False;;360435.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.input_layernorm;True;;171.2;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.query_key_value;True;;119.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.maybe_rotary;True;;608.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.dense;True;;204.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention;False;;358691.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.dense_h_to_4h;True;;127.2;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.act;True;;268.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.dense_4h_to_h;True;;115.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp;False;;895.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7;False;;360196.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.input_layernorm;True;;146.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.query_key_value;True;;116.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.maybe_rotary;True;;556.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.dense;True;;231.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention;False;;358691.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.dense_h_to_4h;True;;193.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.act;True;;129.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.dense_4h_to_h;True;;326.5;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp;False;;1154.4;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8;False;;360450.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.input_layernorm;True;;140.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.query_key_value;True;;118.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.maybe_rotary;True;;537.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.dense;True;;192.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention;False;;358632.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.dense_h_to_4h;True;;113.3;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.act;True;;106.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.dense_4h_to_h;True;;122.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp;False;;722.6;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9;False;;360076.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.input_layernorm;True;;118.6;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.query_key_value;True;;131.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.maybe_rotary;True;;571.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.dense;True;;246.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention;False;;357819.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.dense_h_to_4h;True;;147.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.act;True;;129.5;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.dense_4h_to_h;True;;147.1;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp;False;;835.3;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10;False;;359200.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.input_layernorm;True;;143.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.query_key_value;True;;132.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.maybe_rotary;True;;679.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.dense;True;;183.2;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention;False;;359142.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.dense_h_to_4h;True;;215.4;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.act;True;;154.3;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.dense_4h_to_h;True;;120.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp;False;;1088.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11;False;;361003.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.input_layernorm;True;;138.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.query_key_value;True;;119.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.maybe_rotary;True;;726.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.dense;True;;195.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention;False;;358382.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.dense_h_to_4h;True;;374.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.act;True;;153.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.dense_4h_to_h;True;;127.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp;False;;1097.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12;False;;360219.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.input_layernorm;True;;142.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.query_key_value;True;;140.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.maybe_rotary;True;;436.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.dense;True;;173.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention;False;;358183.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.dense_h_to_4h;True;;117.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.act;True;;263.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.dense_4h_to_h;True;;109.0;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp;False;;842.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13;False;;359602.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.input_layernorm;True;;121.4;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.query_key_value;True;;110.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.maybe_rotary;True;;474.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.dense;True;;165.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention;False;;358976.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.dense_h_to_4h;True;;120.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.act;True;;115.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.dense_4h_to_h;True;;295.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp;False;;917.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14;False;;360421.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.input_layernorm;True;;165.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.query_key_value;True;;118.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.maybe_rotary;True;;472.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.dense;True;;179.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention;False;;359023.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.dense_h_to_4h;True;;158.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.act;True;;119.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.dense_4h_to_h;True;;120.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp;False;;792.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15;False;;360556.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.input_layernorm;True;;129.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.query_key_value;True;;115.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.maybe_rotary;True;;513.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.dense;True;;176.5;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention;False;;359326.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.dense_h_to_4h;True;;125.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.act;True;;108.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.dense_4h_to_h;True;;109.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp;False;;694.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16;False;;360569.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.input_layernorm;True;;133.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.query_key_value;True;;119.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.maybe_rotary;True;;627.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.dense;True;;239.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention;False;;357194.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.dense_h_to_4h;True;;140.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.act;True;;121.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.dense_4h_to_h;True;;122.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp;False;;1026.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17;False;;358972.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.input_layernorm;True;;169.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.query_key_value;True;;143.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.maybe_rotary;True;;644.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.dense;True;;255.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention;False;;353644.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.dense_h_to_4h;True;;130.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.act;True;;117.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.dense_4h_to_h;True;;128.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp;False;;1058.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18;False;;355437.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.input_layernorm;True;;128.4;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.query_key_value;True;;322.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.maybe_rotary;True;;534.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.dense;True;;194.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention;False;;353359.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.dense_h_to_4h;True;;137.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.act;True;;130.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.dense_4h_to_h;True;;122.3;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp;False;;962.0;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19;False;;354908.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.input_layernorm;True;;135.2;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.query_key_value;True;;136.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.maybe_rotary;True;;951.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.dense;True;;188.3;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention;False;;354370.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.dense_h_to_4h;True;;169.4;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.act;True;;168.4;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.dense_4h_to_h;True;;253.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp;False;;1204.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20;False;;356174.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.input_layernorm;True;;129.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.query_key_value;True;;202.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.maybe_rotary;True;;576.0;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.dense;True;;473.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention;False;;353574.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.dense_h_to_4h;True;;126.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.act;True;;164.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.dense_4h_to_h;True;;148.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp;False;;845.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21;False;;354982.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.input_layernorm;True;;162.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.query_key_value;True;;150.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.maybe_rotary;True;;502.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.dense;True;;201.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention;False;;353301.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.dense_h_to_4h;True;;144.3;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.act;True;;124.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.dense_4h_to_h;True;;119.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp;False;;790.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22;False;;354975.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.input_layernorm;True;;320.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.query_key_value;True;;152.2;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.maybe_rotary;True;;662.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.dense;True;;230.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention;False;;353630.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.dense_h_to_4h;True;;461.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.act;True;;188.7;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.dense_4h_to_h;True;;124.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp;False;;1227.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23;False;;355654.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.input_layernorm;True;;134.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.query_key_value;True;;170.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.maybe_rotary;True;;520.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.dense;True;;185.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention;False;;352807.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.dense_h_to_4h;True;;138.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.act;True;;126.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.dense_4h_to_h;True;;122.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp;False;;963.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24;False;;354358.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.input_layernorm;True;;131.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.query_key_value;True;;292.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.maybe_rotary;True;;642.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.dense;True;;189.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention;False;;353775.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.dense_h_to_4h;True;;116.0;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.act;True;;127.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.dense_4h_to_h;True;;120.3;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp;False;;883.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25;False;;355231.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.input_layernorm;True;;120.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.query_key_value;True;;112.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.maybe_rotary;True;;963.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.dense;True;;165.3;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention;False;;354135.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.dense_h_to_4h;True;;172.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.act;True;;112.7;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.dense_4h_to_h;True;;152.2;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp;False;;1047.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26;False;;355727.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.input_layernorm;True;;165.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.query_key_value;True;;115.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.maybe_rotary;True;;640.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.dense;True;;535.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention;False;;354158.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.dense_h_to_4h;True;;222.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.act;True;;158.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.dense_4h_to_h;True;;130.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp;False;;954.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27;False;;355794.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.input_layernorm;True;;142.6;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.query_key_value;True;;188.0;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.maybe_rotary;True;;535.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.dense;True;;202.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention;False;;353124.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.dense_h_to_4h;True;;154.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.act;True;;161.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.dense_4h_to_h;True;;133.0;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp;False;;912.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28;False;;354834.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.input_layernorm;True;;291.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.query_key_value;True;;114.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.maybe_rotary;True;;542.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.dense;True;;182.2;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention;False;;353142.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.dense_h_to_4h;True;;330.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.act;True;;121.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.dense_4h_to_h;True;;143.2;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp;False;;1034.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29;False;;354932.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.input_layernorm;True;;138.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.query_key_value;True;;137.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.maybe_rotary;True;;502.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.dense;True;;195.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention;False;;353290.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.dense_h_to_4h;True;;141.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.act;True;;297.4;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.dense_4h_to_h;True;;119.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp;False;;960.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30;False;;354913.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.input_layernorm;True;;133.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.query_key_value;True;;240.0;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.maybe_rotary;True;;659.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.dense;True;;222.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention;False;;353482.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.dense_h_to_4h;True;;190.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.act;True;;124.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.dense_4h_to_h;True;;281.1;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp;False;;1003.4;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31;False;;355158.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.ln_f;True;;190.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer;False;;11578242.6;;int64[6, 2048]float32[6, 2048];0.1;0.1;367.7;106.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "lm_head;True;;345.3;;bfloat16[6, 2048, 4544];106.5;106.6;1630.6;1630.6;bfloat16[6, 2048, 65024];1524.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "<model>;False;;11579376.7;;int64[6, 2048]float32[6, 2048];0.1;0.1;1630.6;1524.1;bfloat16[6, 2048, 65024];1524.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(6, 2048)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502ae39c-f50f-4e84-8e8e-f400888731b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:17.936510Z",
     "iopub.status.busy": "2023-12-01T22:48:17.935101Z",
     "iopub.status.idle": "2023-12-01T22:48:18.191556Z",
     "shell.execute_reply": "2023-12-01T22:48:18.191105Z",
     "shell.execute_reply.started": "2023-12-01T22:48:17.936461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8519680, 17229268992)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3e674d-af67-42f5-803e-e850768437a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:22.153377Z",
     "iopub.status.busy": "2023-12-01T22:48:22.152979Z",
     "iopub.status.idle": "2023-12-01T22:48:22.213279Z",
     "shell.execute_reply": "2023-12-01T22:48:22.212691Z",
     "shell.execute_reply.started": "2023-12-01T22:48:22.153364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:01:46 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 2714 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      417        0 MB (  0.00%)\n",
      "GPU:      578        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     969  29,606  31,785 MB   3.05% \n",
      "GPU:   1,906  22,657  24,564 MB   7.76% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929970ff-2c7c-422b-a708-f0133021e035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:40.028266Z",
     "iopub.status.busy": "2023-12-01T22:48:40.027866Z",
     "iopub.status.idle": "2023-12-01T22:48:40.066791Z",
     "shell.execute_reply": "2023-12-01T22:48:40.066310Z",
     "shell.execute_reply.started": "2023-12-01T22:48:40.028244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "FalconForCausalLM\n",
      "> submodules\n",
      "- transformer: FalconModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  transformer#FalconModel\n",
      "  > submodules\n",
      "  - word_embeddings: Embedding\n",
      "  - h: ModuleList\n",
      "  - ln_f: LayerNorm\n",
      "    ---------------------\n",
      "    word_embeddings#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [65024, 4544] (563.6 MB)\n",
      "    ---------------------\n",
      "    h#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X FalconDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#FalconDecoderLayer\n",
      "      > submodules\n",
      "      - self_attention: FalconAttention\n",
      "      - mlp: FalconMLP\n",
      "      - input_layernorm: LayerNorm\n",
      "        ---------------------\n",
      "        self_attention#FalconAttention\n",
      "        > submodules\n",
      "        - maybe_rotary: FalconRotaryEmbedding\n",
      "        - query_key_value: FalconLinear\n",
      "        - dense: FalconLinear\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          maybe_rotary#FalconRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [32] (0.0 MB)\n",
      "          ---------------------\n",
      "          query_key_value#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4672, 4544] (40.5 MB)\n",
      "          ---------------------\n",
      "          dense#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4544, 4544] (39.4 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#FalconMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: FalconLinear\n",
      "        - act: GELU\n",
      "        - dense_4h_to_h: FalconLinear\n",
      "          ---------------------\n",
      "          dense_h_to_4h#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [18176, 4544] (157.5 MB)\n",
      "          ---------------------\n",
      "          act#GELU\n",
      "          ---------------------\n",
      "          dense_4h_to_h#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4544, 18176] (157.5 MB)\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4544] (0.0 MB)\n",
      "        - bias: bfloat16 [4544] (0.0 MB)\n",
      "    ---------------------\n",
      "    ln_f#LayerNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [4544] (0.0 MB)\n",
      "    - bias: bfloat16 [4544] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [65024, 4544] (563.6 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "FalconForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=CausalLMOutputWithCrossAttentions,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        inputs_embeds: Optional[torch.Tensor] = None,\n",
      "        labels: Optional[torch.Tensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
      "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
      "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
      "        \"\"\"\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        transformer_outputs = self.transformer(\n",
      "            input_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "        hidden_states = transformer_outputs[0]\n",
      "\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            batch_size, seq_length, vocab_size = shift_logits.shape\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            loss = loss_fct(\n",
      "                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n",
      "            )\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + transformer_outputs[1:]\n",
      "            return ((loss,) + output) if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithCrossAttentions(\n",
      "            loss=loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=transformer_outputs.past_key_values,\n",
      "            hidden_states=transformer_outputs.hidden_states,\n",
      "            attentions=transformer_outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "FalconModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_key_values = tuple([None] * len(self.h))\n",
      "        else:\n",
      "            past_key_values = self._convert_to_rw_cache(past_key_values)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape batch_size x num_heads x N x N\n",
      "        # head_mask has shape n_layer x batch x num_heads x N x N\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.word_embeddings(input_ids)\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "        presents = () if use_cache else None\n",
      "        all_self_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "\n",
      "        # Compute alibi tensor: check build_alibi_tensor documentation\n",
      "        past_key_values_length = 0\n",
      "        if past_key_values[0] is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[1]  # 1 because RW-cache, not standard format\n",
      "\n",
      "        if self.use_alibi:\n",
      "            mask = (\n",
      "                torch.ones(\n",
      "                    (batch_size, seq_length + past_key_values_length), device=inputs_embeds.device, dtype=torch.long\n",
      "                )\n",
      "                if attention_mask is None\n",
      "                else attention_mask\n",
      "            )\n",
      "            alibi = build_alibi_tensor(mask, self.num_heads, dtype=hidden_states.dtype)\n",
      "        else:\n",
      "            alibi = None\n",
      "            if position_ids is None:\n",
      "                device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "                position_ids = torch.arange(\n",
      "                    past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "                )\n",
      "                position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
      "            )\n",
      "\n",
      "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    block.__call__,\n",
      "                    hidden_states,\n",
      "                    alibi,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    layer_past,\n",
      "                    use_cache,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = block(\n",
      "                    hidden_states,\n",
      "                    layer_past=layer_past,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                    alibi=alibi,\n",
      "                )\n",
      "\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        # Add last hidden state\n",
      "        hidden_states = self.ln_f(hidden_states)\n",
      "\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if presents is not None:\n",
      "            presents = self._convert_cache_to_standard_format(presents, batch_size)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPastAndCrossAttentions(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "FalconDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        alibi: Optional[torch.Tensor],\n",
      "        attention_mask: torch.Tensor,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        use_cache: bool = False,\n",
      "        output_attentions: bool = False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        if self.config.new_decoder_architecture:\n",
      "            attention_layernorm_out = self.ln_attn(hidden_states)\n",
      "            mlp_layernorm_out = self.ln_mlp(hidden_states)\n",
      "        else:\n",
      "            attention_layernorm_out = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self attention.\n",
      "        attn_outputs = self.self_attention(\n",
      "            attention_layernorm_out,\n",
      "            layer_past=layer_past,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            alibi=alibi,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attention_output = attn_outputs[0]\n",
      "\n",
      "        if not self.config.new_decoder_architecture:\n",
      "            if self.config.parallel_attn:\n",
      "                mlp_layernorm_out = attention_layernorm_out\n",
      "            else:\n",
      "                residual = dropout_add(\n",
      "                    attention_output, residual, self.config.attention_dropout, training=self.training\n",
      "                )\n",
      "                mlp_layernorm_out = self.post_attention_layernorm(residual)\n",
      "\n",
      "        outputs = attn_outputs[1:]\n",
      "\n",
      "        # MLP.\n",
      "        mlp_output = self.mlp(mlp_layernorm_out)\n",
      "\n",
      "        if self.config.new_decoder_architecture or self.config.parallel_attn:\n",
      "            mlp_output += attention_output\n",
      "\n",
      "        output = dropout_add(mlp_output, residual, self.config.hidden_dropout, training=self.training)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (output,) + outputs\n",
      "        else:\n",
      "            outputs = (output,) + outputs[1:]\n",
      "\n",
      "        return outputs  # hidden_states, present, attentions\n",
      "\n",
      "---------------------\n",
      "FalconAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        alibi: Optional[torch.Tensor],\n",
      "        attention_mask: torch.Tensor,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        use_cache: bool = False,\n",
      "        output_attentions: bool = False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
      "        num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n",
      "        # 3 x [batch_size, seq_length, num_heads, head_dim]\n",
      "        (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n",
      "\n",
      "        batch_size, query_length, _, _ = query_layer.shape\n",
      "\n",
      "        query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n",
      "        key_layer = key_layer.transpose(1, 2).reshape(\n",
      "            batch_size * num_kv_heads,\n",
      "            query_length,\n",
      "            self.head_dim,\n",
      "        )\n",
      "        value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n",
      "\n",
      "        past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n",
      "        query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, past_kv_length, position_ids)\n",
      "\n",
      "        if layer_past is not None:\n",
      "            past_key, past_value = layer_past\n",
      "            # concatenate along seq_length dimension:\n",
      "            #  - key: [batch_size * self.num_heads, kv_length, head_dim]\n",
      "            #  - value: [batch_size * self.num_heads, kv_length, head_dim]\n",
      "            key_layer = torch.cat((past_key, key_layer), dim=1)\n",
      "            value_layer = torch.cat((past_value, value_layer), dim=1)\n",
      "\n",
      "        _, kv_length, _ = key_layer.shape\n",
      "        if use_cache:\n",
      "            present = (key_layer, value_layer)\n",
      "        else:\n",
      "            present = None\n",
      "\n",
      "        query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n",
      "        key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n",
      "        value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n",
      "\n",
      "        if alibi is None:\n",
      "            if hasattr(F, \"scaled_dot_product_attention\") and not output_attentions:\n",
      "                # TODO: deprecate this once we add FA2 support in Falcon\n",
      "                logger.warning_once(\n",
      "                    \"The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the\"\n",
      "                    \" future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call \"\n",
      "                    \"`model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\"\n",
      "                )\n",
      "\n",
      "                attn_output = F.scaled_dot_product_attention(\n",
      "                    query_layer_, key_layer_, value_layer_, attention_mask, 0.0, is_causal=False\n",
      "                )\n",
      "                attention_scores = None\n",
      "            else:\n",
      "                attention_scores = query_layer_ @ key_layer_.transpose(-1, -2)\n",
      "                attention_scores /= math.sqrt(self.head_dim)\n",
      "\n",
      "                attention_scores = F.softmax(attention_scores + attention_mask, dim=-1, dtype=hidden_states.dtype)\n",
      "                attn_output = attention_scores @ value_layer_\n",
      "\n",
      "            attn_output = attn_output.view(batch_size, self.num_heads, query_length, self.head_dim)\n",
      "            attn_output = attn_output.permute(0, 2, 1, 3)\n",
      "            attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n",
      "\n",
      "            output_tensor = self.dense(attn_output)\n",
      "\n",
      "            if output_attentions:\n",
      "                return output_tensor, present, attention_scores\n",
      "            else:\n",
      "                return output_tensor, present\n",
      "\n",
      "        else:\n",
      "            matmul_result = query_layer_ @ key_layer_.transpose(-1, -2)\n",
      "\n",
      "            # change view to [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_scores = matmul_result.view(batch_size, self.num_heads, query_length, kv_length)\n",
      "\n",
      "            # cast attention scores to fp32, compute scaled softmax and cast back to initial dtype - [batch_size, num_heads, q_length, kv_length]\n",
      "            input_dtype = attention_scores.dtype\n",
      "            # `float16` has a minimum value of -65504.0, whereas `bfloat16` and `float32` have a minimum value of `-3.4e+38`\n",
      "            if input_dtype == torch.float16 or input_dtype == torch.bfloat16:\n",
      "                attention_scores = attention_scores.to(torch.float32)\n",
      "            # Matt (HF) note: We could possibly use F.scaled_dot_product_attention here too, by\n",
      "            # adding (alibi * self.inv_norm_factor) to attention_mask. I think this would be mathematically\n",
      "            # equivalent and more performant, but there might be a numerical difference. If you're reading this\n",
      "            # and you'd like to experiment and maybe file a PR, feel free!\n",
      "            attention_logits = attention_scores + alibi.view(batch_size, self.num_heads, 1, -1)\n",
      "            attention_logits *= self.inv_norm_factor\n",
      "            attention_probs = F.softmax(attention_logits + attention_mask, dim=-1, dtype=hidden_states.dtype)\n",
      "            # [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_probs = self.attention_dropout(attention_probs)\n",
      "\n",
      "            if head_mask is not None:\n",
      "                attention_probs = attention_probs * head_mask\n",
      "\n",
      "            # change view [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_probs_reshaped = attention_probs.view(batch_size, self.num_heads, query_length, kv_length)\n",
      "\n",
      "            # matmul: [batch_size * num_heads, q_length, head_dim]\n",
      "            context_layer = (attention_probs_reshaped @ value_layer_).flatten(0, 1)\n",
      "\n",
      "            # change view [batch_size, q_length, num_heads * head_dim]\n",
      "            context_layer = self._merge_heads(context_layer)\n",
      "\n",
      "            output_tensor = self.dense(context_layer)\n",
      "\n",
      "            if output_attentions:\n",
      "                return output_tensor, present, attention_probs\n",
      "            else:\n",
      "                return output_tensor, present\n",
      "\n",
      "---------------------\n",
      "FalconRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, query, key, past_key_values_length, position_ids):\n",
      "        _, seq_len, _ = query.shape\n",
      "        cos, sin = self.cos_sin(seq_len, past_key_values_length, position_ids, query.device, query.dtype)\n",
      "        # Query and key's shapes are [bs * num_heads, seq_len, dim], might need manual expansion. Ifs and elses used to\n",
      "        # avoid unnecessary repeat_interleave operations.\n",
      "        query_expansion_factor = int(query.shape[0] / cos.shape[0])\n",
      "        if query_expansion_factor > 1:\n",
      "            query_cos = torch.repeat_interleave(cos, query_expansion_factor, dim=0)\n",
      "            query_sin = torch.repeat_interleave(sin, query_expansion_factor, dim=0)\n",
      "        else:\n",
      "            query_cos, query_sin = cos, sin\n",
      "\n",
      "        key_expansion_factor = int(key.shape[0] / cos.shape[0])\n",
      "        if key_expansion_factor > 1:\n",
      "            if key_expansion_factor != query_expansion_factor:\n",
      "                key_cos = torch.repeat_interleave(cos, key_expansion_factor, dim=0)\n",
      "                key_sin = torch.repeat_interleave(sin, key_expansion_factor, dim=0)\n",
      "            else:\n",
      "                key_cos, key_sin = query_cos, query_sin\n",
      "        else:\n",
      "            key_cos, key_sin = cos, sin\n",
      "\n",
      "        return (query * query_cos) + (rotate_half(query) * query_sin), (key * key_cos) + (rotate_half(key) * key_sin)\n",
      "\n",
      "---------------------\n",
      "FalconLinear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
      "        hidden_states = input @ self.weight.T\n",
      "        if self.bias is None:\n",
      "            return hidden_states\n",
      "        return hidden_states + self.bias\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "FalconMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.act(self.dense_h_to_4h(x))\n",
      "        x = self.dense_4h_to_h(x)\n",
      "        return x\n",
      "\n",
      "---------------------\n",
      "GELU.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.gelu(input, approximate=self.approximate)\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d466860-fad9-44d6-8894-4a61ff1d9f3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Llama-2-7B\n",
    "\n",
    "meta-llama/Llama-2-7b-hf\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d0c1ba-2b8b-450a-b0e0-2695329f10b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:52:46.130649Z",
     "iopub.status.busy": "2023-12-01T22:52:46.129955Z",
     "iopub.status.idle": "2023-12-01T22:52:46.135177Z",
     "shell.execute_reply": "2023-12-01T22:52:46.134573Z",
     "shell.execute_reply.started": "2023-12-01T22:52:46.130619Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc44f39-4b42-42f3-bee5-1b3f544f9335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:54:49.136597Z",
     "iopub.status.busy": "2023-12-01T22:54:49.136126Z",
     "iopub.status.idle": "2023-12-01T23:09:14.106916Z",
     "shell.execute_reply": "2023-12-01T23:09:14.105899Z",
     "shell.execute_reply.started": "2023-12-01T22:54:49.136580Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     554  29,977  31,785 MB   1.74% \n",
      "GPU:   1,328  23,235  24,564 MB   5.41% \n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-2-7b-hf in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd18ab1617f948a08a52824a211c77f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b53a3bf946473b8bf2bdd49ffa0034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d2784ec2a45f7a3097ddf3ef73535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d05dc267e464cad36b327b6b526f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ace1bdb8d34d0bbba4e58eb793a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0518d53b6eb94ca5a9045061cc65c711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a6e51c27a64b65ad22236202d3a4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7bf6e4271b4b14b237277d8cdfb61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3168681d74ac4b77a74c4849a96aeba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0f43f9ff664b5598c38e28221e97cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7923c2d385964cb3ae6884eea5c81831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 12.55 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:14:24 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 105 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -344        0 MB (  0.18%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     210  30,201  31,785 MB   0.66% \n",
      "GPU:   1,328  23,235  24,564 MB   5.41% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, token=myhftoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99505cc-b7f5-4835-9a51-a229ae5d570a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4596b37f-9579-43d7-a4a7-3be9827fbf49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:36.322556Z",
     "iopub.status.busy": "2023-12-01T23:16:36.321795Z",
     "iopub.status.idle": "2023-12-01T23:16:36.382581Z",
     "shell.execute_reply": "2023-12-01T23:16:36.381987Z",
     "shell.execute_reply.started": "2023-12-01T23:16:36.322524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     739  29,709  31,785 MB   2.33% \n",
      "GPU:   1,422  23,141  24,564 MB   5.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9bcadc5-46a3-458c-a753-ba1a4bb6b9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:37.130632Z",
     "iopub.status.busy": "2023-12-01T23:16:37.129696Z",
     "iopub.status.idle": "2023-12-01T23:16:44.269397Z",
     "shell.execute_reply": "2023-12-01T23:16:44.268949Z",
     "shell.execute_reply.started": "2023-12-01T23:16:37.130577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c040b5a334dadafecadcccfd4428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 12.55 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852)\n",
      "\n",
      "Tokenizer load time : 160.45 ms\n",
      "Tokenizer CPU memory: 13.38 MB\n",
      "\n",
      "Model load time : 6971.62 ms\n",
      "Model CPU memory: 0.14 GB\n",
      "Model GPU memory: 12.61 GB\n",
      "Max   GPU memory: 12.62 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d8db7a8-092b-44ae-9337-87a60de1daf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:09.070057Z",
     "iopub.status.busy": "2023-12-01T23:17:09.069011Z",
     "iopub.status.idle": "2023-12-01T23:17:09.108852Z",
     "shell.execute_reply": "2023-12-01T23:17:09.108415Z",
     "shell.execute_reply.started": "2023-12-01T23:17:09.070027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "LlamaForCausalLM\n",
      "> submodules\n",
      "- model: LlamaModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#LlamaModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: LlamaRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [32000, 4096] (250.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X LlamaDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#LlamaDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: LlamaAttention\n",
      "      - mlp: LlamaMLP\n",
      "      - input_layernorm: LlamaRMSNorm\n",
      "      - post_attention_layernorm: LlamaRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#LlamaAttention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: LlamaRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#LlamaRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "          - cos_cached: float16 [4096, 128] (1.0 MB)\n",
      "          - sin_cached: float16 [4096, 128] (1.0 MB)\n",
      "        ---------------------\n",
      "        mlp#LlamaMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [11008, 4096] (86.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [11008, 4096] (86.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 11008] (86.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [4096] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [4096] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#LlamaRMSNorm\n",
      "    > parameters\n",
      "    - weight: float16 [4096] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [32000, 4096] (250.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "LlamaForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
      "\n",
      "        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
      "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            logits = torch.cat(logits, dim=-1)\n",
      "        else:\n",
      "            logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "LlamaModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length = inputs_embeds.shape[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        past_key_values_length = 0\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
      "            )\n",
      "\n",
      "        # embed positions\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_value,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "LlamaDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*):\n",
      "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
      "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LlamaAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
      "            query_slices = self.q_proj.weight.split(\n",
      "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
      "            )\n",
      "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
      "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
      "\n",
      "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            query_states = torch.cat(query_states, dim=-1)\n",
      "\n",
      "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            key_states = torch.cat(key_states, dim=-1)\n",
      "\n",
      "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            value_states = torch.cat(value_states, dim=-1)\n",
      "\n",
      "        else:\n",
      "            query_states = self.q_proj(hidden_states)\n",
      "            key_states = self.k_proj(hidden_states)\n",
      "            value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
      "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
      "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
      "        else:\n",
      "            attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "LlamaRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "LlamaMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            slice = self.intermediate_size // self.config.pretraining_tp\n",
      "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
      "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
      "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
      "\n",
      "            gate_proj = torch.cat(\n",
      "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
      "            )\n",
      "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
      "\n",
      "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
      "            down_proj = [\n",
      "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
      "            ]\n",
      "            down_proj = sum(down_proj)\n",
      "        else:\n",
      "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "        return down_proj\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "LlamaRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a609566c-7f04-4116-9620-4864bc520031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:09.033747Z",
     "iopub.status.busy": "2023-12-01T23:16:09.032559Z",
     "iopub.status.idle": "2023-12-01T23:16:12.686941Z",
     "shell.execute_reply": "2023-12-01T23:16:12.686415Z",
     "shell.execute_reply.started": "2023-12-01T23:16:09.033700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 6 and sequence length 2048:\n",
      "model.embed_tokens;True;;502.3;;int64[6, 2048];0.1;0.1;96.1;96.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;295.4;;float16[6, 2048, 4096];96.0;144.1;528.2;240.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;192.4;;float16[6, 2048, 4096];96.0;240.1;336.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;130.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;127.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;126.5;;float16[6, 32, 2048, 128];96.0;528.1;528.1;528.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;258.9;;float16[6, 2048, 4096];96.0;2160.1;2256.1;2256.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;10618.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;2256.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;252.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;171.7;;float16[6, 2048, 4096];96.0;336.1;594.1;594.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;274.0;;float16[6, 2048, 11008];258.0;594.1;852.1;852.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;128.0;;float16[6, 2048, 4096];96.0;594.1;852.1;852.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;181.8;;float16[6, 2048, 11008];258.0;594.1;690.1;690.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1419.0;;float16[6, 2048, 4096];96.0;336.1;690.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0;False;;13299.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;144.1;690.1;240.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;240.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;132.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;108.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;104.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;97.3;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;129.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;1605.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;193.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;114.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;101.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;105.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;112.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1036.9;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1;False;;3721.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;192.7;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;113.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;106.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;419.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;100.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;122.5;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;1727.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;188.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;112.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;106.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;107.2;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;108.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp;False;;1010.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2;False;;3595.2;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;240.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;124.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;103.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;102.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;97.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;134.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;1568.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;194.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;113.3;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;101.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;106.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;247.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1014.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3;False;;3634.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;183.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;117.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;106.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;129.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;97.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;133.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;1576.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;195.8;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;255.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;161.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;133.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;119.8;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp;False;;1316.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4;False;;3762.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;355.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;124.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;107.2;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;102.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;101.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;284.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;1615.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;193.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;128.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;103.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;191.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;120.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1178.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5;False;;3896.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;192.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;119.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;248.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;102.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;96.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;138.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1569.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;214.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;122.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;101.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;108.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;109.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp;False;;1034.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6;False;;3511.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;191.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;114.9;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;104.4;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;122.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;228.3;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;132.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1576.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;188.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;113.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;101.3;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;109.6;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;107.9;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1019.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7;False;;3611.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;187.0;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;276.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;112.6;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;103.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;101.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;134.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;1605.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;335.5;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;115.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;98.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;106.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;107.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp;False;;874.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8;False;;3638.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;180.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;114.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;108.4;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;103.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;102.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;131.2;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;1540.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;178.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;112.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;241.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;105.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;235.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp;False;;1181.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9;False;;3576.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;200.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;226.0;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;144.6;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;116.8;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;115.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;168.7;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;1993.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;191.0;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;117.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;102.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;116.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;110.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp;False;;1079.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10;False;;4149.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;195.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;116.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;108.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;254.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;98.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;163.2;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;1570.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;180.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;121.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;114.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;114.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;114.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1274.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11;False;;3699.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;178.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;121.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;113.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;106.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;99.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;124.3;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1558.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;184.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;113.5;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;97.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;110.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;253.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp;False;;1038.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12;False;;3603.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;190.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;114.5;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;103.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;104.0;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;117.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;138.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1540.2;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;194.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;261.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;98.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;105.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;120.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp;False;;1036.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13;False;;3453.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;321.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;128.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;104.9;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;100.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;99.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;152.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1612.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;183.0;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;121.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;100.3;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;254.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;109.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1033.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14;False;;3624.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;175.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;114.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;109.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;107.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;98.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;137.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;1558.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;188.6;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;129.7;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;100.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;116.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;107.6;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp;False;;926.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15;False;;3492.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;215.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;118.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;104.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;103.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;312.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;133.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;1639.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;203.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;113.5;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;101.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;111.8;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;108.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp;False;;1027.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16;False;;3565.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;177.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;257.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;102.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;110.8;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;96.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;124.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;1494.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;347.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;113.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;97.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;103.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;119.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp;False;;917.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17;False;;3568.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;174.0;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;124.0;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;109.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;104.0;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;99.8;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;121.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;1561.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;174.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;164.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;248.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;111.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;106.1;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1088.7;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18;False;;3494.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;187.7;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;119.5;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;127.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;102.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;96.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;124.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;1553.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;181.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;114.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;98.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;124.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;117.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp;False;;1064.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19;False;;3605.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;198.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;116.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;106.7;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;251.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;100.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;137.9;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;1560.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;189.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;121.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;107.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;110.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;112.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp;False;;1079.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20;False;;3507.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;187.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;129.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;105.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;102.6;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;98.0;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;131.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;1566.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;186.6;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;116.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;193.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;141.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;3535.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp;False;;6609.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21;False;;14349.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;10380.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;246.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;312.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;109.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;102.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;182.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;31693.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;1868.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;121.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;289.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;1984.9;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;3551.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp;False;;9963.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22;False;;59548.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;10225.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;191.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;169.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;104.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;181.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;199.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;31801.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;1781.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;172.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;318.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;2015.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;3551.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp;False;;9976.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23;False;;59607.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;10381.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;216.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;128.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;112.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;147.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;179.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;31766.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;1879.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;127.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;257.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;2000.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;3542.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp;False;;9812.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24;False;;59668.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;10160.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;236.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;178.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;113.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;256.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;178.5;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;31767.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;1895.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;117.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;450.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;1812.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;3540.6;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp;False;;9990.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25;False;;59413.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;10385.6;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;290.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;118.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;104.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;100.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;171.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;31778.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;1860.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;121.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;422.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;1991.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;3552.3;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp;False;;9952.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26;False;;59646.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;10416.9;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;214.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;197.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;118.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;108.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;168.3;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;31734.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;1856.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;131.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;418.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;1982.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;3542.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp;False;;10021.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27;False;;59667.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;10326.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;159.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;162.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;109.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;104.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;373.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;32014.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;1341.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;127.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;428.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;1991.2;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;3549.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp;False;;10111.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28;False;;59589.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;10373.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;239.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;178.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;268.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;104.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;206.9;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;31913.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;1755.8;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;147.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;222.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;1950.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;3547.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp;False;;9954.4;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29;False;;59642.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;10353.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;145.2;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;117.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;109.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;105.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;387.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;31808.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;1602.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;119.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;442.1;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;1980.9;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;3546.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp;False;;9949.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30;False;;59614.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;9947.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;223.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;273.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;124.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;106.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;164.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;31770.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;1878.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;127.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;273.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;1993.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;3561.3;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp;False;;9996.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31;False;;59380.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.norm;True;;10446.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model;False;;711749.7;;int64[6, 2048]float32[6, 2048];0.1;0.1;624.2;96.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "lm_head;True;;326.5;;float16[6, 2048, 4096];96.0;96.1;846.1;846.1;float16[6, 2048, 32000];750.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "<model>;False;;715227.4;;int64[6, 2048]float32[6, 2048];0.1;0.1;2346.1;1500.1;float32[6, 2048, 32000];1500.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(6, 2048)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66407881-f833-4fcd-acdf-7de603f72ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:34.264599Z",
     "iopub.status.busy": "2023-12-01T23:17:34.264241Z",
     "iopub.status.idle": "2023-12-01T23:17:34.268467Z",
     "shell.execute_reply": "2023-12-01T23:17:34.267804Z",
     "shell.execute_reply.started": "2023-12-01T23:17:34.264586Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13552476160, 13552476160)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ccc7692-dda4-442d-8d61-66865458177c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:34.621465Z",
     "iopub.status.busy": "2023-12-01T23:17:34.620551Z",
     "iopub.status.idle": "2023-12-01T23:17:34.788039Z",
     "shell.execute_reply": "2023-12-01T23:17:34.787598Z",
     "shell.execute_reply.started": "2023-12-01T23:17:34.621426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:58 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 517 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      158      145 MB ( 92.39%)\n",
      "GPU:   12,918   12,918 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     751  29,685  31,785 MB   2.36% \n",
      "GPU:   1,422  23,141  24,564 MB   5.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcce30-4544-48cc-a375-262dfe1d724b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mistral-7B\n",
    "\n",
    "mistralai/Mistral-7B-v0.1\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d211ba3-af65-431a-9c5b-8e5e15f9cbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:34:12.402566Z",
     "iopub.status.busy": "2023-12-01T23:34:12.402085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     577  30,026  31,785 MB   1.82% \n",
      "GPU:   1,344  23,219  24,564 MB   5.47% \n",
      "\n",
      "\n",
      "Loading model mistralai/Mistral-7B-v0.1 in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482c325420f4a199b71ca5ac96e66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015924f-8c33-4fcc-adfa-6414574c536a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1596c51f-885d-49bd-9a5c-b3eb46fa9424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:38:30.004452Z",
     "iopub.status.busy": "2023-12-01T23:38:30.003610Z",
     "iopub.status.idle": "2023-12-01T23:38:30.060740Z",
     "shell.execute_reply": "2023-12-01T23:38:30.060219Z",
     "shell.execute_reply.started": "2023-12-01T23:38:30.004419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,685  27,943  31,785 MB   8.45% \n",
      "GPU:   1,374  23,189  24,564 MB   5.59% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8390e6ef-d7b5-44dc-8f4e-0fe3e653c0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:38:30.738958Z",
     "iopub.status.busy": "2023-12-01T23:38:30.738496Z",
     "iopub.status.idle": "2023-12-01T23:38:49.152410Z",
     "shell.execute_reply": "2023-12-01T23:38:49.151838Z",
     "shell.execute_reply.started": "2023-12-01T23:38:30.738927Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a740907f684fa8a963a23c9e1a9781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.98 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658)\n",
      "\n",
      "Tokenizer load time : 150.55 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 18255.09 ms\n",
      "Model CPU memory: 0.63 GB\n",
      "Model GPU memory: 13.99 GB\n",
      "Max   GPU memory: 14.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb05fc9f-ce25-49b8-b300-7c499f3bd7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:37:12.632791Z",
     "iopub.status.busy": "2023-12-01T23:37:12.631277Z",
     "iopub.status.idle": "2023-12-01T23:37:12.669446Z",
     "shell.execute_reply": "2023-12-01T23:37:12.668853Z",
     "shell.execute_reply.started": "2023-12-01T23:37:12.632688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "MistralForCausalLM\n",
      "> submodules\n",
      "- model: MistralModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#MistralModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: MistralRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X MistralDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#MistralDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: MistralAttention\n",
      "      - mlp: MistralMLP\n",
      "      - input_layernorm: MistralRMSNorm\n",
      "      - post_attention_layernorm: MistralRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#MistralAttention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: MistralRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#MistralRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "          - cos_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "          - sin_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "        ---------------------\n",
      "        mlp#MistralMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 14336] (112.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#MistralRMSNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [4096] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "MistralForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, MistralForCausalLM\n",
      "\n",
      "        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
      "\n",
      "        seq_length_with_past = seq_length\n",
      "        past_key_values_length = 0\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        else:\n",
      "            position_ids = position_ids.view(-1, seq_length).long()\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if (\n",
      "            attention_mask is not None\n",
      "            and hasattr(self.config, \"_flash_attn_2_enabled\")\n",
      "            and self.config._flash_attn_2_enabled\n",
      "            and past_key_values is not None\n",
      "        ):\n",
      "            is_padding_right = attention_mask[:, -1].sum().item() != batch_size\n",
      "            if is_padding_right:\n",
      "                raise ValueError(\n",
      "                    \"You are attempting to perform batched generation with padding_side='right'\"\n",
      "                    \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n",
      "                    \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n",
      "                )\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask,\n",
      "                (batch_size, seq_length),\n",
      "                inputs_embeds,\n",
      "                past_key_values_length,\n",
      "                sliding_window=self.config.sliding_window,\n",
      "            )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_value,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "MistralDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
      "                `(batch, sequence_length)` where padding elements are indicated by 0.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "MistralAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        # repeat k/v heads if n_kv_heads < n_heads\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "MistralRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "MistralRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0e7ccc-6181-402a-b1c9-50a570cf1d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:39:07.429401Z",
     "iopub.status.busy": "2023-12-01T23:39:07.428851Z",
     "iopub.status.idle": "2023-12-01T23:39:53.451335Z",
     "shell.execute_reply": "2023-12-01T23:39:53.450798Z",
     "shell.execute_reply.started": "2023-12-01T23:39:07.429383Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 6 and sequence length 2048:\n",
      "model.embed_tokens;True;;255.4;;int64[6, 2048];0.1;0.1;96.1;96.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;229.9;;bfloat16[6, 2048, 4096];96.0;144.1;528.2;240.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;247.4;;bfloat16[6, 2048, 4096];96.0;240.1;336.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;110.4;;bfloat16[6, 2048, 4096];96.0;336.1;360.1;360.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;233.2;;bfloat16[6, 2048, 4096];96.0;360.1;384.1;384.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;194.3;;bfloat16[6, 8, 2048, 128];24.0;384.1;384.1;384.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;309.0;;bfloat16[6, 2048, 4096];96.0;2160.1;2256.1;2256.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;2408.4;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;2256.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;200.3;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;119.7;;bfloat16[6, 2048, 4096];96.0;336.1;672.1;672.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;144.6;;bfloat16[6, 2048, 14336];336.0;672.1;1008.1;1008.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;245.4;;bfloat16[6, 2048, 4096];96.0;672.1;1008.1;1008.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;112.3;;bfloat16[6, 2048, 14336];336.0;672.1;768.1;768.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1109.5;;bfloat16[6, 2048, 4096];96.0;336.1;768.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.0;False;;4511.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;144.1;768.1;240.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;226.5;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;121.4;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;118.1;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;103.7;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;142.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;209.5;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;48701.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;526.5;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;205.2;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;171.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;177.0;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;177.7;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1443.3;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.1;False;;51705.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;290.1;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;190.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;188.1;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;171.7;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;160.2;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;179.4;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;2647.5;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;298.9;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;190.6;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;347.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;203.8;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;182.2;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2.mlp;False;;1616.5;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.2;False;;5567.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;295.0;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;194.2;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;169.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;170.8;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;157.9;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;207.2;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;2459.4;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;278.2;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;268.8;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;195.4;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;177.0;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;194.9;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1794.4;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.3;False;;5721.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;316.7;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;191.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;175.0;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;351.4;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;159.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;192.2;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;2475.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;409.0;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;183.0;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;179.7;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;163.7;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;181.9;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4.mlp;False;;1625.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.4;False;;5552.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;293.4;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;191.3;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;177.4;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;144.0;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;157.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;139.1;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;2213.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;175.6;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;110.7;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;94.8;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;105.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;257.5;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1031.0;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.5;False;;4536.4;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;164.8;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;109.2;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;109.3;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;109.6;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;94.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;120.0;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1571.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;200.5;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;261.6;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;100.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;111.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;105.6;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6.mlp;False;;1076.2;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.6;False;;3473.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;398.6;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;114.1;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;104.1;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;98.0;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;96.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;392.0;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1783.6;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;293.3;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;185.9;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;167.8;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;178.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;177.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1560.2;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.7;False;;4721.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;271.3;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;179.4;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;361.4;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;173.6;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;155.5;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;208.7;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;2437.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;278.8;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;190.2;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;170.3;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;184.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;169.8;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8.mlp;False;;1614.6;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.8;False;;5382.4;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;309.8;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;177.6;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;177.7;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;177.0;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;335.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;216.1;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;2523.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;275.2;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;188.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;183.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;190.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;184.9;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9.mlp;False;;1640.2;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.9;False;;5692.6;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;281.2;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;363.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;184.2;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;176.1;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;164.4;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;216.6;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;2503.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;485.9;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;177.7;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;168.7;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;181.9;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;201.7;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10.mlp;False;;1478.5;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.10;False;;5743.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;301.3;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;163.3;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;175.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;192.5;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;174.2;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;234.2;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;2566.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;304.2;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;183.5;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;407.7;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;165.1;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;184.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1689.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.11;False;;5634.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;190.5;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;116.5;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;101.4;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;99.0;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;93.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;134.9;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1567.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;189.4;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;108.8;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;99.0;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;104.6;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;130.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12.mlp;False;;1005.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.12;False;;3553.5;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;186.4;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;112.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;105.1;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;240.8;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;93.5;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;134.5;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1552.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;185.9;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;115.1;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;99.9;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;103.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;104.3;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13.mlp;False;;985.3;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.13;False;;3362.5;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;178.1;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;114.9;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;101.0;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;99.1;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;96.0;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;227.1;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1828.6;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;312.7;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;173.3;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;179.3;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;194.8;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;407.3;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1738.0;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.14;False;;4849.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;317.2;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;198.8;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;191.6;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;198.2;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;149.4;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;176.1;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;2593.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;301.3;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;406.2;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;161.6;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;186.6;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;164.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15.mlp;False;;1688.7;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.15;False;;5645.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;511.3;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;190.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;201.2;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;184.7;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;170.9;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;490.6;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;2690.9;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;319.2;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;201.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;178.4;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;164.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;182.4;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16.mlp;False;;1660.8;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.16;False;;5966.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;304.0;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;198.1;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;349.6;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;186.4;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;163.5;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;196.5;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;2569.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;303.8;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;205.3;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;143.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;188.5;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;182.4;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17.mlp;False;;1663.1;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.17;False;;5611.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;287.0;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;194.3;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;188.1;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;206.2;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;366.6;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;213.8;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;2610.4;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;301.6;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;177.1;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;102.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;109.6;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;106.4;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1138.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.18;False;;5243.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;197.1;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;256.9;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;101.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;98.6;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;95.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;131.6;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;1561.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;331.5;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;111.0;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;95.2;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;102.0;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;103.5;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19.mlp;False;;837.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.19;False;;3529.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;180.9;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;108.1;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;100.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;101.4;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;98.6;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;308.7;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;3895225.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;333999.5;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;230.3;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;304.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;110.5;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;110.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20.mlp;False;;1260.6;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.20;False;;4231277.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;2090.1;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;3098.0;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;3436.6;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;8584.1;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;129.5;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;262.8;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;2440859.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;337050.1;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;229.9;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;127.2;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;132.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;141.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21.mlp;False;;1360.6;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.21;False;;2782233.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;1974.3;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;3316.5;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;3570.0;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;8285.4;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;122.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;347.4;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;2437670.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;337764.7;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;238.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;116.2;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;108.6;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;150.0;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22.mlp;False;;1286.4;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.22;False;;2779331.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;2057.5;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;3180.2;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;3450.7;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;8585.6;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;133.4;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;253.5;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;2440615.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;336749.5;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;227.2;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;124.2;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;122.0;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;271.4;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23.mlp;False;;1256.8;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.23;False;;2781446.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;2083.2;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;3310.3;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;3447.0;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;8351.0;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;118.8;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;291.3;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;2435980.5;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;339827.7;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;469.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;115.9;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;109.1;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;128.2;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24.mlp;False;;1341.4;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.24;False;;2779892.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;1935.9;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;3395.4;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;3300.6;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;8559.2;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;118.3;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;681.9;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;2450560.2;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;343481.1;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;224.0;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;131.0;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;120.0;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;131.7;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25.mlp;False;;1378.7;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.25;False;;2798030.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;1918.9;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;3308.6;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;3464.4;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;8555.1;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;132.4;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;248.6;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;2473158.6;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;343550.8;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;206.5;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;113.0;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;112.8;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;107.9;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26.mlp;False;;1017.7;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.26;False;;2820459.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;2015.5;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;3318.1;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;3502.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;8400.8;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;281.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;259.4;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;2473813.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;344109.1;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;360.9;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;257.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;176.2;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;121.0;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27.mlp;False;;1768.1;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.27;False;;2822411.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;1437.0;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;3257.8;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;3425.9;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;8491.4;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;123.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;250.5;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;2483865.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;344772.4;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;240.7;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;132.1;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;137.6;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;128.1;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28.mlp;False;;1189.1;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.28;False;;2832138.3;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;1934.6;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;3281.1;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;3445.5;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;8568.5;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;141.2;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;247.8;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;2460512.5;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;342629.6;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;215.8;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;300.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;116.1;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;109.8;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29.mlp;False;;1227.6;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.29;False;;2806942.8;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;2132.2;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;3137.3;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;3434.6;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;8480.6;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;113.1;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;245.4;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;2123992.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;3765.3;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;121.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;162.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;135.4;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;171.2;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30.mlp;False;;1257.9;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.30;False;;2131889.0;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;2275.6;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;3287.4;;bfloat16[6, 2048, 4096];96.0;336.1;432.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;3452.2;;bfloat16[6, 2048, 4096];96.0;432.1;456.1;456.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;8326.3;;bfloat16[6, 2048, 4096];96.0;456.1;480.1;480.1;bfloat16[6, 2048, 1024];24.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;110.3;;bfloat16[6, 8, 2048, 128];24.0;480.1;480.1;480.1;bfloat16[2048, 128]bfloat16[2048, 128];1.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;236.9;;bfloat16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;431934.1;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;4014.3;;bfloat16[6, 2048, 4096];96.0;336.1;720.2;432.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;158.4;;bfloat16[6, 2048, 4096];96.0;432.1;768.1;768.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;122.5;;bfloat16[6, 2048, 14336];336.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;121.3;;bfloat16[6, 2048, 4096];96.0;768.1;1104.1;1104.1;bfloat16[6, 2048, 14336];336.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;116.7;;bfloat16[6, 2048, 14336];336.0;768.1;864.1;864.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31.mlp;False;;1181.4;;bfloat16[6, 2048, 4096];96.0;432.1;864.1;528.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.layers.31;False;;439988.7;;bfloat16[6, 2048, 4096]bfloat16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;864.1;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model.norm;True;;2338.5;;bfloat16[6, 2048, 4096];96.0;240.1;624.2;336.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "model;False;;32159508.1;;int64[6, 2048]float32[6, 2048];0.1;0.1;624.2;96.1;bfloat16[6, 2048, 4096];96.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "lm_head;True;;3108.8;;bfloat16[6, 2048, 4096];96.0;96.1;846.1;846.1;bfloat16[6, 2048, 32000];750.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n",
      "<model>;False;;32166687.8;;int64[6, 2048]float32[6, 2048];0.1;0.1;2346.1;1500.1;float32[6, 2048, 32000];1500.0;;0.0;;;-15832.7;-15832.7;-15832.7;0.0;;0.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(6, 2048)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533074ac-3463-4399-bc05-7a826a489d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:37:55.773566Z",
     "iopub.status.busy": "2023-12-01T23:37:55.773079Z",
     "iopub.status.idle": "2023-12-01T23:38:06.113743Z",
     "shell.execute_reply": "2023-12-01T23:38:06.113249Z",
     "shell.execute_reply.started": "2023-12-01T23:37:55.773536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8519680, 19061841920)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9c0212-735d-4fe8-8757-c75c22fdf5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:38:06.114784Z",
     "iopub.status.busy": "2023-12-01T23:38:06.114427Z",
     "iopub.status.idle": "2023-12-01T23:38:06.174825Z",
     "shell.execute_reply": "2023-12-01T23:38:06.174433Z",
     "shell.execute_reply.started": "2023-12-01T23:38:06.114773Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:01:34 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1249 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:    2,123        0 MB (  0.00%)\n",
      "GPU:       29        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,685  27,947  31,785 MB   8.45% \n",
      "GPU:   1,374  23,189  24,564 MB   5.59% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc22d5-b937-4a3f-b940-a24db29be5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
