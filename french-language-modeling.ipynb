{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {},
   "source": [
    "# Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:14:03.286861Z",
     "iopub.status.busy": "2023-11-17T20:14:03.286304Z",
     "iopub.status.idle": "2023-11-17T20:14:03.804335Z",
     "shell.execute_reply": "2023-11-17T20:14:03.803869Z",
     "shell.execute_reply.started": "2023-11-17T20:14:03.286844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {},
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T23:38:05.973188Z",
     "iopub.status.busy": "2023-11-16T23:38:05.972336Z",
     "iopub.status.idle": "2023-11-16T23:38:05.977439Z",
     "shell.execute_reply": "2023-11-16T23:38:05.976555Z",
     "shell.execute_reply.started": "2023-11-16T23:38:05.973159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:22:18.988139Z",
     "iopub.status.busy": "2023-11-15T06:22:18.987564Z",
     "iopub.status.idle": "2023-11-15T06:22:25.849906Z",
     "shell.execute_reply": "2023-11-15T06:22:25.849209Z",
     "shell.execute_reply.started": "2023-11-15T06:22:18.988115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-6B.0b745f4b791d153df9fbd0062e81c45728868b56.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.f7bc352f27bb1c02ee371a4576942a7d96c8bb97.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.d08b22ae22ee52b7fd762bf6e44711d90cc96794.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-34B.3fac38b00c086fb2420a731d53517582d9303843.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {},
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:25:16.477085Z",
     "iopub.status.busy": "2023-11-15T06:25:16.476437Z",
     "iopub.status.idle": "2023-11-15T20:18:05.373331Z",
     "shell.execute_reply": "2023-11-15T20:18:05.361097Z",
     "shell.execute_reply.started": "2023-11-15T06:25:16.477066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466e7dfdc62a4073bf32fd1beb3b099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fad18c8c64ca2bc8f09635d0d7d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26e5a4d1f744dcb6933fe85293ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6edff8851f4abdb00d256bd38919f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fdd7631cc4e2a89580fabf990cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1636c2841b24452b9dd3a0b609300911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-webscraper/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b9087acf2b42248cf5af8a981024df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3407c0b6940c09a7f929cab3bc22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e252a7ffff74a4d909a8bf7b5d4aef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffde3291ccd43aebed0045a6bc157e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efbb66c793e4f02bfa39c35e3d6a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c569195cc44a782e98dbd953ccb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6aefa8c53c42a69a223d33daa7e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cc7b0e0e94dfd9080fc4a7faf9848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff776e00b6cf42e58bd0caeff74a8a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c009ec2f1df34c0c915013bdc3c3ffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f3b8d94964fb18fbc66d9745d21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d622690413b4bd8a9257d48b7a34f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd40479c0b4e4a8bdf4d2d0e850932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d9471d7744e16b18245d0bf57d7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad3b977791546c985708da95bde3fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710cca87f5c34722a8e52d6f0a0f3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c334be2f31e4d34ac6d1c5de47f91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15c1cd12cf4e73b48aa58c95609a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b096d674c8e429ea2383ee3e4d322d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07b3f0c-fa88-49c0-a2a9-74da94b0181d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:36.772930Z",
     "iopub.status.busy": "2023-11-15T22:28:36.772616Z",
     "iopub.status.idle": "2023-11-15T22:28:36.782697Z",
     "shell.execute_reply": "2023-11-15T22:28:36.781762Z",
     "shell.execute_reply.started": "2023-11-15T22:28:36.772887Z"
    }
   },
   "outputs": [],
   "source": [
    "other_models = { \n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b093a3-5c98-4506-8ff6-fd4d3fcf4dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:54.337937Z",
     "iopub.status.busy": "2023-11-15T22:28:54.337640Z",
     "iopub.status.idle": "2023-11-15T22:47:11.856419Z",
     "shell.execute_reply": "2023-11-15T22:47:11.855564Z",
     "shell.execute_reply.started": "2023-11-15T22:28:54.337905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95363c4b2ecb4817aefe35f5efd8e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff40d5c48894470a78b4613e83e09a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670c247f14504a7499e23e49e7e90f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5ed54abf3424580c210b51b182742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "for model in other_models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {},
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {},
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef2623-1568-44c9-b0e1-22d434c8d673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:11.007140Z",
     "iopub.status.busy": "2023-11-18T12:50:11.006435Z",
     "iopub.status.idle": "2023-11-18T12:50:11.012078Z",
     "shell.execute_reply": "2023-11-18T12:50:11.011446Z",
     "shell.execute_reply.started": "2023-11-18T12:50:11.007116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ea611a-0c97-45f6-aa47-4fe8474954b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:16.865613Z",
     "iopub.status.busy": "2023-11-18T12:50:16.864880Z",
     "iopub.status.idle": "2023-11-18T12:50:16.870681Z",
     "shell.execute_reply": "2023-11-18T12:50:16.869546Z",
     "shell.execute_reply.started": "2023-11-18T12:50:16.865578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = models[\"redpajama_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f28d43-e44b-49b2-a29f-20b103d1d1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:51.109646Z",
     "iopub.status.busy": "2023-11-17T23:49:51.109274Z",
     "iopub.status.idle": "2023-11-17T23:49:56.330791Z",
     "shell.execute_reply": "2023-11-17T23:49:56.330376Z",
     "shell.execute_reply.started": "2023-11-17T23:49:51.109630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc4ac7-3333-4310-bb4d-bc555b41ebf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:56.331914Z",
     "iopub.status.busy": "2023-11-17T23:49:56.331453Z",
     "iopub.status.idle": "2023-11-17T23:50:06.822516Z",
     "shell.execute_reply": "2023-11-17T23:50:06.821911Z",
     "shell.execute_reply.started": "2023-11-17T23:49:56.331902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb64419b9e54f1d9271e6a7034fe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8817b2eeb74d83a7ac4fc16bd30f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1290ddeb18645fa93e0fd5058c7949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a3a7bc743445fbe9feaef5900fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda35c0f10d4d15a9a0fbb3aa408b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7eb0a738c40e991f2de95404a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"frenchtext/banque-fr-2311\"\n",
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dbb014-c670-410e-88a6-219a3466770f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.823168Z",
     "iopub.status.busy": "2023-11-17T23:50:06.822994Z",
     "iopub.status.idle": "2023-11-17T23:50:06.826497Z",
     "shell.execute_reply": "2023-11-17T23:50:06.825982Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.823160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = dataset[0][\"Text\"]\n",
    "len(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896de44a-2e1c-45d3-87ce-527e6b3f38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.827305Z",
     "iopub.status.busy": "2023-11-17T23:50:06.827192Z",
     "iopub.status.idle": "2023-11-17T23:50:06.848030Z",
     "shell.execute_reply": "2023-11-17T23:50:06.847618Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.827297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text_example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fa350c-36ef-4615-9c2b-23b845880794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.848944Z",
     "iopub.status.busy": "2023-11-17T23:50:06.848554Z",
     "iopub.status.idle": "2023-11-17T23:50:08.534591Z",
     "shell.execute_reply": "2023-11-17T23:50:08.534182Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.848929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 23:50:07.766211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-17 23:50:07.786800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.375988006591797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenize_input = tokenizer.encode(text_example[:2048], return_tensors=\"pt\")\n",
    "    loss = model(tokenize_input, labels=tokenize_input)[0]\n",
    "torch.exp(loss.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c21247-3ea4-450f-b810-3699c1f9e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.535351Z",
     "iopub.status.busy": "2023-11-17T23:50:08.535080Z",
     "iopub.status.idle": "2023-11-17T23:50:08.541585Z",
     "shell.execute_reply": "2023-11-17T23:50:08.541199Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.535337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375\n",
      "3689\n",
      "3804\n",
      "2104\n",
      "7936\n",
      "3802\n",
      "10669\n",
      "2171\n",
      "3568\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(dataset[i][\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ad0bf0-41ec-4a90-98d7-db8ab7150b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.542513Z",
     "iopub.status.busy": "2023-11-17T23:50:08.542112Z",
     "iopub.status.idle": "2023-11-17T23:50:08.557331Z",
     "shell.execute_reply": "2023-11-17T23:50:08.556940Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.542501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c375df-43ab-44f7-a3e0-4ec139734e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.558032Z",
     "iopub.status.busy": "2023-11-17T23:50:08.557899Z",
     "iopub.status.idle": "2023-11-17T23:50:08.573642Z",
     "shell.execute_reply": "2023-11-17T23:50:08.573227Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.558022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text = dataset[0:10][\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73874f-5726-4e82-941f-ccf68c109291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.574256Z",
     "iopub.status.busy": "2023-11-17T23:50:08.574143Z",
     "iopub.status.idle": "2023-11-17T23:50:08.580661Z",
     "shell.execute_reply": "2023-11-17T23:50:08.580268Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.574248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048 1630\n",
      "1 2048 1160\n",
      "2 2048 1162\n",
      "3 2048 658\n",
      "4 2048 2048\n",
      "4 2048 1741\n",
      "5 2048 1175\n",
      "6 2048 2048\n",
      "6 2048 2048\n",
      "6 2048 1234\n",
      "7 2048 648\n",
      "8 2048 1072\n",
      "9 2048 585\n"
     ]
    }
   ],
   "source": [
    "for sample_mapping,input_ids,attention_mask in zip(encodings[\"overflow_to_sample_mapping\"],encodings[\"input_ids\"],encodings[\"attention_mask\"]):\n",
    "    print(sample_mapping,len(input_ids),sum(attention_mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b66afe1-835e-4f37-b749-9066805d35ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.581766Z",
     "iopub.status.busy": "2023-11-17T23:50:08.581571Z",
     "iopub.status.idle": "2023-11-17T23:50:08.588527Z",
     "shell.execute_reply": "2023-11-17T23:50:08.588112Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.581757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dataset = dataset.sort(\"Words\").filter(lambda example: example[\"Words\"]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f520b9-2e06-4158-ab39-476563ed75fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.589061Z",
     "iopub.status.busy": "2023-11-17T23:50:08.588968Z",
     "iopub.status.idle": "2023-11-17T23:50:08.591277Z",
     "shell.execute_reply": "2023-11-17T23:50:08.590863Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.589053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(batch_size=32):\n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294cc1d6-bb4a-4253-a249-3af30b0ee43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.591941Z",
     "iopub.status.busy": "2023-11-17T23:50:08.591730Z",
     "iopub.status.idle": "2023-11-17T23:50:08.594172Z",
     "shell.execute_reply": "2023-11-17T23:50:08.593798Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.591928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(dataset_batch):\n",
    "    return tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16,\n",
    "                      return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6f843d-e0ab-41db-b6ef-15334d943e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.594732Z",
     "iopub.status.busy": "2023-11-17T23:50:08.594630Z",
     "iopub.status.idle": "2023-11-17T23:50:08.614329Z",
     "shell.execute_reply": "2023-11-17T23:50:08.613824Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.594724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4,  6,  5,  7, 23,  7,  7,  7,  7,  7, 10,  7,  7, 13,  6,  4,  6,\n",
      "         6, 15,  4,  4,  4, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 32])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12,  6, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 16])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         6, 12, 12, 12,  7, 12, 12, 12, 12, 12, 12, 12, 12, 47])\n",
      "torch.Size([32, 48])\n",
      "tensor([45, 45, 44, 48, 55, 39, 51, 46, 43, 42, 40, 47, 41, 44, 47, 42, 40, 62,\n",
      "        43, 55, 34, 44, 42, 53, 53, 30, 56, 52, 51, 53, 37, 41])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for idx,dataset_batch in enumerate(get_dataset_batches()):\n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "    print(encodings[\"attention_mask\"].sum(axis=1))\n",
    "    print(encodings[\"input_ids\"].size())\n",
    "    if idx == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe85da1-182d-4d7b-8cac-452fb26bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nlls = []\n",
    "for idx,dataset_batch in enumerate(get_dataset_batches(batch_size)):\n",
    "    \n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    \n",
    "    if idx%10==0: print(f\"{(idx+1)*batch_size} / {len(dataset)}: {neg_log_likelihood}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74e49-e5f3-48c4-86ba-7f84fbdb6cfb",
   "metadata": {},
   "source": [
    "15 min\n",
    "\n",
    "32 / 85229: 7.71484375\n",
    "352 / 85229: 7.09765625\n",
    "672 / 85229: 8.1015625\n",
    "992 / 85229: 5.12109375\n",
    "1312 / 85229: 5.78125\n",
    "1632 / 85229: 5.51953125\n",
    "1952 / 85229: 6.12109375\n",
    "2272 / 85229: 3.36328125\n",
    "2592 / 85229: 5.26171875\n",
    "2912 / 85229: 5.0703125\n",
    "3232 / 85229: 4.87109375\n",
    "3552 / 85229: 4.74609375\n",
    "3872 / 85229: 4.046875\n",
    "4192 / 85229: 4.3046875\n",
    "4512 / 85229: 4.11328125\n",
    "4832 / 85229: 4.56640625\n",
    "5152 / 85229: 4.51171875\n",
    "5472 / 85229: 4.359375\n",
    "5792 / 85229: 4.32421875\n",
    "6112 / 85229: 4.68359375\n",
    "6432 / 85229: 4.6015625\n",
    "6752 / 85229: 3.658203125\n",
    "7072 / 85229: 4.7734375\n",
    "7392 / 85229: 6.6328125\n",
    "7712 / 85229: 3.869140625\n",
    "8032 / 85229: 4.9375\n",
    "8352 / 85229: 4.359375\n",
    "8672 / 85229: 5.0078125\n",
    "8992 / 85229: 4.09375\n",
    "9312 / 85229: 3.513671875\n",
    "9632 / 85229: 3.578125\n",
    "9952 / 85229: 3.609375\n",
    "10272 / 85229: 3.505859375\n",
    "10592 / 85229: 2.99609375\n",
    "10912 / 85229: 3.5859375\n",
    "11232 / 85229: 3.18359375\n",
    "11552 / 85229: 3.48046875\n",
    "11872 / 85229: 3.11328125\n",
    "12192 / 85229: 3.40234375\n",
    "12512 / 85229: 3.189453125\n",
    "12832 / 85229: 4.3515625\n",
    "13152 / 85229: 3.306640625\n",
    "13472 / 85229: 3.40234375\n",
    "13792 / 85229: 3.162109375\n",
    "14112 / 85229: 3.451171875\n",
    "14432 / 85229: 2.845703125\n",
    "14752 / 85229: 4.3515625\n",
    "15072 / 85229: 4.2890625\n",
    "15392 / 85229: 2.91015625\n",
    "15712 / 85229: 3.1328125\n",
    "16032 / 85229: 4.02734375\n",
    "16352 / 85229: 3.126953125\n",
    "16672 / 85229: 3.478515625\n",
    "16992 / 85229: 3.193359375\n",
    "17312 / 85229: 3.3203125\n",
    "17632 / 85229: 3.0078125\n",
    "\n",
    "OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6177ee4-eb93-44c3-a6ad-52784cc55742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T06:49:07.814995Z",
     "iopub.status.busy": "2023-11-18T06:49:07.814819Z",
     "iopub.status.idle": "2023-11-18T06:49:07.841774Z",
     "shell.execute_reply": "2023-11-18T06:49:07.841206Z",
     "shell.execute_reply.started": "2023-11-18T06:49:07.814984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.5841)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611711-1e58-46d7-84bf-8c4263ed78d8",
   "metadata": {},
   "source": [
    "# Memory study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5319-aca9-41ac-816e-1b65065ac3e5",
   "metadata": {},
   "source": [
    "## CUDA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585551-ae01-4dd9-a0d8-7bb4946bb196",
   "metadata": {},
   "source": [
    "[CUDA semantics / Memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management)\n",
    "\n",
    "[Understanding CUDA Memory Usage](https://pytorch.org/docs/stable/torch_cuda_memory.html#torch-cuda-memory)\n",
    "\n",
    "[CUDA memory management API](https://pytorch.org/docs/stable/cuda.html#cuda-memory-management-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d2a5f9-40a1-4fce-a106-c3f16cf58484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:04:37.814758Z",
     "iopub.status.busy": "2023-11-18T13:04:37.813968Z",
     "iopub.status.idle": "2023-11-18T13:04:37.822574Z",
     "shell.execute_reply": "2023-11-18T13:04:37.822121Z",
     "shell.execute_reply.started": "2023-11-18T13:04:37.814728Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,617.5 MB -   6 %\n",
      "Reserved :  3,160.0 MB -  12 %\n",
      "Free     : 19,786.0 MB -  80 %\n",
      "------------------------------\n",
      "Used     :  3,138.4 MB -  12 %\n",
      "Max used :  3,138.4 MB -  12 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "    \n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeea17f5-409d-4230-bab9-4010f3b6cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:55:15.162845Z",
     "iopub.status.busy": "2023-11-18T12:55:15.162447Z",
     "iopub.status.idle": "2023-11-18T12:55:15.167405Z",
     "shell.execute_reply": "2023-11-18T12:55:15.166833Z",
     "shell.execute_reply.started": "2023-11-18T12:55:15.162820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41785e96-8956-4aee-97d5-d0479c89eb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:56:49.681121Z",
     "iopub.status.busy": "2023-11-18T12:56:49.680290Z",
     "iopub.status.idle": "2023-11-18T12:56:55.117481Z",
     "shell.execute_reply": "2023-11-18T12:56:55.117059Z",
     "shell.execute_reply.started": "2023-11-18T12:56:49.681088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a128d7-8070-4de1-bf66-b6683881921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:00.726679Z",
     "iopub.status.busy": "2023-11-18T13:05:00.725839Z",
     "iopub.status.idle": "2023-11-18T13:05:00.739971Z",
     "shell.execute_reply": "2023-11-18T13:05:00.739570Z",
     "shell.execute_reply.started": "2023-11-18T13:05:00.726644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231118_130500.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22441-8d1d-43c8-90eb-abb78cd3fa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:06.460703Z",
     "iopub.status.busy": "2023-11-18T13:05:06.459810Z",
     "iopub.status.idle": "2023-11-18T13:05:06.468484Z",
     "shell.execute_reply": "2023-11-18T13:05:06.467915Z",
     "shell.execute_reply.started": "2023-11-18T13:05:06.460670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0d6c-289c-4e79-9c6b-c95c0d033581",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
