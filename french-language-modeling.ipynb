{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {},
   "source": [
    "# Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:14:03.286861Z",
     "iopub.status.busy": "2023-11-17T20:14:03.286304Z",
     "iopub.status.idle": "2023-11-17T20:14:03.804335Z",
     "shell.execute_reply": "2023-11-17T20:14:03.803869Z",
     "shell.execute_reply.started": "2023-11-17T20:14:03.286844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {},
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T23:38:05.973188Z",
     "iopub.status.busy": "2023-11-16T23:38:05.972336Z",
     "iopub.status.idle": "2023-11-16T23:38:05.977439Z",
     "shell.execute_reply": "2023-11-16T23:38:05.976555Z",
     "shell.execute_reply.started": "2023-11-16T23:38:05.973159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:22:18.988139Z",
     "iopub.status.busy": "2023-11-15T06:22:18.987564Z",
     "iopub.status.idle": "2023-11-15T06:22:25.849906Z",
     "shell.execute_reply": "2023-11-15T06:22:25.849209Z",
     "shell.execute_reply.started": "2023-11-15T06:22:18.988115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-6B.0b745f4b791d153df9fbd0062e81c45728868b56.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.f7bc352f27bb1c02ee371a4576942a7d96c8bb97.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.d08b22ae22ee52b7fd762bf6e44711d90cc96794.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-34B.3fac38b00c086fb2420a731d53517582d9303843.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {},
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:25:16.477085Z",
     "iopub.status.busy": "2023-11-15T06:25:16.476437Z",
     "iopub.status.idle": "2023-11-15T20:18:05.373331Z",
     "shell.execute_reply": "2023-11-15T20:18:05.361097Z",
     "shell.execute_reply.started": "2023-11-15T06:25:16.477066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466e7dfdc62a4073bf32fd1beb3b099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fad18c8c64ca2bc8f09635d0d7d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26e5a4d1f744dcb6933fe85293ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6edff8851f4abdb00d256bd38919f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fdd7631cc4e2a89580fabf990cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1636c2841b24452b9dd3a0b609300911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-webscraper/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b9087acf2b42248cf5af8a981024df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3407c0b6940c09a7f929cab3bc22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e252a7ffff74a4d909a8bf7b5d4aef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffde3291ccd43aebed0045a6bc157e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efbb66c793e4f02bfa39c35e3d6a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c569195cc44a782e98dbd953ccb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6aefa8c53c42a69a223d33daa7e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cc7b0e0e94dfd9080fc4a7faf9848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff776e00b6cf42e58bd0caeff74a8a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c009ec2f1df34c0c915013bdc3c3ffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f3b8d94964fb18fbc66d9745d21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d622690413b4bd8a9257d48b7a34f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd40479c0b4e4a8bdf4d2d0e850932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d9471d7744e16b18245d0bf57d7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad3b977791546c985708da95bde3fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710cca87f5c34722a8e52d6f0a0f3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c334be2f31e4d34ac6d1c5de47f91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15c1cd12cf4e73b48aa58c95609a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b096d674c8e429ea2383ee3e4d322d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07b3f0c-fa88-49c0-a2a9-74da94b0181d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:36.772930Z",
     "iopub.status.busy": "2023-11-15T22:28:36.772616Z",
     "iopub.status.idle": "2023-11-15T22:28:36.782697Z",
     "shell.execute_reply": "2023-11-15T22:28:36.781762Z",
     "shell.execute_reply.started": "2023-11-15T22:28:36.772887Z"
    }
   },
   "outputs": [],
   "source": [
    "other_models = { \n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b093a3-5c98-4506-8ff6-fd4d3fcf4dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:54.337937Z",
     "iopub.status.busy": "2023-11-15T22:28:54.337640Z",
     "iopub.status.idle": "2023-11-15T22:47:11.856419Z",
     "shell.execute_reply": "2023-11-15T22:47:11.855564Z",
     "shell.execute_reply.started": "2023-11-15T22:28:54.337905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95363c4b2ecb4817aefe35f5efd8e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff40d5c48894470a78b4613e83e09a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670c247f14504a7499e23e49e7e90f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5ed54abf3424580c210b51b182742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "for model in other_models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {},
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {},
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef2623-1568-44c9-b0e1-22d434c8d673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:11.007140Z",
     "iopub.status.busy": "2023-11-18T12:50:11.006435Z",
     "iopub.status.idle": "2023-11-18T12:50:11.012078Z",
     "shell.execute_reply": "2023-11-18T12:50:11.011446Z",
     "shell.execute_reply.started": "2023-11-18T12:50:11.007116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ea611a-0c97-45f6-aa47-4fe8474954b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:16.865613Z",
     "iopub.status.busy": "2023-11-18T12:50:16.864880Z",
     "iopub.status.idle": "2023-11-18T12:50:16.870681Z",
     "shell.execute_reply": "2023-11-18T12:50:16.869546Z",
     "shell.execute_reply.started": "2023-11-18T12:50:16.865578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = models[\"redpajama_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f28d43-e44b-49b2-a29f-20b103d1d1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:51.109646Z",
     "iopub.status.busy": "2023-11-17T23:49:51.109274Z",
     "iopub.status.idle": "2023-11-17T23:49:56.330791Z",
     "shell.execute_reply": "2023-11-17T23:49:56.330376Z",
     "shell.execute_reply.started": "2023-11-17T23:49:51.109630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc4ac7-3333-4310-bb4d-bc555b41ebf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:56.331914Z",
     "iopub.status.busy": "2023-11-17T23:49:56.331453Z",
     "iopub.status.idle": "2023-11-17T23:50:06.822516Z",
     "shell.execute_reply": "2023-11-17T23:50:06.821911Z",
     "shell.execute_reply.started": "2023-11-17T23:49:56.331902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb64419b9e54f1d9271e6a7034fe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8817b2eeb74d83a7ac4fc16bd30f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1290ddeb18645fa93e0fd5058c7949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a3a7bc743445fbe9feaef5900fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda35c0f10d4d15a9a0fbb3aa408b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7eb0a738c40e991f2de95404a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"frenchtext/banque-fr-2311\"\n",
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dbb014-c670-410e-88a6-219a3466770f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.823168Z",
     "iopub.status.busy": "2023-11-17T23:50:06.822994Z",
     "iopub.status.idle": "2023-11-17T23:50:06.826497Z",
     "shell.execute_reply": "2023-11-17T23:50:06.825982Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.823160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = dataset[0][\"Text\"]\n",
    "len(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896de44a-2e1c-45d3-87ce-527e6b3f38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.827305Z",
     "iopub.status.busy": "2023-11-17T23:50:06.827192Z",
     "iopub.status.idle": "2023-11-17T23:50:06.848030Z",
     "shell.execute_reply": "2023-11-17T23:50:06.847618Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.827297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text_example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fa350c-36ef-4615-9c2b-23b845880794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.848944Z",
     "iopub.status.busy": "2023-11-17T23:50:06.848554Z",
     "iopub.status.idle": "2023-11-17T23:50:08.534591Z",
     "shell.execute_reply": "2023-11-17T23:50:08.534182Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.848929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 23:50:07.766211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-17 23:50:07.786800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.375988006591797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenize_input = tokenizer.encode(text_example[:2048], return_tensors=\"pt\")\n",
    "    loss = model(tokenize_input, labels=tokenize_input)[0]\n",
    "torch.exp(loss.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c21247-3ea4-450f-b810-3699c1f9e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.535351Z",
     "iopub.status.busy": "2023-11-17T23:50:08.535080Z",
     "iopub.status.idle": "2023-11-17T23:50:08.541585Z",
     "shell.execute_reply": "2023-11-17T23:50:08.541199Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.535337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375\n",
      "3689\n",
      "3804\n",
      "2104\n",
      "7936\n",
      "3802\n",
      "10669\n",
      "2171\n",
      "3568\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(dataset[i][\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ad0bf0-41ec-4a90-98d7-db8ab7150b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.542513Z",
     "iopub.status.busy": "2023-11-17T23:50:08.542112Z",
     "iopub.status.idle": "2023-11-17T23:50:08.557331Z",
     "shell.execute_reply": "2023-11-17T23:50:08.556940Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.542501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c375df-43ab-44f7-a3e0-4ec139734e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.558032Z",
     "iopub.status.busy": "2023-11-17T23:50:08.557899Z",
     "iopub.status.idle": "2023-11-17T23:50:08.573642Z",
     "shell.execute_reply": "2023-11-17T23:50:08.573227Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.558022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text = dataset[0:10][\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73874f-5726-4e82-941f-ccf68c109291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.574256Z",
     "iopub.status.busy": "2023-11-17T23:50:08.574143Z",
     "iopub.status.idle": "2023-11-17T23:50:08.580661Z",
     "shell.execute_reply": "2023-11-17T23:50:08.580268Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.574248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048 1630\n",
      "1 2048 1160\n",
      "2 2048 1162\n",
      "3 2048 658\n",
      "4 2048 2048\n",
      "4 2048 1741\n",
      "5 2048 1175\n",
      "6 2048 2048\n",
      "6 2048 2048\n",
      "6 2048 1234\n",
      "7 2048 648\n",
      "8 2048 1072\n",
      "9 2048 585\n"
     ]
    }
   ],
   "source": [
    "for sample_mapping,input_ids,attention_mask in zip(encodings[\"overflow_to_sample_mapping\"],encodings[\"input_ids\"],encodings[\"attention_mask\"]):\n",
    "    print(sample_mapping,len(input_ids),sum(attention_mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b66afe1-835e-4f37-b749-9066805d35ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.581766Z",
     "iopub.status.busy": "2023-11-17T23:50:08.581571Z",
     "iopub.status.idle": "2023-11-17T23:50:08.588527Z",
     "shell.execute_reply": "2023-11-17T23:50:08.588112Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.581757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dataset = dataset.sort(\"Words\").filter(lambda example: example[\"Words\"]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f520b9-2e06-4158-ab39-476563ed75fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.589061Z",
     "iopub.status.busy": "2023-11-17T23:50:08.588968Z",
     "iopub.status.idle": "2023-11-17T23:50:08.591277Z",
     "shell.execute_reply": "2023-11-17T23:50:08.590863Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.589053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(batch_size=32):\n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294cc1d6-bb4a-4253-a249-3af30b0ee43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.591941Z",
     "iopub.status.busy": "2023-11-17T23:50:08.591730Z",
     "iopub.status.idle": "2023-11-17T23:50:08.594172Z",
     "shell.execute_reply": "2023-11-17T23:50:08.593798Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.591928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(dataset_batch):\n",
    "    return tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16,\n",
    "                      return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6f843d-e0ab-41db-b6ef-15334d943e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.594732Z",
     "iopub.status.busy": "2023-11-17T23:50:08.594630Z",
     "iopub.status.idle": "2023-11-17T23:50:08.614329Z",
     "shell.execute_reply": "2023-11-17T23:50:08.613824Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.594724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4,  6,  5,  7, 23,  7,  7,  7,  7,  7, 10,  7,  7, 13,  6,  4,  6,\n",
      "         6, 15,  4,  4,  4, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 32])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12,  6, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 16])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         6, 12, 12, 12,  7, 12, 12, 12, 12, 12, 12, 12, 12, 47])\n",
      "torch.Size([32, 48])\n",
      "tensor([45, 45, 44, 48, 55, 39, 51, 46, 43, 42, 40, 47, 41, 44, 47, 42, 40, 62,\n",
      "        43, 55, 34, 44, 42, 53, 53, 30, 56, 52, 51, 53, 37, 41])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for idx,dataset_batch in enumerate(get_dataset_batches()):\n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "    print(encodings[\"attention_mask\"].sum(axis=1))\n",
    "    print(encodings[\"input_ids\"].size())\n",
    "    if idx == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe85da1-182d-4d7b-8cac-452fb26bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nlls = []\n",
    "for idx,dataset_batch in enumerate(get_dataset_batches(batch_size)):\n",
    "    \n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    \n",
    "    if idx%10==0: print(f\"{(idx+1)*batch_size} / {len(dataset)}: {neg_log_likelihood}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74e49-e5f3-48c4-86ba-7f84fbdb6cfb",
   "metadata": {},
   "source": [
    "15 min\n",
    "\n",
    "32 / 85229: 7.71484375\n",
    "352 / 85229: 7.09765625\n",
    "672 / 85229: 8.1015625\n",
    "992 / 85229: 5.12109375\n",
    "1312 / 85229: 5.78125\n",
    "1632 / 85229: 5.51953125\n",
    "1952 / 85229: 6.12109375\n",
    "2272 / 85229: 3.36328125\n",
    "2592 / 85229: 5.26171875\n",
    "2912 / 85229: 5.0703125\n",
    "3232 / 85229: 4.87109375\n",
    "3552 / 85229: 4.74609375\n",
    "3872 / 85229: 4.046875\n",
    "4192 / 85229: 4.3046875\n",
    "4512 / 85229: 4.11328125\n",
    "4832 / 85229: 4.56640625\n",
    "5152 / 85229: 4.51171875\n",
    "5472 / 85229: 4.359375\n",
    "5792 / 85229: 4.32421875\n",
    "6112 / 85229: 4.68359375\n",
    "6432 / 85229: 4.6015625\n",
    "6752 / 85229: 3.658203125\n",
    "7072 / 85229: 4.7734375\n",
    "7392 / 85229: 6.6328125\n",
    "7712 / 85229: 3.869140625\n",
    "8032 / 85229: 4.9375\n",
    "8352 / 85229: 4.359375\n",
    "8672 / 85229: 5.0078125\n",
    "8992 / 85229: 4.09375\n",
    "9312 / 85229: 3.513671875\n",
    "9632 / 85229: 3.578125\n",
    "9952 / 85229: 3.609375\n",
    "10272 / 85229: 3.505859375\n",
    "10592 / 85229: 2.99609375\n",
    "10912 / 85229: 3.5859375\n",
    "11232 / 85229: 3.18359375\n",
    "11552 / 85229: 3.48046875\n",
    "11872 / 85229: 3.11328125\n",
    "12192 / 85229: 3.40234375\n",
    "12512 / 85229: 3.189453125\n",
    "12832 / 85229: 4.3515625\n",
    "13152 / 85229: 3.306640625\n",
    "13472 / 85229: 3.40234375\n",
    "13792 / 85229: 3.162109375\n",
    "14112 / 85229: 3.451171875\n",
    "14432 / 85229: 2.845703125\n",
    "14752 / 85229: 4.3515625\n",
    "15072 / 85229: 4.2890625\n",
    "15392 / 85229: 2.91015625\n",
    "15712 / 85229: 3.1328125\n",
    "16032 / 85229: 4.02734375\n",
    "16352 / 85229: 3.126953125\n",
    "16672 / 85229: 3.478515625\n",
    "16992 / 85229: 3.193359375\n",
    "17312 / 85229: 3.3203125\n",
    "17632 / 85229: 3.0078125\n",
    "\n",
    "OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6177ee4-eb93-44c3-a6ad-52784cc55742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T06:49:07.814995Z",
     "iopub.status.busy": "2023-11-18T06:49:07.814819Z",
     "iopub.status.idle": "2023-11-18T06:49:07.841774Z",
     "shell.execute_reply": "2023-11-18T06:49:07.841206Z",
     "shell.execute_reply.started": "2023-11-18T06:49:07.814984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.5841)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611711-1e58-46d7-84bf-8c4263ed78d8",
   "metadata": {},
   "source": [
    "# Memory study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5319-aca9-41ac-816e-1b65065ac3e5",
   "metadata": {},
   "source": [
    "## CUDA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585551-ae01-4dd9-a0d8-7bb4946bb196",
   "metadata": {},
   "source": [
    "[CUDA semantics / Memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management)\n",
    "\n",
    "[Understanding CUDA Memory Usage](https://pytorch.org/docs/stable/torch_cuda_memory.html#torch-cuda-memory)\n",
    "\n",
    "[CUDA memory management API](https://pytorch.org/docs/stable/cuda.html#cuda-memory-management-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d2a5f9-40a1-4fce-a106-c3f16cf58484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T23:29:17.243305Z",
     "iopub.status.busy": "2023-11-20T23:29:17.242490Z",
     "iopub.status.idle": "2023-11-20T23:29:17.251858Z",
     "shell.execute_reply": "2023-11-20T23:29:17.251286Z",
     "shell.execute_reply.started": "2023-11-20T23:29:17.243272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,633.5 MB -   6 %\n",
      "Reserved :  7,442.0 MB -  30 %\n",
      "Free     : 15,488.0 MB -  63 %\n",
      "------------------------------\n",
      "Used     :  6,718.8 MB -  27 %\n",
      "Max used :  6,921.0 MB -  28 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "    \n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeea17f5-409d-4230-bab9-4010f3b6cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:55:15.162845Z",
     "iopub.status.busy": "2023-11-18T12:55:15.162447Z",
     "iopub.status.idle": "2023-11-18T12:55:15.167405Z",
     "shell.execute_reply": "2023-11-18T12:55:15.166833Z",
     "shell.execute_reply.started": "2023-11-18T12:55:15.162820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41785e96-8956-4aee-97d5-d0479c89eb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:56:49.681121Z",
     "iopub.status.busy": "2023-11-18T12:56:49.680290Z",
     "iopub.status.idle": "2023-11-18T12:56:55.117481Z",
     "shell.execute_reply": "2023-11-18T12:56:55.117059Z",
     "shell.execute_reply.started": "2023-11-18T12:56:49.681088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a128d7-8070-4de1-bf66-b6683881921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:00.726679Z",
     "iopub.status.busy": "2023-11-18T13:05:00.725839Z",
     "iopub.status.idle": "2023-11-18T13:05:00.739971Z",
     "shell.execute_reply": "2023-11-18T13:05:00.739570Z",
     "shell.execute_reply.started": "2023-11-18T13:05:00.726644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231118_130500.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22441-8d1d-43c8-90eb-abb78cd3fa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:06.460703Z",
     "iopub.status.busy": "2023-11-18T13:05:06.459810Z",
     "iopub.status.idle": "2023-11-18T13:05:06.468484Z",
     "shell.execute_reply": "2023-11-18T13:05:06.467915Z",
     "shell.execute_reply.started": "2023-11-18T13:05:06.460670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0d6c-289c-4e79-9c6b-c95c0d033581",
   "metadata": {},
   "source": [
    "## Pytorch models exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85be5910-7850-4787-b36c-203bdf93ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:40:02.824012Z",
     "iopub.status.busy": "2023-11-19T16:40:02.823531Z",
     "iopub.status.idle": "2023-11-19T16:40:02.826926Z",
     "shell.execute_reply": "2023-11-19T16:40:02.826475Z",
     "shell.execute_reply.started": "2023-11-19T16:40:02.823996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def find_attribute_origin(obj, attr_name):\n",
    "    for cls in obj.__class__.__mro__:\n",
    "        if attr_name in dir(cls):\n",
    "            return cls.__name__\n",
    "    return obj.__class__.__name__\n",
    "\n",
    "def display_members(obj):\n",
    "    obj_attributes = {}\n",
    "    for member_name in dir(obj):\n",
    "        if member_name[0:1]!=\"_\":\n",
    "            obj_attributes[getattr(obj,member_name).__qualname__ if hasattr(getattr(obj,member_name),\"__qualname__\") else f\"{find_attribute_origin(obj,member_name)}.{member_name}\"] = str(type(getattr(obj,member_name)))\n",
    "    obj_attributes = {k: obj_attributes[k] for k in sorted(obj_attributes)}\n",
    "    for member_name in obj_attributes.keys():\n",
    "        print(member_name, obj_attributes[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "663d0ad3-8085-4699-a6b4-c176eb76aca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T16:24:40.156700Z",
     "iopub.status.busy": "2023-11-18T16:24:40.156401Z",
     "iopub.status.idle": "2023-11-18T16:24:40.186241Z",
     "shell.execute_reply": "2023-11-18T16:24:40.185806Z",
     "shell.execute_reply.started": "2023-11-18T16:24:40.156689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXConfig <class 'type'>\n",
      "GPTNeoXForCausalLM.T_destination <class 'typing.TypeVar'>\n",
      "GPTNeoXForCausalLM.base_model <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.base_model_prefix <class 'str'>\n",
      "GPTNeoXForCausalLM.call_super_init <class 'bool'>\n",
      "GPTNeoXForCausalLM.config <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>\n",
      "GPTNeoXForCausalLM.device <class 'torch.device'>\n",
      "GPTNeoXForCausalLM.dtype <class 'torch.dtype'>\n",
      "GPTNeoXForCausalLM.dummy_inputs <class 'dict'>\n",
      "GPTNeoXForCausalLM.dump_patches <class 'bool'>\n",
      "GPTNeoXForCausalLM.embed_out <class 'torch.nn.modules.linear.Linear'>\n",
      "GPTNeoXForCausalLM.forward <class 'functools.partial'>\n",
      "GPTNeoXForCausalLM.framework <class 'str'>\n",
      "GPTNeoXForCausalLM.generation_config <class 'transformers.generation.configuration_utils.GenerationConfig'>\n",
      "GPTNeoXForCausalLM.get_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.gpt_neox <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.hf_device_map <class 'dict'>\n",
      "GPTNeoXForCausalLM.is_8bit_serializable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_4bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_8bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_parallelizable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_quantized <class 'bool'>\n",
      "GPTNeoXForCausalLM.main_input_name <class 'str'>\n",
      "GPTNeoXForCausalLM.name_or_path <class 'str'>\n",
      "GPTNeoXForCausalLM.prepare_inputs_for_generation <class 'method'>\n",
      "GPTNeoXForCausalLM.quantization_method <enum 'QuantizationMethod'>\n",
      "GPTNeoXForCausalLM.set_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.supports_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.training <class 'bool'>\n",
      "GPTNeoXForCausalLM.warnings_issued <class 'dict'>\n",
      "GenerationMixin.assisted_decoding <class 'method'>\n",
      "GenerationMixin.beam_sample <class 'method'>\n",
      "GenerationMixin.beam_search <class 'method'>\n",
      "GenerationMixin.compute_transition_scores <class 'method'>\n",
      "GenerationMixin.constrained_beam_search <class 'method'>\n",
      "GenerationMixin.contrastive_search <class 'method'>\n",
      "GenerationMixin.generate <class 'method'>\n",
      "GenerationMixin.greedy_search <class 'method'>\n",
      "GenerationMixin.group_beam_search <class 'method'>\n",
      "GenerationMixin.sample <class 'method'>\n",
      "Module.add_module <class 'method'>\n",
      "Module.apply <class 'method'>\n",
      "Module.bfloat16 <class 'method'>\n",
      "Module.buffers <class 'method'>\n",
      "Module.children <class 'method'>\n",
      "Module.cpu <class 'method'>\n",
      "Module.cuda <class 'function'>\n",
      "Module.double <class 'method'>\n",
      "Module.eval <class 'method'>\n",
      "Module.extra_repr <class 'method'>\n",
      "Module.get_buffer <class 'method'>\n",
      "Module.get_extra_state <class 'method'>\n",
      "Module.get_parameter <class 'method'>\n",
      "Module.get_submodule <class 'method'>\n",
      "Module.ipu <class 'method'>\n",
      "Module.load_state_dict <class 'method'>\n",
      "Module.modules <class 'method'>\n",
      "Module.named_buffers <class 'method'>\n",
      "Module.named_children <class 'method'>\n",
      "Module.named_modules <class 'method'>\n",
      "Module.named_parameters <class 'method'>\n",
      "Module.parameters <class 'method'>\n",
      "Module.register_backward_hook <class 'method'>\n",
      "Module.register_buffer <class 'method'>\n",
      "Module.register_forward_hook <class 'method'>\n",
      "Module.register_forward_pre_hook <class 'method'>\n",
      "Module.register_full_backward_hook <class 'method'>\n",
      "Module.register_full_backward_pre_hook <class 'method'>\n",
      "Module.register_load_state_dict_post_hook <class 'method'>\n",
      "Module.register_module <class 'method'>\n",
      "Module.register_parameter <class 'method'>\n",
      "Module.register_state_dict_pre_hook <class 'method'>\n",
      "Module.requires_grad_ <class 'method'>\n",
      "Module.set_extra_state <class 'method'>\n",
      "Module.share_memory <class 'method'>\n",
      "Module.state_dict <class 'method'>\n",
      "Module.to <class 'function'>\n",
      "Module.to_empty <class 'method'>\n",
      "Module.train <class 'method'>\n",
      "Module.type <class 'method'>\n",
      "Module.xpu <class 'method'>\n",
      "Module.zero_grad <class 'method'>\n",
      "ModuleUtilsMixin.add_memory_hooks <class 'method'>\n",
      "ModuleUtilsMixin.create_extended_attention_mask_for_decoder <class 'function'>\n",
      "ModuleUtilsMixin.estimate_tokens <class 'method'>\n",
      "ModuleUtilsMixin.floating_point_ops <class 'method'>\n",
      "ModuleUtilsMixin.get_extended_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.get_head_mask <class 'method'>\n",
      "ModuleUtilsMixin.invert_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.num_parameters <class 'method'>\n",
      "ModuleUtilsMixin.reset_memory_hooks_state <class 'method'>\n",
      "PeftAdapterMixin.active_adapter <class 'method'>\n",
      "PeftAdapterMixin.active_adapters <class 'method'>\n",
      "PeftAdapterMixin.add_adapter <class 'method'>\n",
      "PeftAdapterMixin.disable_adapters <class 'method'>\n",
      "PeftAdapterMixin.enable_adapters <class 'method'>\n",
      "PeftAdapterMixin.get_adapter_state_dict <class 'method'>\n",
      "PeftAdapterMixin.load_adapter <class 'method'>\n",
      "PeftAdapterMixin.set_adapter <class 'method'>\n",
      "PreTrainedModel.can_generate <class 'method'>\n",
      "PreTrainedModel.disable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.enable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.float <class 'method'>\n",
      "PreTrainedModel.from_pretrained <class 'method'>\n",
      "PreTrainedModel.get_input_embeddings <class 'method'>\n",
      "PreTrainedModel.get_memory_footprint <class 'method'>\n",
      "PreTrainedModel.get_position_embeddings <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_disable <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_enable <class 'method'>\n",
      "PreTrainedModel.half <class 'method'>\n",
      "PreTrainedModel.init_weights <class 'method'>\n",
      "PreTrainedModel.post_init <class 'method'>\n",
      "PreTrainedModel.prune_heads <class 'method'>\n",
      "PreTrainedModel.register_for_auto_class <class 'method'>\n",
      "PreTrainedModel.resize_position_embeddings <class 'method'>\n",
      "PreTrainedModel.resize_token_embeddings <class 'method'>\n",
      "PreTrainedModel.retrieve_modules_from_names <class 'method'>\n",
      "PreTrainedModel.reverse_bettertransformer <class 'method'>\n",
      "PreTrainedModel.save_pretrained <class 'method'>\n",
      "PreTrainedModel.set_input_embeddings <class 'method'>\n",
      "PreTrainedModel.tie_weights <class 'method'>\n",
      "PreTrainedModel.to_bettertransformer <class 'method'>\n",
      "PreTrainedModel.warn_if_padding_and_no_attention_mask <class 'method'>\n",
      "PushToHubMixin.push_to_hub <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "display_members(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd82911-00ea-48a4-80d2-709a0797818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:41:51.525145Z",
     "iopub.status.busy": "2023-11-19T16:41:51.524456Z",
     "iopub.status.idle": "2023-11-19T16:41:51.534152Z",
     "shell.execute_reply": "2023-11-19T16:41:51.533665Z",
     "shell.execute_reply.started": "2023-11-19T16:41:51.525118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "56d76159-52cd-4a11-acae-b3461ce566b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:31:35.570614Z",
     "iopub.status.busy": "2023-11-19T07:31:35.569668Z",
     "iopub.status.idle": "2023-11-19T07:31:35.592081Z",
     "shell.execute_reply": "2023-11-19T07:31:35.591622Z",
     "shell.execute_reply.started": "2023-11-19T07:31:35.570571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "GPTNeoXForCausalLM\n",
      "> submodules\n",
      "- gpt_neox: GPTNeoXModel\n",
      "- embed_out: Linear\n",
      "  ---------------------\n",
      "  gpt_neox#GPTNeoXModel\n",
      "  > submodules\n",
      "  - embed_in: Embedding\n",
      "  - emb_dropout: Dropout\n",
      "  - layers: ModuleList\n",
      "  - final_layer_norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_in#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [50432, 2560] (246.2 MB)\n",
      "    ---------------------\n",
      "    emb_dropout#Dropout\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X GPTNeoXLayer\n",
      "      ---------------------\n",
      "      0..31#GPTNeoXLayer\n",
      "      > submodules\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "      - post_attention_dropout: Dropout\n",
      "      - post_mlp_dropout: Dropout\n",
      "      - attention: GPTNeoXAttention\n",
      "      - mlp: GPTNeoXMLP\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        post_mlp_dropout#Dropout\n",
      "        ---------------------\n",
      "        attention#GPTNeoXAttention\n",
      "        > buffers\n",
      "        - bias: bool [1, 1, 2048, 2048] (4.0 MB)\n",
      "        - masked_bias: float16 [] (0.0 MB)\n",
      "        > submodules\n",
      "        - rotary_emb: GPTNeoXRotaryEmbedding\n",
      "        - query_key_value: Linear8bitLt\n",
      "        - dense: Linear8bitLt\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          rotary_emb#GPTNeoXRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [40] (0.0 MB)\n",
      "          - cos_cached: float16 [2048, 80] (0.3 MB)\n",
      "          - sin_cached: float16 [2048, 80] (0.3 MB)\n",
      "          ---------------------\n",
      "          query_key_value#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [7680, 2560] (18.8 MB)\n",
      "          - bias: float16 [7680] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 2560] (6.2 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#GPTNeoXMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: Linear8bitLt\n",
      "        - dense_4h_to_h: Linear8bitLt\n",
      "        - act: GELUActivation\n",
      "          ---------------------\n",
      "          dense_h_to_4h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [10240, 2560] (25.0 MB)\n",
      "          - bias: float16 [10240] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense_4h_to_h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 10240] (25.0 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          act#GELUActivation\n",
      "    ---------------------\n",
      "    final_layer_norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: float16 [2560] (0.0 MB)\n",
      "    - bias: float16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  embed_out#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [50432, 2560] (246.2 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "GPTNeoXForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "            only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "            `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "        >>> import torch\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config.is_decoder = True\n",
      "        >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "        >>> outputs = model(**inputs)\n",
      "\n",
      "        >>> prediction_logits = outputs.logits\n",
      "        ```\"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.gpt_neox(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            past_key_values=past_key_values,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        lm_logits = self.embed_out(hidden_states)\n",
      "\n",
      "        lm_loss = None\n",
      "        if labels is not None:\n",
      "            # move labels to correct device to enable model parallelism\n",
      "            labels = labels.to(lm_logits.device)\n",
      "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
      "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
      "            labels = labels[:, 1:].contiguous()\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + outputs[1:]\n",
      "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=lm_loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "GPTNeoXModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPast,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \"\"\"\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "            input_shape = input_ids.size()\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        batch_size, seq_length = input_shape\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_length = 0\n",
      "            past_key_values = tuple([None] * self.config.num_hidden_layers)\n",
      "        else:\n",
      "            past_length = past_key_values[0][0].size(-2)\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        # Attention mask.\n",
      "        if attention_mask is not None:\n",
      "            assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "            attention_mask = attention_mask.view(batch_size, -1)\n",
      "            # We create a 3D attention mask from a 2D tensor mask.\n",
      "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
      "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
      "            # this attention mask is more simple than the triangular masking of causal attention\n",
      "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
      "            attention_mask = attention_mask[:, None, None, :]\n",
      "\n",
      "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "            # masked positions, this operation will create a tensor which is 0.0 for\n",
      "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
      "            # Since we are adding it to the raw scores before the softmax, this is\n",
      "            # effectively the same as removing these entirely.\n",
      "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
      "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
      "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_in(input_ids)\n",
      "\n",
      "        hidden_states = self.emb_dropout(inputs_embeds)\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        presents = () if use_cache else None\n",
      "        all_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    use_cache,\n",
      "                    None,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    layer_past=layer_past,\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                )\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "            if output_attentions:\n",
      "                all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        hidden_states = self.final_layer_norm(hidden_states)\n",
      "        # Add last hidden state\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        attention_layer_outputs = self.attention(\n",
      "            self.input_layernorm(hidden_states),\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            layer_past=layer_past,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "        )\n",
      "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
      "        attn_output = self.post_attention_dropout(attn_output)\n",
      "        outputs = attention_layer_outputs[1:]\n",
      "\n",
      "        if self.use_parallel_residual:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output + hidden_states\n",
      "        else:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x))\n",
      "            # x = x + mlp(ln2(x))\n",
      "            attn_output = attn_output + hidden_states\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
      "        else:\n",
      "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        has_layer_past = layer_past is not None\n",
      "\n",
      "        # Compute QKV\n",
      "        # Attention heads [batch, seq_len, hidden_size]\n",
      "        #   --> [batch, seq_len, (np * 3 * head_size)]\n",
      "        qkv = self.query_key_value(hidden_states)\n",
      "\n",
      "        # [batch, seq_len, (num_heads * 3 * head_size)]\n",
      "        #   --> [batch, seq_len, num_heads, 3 * head_size]\n",
      "        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
      "        qkv = qkv.view(*new_qkv_shape)\n",
      "\n",
      "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
      "        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
      "        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
      "        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
      "\n",
      "        # Compute rotary embeddings on rotary_ndims\n",
      "        query_rot = query[..., : self.rotary_ndims]\n",
      "        query_pass = query[..., self.rotary_ndims :]\n",
      "        key_rot = key[..., : self.rotary_ndims]\n",
      "        key_pass = key[..., self.rotary_ndims :]\n",
      "\n",
      "        # Compute token offset for rotary embeddings (when decoding)\n",
      "        seq_len = key.shape[-2]\n",
      "        if has_layer_past:\n",
      "            seq_len += layer_past[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
      "        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "        query = torch.cat((query, query_pass), dim=-1)\n",
      "        key = torch.cat((key, key_pass), dim=-1)\n",
      "\n",
      "        # Cache QKV values\n",
      "        if has_layer_past:\n",
      "            past_key = layer_past[0]\n",
      "            past_value = layer_past[1]\n",
      "            key = torch.cat((past_key, key), dim=-2)\n",
      "            value = torch.cat((past_value, value), dim=-2)\n",
      "        present = (key, value) if use_cache else None\n",
      "\n",
      "        # Compute attention\n",
      "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "\n",
      "        # Reshape outputs\n",
      "        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n",
      "        attn_output = self.dense(attn_output)\n",
      "\n",
      "        outputs = (attn_output, present)\n",
      "        if output_attentions:\n",
      "            outputs += (attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "GPTNeoXRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Linear8bitLt.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor):\n",
      "        self.state.is_training = self.training\n",
      "        if self.weight.CB is not None:\n",
      "            self.init_8bit_state()\n",
      "\n",
      "        # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
      "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
      "            self.bias.data = self.bias.data.to(x.dtype)\n",
      "\n",
      "        out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "\n",
      "        if not self.state.has_fp16_weights:\n",
      "            if self.state.CB is not None and self.state.CxB is not None:\n",
      "                # we converted 8-bit row major to turing/ampere format in the first inference pass\n",
      "                # we no longer need the row-major weight\n",
      "                del self.state.CB\n",
      "                self.weight.data = self.state.CxB\n",
      "        return out\n",
      "\n",
      "---------------------\n",
      "GPTNeoXMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "---------------------\n",
      "GELUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self.act(input)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72c72-5447-48ac-a895-4fda1ae5d6fe",
   "metadata": {},
   "source": [
    "# Huggingface language models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526914e-3ed9-454a-8005-aa3bf5b85594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T14:24:29.139926Z",
     "iopub.status.busy": "2023-11-19T14:24:29.139496Z",
     "iopub.status.idle": "2023-11-19T14:24:30.216800Z",
     "shell.execute_reply": "2023-11-19T14:24:30.214848Z",
     "shell.execute_reply.started": "2023-11-19T14:24:29.139898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757796c0-57bd-419f-828e-2d845e430877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T22:26:44.966788Z",
     "iopub.status.busy": "2023-11-23T22:26:44.966388Z",
     "iopub.status.idle": "2023-11-23T22:26:45.597270Z",
     "shell.execute_reply": "2023-11-23T22:26:45.596882Z",
     "shell.execute_reply.started": "2023-11-23T22:26:44.966753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            total_size += os.path.getsize(full_entry_path)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_used_cpu_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_memory = process.memory_info().rss\n",
    "    return process_memory\n",
    "\n",
    "def get_used_and_max_gpu_memory():\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    return used_memory,max_used_memory\n",
    "\n",
    "def reset_max_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62b88c-94fd-4557-a15a-45d6a01b6d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path,size = get_model_path_and_size_on_disk(model)\n",
    "print(f\"Model files: {(size/1024/1024/1024):.2f} GB stored in {path}\")\n",
    "print()\n",
    "cpu_memory = get_used_cpu_memory()\n",
    "print(f\"The current process uses {(cpu_memory/1024/1024/1024):.2f} GB of CPU memory\")\n",
    "print()\n",
    "gpu_memory, max_gpu_memory = get_used_and_max_gpu_memory()\n",
    "print(f\"The current process uses {(gpu_memory/1024/1024/1024):.2f} GB of GPU memory\")\n",
    "print(f\"The maximum use of GPU so far was {(gpu_memory/1024/1024/1024):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c95b2c-f905-477a-aa52-f685e848ff9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T22:26:46.895878Z",
     "iopub.status.busy": "2023-11-23T22:26:46.895516Z",
     "iopub.status.idle": "2023-11-23T22:26:46.947120Z",
     "shell.execute_reply": "2023-11-23T22:26:46.946709Z",
     "shell.execute_reply.started": "2023-11-23T22:26:46.895857Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000\n",
    "\n",
    "def get_tensor_params_size_and_dim(param):\n",
    "    if param is None:\n",
    "        return 0,\"\"\n",
    "    elif isinstance(param, torch.Tensor):\n",
    "        psize = param.numel() * param.element_size()\n",
    "        pdim = f\"{str(param.dtype)[6:]}{str(param.size())[11:-1]}\"\n",
    "        return psize,pdim\n",
    "    elif isinstance(param, dict):\n",
    "        size = 0\n",
    "        dim = \"\"\n",
    "        for value in param.values():            \n",
    "            psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "            size += psize\n",
    "            dim += pdim\n",
    "        return size, dim\n",
    "    else:\n",
    "        try:\n",
    "            iter(param)\n",
    "            size = 0\n",
    "            dim = \"\"\n",
    "            for value in param:            \n",
    "                psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "                size += psize\n",
    "                dim += pdim\n",
    "            return size, dim\n",
    "        except TypeError:\n",
    "            return 0,\"\"\n",
    "\n",
    "class ModulePerf:\n",
    "    \n",
    "    def __init__(self, module_name, module, is_leaf_module):\n",
    "        self.module_name = module_name\n",
    "        self.module = module\n",
    "        self.is_leaf_module = is_leaf_module\n",
    "        \n",
    "        self.before_forward_time_ns = 0\n",
    "        self.before_forward_used_memory = 0\n",
    "        self.forward_inputs_memory_size = 0 \n",
    "        self.forward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_forward_time_ns = 0\n",
    "        self.after_forward_used_memory = 0\n",
    "        self.forward_max_used_memory = 0        \n",
    "        self.forward_outputs_memory_size = 0\n",
    "        self.forward_outputs_memory_dim = \"\"\n",
    "        \n",
    "        self.before_backward_time_ns = 0\n",
    "        self.before_backward_used_memory = 0\n",
    "        self.backward_inputs_memory_size = 0\n",
    "        self.backward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_backward_time_ns = 0\n",
    "        self.after_backward_used_memory = 0\n",
    "        self.backward_max_used_memory = 0\n",
    "        self.backward_outputs_memory_size = 0\n",
    "        self.backward_outputs_memory_dim = \"\"\n",
    "        \n",
    "    def before_forward(self, module, args, kwargs):\n",
    "        self.before_forward_time_ns = perf_counter_ns()\n",
    "        self.before_forward_used_memory,_ = get_used_and_max_gpu_memory()   \n",
    "        args_size,args_dim = get_tensor_params_size_and_dim(args)\n",
    "        kwargs_size,kwargs_dim = get_tensor_params_size_and_dim(kwargs) \n",
    "        self.forward_inputs_memory_size = args_size + kwargs_size\n",
    "        self.forward_inputs_memory_dim = args_dim + kwargs_dim\n",
    "        if self.is_leaf_module: reset_max_gpu_memory()\n",
    "        \n",
    "    def after_forward(self, module, args, kwargs, output):\n",
    "        self.after_forward_time_ns = perf_counter_ns()\n",
    "        self.after_forward_used_memory, self.forward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.forward_outputs_memory_size, self.forward_outputs_memory_dim = get_tensor_params_size_and_dim(output) \n",
    "        \n",
    "    def before_backward(self, module, grad_output):\n",
    "        self.before_backward_time_ns = perf_counter_ns()\n",
    "        self.before_backward_used_memory,_ = get_used_and_max_gpu_memory()\n",
    "        self.backward_inputs_memory_size, self.backward_inputs_memory_dim = get_tensor_params_size_and_dim(grad_output) \n",
    "        \n",
    "    def after_backward(self, module, grad_input, grad_output):\n",
    "        self.after_backward_time_ns = perf_counter_ns()\n",
    "        self.after_backward_used_memory, self.backward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.backward_outputs_memory_size, self.backward_outputs_memory_dim = get_tensor_params_size_and_dim(grad_input) \n",
    "    \n",
    "    def get_stats_line(self, initial_used_memory):\n",
    "        return f\"{self.module_name};{self.is_leaf_module};;{(self.after_forward_time_ns-self.before_forward_time_ns)/time_unit_µs:.1f};;{self.forward_inputs_memory_dim};{self.forward_inputs_memory_size/memory_unit_mb:.1f};{(self.before_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.forward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.after_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.forward_outputs_memory_dim};{self.forward_outputs_memory_size/memory_unit_mb:.1f};;{(self.after_backward_time_ns-self.before_backward_time_ns)/time_unit_µs:.1f};;{self.backward_inputs_memory_dim};{(self.backward_inputs_memory_size-initial_used_memory)/memory_unit_mb:.1f};{(self.before_backward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.backward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.after_backward_used_memory/memory_unit_mb:.1f};{self.backward_outputs_memory_dim};{self.backward_outputs_memory_size/memory_unit_mb:.1f}\"\n",
    "    \n",
    "class ModelForCausalLMBenchmark:   \n",
    "    \n",
    "    @staticmethod\n",
    "    def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id)\n",
    "        AutoModelForCausalLM.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        print(\"--> Reset the kernel now before going further\")\n",
    "    \n",
    "    def __init__(self, pretrained_model_id):\n",
    "        self.pretrained_model_id = pretrained_model_id\n",
    "        self.tokenizer = None \n",
    "        self.model = None\n",
    "        \n",
    "        self.model_path = None\n",
    "        self.model_size_on_disk = 0\n",
    "        self.tokenizer_load_time_ns = 0\n",
    "        self.tokenizer_cpu_memory = 0\n",
    "        self.model_load_time_ns = 0\n",
    "        self.model_cpu_memory = 0\n",
    "        self.model_gpu_memory = 0\n",
    "        self.model_load_max_gpu_memory = 0\n",
    "        \n",
    "    def trace_load_from_cache(self, **kwargs):\n",
    "        cpu_memory_before = get_used_cpu_memory()\n",
    "        gpu_memory_before = get_used_and_max_gpu_memory()[0]\n",
    "        reset_max_gpu_memory()        \n",
    "        time_before = perf_counter_ns()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_id)\n",
    "        cpu_memory_tokenizer = get_used_cpu_memory()\n",
    "        time_tokenizer = perf_counter_ns()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_model = get_used_cpu_memory()\n",
    "        gpu_memory_model,max_gpu_memory_model = get_used_and_max_gpu_memory()     \n",
    "        time_model = perf_counter_ns()\n",
    "        \n",
    "        self.model_path,self.model_size_on_disk = get_model_path_and_size_on_disk(self.model)\n",
    "        self.tokenizer_load_time_ns = time_tokenizer-time_before\n",
    "        self.tokenizer_cpu_memory = cpu_memory_tokenizer-cpu_memory_before\n",
    "        self.model_load_time_ns = time_model-time_tokenizer\n",
    "        self.model_cpu_memory = cpu_memory_model-cpu_memory_tokenizer\n",
    "        self.model_gpu_memory = gpu_memory_model-gpu_memory_before\n",
    "        self.model_load_max_gpu_memory = max_gpu_memory_model\n",
    "        \n",
    "        self.display_load_results()            \n",
    "    \n",
    "    def display_load_results(self):\n",
    "        print(f\"Model files: {(self.model_size_on_disk/1024/1024/1024):.2f} GB on disk\")\n",
    "        print(\"\"f\"(cache path: {self.model_path})\")\n",
    "        print()\n",
    "        print(f\"Tokenizer load time : {(self.tokenizer_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Tokenizer CPU memory: {(self.tokenizer_cpu_memory/memory_unit_mb):.2f} MB\")\n",
    "        print()\n",
    "        print(f\"Model load time : {(self.model_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Model CPU memory: {(self.model_cpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Model GPU memory: {(self.model_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Max   GPU memory: {(self.model_load_max_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print()\n",
    "        \n",
    "    def trace_prefill(self, batch_size, seq_length):\n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "        \n",
    "        # measure perfs\n",
    "        moduleperfs = []\n",
    "        hookhandles = []\n",
    "        try:\n",
    "            for module_name,module in self.model.named_modules():\n",
    "                if module_name==\"\": module_name=\"<model>\"\n",
    "                mperf = ModulePerf(module_name, module, len(list(module.children())) == 0)\n",
    "                moduleperfs.append(mperf)                \n",
    "                hookhandles.append(module.register_forward_pre_hook(mperf.before_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_forward_hook(mperf.after_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_full_backward_pre_hook(mperf.before_backward))\n",
    "                hookhandles.append(module.register_full_backward_hook(mperf.after_backward))\n",
    "            \n",
    "            # perf test\n",
    "            input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "            attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "        finally:\n",
    "            for handle in hookhandles:\n",
    "                handle.remove()    \n",
    "                \n",
    "        # sort modules\n",
    "        sorted_moduleperfs = sorted(moduleperfs, key=lambda mp: mp.after_forward_time_ns)\n",
    "        first_mperf = None\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.before_forward_used_memory>0:\n",
    "                first_mperf = mperf\n",
    "                break\n",
    "        initial_used_memory = first_mperf.before_forward_used_memory - first_mperf.forward_inputs_memory_size\n",
    "        \n",
    "        # display results\n",
    "        print(f\"Prefill test for batch size {batch_size} and sequence length {seq_length}:\")\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.after_forward_time_ns>0:\n",
    "                print(mperf.get_stats_line(initial_used_memory))\n",
    "    \n",
    "    def check_prefill(self):\n",
    "        seq_length = self.tokenizer.model_max_length\n",
    "        batch_size = 1\n",
    "        \n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "        # perf test\n",
    "        base_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "        for batch_size in range(7,100):\n",
    "            print(f\"--- {batch_size} x {seq_length} ---\")\n",
    "            reset_max_gpu_memory()\n",
    "            initial_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "            input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "            attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "            before_forward_time_ns = perf_counter_ns()\n",
    "            with torch.no_grad():\n",
    "                self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "            after_forward_time_ns = perf_counter_ns()\n",
    "            gpu_memory, max_gpu_memory = get_used_and_max_gpu_memory()\n",
    "            print(f\"Forward pass  : {(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.1f} ms\")\n",
    "            print(f\"Initial memory  : {((initial_gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "            print(f\"Maximum memory: {((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "            print(f\"Final memory  : {((gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "    \n",
    "    def trace_generate(self):\n",
    "        return\n",
    "    \n",
    "    def trace_train(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49ec7ab-3280-46c9-a6a8-d6d4513c07d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:38:07.497380Z",
     "iopub.status.busy": "2023-11-19T16:38:07.496223Z",
     "iopub.status.idle": "2023-11-19T16:38:24.438368Z",
     "shell.execute_reply": "2023-11-19T16:38:24.426495Z",
     "shell.execute_reply.started": "2023-11-19T16:38:07.497299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model togethercomputer/RedPajama-INCITE-Base-3B-v1 in local cache ...\n",
      "--> Reset the kernel now before going further\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85524c6f-dced-412b-8852-1cd78ca99260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T22:26:48.958710Z",
     "iopub.status.busy": "2023-11-23T22:26:48.957542Z",
     "iopub.status.idle": "2023-11-23T22:26:52.895192Z",
     "shell.execute_reply": "2023-11-23T22:26:52.894656Z",
     "shell.execute_reply.started": "2023-11-23T22:26:48.958656Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 241.08 ms\n",
      "Tokenizer CPU memory: 43.74 MB\n",
      "\n",
      "Model load time : 3607.25 ms\n",
      "Model CPU memory: 0.35 GB\n",
      "Model GPU memory: 5.33 GB\n",
      "Max   GPU memory: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"cuda\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7d86f-040d-4af2-8c6c-6decdb8a894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.trace_prefill(2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0493ffec-98d8-4974-85dc-bbf419f824cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T22:26:52.895940Z",
     "iopub.status.busy": "2023-11-23T22:26:52.895813Z",
     "iopub.status.idle": "2023-11-23T22:27:24.664056Z",
     "shell.execute_reply": "2023-11-23T22:27:24.663520Z",
     "shell.execute_reply.started": "2023-11-23T22:26:52.895932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7 x 2048 ---\n",
      "Forward pass  : 1381.7 ms\n",
      "Initial memory  : 0.00 GB\n",
      "Maximum memory: 4.18 GB\n",
      "Final memory  : 0.00 GB\n",
      "--- 8 x 2048 ---\n",
      "Forward pass  : 1576.7 ms\n",
      "Initial memory  : 0.00 GB\n",
      "Maximum memory: 4.78 GB\n",
      "Final memory  : 0.00 GB\n",
      "--- 9 x 2048 ---\n",
      "Forward pass  : 1775.4 ms\n",
      "Initial memory  : 0.00 GB\n",
      "Maximum memory: 5.38 GB\n",
      "Final memory  : 0.00 GB\n",
      "--- 10 x 2048 ---\n",
      "Forward pass  : 10473.8 ms\n",
      "Initial memory  : 0.00 GB\n",
      "Maximum memory: 5.98 GB\n",
      "Final memory  : 0.00 GB\n",
      "--- 11 x 2048 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20676/4111493346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prefill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_20676/175166287.py\u001b[0m in \u001b[0;36mcheck_prefill\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mbefore_forward_time_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mafter_forward_time_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mgpu_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gpu_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_used_and_max_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         outputs = self.gpt_neox(\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 )\n\u001b[1;32m    670\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 outputs = layer(\n\u001b[0m\u001b[1;32m    672\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     ):\n\u001b[0;32m--> 440\u001b[0;31m         attention_layer_outputs = self.attention(\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Compute attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Reshape outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mmask_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bcae39-2ec1-4674-9872-bd07b944499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:28.932661Z",
     "iopub.status.busy": "2023-11-22T23:14:28.931840Z",
     "iopub.status.idle": "2023-11-22T23:14:28.935824Z",
     "shell.execute_reply": "2023-11-22T23:14:28.935080Z",
     "shell.execute_reply.started": "2023-11-22T23:14:28.932634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs['logits']\n",
    "past_key_values = outputs['past_key_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e6b133-7b78-48ac-ba06-938daf8a44f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:59.065051Z",
     "iopub.status.busy": "2023-11-22T23:14:59.064635Z",
     "iopub.status.idle": "2023-11-22T23:14:59.067809Z",
     "shell.execute_reply": "2023-11-22T23:14:59.067428Z",
     "shell.execute_reply.started": "2023-11-22T23:14:59.065034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 50432]), torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size(),logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915d0c6-9bae-47eb-9712-44a9de6892e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:12:26.134495Z",
     "iopub.status.busy": "2023-11-22T23:12:26.133905Z",
     "iopub.status.idle": "2023-11-22T23:12:26.141171Z",
     "shell.execute_reply": "2023-11-22T23:12:26.140688Z",
     "shell.execute_reply.started": "2023-11-22T23:12:26.134464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_key_values), past_key_values[0][0].size(), past_key_values[0][0].dtype, past_key_values[0][1].size(), past_key_values[0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d13c74a-e0c1-449e-b2fa-59943661c78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:56.688325Z",
     "iopub.status.busy": "2023-11-20T21:23:56.687590Z",
     "iopub.status.idle": "2023-11-20T21:23:57.046020Z",
     "shell.execute_reply": "2023-11-20T21:23:57.045602Z",
     "shell.execute_reply.started": "2023-11-20T21:23:56.688294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291d9889-5bb6-4635-8ec1-17dfd0439542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:57.100183Z",
     "iopub.status.busy": "2023-11-20T21:23:57.099421Z",
     "iopub.status.idle": "2023-11-20T21:23:57.109529Z",
     "shell.execute_reply": "2023-11-20T21:23:57.109088Z",
     "shell.execute_reply.started": "2023-11-20T21:23:57.100149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer([\"un test\",\"un deuxième test\"], padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf3dd39-1341-44a7-9119-85b4d1298d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:24:01.082171Z",
     "iopub.status.busy": "2023-11-20T21:24:01.081390Z",
     "iopub.status.idle": "2023-11-20T21:24:01.087945Z",
     "shell.execute_reply": "2023-11-20T21:24:01.087300Z",
     "shell.execute_reply.started": "2023-11-20T21:24:01.082139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  328,  1071,     0,     0,     0],\n",
       "        [  328, 23156,    74, 22722,  1071]]), 'attention_mask': tensor([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fe31a0-9b15-421f-b069-e67546d7ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:32:12.116422Z",
     "iopub.status.busy": "2023-11-20T21:32:12.116190Z",
     "iopub.status.idle": "2023-11-20T21:32:12.120878Z",
     "shell.execute_reply": "2023-11-20T21:32:12.120463Z",
     "shell.execute_reply.started": "2023-11-20T21:32:12.116408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"input_ids\"].size(), encodings[\"input_ids\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef91e00-7d3c-452f-9d93-ec2d707215d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:34:04.936740Z",
     "iopub.status.busy": "2023-11-20T21:34:04.936503Z",
     "iopub.status.idle": "2023-11-20T21:34:04.939242Z",
     "shell.execute_reply": "2023-11-20T21:34:04.938877Z",
     "shell.execute_reply.started": "2023-11-20T21:34:04.936731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"attention_mask\"].size(), encodings[\"attention_mask\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca42a0a-eabb-46a6-aa77-4e925029d168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:22.386686Z",
     "iopub.status.busy": "2023-11-20T21:41:22.385561Z",
     "iopub.status.idle": "2023-11-20T21:41:22.393072Z",
     "shell.execute_reply": "2023-11-20T21:41:22.392631Z",
     "shell.execute_reply.started": "2023-11-20T21:41:22.386648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 50432]), torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"logits\"].size(), output[\"logits\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e887edcc-6554-466d-970e-9a800e7193fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:02.274392Z",
     "iopub.status.busy": "2023-11-20T21:41:02.273974Z",
     "iopub.status.idle": "2023-11-20T21:41:02.281515Z",
     "shell.execute_reply": "2023-11-20T21:41:02.280358Z",
     "shell.execute_reply.started": "2023-11-20T21:41:02.274364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172ffae5-19b4-42e4-a072-57882f130641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:46:28.031823Z",
     "iopub.status.busy": "2023-11-20T21:46:28.031023Z",
     "iopub.status.idle": "2023-11-20T21:46:28.037420Z",
     "shell.execute_reply": "2023-11-20T21:46:28.036755Z",
     "shell.execute_reply.started": "2023-11-20T21:46:28.031790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, torch.Size([2, 32, 5, 80]), torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"past_key_values\"]),output[\"past_key_values\"][0][1].size(),output[\"past_key_values\"][0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67521338-e1f2-4ed2-bb0e-a7d4f77aef74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
