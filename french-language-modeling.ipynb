{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T08:29:50.671396Z",
     "iopub.status.busy": "2023-12-16T08:29:50.671223Z",
     "iopub.status.idle": "2023-12-16T08:29:51.115729Z",
     "shell.execute_reply": "2023-12-16T08:29:51.115304Z",
     "shell.execute_reply.started": "2023-12-16T08:29:50.671386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:53:40.369711Z",
     "iopub.status.busy": "2023-12-16T07:53:40.369287Z",
     "iopub.status.idle": "2023-12-16T07:53:40.381296Z",
     "shell.execute_reply": "2023-12-16T07:53:40.380948Z",
     "shell.execute_reply.started": "2023-12-16T07:53:40.369683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\",\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",      \n",
    "    \n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\",\n",
    "    \n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\",\n",
    "    \n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "    \n",
    "    \"mixtral_8x7B\" : \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbc2a24-55f1-4547-8a21-daaff40271d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:55:07.680138Z",
     "iopub.status.busy": "2023-12-16T07:55:07.679266Z",
     "iopub.status.idle": "2023-12-16T07:55:07.695674Z",
     "shell.execute_reply": "2023-12-16T07:55:07.694944Z",
     "shell.execute_reply.started": "2023-12-16T07:55:07.680093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "135024e9-d16d-499b-acb7-aca76d8e75cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T11:35:56.994402Z",
     "iopub.status.busy": "2023-12-16T11:35:56.993977Z",
     "iopub.status.idle": "2023-12-16T11:35:57.501587Z",
     "shell.execute_reply": "2023-12-16T11:35:57.500979Z",
     "shell.execute_reply.started": "2023-12-16T11:35:56.994374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( \"Qwen/Qwen-7B\", trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T11:33:04.257682Z",
     "iopub.status.busy": "2023-12-16T11:33:04.255680Z",
     "iopub.status.idle": "2023-12-16T11:33:10.880320Z",
     "shell.execute_reply": "2023-12-16T11:33:10.879884Z",
     "shell.execute_reply.started": "2023-12-16T11:33:04.257644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading phi2_3b tokenizer\n",
      "4837434508465586422\n",
      "{'type': <class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50257, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "8272226667653940756\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n",
      "-2792885599119712379\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "-2020660444933683167\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}]}}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "-5253717437475191100\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "-2792885599119712379\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "-5546107980889120355\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.ffe04dd57f85293043ba999a2c0daa788d6182e9.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 32768, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "-1357707804698952073\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading decilm_7b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading solar_10b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "-5546107980889120355\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.c4051215126d906ac22bb67fe5edb39a921cd831.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mixtral_8x7B tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "-2020660444933683167\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}]}}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "-5253717437475191100\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True, token=myhftoken)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    if model[:5]==\"qwen_\":\n",
    "        print(hash(tuple(sorted(tokenizer.mergeable_ranks.items()))))\n",
    "    else:\n",
    "        print(hash(tuple(sorted(tokenizer.vocab.items()))))\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    if model[:5]==\"qwen_\":\n",
    "         type(tokenizer.tokenizer)\n",
    "    else:\n",
    "        config[\"backend_type\"] = type(tokenizer.backend_tokenizer.model)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T08:30:08.487681Z",
     "iopub.status.busy": "2023-12-16T08:30:08.486702Z",
     "iopub.status.idle": "2023-12-16T10:54:36.991466Z",
     "shell.execute_reply": "2023-12-16T10:54:36.987356Z",
     "shell.execute_reply.started": "2023-12-16T08:30:08.487645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d664e99727466da664ce7c3e18f8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5886aff7371249988ade4ac4d96d7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec246a5dd914ab691af0f14465f3fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991b0cb843f042b3aded8dcf77be7fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728a5339fd5d4079ad522afdb5b23f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caba6e3932f349a7840ddfdf173f43b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb175c6f72c4553a8a522cc2db3b5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3094 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "phi2_3b: 50257 vocab => 2.329939242686227 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e803e05e141242fb80252a7e527ce7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10506 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4a51cac60f4585aef9463c51647f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfdfd0b114743a3b8d012338333315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d8b16db5544ab7bc2fb37674fede03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748aaf52c5224fd6ad77cce0b881678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612843758054166 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed020d9d85f47bea9ed5d5c3d06628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412dec40f92441b487e88f9560b71f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfcb7c161794325957528188e6baf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab62bce6b9e4574b13e83bdb1f1575b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc74ce5295e4cde942621de75c5e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09932ab593c4908b284452569f4f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5f209297bd450ba20cbe7c91ac5c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229fc3fadf414d56bf6353d56c5aeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112350 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79148d520c647f69aa89de137bb1987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a929b0e788243cc90adc335834a2511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322654c675d4a6daa6afedfd158f893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "decilm_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c1ffa70804c30979c637eb0ec7635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "solar_10b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e754d26fd004b8287ec1bca76af34a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b8ca6756b408b9467aa25c8eebcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f514c4fa334454ca2160174c0ff2064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mixtral_8x7B: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4433a090f3d74c62a02406a94ac050f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b16778005a04127ab7b6d29d71fcf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612843758054166 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0917cd54e4242499a7b5d89c7bf178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef2623-1568-44c9-b0e1-22d434c8d673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:11.007140Z",
     "iopub.status.busy": "2023-11-18T12:50:11.006435Z",
     "iopub.status.idle": "2023-11-18T12:50:11.012078Z",
     "shell.execute_reply": "2023-11-18T12:50:11.011446Z",
     "shell.execute_reply.started": "2023-11-18T12:50:11.007116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ea611a-0c97-45f6-aa47-4fe8474954b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:16.865613Z",
     "iopub.status.busy": "2023-11-18T12:50:16.864880Z",
     "iopub.status.idle": "2023-11-18T12:50:16.870681Z",
     "shell.execute_reply": "2023-11-18T12:50:16.869546Z",
     "shell.execute_reply.started": "2023-11-18T12:50:16.865578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = models[\"redpajama_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f28d43-e44b-49b2-a29f-20b103d1d1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:51.109646Z",
     "iopub.status.busy": "2023-11-17T23:49:51.109274Z",
     "iopub.status.idle": "2023-11-17T23:49:56.330791Z",
     "shell.execute_reply": "2023-11-17T23:49:56.330376Z",
     "shell.execute_reply.started": "2023-11-17T23:49:51.109630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc4ac7-3333-4310-bb4d-bc555b41ebf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:56.331914Z",
     "iopub.status.busy": "2023-11-17T23:49:56.331453Z",
     "iopub.status.idle": "2023-11-17T23:50:06.822516Z",
     "shell.execute_reply": "2023-11-17T23:50:06.821911Z",
     "shell.execute_reply.started": "2023-11-17T23:49:56.331902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb64419b9e54f1d9271e6a7034fe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8817b2eeb74d83a7ac4fc16bd30f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1290ddeb18645fa93e0fd5058c7949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a3a7bc743445fbe9feaef5900fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda35c0f10d4d15a9a0fbb3aa408b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7eb0a738c40e991f2de95404a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"frenchtext/banque-fr-2311\"\n",
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dbb014-c670-410e-88a6-219a3466770f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.823168Z",
     "iopub.status.busy": "2023-11-17T23:50:06.822994Z",
     "iopub.status.idle": "2023-11-17T23:50:06.826497Z",
     "shell.execute_reply": "2023-11-17T23:50:06.825982Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.823160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = dataset[0][\"Text\"]\n",
    "len(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896de44a-2e1c-45d3-87ce-527e6b3f38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.827305Z",
     "iopub.status.busy": "2023-11-17T23:50:06.827192Z",
     "iopub.status.idle": "2023-11-17T23:50:06.848030Z",
     "shell.execute_reply": "2023-11-17T23:50:06.847618Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.827297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text_example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fa350c-36ef-4615-9c2b-23b845880794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.848944Z",
     "iopub.status.busy": "2023-11-17T23:50:06.848554Z",
     "iopub.status.idle": "2023-11-17T23:50:08.534591Z",
     "shell.execute_reply": "2023-11-17T23:50:08.534182Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.848929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 23:50:07.766211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-17 23:50:07.786800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.375988006591797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenize_input = tokenizer.encode(text_example[:2048], return_tensors=\"pt\")\n",
    "    loss = model(tokenize_input, labels=tokenize_input)[0]\n",
    "torch.exp(loss.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c21247-3ea4-450f-b810-3699c1f9e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.535351Z",
     "iopub.status.busy": "2023-11-17T23:50:08.535080Z",
     "iopub.status.idle": "2023-11-17T23:50:08.541585Z",
     "shell.execute_reply": "2023-11-17T23:50:08.541199Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.535337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375\n",
      "3689\n",
      "3804\n",
      "2104\n",
      "7936\n",
      "3802\n",
      "10669\n",
      "2171\n",
      "3568\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(dataset[i][\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ad0bf0-41ec-4a90-98d7-db8ab7150b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.542513Z",
     "iopub.status.busy": "2023-11-17T23:50:08.542112Z",
     "iopub.status.idle": "2023-11-17T23:50:08.557331Z",
     "shell.execute_reply": "2023-11-17T23:50:08.556940Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.542501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c375df-43ab-44f7-a3e0-4ec139734e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.558032Z",
     "iopub.status.busy": "2023-11-17T23:50:08.557899Z",
     "iopub.status.idle": "2023-11-17T23:50:08.573642Z",
     "shell.execute_reply": "2023-11-17T23:50:08.573227Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.558022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text = dataset[0:10][\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73874f-5726-4e82-941f-ccf68c109291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.574256Z",
     "iopub.status.busy": "2023-11-17T23:50:08.574143Z",
     "iopub.status.idle": "2023-11-17T23:50:08.580661Z",
     "shell.execute_reply": "2023-11-17T23:50:08.580268Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.574248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048 1630\n",
      "1 2048 1160\n",
      "2 2048 1162\n",
      "3 2048 658\n",
      "4 2048 2048\n",
      "4 2048 1741\n",
      "5 2048 1175\n",
      "6 2048 2048\n",
      "6 2048 2048\n",
      "6 2048 1234\n",
      "7 2048 648\n",
      "8 2048 1072\n",
      "9 2048 585\n"
     ]
    }
   ],
   "source": [
    "for sample_mapping,input_ids,attention_mask in zip(encodings[\"overflow_to_sample_mapping\"],encodings[\"input_ids\"],encodings[\"attention_mask\"]):\n",
    "    print(sample_mapping,len(input_ids),sum(attention_mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b66afe1-835e-4f37-b749-9066805d35ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.581766Z",
     "iopub.status.busy": "2023-11-17T23:50:08.581571Z",
     "iopub.status.idle": "2023-11-17T23:50:08.588527Z",
     "shell.execute_reply": "2023-11-17T23:50:08.588112Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.581757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dataset = dataset.sort(\"Words\").filter(lambda example: example[\"Words\"]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f520b9-2e06-4158-ab39-476563ed75fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.589061Z",
     "iopub.status.busy": "2023-11-17T23:50:08.588968Z",
     "iopub.status.idle": "2023-11-17T23:50:08.591277Z",
     "shell.execute_reply": "2023-11-17T23:50:08.590863Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.589053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(batch_size=32):\n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294cc1d6-bb4a-4253-a249-3af30b0ee43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.591941Z",
     "iopub.status.busy": "2023-11-17T23:50:08.591730Z",
     "iopub.status.idle": "2023-11-17T23:50:08.594172Z",
     "shell.execute_reply": "2023-11-17T23:50:08.593798Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.591928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(dataset_batch):\n",
    "    return tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16,\n",
    "                      return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6f843d-e0ab-41db-b6ef-15334d943e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.594732Z",
     "iopub.status.busy": "2023-11-17T23:50:08.594630Z",
     "iopub.status.idle": "2023-11-17T23:50:08.614329Z",
     "shell.execute_reply": "2023-11-17T23:50:08.613824Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.594724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4,  6,  5,  7, 23,  7,  7,  7,  7,  7, 10,  7,  7, 13,  6,  4,  6,\n",
      "         6, 15,  4,  4,  4, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 32])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12,  6, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 16])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         6, 12, 12, 12,  7, 12, 12, 12, 12, 12, 12, 12, 12, 47])\n",
      "torch.Size([32, 48])\n",
      "tensor([45, 45, 44, 48, 55, 39, 51, 46, 43, 42, 40, 47, 41, 44, 47, 42, 40, 62,\n",
      "        43, 55, 34, 44, 42, 53, 53, 30, 56, 52, 51, 53, 37, 41])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for idx,dataset_batch in enumerate(get_dataset_batches()):\n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "    print(encodings[\"attention_mask\"].sum(axis=1))\n",
    "    print(encodings[\"input_ids\"].size())\n",
    "    if idx == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe85da1-182d-4d7b-8cac-452fb26bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nlls = []\n",
    "for idx,dataset_batch in enumerate(get_dataset_batches(batch_size)):\n",
    "    \n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    \n",
    "    if idx%10==0: print(f\"{(idx+1)*batch_size} / {len(dataset)}: {neg_log_likelihood}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74e49-e5f3-48c4-86ba-7f84fbdb6cfb",
   "metadata": {},
   "source": [
    "15 min\n",
    "\n",
    "32 / 85229: 7.71484375\n",
    "352 / 85229: 7.09765625\n",
    "672 / 85229: 8.1015625\n",
    "992 / 85229: 5.12109375\n",
    "1312 / 85229: 5.78125\n",
    "1632 / 85229: 5.51953125\n",
    "1952 / 85229: 6.12109375\n",
    "2272 / 85229: 3.36328125\n",
    "2592 / 85229: 5.26171875\n",
    "2912 / 85229: 5.0703125\n",
    "3232 / 85229: 4.87109375\n",
    "3552 / 85229: 4.74609375\n",
    "3872 / 85229: 4.046875\n",
    "4192 / 85229: 4.3046875\n",
    "4512 / 85229: 4.11328125\n",
    "4832 / 85229: 4.56640625\n",
    "5152 / 85229: 4.51171875\n",
    "5472 / 85229: 4.359375\n",
    "5792 / 85229: 4.32421875\n",
    "6112 / 85229: 4.68359375\n",
    "6432 / 85229: 4.6015625\n",
    "6752 / 85229: 3.658203125\n",
    "7072 / 85229: 4.7734375\n",
    "7392 / 85229: 6.6328125\n",
    "7712 / 85229: 3.869140625\n",
    "8032 / 85229: 4.9375\n",
    "8352 / 85229: 4.359375\n",
    "8672 / 85229: 5.0078125\n",
    "8992 / 85229: 4.09375\n",
    "9312 / 85229: 3.513671875\n",
    "9632 / 85229: 3.578125\n",
    "9952 / 85229: 3.609375\n",
    "10272 / 85229: 3.505859375\n",
    "10592 / 85229: 2.99609375\n",
    "10912 / 85229: 3.5859375\n",
    "11232 / 85229: 3.18359375\n",
    "11552 / 85229: 3.48046875\n",
    "11872 / 85229: 3.11328125\n",
    "12192 / 85229: 3.40234375\n",
    "12512 / 85229: 3.189453125\n",
    "12832 / 85229: 4.3515625\n",
    "13152 / 85229: 3.306640625\n",
    "13472 / 85229: 3.40234375\n",
    "13792 / 85229: 3.162109375\n",
    "14112 / 85229: 3.451171875\n",
    "14432 / 85229: 2.845703125\n",
    "14752 / 85229: 4.3515625\n",
    "15072 / 85229: 4.2890625\n",
    "15392 / 85229: 2.91015625\n",
    "15712 / 85229: 3.1328125\n",
    "16032 / 85229: 4.02734375\n",
    "16352 / 85229: 3.126953125\n",
    "16672 / 85229: 3.478515625\n",
    "16992 / 85229: 3.193359375\n",
    "17312 / 85229: 3.3203125\n",
    "17632 / 85229: 3.0078125\n",
    "\n",
    "OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6177ee4-eb93-44c3-a6ad-52784cc55742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T06:49:07.814995Z",
     "iopub.status.busy": "2023-11-18T06:49:07.814819Z",
     "iopub.status.idle": "2023-11-18T06:49:07.841774Z",
     "shell.execute_reply": "2023-11-18T06:49:07.841206Z",
     "shell.execute_reply.started": "2023-11-18T06:49:07.814984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.5841)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611711-1e58-46d7-84bf-8c4263ed78d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Memory study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5319-aca9-41ac-816e-1b65065ac3e5",
   "metadata": {},
   "source": [
    "## CUDA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585551-ae01-4dd9-a0d8-7bb4946bb196",
   "metadata": {},
   "source": [
    "[CUDA semantics / Memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management)\n",
    "\n",
    "[Understanding CUDA Memory Usage](https://pytorch.org/docs/stable/torch_cuda_memory.html#torch-cuda-memory)\n",
    "\n",
    "[CUDA memory management API](https://pytorch.org/docs/stable/cuda.html#cuda-memory-management-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d2a5f9-40a1-4fce-a106-c3f16cf58484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:55:37.303562Z",
     "iopub.status.busy": "2023-11-24T21:55:37.302740Z",
     "iopub.status.idle": "2023-11-24T21:55:38.016517Z",
     "shell.execute_reply": "2023-11-24T21:55:38.016087Z",
     "shell.execute_reply.started": "2023-11-24T21:55:37.303527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved :      0.0 MB -   0 %\n",
      "Free     : 23,008.0 MB -  93 %\n",
      "------------------------------\n",
      "Used     :      0.0 MB -   0 %\n",
      "Max used :      0.0 MB -   0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "\n",
    "# https://zdevito.github.io/2022/08/16/memory-snapshots.html\n",
    "# https://zdevito.github.io/2022/12/09/memory-traces.html\n",
    "\n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeea17f5-409d-4230-bab9-4010f3b6cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:55:15.162845Z",
     "iopub.status.busy": "2023-11-18T12:55:15.162447Z",
     "iopub.status.idle": "2023-11-18T12:55:15.167405Z",
     "shell.execute_reply": "2023-11-18T12:55:15.166833Z",
     "shell.execute_reply.started": "2023-11-18T12:55:15.162820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41785e96-8956-4aee-97d5-d0479c89eb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:56:49.681121Z",
     "iopub.status.busy": "2023-11-18T12:56:49.680290Z",
     "iopub.status.idle": "2023-11-18T12:56:55.117481Z",
     "shell.execute_reply": "2023-11-18T12:56:55.117059Z",
     "shell.execute_reply.started": "2023-11-18T12:56:49.681088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a128d7-8070-4de1-bf66-b6683881921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:00.726679Z",
     "iopub.status.busy": "2023-11-18T13:05:00.725839Z",
     "iopub.status.idle": "2023-11-18T13:05:00.739971Z",
     "shell.execute_reply": "2023-11-18T13:05:00.739570Z",
     "shell.execute_reply.started": "2023-11-18T13:05:00.726644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231118_130500.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22441-8d1d-43c8-90eb-abb78cd3fa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:06.460703Z",
     "iopub.status.busy": "2023-11-18T13:05:06.459810Z",
     "iopub.status.idle": "2023-11-18T13:05:06.468484Z",
     "shell.execute_reply": "2023-11-18T13:05:06.467915Z",
     "shell.execute_reply.started": "2023-11-18T13:05:06.460670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0d6c-289c-4e79-9c6b-c95c0d033581",
   "metadata": {},
   "source": [
    "## Pytorch models exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85be5910-7850-4787-b36c-203bdf93ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:40:02.824012Z",
     "iopub.status.busy": "2023-11-19T16:40:02.823531Z",
     "iopub.status.idle": "2023-11-19T16:40:02.826926Z",
     "shell.execute_reply": "2023-11-19T16:40:02.826475Z",
     "shell.execute_reply.started": "2023-11-19T16:40:02.823996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def find_attribute_origin(obj, attr_name):\n",
    "    for cls in obj.__class__.__mro__:\n",
    "        if attr_name in dir(cls):\n",
    "            return cls.__name__\n",
    "    return obj.__class__.__name__\n",
    "\n",
    "def display_members(obj):\n",
    "    obj_attributes = {}\n",
    "    for member_name in dir(obj):\n",
    "        if member_name[0:1]!=\"_\":\n",
    "            obj_attributes[getattr(obj,member_name).__qualname__ if hasattr(getattr(obj,member_name),\"__qualname__\") else f\"{find_attribute_origin(obj,member_name)}.{member_name}\"] = str(type(getattr(obj,member_name)))\n",
    "    obj_attributes = {k: obj_attributes[k] for k in sorted(obj_attributes)}\n",
    "    for member_name in obj_attributes.keys():\n",
    "        print(member_name, obj_attributes[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "663d0ad3-8085-4699-a6b4-c176eb76aca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T16:24:40.156700Z",
     "iopub.status.busy": "2023-11-18T16:24:40.156401Z",
     "iopub.status.idle": "2023-11-18T16:24:40.186241Z",
     "shell.execute_reply": "2023-11-18T16:24:40.185806Z",
     "shell.execute_reply.started": "2023-11-18T16:24:40.156689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXConfig <class 'type'>\n",
      "GPTNeoXForCausalLM.T_destination <class 'typing.TypeVar'>\n",
      "GPTNeoXForCausalLM.base_model <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.base_model_prefix <class 'str'>\n",
      "GPTNeoXForCausalLM.call_super_init <class 'bool'>\n",
      "GPTNeoXForCausalLM.config <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>\n",
      "GPTNeoXForCausalLM.device <class 'torch.device'>\n",
      "GPTNeoXForCausalLM.dtype <class 'torch.dtype'>\n",
      "GPTNeoXForCausalLM.dummy_inputs <class 'dict'>\n",
      "GPTNeoXForCausalLM.dump_patches <class 'bool'>\n",
      "GPTNeoXForCausalLM.embed_out <class 'torch.nn.modules.linear.Linear'>\n",
      "GPTNeoXForCausalLM.forward <class 'functools.partial'>\n",
      "GPTNeoXForCausalLM.framework <class 'str'>\n",
      "GPTNeoXForCausalLM.generation_config <class 'transformers.generation.configuration_utils.GenerationConfig'>\n",
      "GPTNeoXForCausalLM.get_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.gpt_neox <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.hf_device_map <class 'dict'>\n",
      "GPTNeoXForCausalLM.is_8bit_serializable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_4bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_8bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_parallelizable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_quantized <class 'bool'>\n",
      "GPTNeoXForCausalLM.main_input_name <class 'str'>\n",
      "GPTNeoXForCausalLM.name_or_path <class 'str'>\n",
      "GPTNeoXForCausalLM.prepare_inputs_for_generation <class 'method'>\n",
      "GPTNeoXForCausalLM.quantization_method <enum 'QuantizationMethod'>\n",
      "GPTNeoXForCausalLM.set_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.supports_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.training <class 'bool'>\n",
      "GPTNeoXForCausalLM.warnings_issued <class 'dict'>\n",
      "GenerationMixin.assisted_decoding <class 'method'>\n",
      "GenerationMixin.beam_sample <class 'method'>\n",
      "GenerationMixin.beam_search <class 'method'>\n",
      "GenerationMixin.compute_transition_scores <class 'method'>\n",
      "GenerationMixin.constrained_beam_search <class 'method'>\n",
      "GenerationMixin.contrastive_search <class 'method'>\n",
      "GenerationMixin.generate <class 'method'>\n",
      "GenerationMixin.greedy_search <class 'method'>\n",
      "GenerationMixin.group_beam_search <class 'method'>\n",
      "GenerationMixin.sample <class 'method'>\n",
      "Module.add_module <class 'method'>\n",
      "Module.apply <class 'method'>\n",
      "Module.bfloat16 <class 'method'>\n",
      "Module.buffers <class 'method'>\n",
      "Module.children <class 'method'>\n",
      "Module.cpu <class 'method'>\n",
      "Module.cuda <class 'function'>\n",
      "Module.double <class 'method'>\n",
      "Module.eval <class 'method'>\n",
      "Module.extra_repr <class 'method'>\n",
      "Module.get_buffer <class 'method'>\n",
      "Module.get_extra_state <class 'method'>\n",
      "Module.get_parameter <class 'method'>\n",
      "Module.get_submodule <class 'method'>\n",
      "Module.ipu <class 'method'>\n",
      "Module.load_state_dict <class 'method'>\n",
      "Module.modules <class 'method'>\n",
      "Module.named_buffers <class 'method'>\n",
      "Module.named_children <class 'method'>\n",
      "Module.named_modules <class 'method'>\n",
      "Module.named_parameters <class 'method'>\n",
      "Module.parameters <class 'method'>\n",
      "Module.register_backward_hook <class 'method'>\n",
      "Module.register_buffer <class 'method'>\n",
      "Module.register_forward_hook <class 'method'>\n",
      "Module.register_forward_pre_hook <class 'method'>\n",
      "Module.register_full_backward_hook <class 'method'>\n",
      "Module.register_full_backward_pre_hook <class 'method'>\n",
      "Module.register_load_state_dict_post_hook <class 'method'>\n",
      "Module.register_module <class 'method'>\n",
      "Module.register_parameter <class 'method'>\n",
      "Module.register_state_dict_pre_hook <class 'method'>\n",
      "Module.requires_grad_ <class 'method'>\n",
      "Module.set_extra_state <class 'method'>\n",
      "Module.share_memory <class 'method'>\n",
      "Module.state_dict <class 'method'>\n",
      "Module.to <class 'function'>\n",
      "Module.to_empty <class 'method'>\n",
      "Module.train <class 'method'>\n",
      "Module.type <class 'method'>\n",
      "Module.xpu <class 'method'>\n",
      "Module.zero_grad <class 'method'>\n",
      "ModuleUtilsMixin.add_memory_hooks <class 'method'>\n",
      "ModuleUtilsMixin.create_extended_attention_mask_for_decoder <class 'function'>\n",
      "ModuleUtilsMixin.estimate_tokens <class 'method'>\n",
      "ModuleUtilsMixin.floating_point_ops <class 'method'>\n",
      "ModuleUtilsMixin.get_extended_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.get_head_mask <class 'method'>\n",
      "ModuleUtilsMixin.invert_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.num_parameters <class 'method'>\n",
      "ModuleUtilsMixin.reset_memory_hooks_state <class 'method'>\n",
      "PeftAdapterMixin.active_adapter <class 'method'>\n",
      "PeftAdapterMixin.active_adapters <class 'method'>\n",
      "PeftAdapterMixin.add_adapter <class 'method'>\n",
      "PeftAdapterMixin.disable_adapters <class 'method'>\n",
      "PeftAdapterMixin.enable_adapters <class 'method'>\n",
      "PeftAdapterMixin.get_adapter_state_dict <class 'method'>\n",
      "PeftAdapterMixin.load_adapter <class 'method'>\n",
      "PeftAdapterMixin.set_adapter <class 'method'>\n",
      "PreTrainedModel.can_generate <class 'method'>\n",
      "PreTrainedModel.disable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.enable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.float <class 'method'>\n",
      "PreTrainedModel.from_pretrained <class 'method'>\n",
      "PreTrainedModel.get_input_embeddings <class 'method'>\n",
      "PreTrainedModel.get_memory_footprint <class 'method'>\n",
      "PreTrainedModel.get_position_embeddings <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_disable <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_enable <class 'method'>\n",
      "PreTrainedModel.half <class 'method'>\n",
      "PreTrainedModel.init_weights <class 'method'>\n",
      "PreTrainedModel.post_init <class 'method'>\n",
      "PreTrainedModel.prune_heads <class 'method'>\n",
      "PreTrainedModel.register_for_auto_class <class 'method'>\n",
      "PreTrainedModel.resize_position_embeddings <class 'method'>\n",
      "PreTrainedModel.resize_token_embeddings <class 'method'>\n",
      "PreTrainedModel.retrieve_modules_from_names <class 'method'>\n",
      "PreTrainedModel.reverse_bettertransformer <class 'method'>\n",
      "PreTrainedModel.save_pretrained <class 'method'>\n",
      "PreTrainedModel.set_input_embeddings <class 'method'>\n",
      "PreTrainedModel.tie_weights <class 'method'>\n",
      "PreTrainedModel.to_bettertransformer <class 'method'>\n",
      "PreTrainedModel.warn_if_padding_and_no_attention_mask <class 'method'>\n",
      "PushToHubMixin.push_to_hub <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "display_members(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd82911-00ea-48a4-80d2-709a0797818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:17:20.621530Z",
     "iopub.status.busy": "2023-11-27T21:17:20.621108Z",
     "iopub.status.idle": "2023-11-27T21:17:21.722775Z",
     "shell.execute_reply": "2023-11-27T21:17:21.722342Z",
     "shell.execute_reply.started": "2023-11-27T21:17:20.621499Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "56d76159-52cd-4a11-acae-b3461ce566b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:31:35.570614Z",
     "iopub.status.busy": "2023-11-19T07:31:35.569668Z",
     "iopub.status.idle": "2023-11-19T07:31:35.592081Z",
     "shell.execute_reply": "2023-11-19T07:31:35.591622Z",
     "shell.execute_reply.started": "2023-11-19T07:31:35.570571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "GPTNeoXForCausalLM\n",
      "> submodules\n",
      "- gpt_neox: GPTNeoXModel\n",
      "- embed_out: Linear\n",
      "  ---------------------\n",
      "  gpt_neox#GPTNeoXModel\n",
      "  > submodules\n",
      "  - embed_in: Embedding\n",
      "  - emb_dropout: Dropout\n",
      "  - layers: ModuleList\n",
      "  - final_layer_norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_in#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [50432, 2560] (246.2 MB)\n",
      "    ---------------------\n",
      "    emb_dropout#Dropout\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X GPTNeoXLayer\n",
      "      ---------------------\n",
      "      0..31#GPTNeoXLayer\n",
      "      > submodules\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "      - post_attention_dropout: Dropout\n",
      "      - post_mlp_dropout: Dropout\n",
      "      - attention: GPTNeoXAttention\n",
      "      - mlp: GPTNeoXMLP\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        post_mlp_dropout#Dropout\n",
      "        ---------------------\n",
      "        attention#GPTNeoXAttention\n",
      "        > buffers\n",
      "        - bias: bool [1, 1, 2048, 2048] (4.0 MB)\n",
      "        - masked_bias: float16 [] (0.0 MB)\n",
      "        > submodules\n",
      "        - rotary_emb: GPTNeoXRotaryEmbedding\n",
      "        - query_key_value: Linear8bitLt\n",
      "        - dense: Linear8bitLt\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          rotary_emb#GPTNeoXRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [40] (0.0 MB)\n",
      "          - cos_cached: float16 [2048, 80] (0.3 MB)\n",
      "          - sin_cached: float16 [2048, 80] (0.3 MB)\n",
      "          ---------------------\n",
      "          query_key_value#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [7680, 2560] (18.8 MB)\n",
      "          - bias: float16 [7680] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 2560] (6.2 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#GPTNeoXMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: Linear8bitLt\n",
      "        - dense_4h_to_h: Linear8bitLt\n",
      "        - act: GELUActivation\n",
      "          ---------------------\n",
      "          dense_h_to_4h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [10240, 2560] (25.0 MB)\n",
      "          - bias: float16 [10240] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense_4h_to_h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 10240] (25.0 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          act#GELUActivation\n",
      "    ---------------------\n",
      "    final_layer_norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: float16 [2560] (0.0 MB)\n",
      "    - bias: float16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  embed_out#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [50432, 2560] (246.2 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "GPTNeoXForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "            only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "            `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "        >>> import torch\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config.is_decoder = True\n",
      "        >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "        >>> outputs = model(**inputs)\n",
      "\n",
      "        >>> prediction_logits = outputs.logits\n",
      "        ```\"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.gpt_neox(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            past_key_values=past_key_values,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        lm_logits = self.embed_out(hidden_states)\n",
      "\n",
      "        lm_loss = None\n",
      "        if labels is not None:\n",
      "            # move labels to correct device to enable model parallelism\n",
      "            labels = labels.to(lm_logits.device)\n",
      "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
      "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
      "            labels = labels[:, 1:].contiguous()\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + outputs[1:]\n",
      "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=lm_loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "GPTNeoXModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPast,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \"\"\"\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "            input_shape = input_ids.size()\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        batch_size, seq_length = input_shape\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_length = 0\n",
      "            past_key_values = tuple([None] * self.config.num_hidden_layers)\n",
      "        else:\n",
      "            past_length = past_key_values[0][0].size(-2)\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        # Attention mask.\n",
      "        if attention_mask is not None:\n",
      "            assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "            attention_mask = attention_mask.view(batch_size, -1)\n",
      "            # We create a 3D attention mask from a 2D tensor mask.\n",
      "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
      "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
      "            # this attention mask is more simple than the triangular masking of causal attention\n",
      "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
      "            attention_mask = attention_mask[:, None, None, :]\n",
      "\n",
      "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "            # masked positions, this operation will create a tensor which is 0.0 for\n",
      "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
      "            # Since we are adding it to the raw scores before the softmax, this is\n",
      "            # effectively the same as removing these entirely.\n",
      "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
      "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
      "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_in(input_ids)\n",
      "\n",
      "        hidden_states = self.emb_dropout(inputs_embeds)\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        presents = () if use_cache else None\n",
      "        all_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    use_cache,\n",
      "                    None,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    layer_past=layer_past,\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                )\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "            if output_attentions:\n",
      "                all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        hidden_states = self.final_layer_norm(hidden_states)\n",
      "        # Add last hidden state\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        attention_layer_outputs = self.attention(\n",
      "            self.input_layernorm(hidden_states),\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            layer_past=layer_past,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "        )\n",
      "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
      "        attn_output = self.post_attention_dropout(attn_output)\n",
      "        outputs = attention_layer_outputs[1:]\n",
      "\n",
      "        if self.use_parallel_residual:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output + hidden_states\n",
      "        else:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x))\n",
      "            # x = x + mlp(ln2(x))\n",
      "            attn_output = attn_output + hidden_states\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
      "        else:\n",
      "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        has_layer_past = layer_past is not None\n",
      "\n",
      "        # Compute QKV\n",
      "        # Attention heads [batch, seq_len, hidden_size]\n",
      "        #   --> [batch, seq_len, (np * 3 * head_size)]\n",
      "        qkv = self.query_key_value(hidden_states)\n",
      "\n",
      "        # [batch, seq_len, (num_heads * 3 * head_size)]\n",
      "        #   --> [batch, seq_len, num_heads, 3 * head_size]\n",
      "        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
      "        qkv = qkv.view(*new_qkv_shape)\n",
      "\n",
      "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
      "        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
      "        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
      "        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
      "\n",
      "        # Compute rotary embeddings on rotary_ndims\n",
      "        query_rot = query[..., : self.rotary_ndims]\n",
      "        query_pass = query[..., self.rotary_ndims :]\n",
      "        key_rot = key[..., : self.rotary_ndims]\n",
      "        key_pass = key[..., self.rotary_ndims :]\n",
      "\n",
      "        # Compute token offset for rotary embeddings (when decoding)\n",
      "        seq_len = key.shape[-2]\n",
      "        if has_layer_past:\n",
      "            seq_len += layer_past[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
      "        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "        query = torch.cat((query, query_pass), dim=-1)\n",
      "        key = torch.cat((key, key_pass), dim=-1)\n",
      "\n",
      "        # Cache QKV values\n",
      "        if has_layer_past:\n",
      "            past_key = layer_past[0]\n",
      "            past_value = layer_past[1]\n",
      "            key = torch.cat((past_key, key), dim=-2)\n",
      "            value = torch.cat((past_value, value), dim=-2)\n",
      "        present = (key, value) if use_cache else None\n",
      "\n",
      "        # Compute attention\n",
      "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "\n",
      "        # Reshape outputs\n",
      "        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n",
      "        attn_output = self.dense(attn_output)\n",
      "\n",
      "        outputs = (attn_output, present)\n",
      "        if output_attentions:\n",
      "            outputs += (attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "GPTNeoXRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Linear8bitLt.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor):\n",
      "        self.state.is_training = self.training\n",
      "        if self.weight.CB is not None:\n",
      "            self.init_8bit_state()\n",
      "\n",
      "        # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
      "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
      "            self.bias.data = self.bias.data.to(x.dtype)\n",
      "\n",
      "        out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "\n",
      "        if not self.state.has_fp16_weights:\n",
      "            if self.state.CB is not None and self.state.CxB is not None:\n",
      "                # we converted 8-bit row major to turing/ampere format in the first inference pass\n",
      "                # we no longer need the row-major weight\n",
      "                del self.state.CB\n",
      "                self.weight.data = self.state.CxB\n",
      "        return out\n",
      "\n",
      "---------------------\n",
      "GPTNeoXMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "---------------------\n",
      "GELUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self.act(input)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72c72-5447-48ac-a895-4fda1ae5d6fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Huggingface language models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c480c-21f4-4e01-a4ad-4e2808fbd5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T22:14:40.901070Z",
     "iopub.status.busy": "2023-11-29T22:14:40.895922Z",
     "iopub.status.idle": "2023-11-29T22:14:40.964250Z",
     "shell.execute_reply": "2023-11-29T22:14:40.960213Z",
     "shell.execute_reply.started": "2023-11-29T22:14:40.900411Z"
    },
    "tags": []
   },
   "source": [
    "### Install flash attention\n",
    "\n",
    "https://pypi.org/project/flash-attn/\n",
    "\n",
    "See supported combinations of Pytorch / Python / Cuda versions for prebuilt wheels at:\n",
    "\n",
    "https://github.com/Dao-AILab/flash-attention/releases\n",
    "\n",
    "Run this commands as root to upgrade Pytorch 2.0 to Pytorch 2.1\n",
    "\n",
    "> pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5555e-7bb7-4502-863d-a2cbbe5f3db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d191215-bda3-4727-abb6-4218efd96e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60d14e0-b415-49f2-9066-303a18b7f8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:17:43.378089Z",
     "iopub.status.busy": "2023-12-01T22:17:43.377975Z",
     "iopub.status.idle": "2023-12-01T22:17:43.993456Z",
     "shell.execute_reply": "2023-12-01T22:17:43.992954Z",
     "shell.execute_reply.started": "2023-12-01T22:17:43.378080Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e511f42-0b16-4317-8882-cd38b2a9708e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:17:45.956174Z",
     "iopub.status.busy": "2023-12-01T22:17:45.955370Z",
     "iopub.status.idle": "2023-12-01T22:17:45.965456Z",
     "shell.execute_reply": "2023-12-01T22:17:45.965051Z",
     "shell.execute_reply.started": "2023-12-01T22:17:45.956137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flash_attn\n",
    "flash_attn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526914e-3ed9-454a-8005-aa3bf5b85594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T14:24:29.139926Z",
     "iopub.status.busy": "2023-11-19T14:24:29.139496Z",
     "iopub.status.idle": "2023-11-19T14:24:30.216800Z",
     "shell.execute_reply": "2023-11-19T14:24:30.214848Z",
     "shell.execute_reply.started": "2023-11-19T14:24:29.139898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c3cb8-66ec-4acb-b67a-674a58448856",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d293db7f-e956-44f1-8507-13438116cd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T07:52:42.612306Z",
     "iopub.status.busy": "2023-12-17T07:52:42.611510Z",
     "iopub.status.idle": "2023-12-17T07:52:43.297505Z",
     "shell.execute_reply": "2023-12-17T07:52:43.297094Z",
     "shell.execute_reply.started": "2023-12-17T07:52:42.612268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 2.1.1+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\n",
      "Python platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.2.128\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\n",
      "Nvidia driver version: 546.33\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Address sizes:                      46 bits physical, 48 bits virtual\n",
      "Byte Order:                         Little Endian\n",
      "CPU(s):                             24\n",
      "On-line CPU(s) list:                0-23\n",
      "Vendor ID:                          GenuineIntel\n",
      "Model name:                         13th Gen Intel(R) Core(TM) i7-13700K\n",
      "CPU family:                         6\n",
      "Model:                              183\n",
      "Thread(s) per core:                 2\n",
      "Core(s) per socket:                 12\n",
      "Socket(s):                          1\n",
      "Stepping:                           1\n",
      "BogoMIPS:                           6835.20\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm serialize flush_l1d arch_capabilities\n",
      "Virtualization:                     VT-x\n",
      "Hypervisor vendor:                  Microsoft\n",
      "Virtualization type:                full\n",
      "L1d cache:                          576 KiB (12 instances)\n",
      "L1i cache:                          384 KiB (12 instances)\n",
      "L2 cache:                           24 MiB (12 instances)\n",
      "L3 cache:                           30 MiB (1 instance)\n",
      "Vulnerability Gather data sampling: Not affected\n",
      "Vulnerability Itlb multihit:        Not affected\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Not affected\n",
      "Vulnerability Retbleed:             Mitigation; Enhanced IBRS\n",
      "Vulnerability Spec rstack overflow: Not affected\n",
      "Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==4.0.1\n",
      "[pip3] numpy==1.21.5\n",
      "[pip3] torch==2.1.1\n",
      "[pip3] torchaudio==2.1.1\n",
      "[pip3] torchvision==0.15.2\n",
      "[pip3] triton==2.1.0\n",
      "[conda] Could not collect\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import collect_env\n",
    "\n",
    "collect_env.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b47fdce-d106-461b-94df-774b3f5bdc14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T13:02:07.777551Z",
     "iopub.status.busy": "2023-12-16T13:02:07.775129Z",
     "iopub.status.idle": "2023-12-16T13:02:16.049125Z",
     "shell.execute_reply": "2023-12-16T13:02:16.048609Z",
     "shell.execute_reply.started": "2023-12-16T13:02:07.777310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Base-3B-v1\", torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "79bd4951-2c10-4677-9f43-ec626c4c744c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:52:34.927811Z",
     "iopub.status.busy": "2023-12-16T17:52:34.927115Z",
     "iopub.status.idle": "2023-12-16T17:52:34.934777Z",
     "shell.execute_reply": "2023-12-16T17:52:34.933998Z",
     "shell.execute_reply.started": "2023-12-16T17:52:34.927780Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8967c84-c3ff-4d1d-ac12-e053462ab2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T13:35:47.554163Z",
     "iopub.status.busy": "2023-12-16T13:35:47.553299Z",
     "iopub.status.idle": "2023-12-16T13:35:47.561719Z",
     "shell.execute_reply": "2023-12-16T13:35:47.561174Z",
     "shell.execute_reply.started": "2023-12-16T13:35:47.554131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import textwrap\n",
    "\n",
    "def find_function_calls(func):\n",
    "    if isinstance(func, types.BuiltinFunctionType):\n",
    "        raise \n",
    "    \n",
    "    source = inspect.getsource(func)\n",
    "    source = textwrap.dedent(source)\n",
    "    tree = ast.parse(source)\n",
    "    \n",
    "    function_calls = []\n",
    "\n",
    "    class FunctionCallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            function_calls.append(node)\n",
    "            #if isinstance(node.func, ast.Name):\n",
    "            #    function_calls.append(node.func.id)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    FunctionCallVisitor().visit(tree)\n",
    "    \n",
    "    function_calls_source = []\n",
    "    \n",
    "    for ast_call in function_calls:\n",
    "        function_calls_source.append(get_function_call_source(ast_call, source))\n",
    "    \n",
    "    return function_calls, function_calls_source\n",
    "\n",
    "def get_function_call_source(ast_call, source):\n",
    "    function_call_source = \"\"\n",
    "    \n",
    "    for idx,line in enumerate(source.splitlines()):\n",
    "        if idx+1 == ast_call.lineno:\n",
    "            if ast_call.end_lineno > ast_call.lineno:\n",
    "                function_call_source += line[ast_call.col_offset:].strip()\n",
    "            else:\n",
    "                function_call_source += line[ast_call.col_offset:ast_call.end_col_offset] \n",
    "        elif idx+1 > ast_call.lineno and idx+1 < ast_call.end_lineno:\n",
    "            function_call_source += \" \" + line.strip()\n",
    "        elif ast_call.end_lineno > ast_call.lineno and idx+1 == ast_call.end_lineno:\n",
    "            function_call_source += \" \" + line[:ast_call.end_col_offset].strip()\n",
    "        elif idx+1 > ast_call.end_lineno:\n",
    "            break\n",
    "    \n",
    "    return f\"[{ast_call.lineno},{ast_call.col_offset}...{ast_call.end_lineno},{ast_call.end_col_offset}] \" + function_call_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2a34e-7fad-4de7-ad21-5de475891779",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Fix to enbale CUDA profiling on WSL :\n",
    "\n",
    "https://github.com/pytorch/pytorch/issues/99615#issuecomment-1827386273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "288840d8-f10c-4e2a-8fd3-b872e7f18ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:59:02.144516Z",
     "iopub.status.busy": "2023-12-16T17:59:02.143859Z",
     "iopub.status.idle": "2023-12-16T17:59:02.150239Z",
     "shell.execute_reply": "2023-12-16T17:59:02.149586Z",
     "shell.execute_reply.started": "2023-12-16T17:59:02.144488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def profile_forward(model, batch_size, seq_length):\n",
    "    input_ids = torch.randint(low=0, high=32000, size=(batch_size,seq_length), dtype=torch.int64).to(model.device)\n",
    "    attention_mask = torch.ones(batch_size,seq_length).to(model.device)\n",
    "    model.eval()\n",
    "    with torch.profiler.profile(use_cuda=True, record_shapes=True, profile_memory=True, with_stack=True, with_flops=True, with_modules=True, experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        with torch.profiler.record_function(\"MODEL INFERENCE\"):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "    return prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "67d0205d-9e43-494f-be23-0df4beacd09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:11.557507Z",
     "iopub.status.busy": "2023-12-16T18:05:11.556703Z",
     "iopub.status.idle": "2023-12-16T18:05:34.355101Z",
     "shell.execute_reply": "2023-12-16T18:05:34.354679Z",
     "shell.execute_reply.started": "2023-12-16T18:05:11.557474Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-12-16 18:05:11 118:118 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-12-16 18:05:34 118:118 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-12-16 18:05:34 118:118 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "prof = profile_forward(model, 100, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d6850c32-dcf7-41b2-886d-dae266ed746f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:37.422046Z",
     "iopub.status.busy": "2023-12-16T18:05:37.420708Z",
     "iopub.status.idle": "2023-12-16T18:05:37.425494Z",
     "shell.execute_reply": "2023-12-16T18:05:37.424881Z",
     "shell.execute_reply.started": "2023-12-16T18:05:37.422009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "events = prof.events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "95d38b56-3927-40a6-bcb0-40676a9e8411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:39.167543Z",
     "iopub.status.busy": "2023-12-16T18:05:39.166860Z",
     "iopub.status.idle": "2023-12-16T18:05:39.173987Z",
     "shell.execute_reply": "2023-12-16T18:05:39.173515Z",
     "shell.execute_reply.started": "2023-12-16T18:05:39.167519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_profiler_event(event, show_code_lines=0):\n",
    "    csindex = 0\n",
    "    call_site = event.stack[csindex].split(\": \")[0]\n",
    "    while call_site.startswith(\"<built-in\") or call_site.startswith(\"torch/\"):\n",
    "        csindex += 1\n",
    "        call_site = event.stack[csindex].split(\": \")[0]\n",
    "        \n",
    "    if show_code_lines>0:\n",
    "        base_dir = \"/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/\"\n",
    "        relative_path = call_site.split(\"(\")[0]\n",
    "        line_number = int(call_site.split(\"(\")[1][:-1])\n",
    "        try:\n",
    "            with open(base_dir+relative_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                call_line = \"\"\n",
    "                for idx,line in enumerate(file_content.splitlines()):\n",
    "                    if idx>=line_number and idx<line_number+show_code_lines:\n",
    "                        if idx==line_number:\n",
    "                            call_line += \">>> \"\n",
    "                        else:\n",
    "                            call_line += f\"{idx} \"\n",
    "                        call_line += f\"{line}\\n\"\n",
    "        except:\n",
    "            call_line = \"\"\n",
    "    \n",
    "    filtered_stack = []\n",
    "    for frame in event.stack:\n",
    "        if frame.endswith(\"profile_forward\"):\n",
    "            break\n",
    "        elif not frame.startswith(\"<built-in\") and not frame.startswith(\"torch/\"):\n",
    "            function = frame.split(\": \")[1]\n",
    "            if not frame.startswith(\"nn.Module\") and not function==\"forward\":\n",
    "                function = function\n",
    "            filtered_stack.append(function)\n",
    "    filtered_stack.reverse()\n",
    "    call_stack = \".\".join(filtered_stack)\n",
    "\n",
    "    filtered_inputs = []\n",
    "    for input_shape in event.input_shapes:\n",
    "        if len(input_shape)>0:\n",
    "            filtered_inputs.append(input_shape)\n",
    "    \n",
    "    print(f\"- call stack: {call_stack}\")\n",
    "    print(f\"- operation : {event.name}\")\n",
    "    print(f\"- inputs    : {filtered_inputs}\")\n",
    "    print(f\"- cpu time  : {event.cpu_time}\")\n",
    "    print(f\"- cpu memory: {event.cpu_memory_usage}\")\n",
    "    print(f\"- gpu time  : {event.cuda_time}\")\n",
    "    print(f\"- gpu memory: {event.cuda_memory_usage}\")\n",
    "    print(f\"- flops     : {event.flops}\")    \n",
    "    if show_code_lines>0:\n",
    "        print(f\"- func src  : {call_site}\")\n",
    "        print(call_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0c2a713c-bba1-40e0-8600-0cf82148e630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T17:59:15.230693Z",
     "iopub.status.busy": "2023-12-16T17:59:15.230152Z",
     "iopub.status.idle": "2023-12-16T17:59:15.235392Z",
     "shell.execute_reply": "2023-12-16T17:59:15.234683Z",
     "shell.execute_reply.started": "2023-12-16T17:59:15.230670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FunctionEvent id=33831 name=aten::linear device_type=DeviceType.CPU node_id=-1 cpu_time=59.000us start_us=1147 end_us=1206 cpu_children=[33832, 33833, 33834, 33837] cuda_time=0.000us name=aten::linear thread=1 input_shapes=[[4, 1024, 2560], [7680, 2560], [7680]] cpu_memory_usage=0 cuda_memory_usage=62914560 is_async=False is_remote=False seq_nr=-1 is_legacy=False>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7b2dc305-c4a5-4379-b176-8ff2fcb7d51f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:05:46.896558Z",
     "iopub.status.busy": "2023-12-16T18:05:46.895419Z",
     "iopub.status.idle": "2023-12-16T18:05:46.910189Z",
     "shell.execute_reply": "2023-12-16T18:05:46.909500Z",
     "shell.execute_reply.started": "2023-12-16T18:05:46.896520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx,event in enumerate(events[1:10000]):\n",
    "    #if event.cpu_parent.id==events[0].id and (event.cpu_time+event.cuda_time)>=30:\n",
    "    if event.cuda_time>0:\n",
    "        print(idx+1)\n",
    "        print_profiler_event(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1bbf7898-cc79-40f5-9b18-85bb84d3bf24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:01:52.907929Z",
     "iopub.status.busy": "2023-12-16T18:01:52.907349Z",
     "iopub.status.idle": "2023-12-16T18:01:52.913696Z",
     "shell.execute_reply": "2023-12-16T18:01:52.913211Z",
     "shell.execute_reply.started": "2023-12-16T18:01:52.907896Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('concrete_inputs',\n",
       "  [None, [4, 32, 1024, 1024], [33554432, 1048576, 1024, 1], None]),\n",
       " ('count', 1),\n",
       " ('cpu_memory_usage', 0),\n",
       " ('cpu_time', 0.0),\n",
       " ('cpu_time_str', '0.000us'),\n",
       " ('cpu_time_total', 0),\n",
       " ('cpu_time_total_str', '0.000us'),\n",
       " ('cuda_memory_usage', 0),\n",
       " ('cuda_time', 0.0),\n",
       " ('cuda_time_str', '0.000us'),\n",
       " ('cuda_time_total', 0),\n",
       " ('cuda_time_total_str', '0.000us'),\n",
       " ('device_index', 118),\n",
       " ('device_type', <DeviceType.CPU: 0>),\n",
       " ('flops', 0),\n",
       " ('fwd_thread', 0),\n",
       " ('id', 38083),\n",
       " ('input_shapes', [[4, 32, 1024, 1024], [], [], []]),\n",
       " ('is_async', False),\n",
       " ('is_legacy', False),\n",
       " ('is_remote', False),\n",
       " ('kernels', []),\n",
       " ('key', 'aten::as_strided'),\n",
       " ('name', 'aten::as_strided'),\n",
       " ('node_id', -1),\n",
       " ('privateuse1_memory_usage', 0),\n",
       " ('privateuse1_time', 0.0),\n",
       " ('privateuse1_time_str', '0.000us'),\n",
       " ('privateuse1_time_total', 0),\n",
       " ('privateuse1_time_total_str', '0.000us'),\n",
       " ('scope', 0),\n",
       " ('self_cpu_memory_usage', 0),\n",
       " ('self_cpu_time_total', 0),\n",
       " ('self_cpu_time_total_str', '0.000us'),\n",
       " ('self_cuda_memory_usage', 0),\n",
       " ('self_cuda_time_total', 0),\n",
       " ('self_cuda_time_total_str', '0.000us'),\n",
       " ('self_privateuse1_memory_usage', 0),\n",
       " ('self_privateuse1_time_total', 0),\n",
       " ('self_privateuse1_time_total_str', '0.000us'),\n",
       " ('sequence_nr', -1),\n",
       " ('stack',\n",
       "  ['<built-in method matmul of type object at 0x7fb3d6298a40>',\n",
       "   'transformers/models/gpt_neox/modeling_gpt_neox.py(228): _attn',\n",
       "   'transformers/models/gpt_neox/modeling_gpt_neox.py(140): forward',\n",
       "   'torch/nn/modules/module.py(1520): _call_impl',\n",
       "   'nn.Module: GPTNeoXAttention_25',\n",
       "   'transformers/models/gpt_neox/modeling_gpt_neox.py(430): forward',\n",
       "   'torch/nn/modules/module.py(1520): _call_impl',\n",
       "   'nn.Module: GPTNeoXLayer_25',\n",
       "   'transformers/models/gpt_neox/modeling_gpt_neox.py(554): forward',\n",
       "   'torch/nn/modules/module.py(1520): _call_impl',\n",
       "   'nn.Module: GPTNeoXModel_0',\n",
       "   'transformers/models/gpt_neox/modeling_gpt_neox.py(723): forward',\n",
       "   'torch/nn/modules/module.py(1520): _call_impl',\n",
       "   'nn.Module: GPTNeoXForCausalLM_0',\n",
       "   '/tmp/ipykernel_118/795888963.py(5): profile_forward',\n",
       "   '/tmp/ipykernel_118/3532100087.py(1): <module>',\n",
       "   'IPython/core/interactiveshell.py(3457): run_code',\n",
       "   'IPython/core/interactiveshell.py(3377): run_ast_nodes',\n",
       "   'IPython/core/interactiveshell.py(3185): run_cell_async',\n",
       "   'IPython/core/async_helpers.py(78): _pseudo_sync_runner',\n",
       "   'IPython/core/interactiveshell.py(2960): _run_cell',\n",
       "   'IPython/core/interactiveshell.py(2914): run_cell',\n",
       "   'ipykernel/zmqshell.py(532): run_cell',\n",
       "   'ipykernel/ipkernel.py(353): do_execute',\n",
       "   'ipykernel/kernelbase.py(652): execute_request',\n",
       "   'ipykernel/kernelbase.py(357): dispatch_shell',\n",
       "   'ipykernel/kernelbase.py(450): process_one',\n",
       "   'ipykernel/kernelbase.py(461): dispatch_queue',\n",
       "   'asyncio/events.py(80): _run',\n",
       "   'asyncio/base_events.py(1909): _run_once',\n",
       "   'asyncio/base_events.py(603): run_forever',\n",
       "   'tornado/platform/asyncio.py(215): start',\n",
       "   'ipykernel/kernelapp.py(677): start',\n",
       "   'traitlets/config/application.py(1043): launch_instance',\n",
       "   'ipykernel_launcher.py(16): <module>',\n",
       "   'runpy.py(86): _run_code',\n",
       "   'runpy.py(196): _run_module_as_main']),\n",
       " ('thread', 1),\n",
       " ('time_range', <torch.autograd.profiler_util.Interval at 0x7fb226d9c6d0>),\n",
       " ('trace_name', 'aten::as_strided'),\n",
       " ('use_device', None)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event0 = events[4290]\n",
    "[(attr,getattr(event0,attr)) for attr in dir(event0) if not attr.startswith(\"__\") and not attr==\"cpu_children\" and not attr==\"cpu_parent\" and not callable(getattr(event0, attr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b21b728-de1f-47d2-884c-9dcfe780361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:22:59.203254Z",
     "iopub.status.busy": "2023-11-25T13:22:59.201912Z",
     "iopub.status.idle": "2023-11-25T13:22:59.260171Z",
     "shell.execute_reply": "2023-11-25T13:22:59.259714Z",
     "shell.execute_reply.started": "2023-11-25T13:22:59.203213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Script: '/usr/lib/python3/dist-packages/torch/nn/modules/linear.py' <SameEnvironment: 3.10.12 in /workspace/wordslab-llms/.venv>>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import jedi\n",
    "\n",
    "classobj = torch.nn.modules.Linear\n",
    "script = jedi.Script(path=inspect.getfile(classobj))\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a87b7371-faa8-45cd-9212-cfe1db3e713b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:42:30.005791Z",
     "iopub.status.busy": "2023-11-25T13:42:30.005055Z",
     "iopub.status.idle": "2023-11-25T13:42:30.017508Z",
     "shell.execute_reply": "2023-11-25T13:42:30.017186Z",
     "shell.execute_reply.started": "2023-11-25T13:42:30.005760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Name full_name='torch.nn.modules.linear.Linear.forward.linear', description='linear'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall = None\n",
    "\n",
    "for name in script.get_names(all_scopes=True, definitions=True, references=True):\n",
    "    if name.full_name == \"torch.nn.modules.linear.Linear.forward.linear\":\n",
    "        myfunctioncall = name\n",
    "        break\n",
    "\n",
    "myfunctioncall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0501354c-ee1d-44cc-a191-47a03f6b7af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:48:57.920380Z",
     "iopub.status.busy": "2023-11-25T13:48:57.919635Z",
     "iopub.status.idle": "2023-11-25T13:48:57.927712Z",
     "shell.execute_reply": "2023-11-25T13:48:57.926688Z",
     "shell.execute_reply.started": "2023-11-25T13:48:57.920331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name full_name='torch.nn.functional.linear', description='linear = _add_docstr( torch._C._nn.linear, r\"\"\" linear(input, weight, bias=None) -> Tensor Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. This operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>` {sparse_beta_warning} This operator supports :ref:`TensorFloat32<tf32_on_ampere>`. Shape: - Input: :math:`(*, in\\\\_features)` where `*` means any number of additional dimensions, including none - Weight: :math:`(out\\\\_features, in\\\\_features)` or :math:`(in\\\\_features)` - Bias: :math:`(out\\\\_features)` or :math:`()` - Output: :math:`(*, out\\\\_features)` or :math:`(*)`, based on the shape of the weight \"\"\".format(**sparse_support_notes))'>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall.goto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "432af689-ab81-406a-b4ec-d6bf5c860b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:43:31.131568Z",
     "iopub.status.busy": "2023-11-25T13:43:31.130642Z",
     "iopub.status.idle": "2023-11-25T13:43:31.140520Z",
     "shell.execute_reply": "2023-11-25T13:43:31.139446Z",
     "shell.execute_reply.started": "2023-11-25T13:43:31.131514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attr for attr in dir(myfunctioncall) if not callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunctioncall) if callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "634c5e15-19dc-4cd4-9d98-273bc123aef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:54:02.333093Z",
     "iopub.status.busy": "2023-11-25T13:54:02.332278Z",
     "iopub.status.idle": "2023-11-25T13:54:02.343608Z",
     "shell.execute_reply": "2023-11-25T13:54:02.343184Z",
     "shell.execute_reply.started": "2023-11-25T13:54:02.333056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function torch._C._nn.linear>, True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "called_function = eval(script.search(\"F.linear\")[0].full_name)\n",
    "called_function, isinstance(called_function, types.BuiltinFunctionType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01f57ae2-097b-4ccf-9c53-122b97363826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:09.977323Z",
     "iopub.status.busy": "2023-11-25T13:17:09.976180Z",
     "iopub.status.idle": "2023-11-25T13:17:09.986413Z",
     "shell.execute_reply": "2023-11-25T13:17:09.985735Z",
     "shell.execute_reply.started": "2023-11-25T13:17:09.977274Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://jedi.readthedocs.io/en/latest/docs/api-classes.html#name\n",
    "\n",
    "myfunction = None\n",
    "\n",
    "for function in script.get_names(all_scopes=True):\n",
    "    if function.name == \"forward\" and function.type == 'function':\n",
    "        myfunction = function\n",
    "        break\n",
    "        \n",
    "[attr for attr in dir(myfunction) if not callable(getattr(myfunction, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunction) if callable(getattr(myfunction, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "916f8095-b82e-4f35-9a70-adf9afb71aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:21:10.665309Z",
     "iopub.status.busy": "2023-11-25T13:21:10.664447Z",
     "iopub.status.idle": "2023-11-25T13:21:10.673674Z",
     "shell.execute_reply": "2023-11-25T13:21:10.672333Z",
     "shell.execute_reply.started": "2023-11-25T13:21:10.665259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name name='self', description='param self'>,\n",
       " <Name name='input', description='param input: Tensor'>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e226688-03ec-4a9b-8e5b-7d5383215253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:29.900165Z",
     "iopub.status.busy": "2023-11-25T13:17:29.899925Z",
     "iopub.status.idle": "2023-11-25T13:17:29.903541Z",
     "shell.execute_reply": "2023-11-25T13:17:29.903082Z",
     "shell.execute_reply.started": "2023-11-25T13:17:29.900152Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Call at 0x7f23381ad4e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_call = called_functions[0]\n",
    "ast_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "374e43f5-7461-49e9-9f9e-ce48991df618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:41:06.795032Z",
     "iopub.status.busy": "2023-11-25T12:41:06.794448Z",
     "iopub.status.idle": "2023-11-25T12:41:06.800139Z",
     "shell.execute_reply": "2023-11-25T12:41:06.799738Z",
     "shell.execute_reply.started": "2023-11-25T12:41:06.795005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F.linear'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inspect.getsource(torch.nn.modules.Linear.forward)\n",
    "source = textwrap.dedent(source)\n",
    "function_called = \"\"\n",
    "for idx,line in enumerate(source.splitlines()):\n",
    "    if idx+1 == ast_func.lineno:\n",
    "        if ast_func.end_lineno > ast_func.lineno:\n",
    "            function_called += line[ast_func.col_offset:].strip()\n",
    "        else:\n",
    "            function_called += line[ast_func.col_offset:ast_func.end_col_offset] \n",
    "    elif idx+1 > ast_func.lineno and idx+1 < ast_func.end_lineno:\n",
    "        function_called += \" \" + line.strip()\n",
    "    elif ast_func.end_lineno > ast_func.lineno and idx+1 == ast_func.end_lineno:\n",
    "        function_called += \" \" + line[:ast_func.end_col_offset].strip()\n",
    "    elif idx+1 > ast_func.end_lineno:\n",
    "        break\n",
    "        \n",
    "function_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9287d570-50bf-4676-bd3b-18992ae4b7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:45:56.280459Z",
     "iopub.status.busy": "2023-11-25T11:45:56.280108Z",
     "iopub.status.idle": "2023-11-25T11:45:56.285847Z",
     "shell.execute_reply": "2023-11-25T11:45:56.285195Z",
     "shell.execute_reply.started": "2023-11-25T11:45:56.280437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ast.Attribute at 0x7f23381ad5d0>,\n",
       " [<ast.Name at 0x7f23381aec20>,\n",
       "  <ast.Attribute at 0x7f23381ae4d0>,\n",
       "  <ast.Attribute at 0x7f23381ae290>],\n",
       " [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func = ast_call.func\n",
    "ast_args = ast_call.args\n",
    "ast_keywords = ast_call.keywords\n",
    "(ast_func,ast_args,ast_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3637b890-a087-4b96-8042-9f9e2d47f96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:14:06.116817Z",
     "iopub.status.busy": "2023-11-25T12:14:06.116420Z",
     "iopub.status.idle": "2023-11-25T12:14:06.123582Z",
     "shell.execute_reply": "2023-11-25T12:14:06.123077Z",
     "shell.execute_reply.started": "2023-11-25T12:14:06.116790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F', 'linear')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func.value.id, ast_func.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3c05174-7530-4616-bad1-5cb1d757a0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:19:56.383491Z",
     "iopub.status.busy": "2023-11-25T12:19:56.382989Z",
     "iopub.status.idle": "2023-11-25T12:19:56.390702Z",
     "shell.execute_reply": "2023-11-25T12:19:56.390280Z",
     "shell.execute_reply.started": "2023-11-25T12:19:56.383469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input', 'self', 'weight', 'self', 'bias')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_args[0].id,  ast_args[1].value.id, ast_args[1].attr, ast_args[2].value.id, ast_args[2].attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e86d8aa-616c-4b74-9959-444e3a46933d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:20:09.664297Z",
     "iopub.status.busy": "2023-11-25T12:20:09.663902Z",
     "iopub.status.idle": "2023-11-25T12:20:09.666740Z",
     "shell.execute_reply": "2023-11-25T12:20:09.666369Z",
     "shell.execute_reply.started": "2023-11-25T12:20:09.664282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9847189a-0abb-41d7-9c50-648b313104b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:31:04.364054Z",
     "iopub.status.busy": "2023-11-25T12:31:04.363488Z",
     "iopub.status.idle": "2023-11-25T12:31:04.368369Z",
     "shell.execute_reply": "2023-11-25T12:31:04.367844Z",
     "shell.execute_reply.started": "2023-11-25T12:31:04.364015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_attributes',\n",
       " '_fields',\n",
       " 'attr',\n",
       " 'col_offset',\n",
       " 'ctx',\n",
       " 'end_col_offset',\n",
       " 'end_lineno',\n",
       " 'lineno',\n",
       " 'value']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = ast_func\n",
    "[attr for attr in dir(obj) if not callable(getattr(obj, attr)) and not attr.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238de63e-d6f1-4d6e-8a49-f25dc14fd621",
   "metadata": {},
   "source": [
    "### Ipyexperiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38f749-d8fe-46ad-8efe-1d08d696ebe0",
   "metadata": {},
   "source": [
    "https://github.com/stas00/ipyexperiments/blob/master/README.md\n",
    "\n",
    "> pip install ipyexperiments\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "exp1 = IPyExperimentsPytorch()\n",
    "...\n",
    "exp1.keep_var_names('var1', 'var2')\n",
    "\n",
    "# optional\n",
    "data = exp1.finish()\n",
    "cpu_data_final = data.cpu\n",
    "gpu_data_final = data.gpu\n",
    "\n",
    "del exp1\n",
    "```\n",
    "\n",
    "Detailed syntax:\n",
    "\n",
    "```python\n",
    "exp = IPyExperimentsPytorch(cl_enable=True, cl_compact=False, cl_gc_collect=True, cl_set_seed=0)\n",
    "```\n",
    "\n",
    "- cl_enable - enable the subsystem\n",
    "- cl_compact - use compact one line printouts\n",
    "- cl_gc_collect - get correct memory usage reports. Don't use when tracking memory leaks (objects with circular reference).\n",
    "- cl_set_seed - set RNG seed before each cell is run to the provided seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56717d6-49ea-4e04-8f68-59dc6b92fac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T00:57:06.047940Z",
     "iopub.status.busy": "2023-12-02T00:57:06.047664Z",
     "iopub.status.idle": "2023-12-02T00:57:07.488516Z",
     "shell.execute_reply": "2023-12-02T00:57:07.487924Z",
     "shell.execute_reply.started": "2023-12-02T00:57:06.047929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipyexperiments in ./.venv/lib/python3.10/site-packages (0.1.28)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from ipyexperiments) (11.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipyexperiments) (5.9.5)\n",
      "Requirement already satisfied: ipython in /usr/lib/python3/dist-packages (from ipyexperiments) (7.31.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab18c80d-f840-40b2-b17e-de2167fa9a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:51:53.322350Z",
     "iopub.status.busy": "2023-12-09T12:51:53.321890Z",
     "iopub.status.idle": "2023-12-09T12:51:54.012097Z",
     "shell.execute_reply": "2023-12-09T12:51:54.011527Z",
     "shell.execute_reply.started": "2023-12-09T12:51:53.322306Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757796c0-57bd-419f-828e-2d845e430877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:51:54.012907Z",
     "iopub.status.busy": "2023-12-09T12:51:54.012739Z",
     "iopub.status.idle": "2023-12-09T12:51:54.100475Z",
     "shell.execute_reply": "2023-12-09T12:51:54.099603Z",
     "shell.execute_reply.started": "2023-12-09T12:51:54.012898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "# https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html\n",
    "# Need to test the line below with Pytorch 2.1\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"roundup_power2_divisions:4\"\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            total_size += os.path.getsize(full_entry_path)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_used_cpu_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_memory = process.memory_info().rss\n",
    "    return process_memory\n",
    "\n",
    "def get_used_and_max_gpu_memory():\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    return used_memory,max_used_memory\n",
    "\n",
    "def reset_max_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "\n",
    "# https://zdevito.github.io/2022/08/16/memory-snapshots.html\n",
    "# https://zdevito.github.io/2022/12/09/memory-traces.html\n",
    "\n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c95b2c-f905-477a-aa52-f685e848ff9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:51:54.211205Z",
     "iopub.status.busy": "2023-12-09T12:51:54.210958Z",
     "iopub.status.idle": "2023-12-09T12:51:54.259964Z",
     "shell.execute_reply": "2023-12-09T12:51:54.259329Z",
     "shell.execute_reply.started": "2023-12-09T12:51:54.211193Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000\n",
    "\n",
    "def get_tensor_params_size_and_dim(param):\n",
    "    if param is None:\n",
    "        return 0,\"\"\n",
    "    elif isinstance(param, torch.Tensor):\n",
    "        psize = param.numel() * param.element_size()\n",
    "        pdim = f\"{str(param.dtype)[6:]}{str(param.size())[11:-1]}\"\n",
    "        return psize,pdim\n",
    "    elif isinstance(param, dict):\n",
    "        size = 0\n",
    "        dim = \"\"\n",
    "        for value in param.values():            \n",
    "            psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "            size += psize\n",
    "            dim += pdim\n",
    "        return size, dim\n",
    "    else:\n",
    "        try:\n",
    "            iter(param)\n",
    "            size = 0\n",
    "            dim = \"\"\n",
    "            for value in param:            \n",
    "                psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "                size += psize\n",
    "                dim += pdim\n",
    "            return size, dim\n",
    "        except TypeError:\n",
    "            return 0,\"\"\n",
    "\n",
    "class ModulePerf:\n",
    "    \n",
    "    def __init__(self, module_name, module, is_leaf_module):\n",
    "        self.module_name = module_name\n",
    "        self.module = module\n",
    "        self.is_leaf_module = is_leaf_module\n",
    "        \n",
    "        self.before_forward_time_ns = 0\n",
    "        self.before_forward_used_memory = 0\n",
    "        self.forward_inputs_memory_size = 0 \n",
    "        self.forward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_forward_time_ns = 0\n",
    "        self.after_forward_used_memory = 0\n",
    "        self.forward_max_used_memory = 0        \n",
    "        self.forward_outputs_memory_size = 0\n",
    "        self.forward_outputs_memory_dim = \"\"\n",
    "        \n",
    "        self.before_backward_time_ns = 0\n",
    "        self.before_backward_used_memory = 0\n",
    "        self.backward_inputs_memory_size = 0\n",
    "        self.backward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_backward_time_ns = 0\n",
    "        self.after_backward_used_memory = 0\n",
    "        self.backward_max_used_memory = 0\n",
    "        self.backward_outputs_memory_size = 0\n",
    "        self.backward_outputs_memory_dim = \"\"\n",
    "        \n",
    "    def before_forward(self, module, args, kwargs):\n",
    "        self.before_forward_time_ns = perf_counter_ns()\n",
    "        self.before_forward_used_memory,_ = get_used_and_max_gpu_memory()   \n",
    "        args_size,args_dim = get_tensor_params_size_and_dim(args)\n",
    "        kwargs_size,kwargs_dim = get_tensor_params_size_and_dim(kwargs) \n",
    "        self.forward_inputs_memory_size = args_size + kwargs_size\n",
    "        self.forward_inputs_memory_dim = args_dim + kwargs_dim\n",
    "        if self.is_leaf_module: reset_max_gpu_memory()\n",
    "        \n",
    "    def after_forward(self, module, args, kwargs, output):\n",
    "        self.after_forward_time_ns = perf_counter_ns()\n",
    "        self.after_forward_used_memory, self.forward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.forward_outputs_memory_size, self.forward_outputs_memory_dim = get_tensor_params_size_and_dim(output) \n",
    "        \n",
    "    def before_backward(self, module, grad_output):\n",
    "        self.before_backward_time_ns = perf_counter_ns()\n",
    "        self.before_backward_used_memory,_ = get_used_and_max_gpu_memory()\n",
    "        self.backward_inputs_memory_size, self.backward_inputs_memory_dim = get_tensor_params_size_and_dim(grad_output) \n",
    "        \n",
    "    def after_backward(self, module, grad_input, grad_output):\n",
    "        self.after_backward_time_ns = perf_counter_ns()\n",
    "        self.after_backward_used_memory, self.backward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.backward_outputs_memory_size, self.backward_outputs_memory_dim = get_tensor_params_size_and_dim(grad_input) \n",
    "    \n",
    "    def get_stats_line(self, initial_used_memory):\n",
    "        return f\"{self.module_name};{self.is_leaf_module};;{(self.after_forward_time_ns-self.before_forward_time_ns)/time_unit_µs:.1f};;{self.forward_inputs_memory_dim};{self.forward_inputs_memory_size/memory_unit_mb:.1f};{(self.before_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.forward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.after_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.forward_outputs_memory_dim};{self.forward_outputs_memory_size/memory_unit_mb:.1f};;{(self.after_backward_time_ns-self.before_backward_time_ns)/time_unit_µs:.1f};;{self.backward_inputs_memory_dim};{(self.backward_inputs_memory_size-initial_used_memory)/memory_unit_mb:.1f};{(self.before_backward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.backward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.after_backward_used_memory/memory_unit_mb:.1f};{self.backward_outputs_memory_dim};{self.backward_outputs_memory_size/memory_unit_mb:.1f}\"\n",
    "    \n",
    "class ModelForCausalLMBenchmark:   \n",
    "    \n",
    "    @staticmethod\n",
    "    def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        path,size = get_model_path_and_size_on_disk(model)\n",
    "        print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"--> stored in directory: {path}\")\n",
    "        print()\n",
    "    \n",
    "    def __init__(self, pretrained_model_id):\n",
    "        self.pretrained_model_id = pretrained_model_id\n",
    "        self.tokenizer = None \n",
    "        self.model = None\n",
    "        \n",
    "        self.model_path = None\n",
    "        self.model_size_on_disk = 0\n",
    "        self.tokenizer_load_time_ns = 0\n",
    "        self.tokenizer_cpu_memory = 0\n",
    "        self.model_load_time_ns = 0\n",
    "        self.model_cpu_memory = 0\n",
    "        self.model_gpu_memory = 0\n",
    "        self.model_load_max_gpu_memory = 0\n",
    "        \n",
    "    def trace_load_from_cache(self, **kwargs):\n",
    "        cpu_memory_before = get_used_cpu_memory()\n",
    "        gpu_memory_before = get_used_and_max_gpu_memory()[0]\n",
    "        reset_max_gpu_memory()        \n",
    "        time_before = perf_counter_ns()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_tokenizer = get_used_cpu_memory()\n",
    "        time_tokenizer = perf_counter_ns()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_model = get_used_cpu_memory()\n",
    "        gpu_memory_model,max_gpu_memory_model = get_used_and_max_gpu_memory()     \n",
    "        time_model = perf_counter_ns()\n",
    "        \n",
    "        self.model_path,self.model_size_on_disk = get_model_path_and_size_on_disk(self.model)\n",
    "        self.tokenizer_load_time_ns = time_tokenizer-time_before\n",
    "        self.tokenizer_cpu_memory = cpu_memory_tokenizer-cpu_memory_before\n",
    "        self.model_load_time_ns = time_model-time_tokenizer\n",
    "        self.model_cpu_memory = cpu_memory_model-cpu_memory_tokenizer\n",
    "        self.model_gpu_memory = gpu_memory_model-gpu_memory_before\n",
    "        self.model_load_max_gpu_memory = max_gpu_memory_model\n",
    "        \n",
    "        self.display_load_results()            \n",
    "    \n",
    "    def display_load_results(self):\n",
    "        print(f\"Model files: {(self.model_size_on_disk/1024/1024/1024):.2f} GB on disk\")\n",
    "        print(\"\"f\"(cache path: {self.model_path})\")\n",
    "        print()\n",
    "        print(f\"Tokenizer load time : {(self.tokenizer_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Tokenizer CPU memory: {(self.tokenizer_cpu_memory/memory_unit_mb):.2f} MB\")\n",
    "        print()\n",
    "        print(f\"Model load time : {(self.model_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Model CPU memory: {(self.model_cpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Model GPU memory: {(self.model_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Max   GPU memory: {(self.model_load_max_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print()\n",
    "        \n",
    "    def trace_prefill(self, batch_size, seq_length):\n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "        \n",
    "        # measure perfs\n",
    "        moduleperfs = []\n",
    "        hookhandles = []\n",
    "        try:\n",
    "            for module_name,module in self.model.named_modules():\n",
    "                if module_name==\"\": module_name=\"<model>\"\n",
    "                mperf = ModulePerf(module_name, module, len(list(module.children())) == 0)\n",
    "                moduleperfs.append(mperf)                \n",
    "                hookhandles.append(module.register_forward_pre_hook(mperf.before_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_forward_hook(mperf.after_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_full_backward_pre_hook(mperf.before_backward))\n",
    "                hookhandles.append(module.register_full_backward_hook(mperf.after_backward))\n",
    "            \n",
    "            # perf test\n",
    "            input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "            attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "        finally:\n",
    "            for handle in hookhandles:\n",
    "                handle.remove()    \n",
    "                \n",
    "        # sort modules\n",
    "        sorted_moduleperfs = sorted(moduleperfs, key=lambda mp: mp.after_forward_time_ns)\n",
    "        first_mperf = None\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.before_forward_used_memory>0:\n",
    "                first_mperf = mperf\n",
    "                break\n",
    "        initial_used_memory = first_mperf.before_forward_used_memory - first_mperf.forward_inputs_memory_size\n",
    "        \n",
    "        # display results\n",
    "        print(f\"Prefill test for batch size {batch_size} and sequence length {seq_length}:\")\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.after_forward_time_ns>0:\n",
    "                print(mperf.get_stats_line(initial_used_memory))\n",
    "    \n",
    "    def check_prefill(self, max_batch_size):\n",
    "        seq_length = self.tokenizer.model_max_length\n",
    "        batch_size = 1\n",
    "        \n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(\"cuda\")\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(\"cuda\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "        # perf test\n",
    "        base_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "        seq_length = 128\n",
    "        while seq_length <= self.tokenizer.model_max_length:\n",
    "            for batch_size in range(1,max_batch_size+1):\n",
    "                #print(f\"--- {batch_size} x {seq_length} ---\")\n",
    "                reset_max_gpu_memory()\n",
    "                initial_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "                input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "                attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "                before_forward_time_ns = perf_counter_ns()\n",
    "                with torch.no_grad():\n",
    "                    self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "                after_forward_time_ns = perf_counter_ns()\n",
    "                before_release = perf_counter_ns()\n",
    "                # https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "                # see \"expandable_segments\": Pytorch allocator doesn't work when we gradually increase batch size !\n",
    "                # when inferencing with a constant batch size, this should not be needed\n",
    "                release_cached_memory()\n",
    "                after_release = perf_counter_ns()            \n",
    "                gpu_memory, max_gpu_memory = get_used_and_max_gpu_memory()\n",
    "                #print(f\"Forward pass  : {(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.1f} ms\")\n",
    "                #print(f\"Initial memory  : {((initial_gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Maximum memory: {((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Final memory  : {((gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"+ GPU cache release  : {(after_release-before_release)/time_unit_ms:.1f} ms\")\n",
    "                print(f\"{batch_size},{seq_length},{batch_size*seq_length/(after_forward_time_ns-before_forward_time_ns)*time_unit_s:.2f},{(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.2f},{(initial_gpu_memory/memory_unit_gb):.2f},{((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f},{(max_gpu_memory/memory_unit_gb):.2f}\")\n",
    "            seq_length *= 2\n",
    "    \n",
    "    def trace_generate(self):\n",
    "        return\n",
    "    \n",
    "    def trace_train(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e2e674-839c-4675-8589-13f17e33b6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:51:54.877830Z",
     "iopub.status.busy": "2023-12-09T12:51:54.877343Z",
     "iopub.status.idle": "2023-12-09T12:51:54.881371Z",
     "shell.execute_reply": "2023-12-09T12:51:54.880998Z",
     "shell.execute_reply.started": "2023-12-09T12:51:54.877813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipyexperiments import IPyExperimentsPytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5635-beae-4071-9a4b-c9707160f30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Redpajama-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faf697d-93d3-4712-88ff-b524e9e0ac42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:16:09.894748Z",
     "iopub.status.busy": "2023-11-25T18:16:09.894043Z",
     "iopub.status.idle": "2023-11-25T18:16:30.782608Z",
     "shell.execute_reply": "2023-11-25T18:16:30.781887Z",
     "shell.execute_reply.started": "2023-11-25T18:16:09.894725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     187  14,273  15,837 MB   1.18% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n",
      "Loading model togethercomputer/RedPajama-INCITE-Base-3B-v1 in local cache ...\n",
      "--> model files size   : 5.30 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:19 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      993     -111 MB (-11.25%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,293  13,175  15,837 MB   8.17% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f082f-d604-4a13-a8e8-198824f5723f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78895cd2-c609-4c2b-b772-26084c30bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd6f6f-2c07-4126-a332-919716b51166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7d86f-040d-4af2-8c6c-6decdb8a894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.trace_prefill(20, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8559f3-f244-41c6-9f88-bca7ff7ca1f6",
   "metadata": {},
   "source": [
    "For MPT-3B, the line of code which triggers the maximum memory is this one:\n",
    "\n",
    "transformers/models/gpt_neox/modeling_gpt_neox.py\n",
    "```\n",
    "def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "...\n",
    "    attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    " \n",
    "attn_weights is a very large matrix of size: float16[20, 32, 2048, 2048] => 5120 MB of memory.\n",
    "\n",
    "On this line we need to allocate it twice.\n",
    "An inplace softmax would divide the memory requirements by a factor of 2 !\n",
    "\n",
    "https://lernapparat.de/pytorch-inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48c2cb-ff83-43dc-8fdd-b69b7dd24b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5011ed-6662-4dd1-825a-20a0f4584e31",
   "metadata": {},
   "source": [
    "## 16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cc6fab-c238-4ecd-8cd2-fdfe20baf888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:20:43.832126Z",
     "iopub.status.busy": "2023-11-25T18:20:43.831302Z",
     "iopub.status.idle": "2023-11-25T18:20:43.885587Z",
     "shell.execute_reply": "2023-11-25T18:20:43.885119Z",
     "shell.execute_reply.started": "2023-11-25T18:20:43.832094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,303  13,105  15,837 MB   8.23% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85524c6f-dced-412b-8852-1cd78ca99260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:21:20.689065Z",
     "iopub.status.busy": "2023-11-25T18:21:20.687548Z",
     "iopub.status.idle": "2023-11-25T18:21:28.427278Z",
     "shell.execute_reply": "2023-11-25T18:21:28.426716Z",
     "shell.execute_reply.started": "2023-11-25T18:21:20.689015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 403.71 ms\n",
      "Tokenizer CPU memory: 24.60 MB\n",
      "\n",
      "Model load time : 7301.54 ms\n",
      "Model CPU memory: 0.48 GB\n",
      "Model GPU memory: 5.33 GB\n",
      "Max   GPU memory: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeae00e8-b62a-4307-9fe8-c98b5af4551d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:22:15.150822Z",
     "iopub.status.busy": "2023-11-25T18:22:15.150032Z",
     "iopub.status.idle": "2023-11-25T18:24:42.179098Z",
     "shell.execute_reply": "2023-11-25T18:24:42.178685Z",
     "shell.execute_reply.started": "2023-11-25T18:22:15.150789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,26.00,0.01\n",
      "2,128,21.64,0.03\n",
      "3,128,23.94,0.04\n",
      "4,128,26.08,0.05\n",
      "5,128,33.23,0.06\n",
      "6,128,37.86,0.08\n",
      "7,128,44.32,0.09\n",
      "8,128,44.16,0.10\n",
      "9,128,51.76,0.11\n",
      "10,128,57.60,0.13\n",
      "11,128,61.57,0.14\n",
      "12,128,78.21,0.15\n",
      "13,128,70.41,0.16\n",
      "14,128,75.92,0.18\n",
      "15,128,79.01,0.19\n",
      "16,128,83.47,0.20\n",
      "17,128,89.58,0.22\n",
      "18,128,95.11,0.23\n",
      "19,128,99.08,0.24\n",
      "20,128,104.85,0.25\n",
      "21,128,111.69,0.27\n",
      "22,128,130.38,0.28\n",
      "23,128,120.73,0.29\n",
      "24,128,124.92,0.30\n",
      "25,128,132.10,0.32\n",
      "26,128,140.93,0.33\n",
      "27,128,141.85,0.34\n",
      "28,128,148.08,0.35\n",
      "29,128,154.12,0.37\n",
      "1,256,31.73,0.03\n",
      "2,256,28.20,0.05\n",
      "3,256,39.70,0.08\n",
      "4,256,45.57,0.10\n",
      "5,256,57.65,0.13\n",
      "6,256,66.39,0.15\n",
      "7,256,90.81,0.18\n",
      "8,256,88.43,0.20\n",
      "9,256,102.81,0.23\n",
      "10,256,115.68,0.25\n",
      "11,256,127.42,0.28\n",
      "12,256,138.54,0.30\n",
      "13,256,152.49,0.33\n",
      "14,256,163.05,0.35\n",
      "15,256,174.82,0.38\n",
      "16,256,185.17,0.40\n",
      "17,256,200.74,0.43\n",
      "18,256,211.15,0.46\n",
      "19,256,224.28,0.48\n",
      "20,256,235.55,0.51\n",
      "21,256,249.25,0.53\n",
      "22,256,262.17,0.56\n",
      "23,256,279.91,0.58\n",
      "24,256,284.02,0.61\n",
      "25,256,298.43,0.63\n",
      "26,256,348.39,0.66\n",
      "27,256,331.03,0.68\n",
      "28,256,336.68,0.71\n",
      "29,256,349.37,0.73\n",
      "1,512,30.82,0.05\n",
      "2,512,49.98,0.11\n",
      "3,512,79.85,0.17\n",
      "4,512,108.43,0.22\n",
      "5,512,136.74,0.28\n",
      "6,512,162.38,0.34\n",
      "7,512,190.85,0.39\n",
      "8,512,215.60,0.45\n",
      "9,512,245.34,0.50\n",
      "10,512,271.71,0.56\n",
      "11,512,301.73,0.62\n",
      "12,512,328.14,0.67\n",
      "13,512,356.85,0.72\n",
      "14,512,390.02,0.79\n",
      "15,512,415.09,0.84\n",
      "16,512,440.04,0.89\n",
      "17,512,474.15,0.95\n",
      "18,512,502.06,1.01\n",
      "19,512,524.08,1.06\n",
      "20,512,552.86,1.11\n",
      "21,512,584.60,1.17\n",
      "22,512,607.31,1.23\n",
      "23,512,634.40,1.28\n",
      "24,512,661.45,1.34\n",
      "25,512,694.51,1.39\n",
      "26,512,719.78,1.46\n",
      "27,512,752.86,1.51\n",
      "28,512,769.67,1.56\n",
      "29,512,803.23,1.61\n",
      "1,1024,81.24,0.17\n",
      "2,1024,140.51,0.35\n",
      "3,1024,207.61,0.53\n",
      "4,1024,275.44,0.70\n",
      "5,1024,348.17,0.88\n",
      "6,1024,443.23,1.04\n",
      "7,1024,494.68,1.22\n",
      "8,1024,559.51,1.39\n",
      "9,1024,637.32,1.57\n",
      "10,1024,715.64,1.74\n",
      "11,1024,772.57,1.92\n",
      "12,1024,861.62,2.09\n",
      "13,1024,914.35,2.27\n",
      "14,1024,980.50,2.43\n",
      "15,1024,1050.63,2.62\n",
      "16,1024,1119.38,2.78\n",
      "17,1024,1191.98,2.96\n",
      "18,1024,1264.71,3.13\n",
      "19,1024,1329.68,3.31\n",
      "20,1024,1408.70,3.48\n",
      "21,1024,1510.00,3.66\n",
      "22,1024,1568.26,3.82\n",
      "23,1024,1635.51,4.01\n",
      "24,1024,1681.92,4.17\n",
      "25,1024,1753.95,4.35\n",
      "26,1024,1823.89,4.52\n",
      "27,1024,1894.38,4.70\n",
      "28,1024,1961.38,4.87\n",
      "29,1024,2033.24,5.05\n",
      "1,2048,198.67,0.59\n",
      "2,2048,404.08,1.20\n",
      "3,2048,596.81,1.79\n",
      "4,2048,794.71,2.39\n",
      "5,2048,997.44,2.99\n",
      "6,2048,1203.87,3.59\n",
      "7,2048,1397.73,4.18\n",
      "8,2048,1593.36,4.78\n",
      "9,2048,1793.75,5.38\n",
      "10,2048,1990.36,5.98\n",
      "11,2048,2190.12,6.57\n",
      "12,2048,2387.64,7.17\n",
      "13,2048,2587.98,7.77\n",
      "14,2048,2796.69,8.37\n",
      "15,2048,2987.08,8.96\n",
      "16,2048,3208.74,9.56\n",
      "17,2048,3395.49,10.16\n",
      "18,2048,3589.17,10.76\n",
      "19,2048,3809.31,11.36\n",
      "20,2048,4016.76,11.95\n",
      "21,2048,4213.65,12.55\n",
      "22,2048,4410.30,13.15\n",
      "23,2048,4610.12,13.75\n",
      "24,2048,4811.09,14.34\n",
      "25,2048,5012.10,14.94\n",
      "26,2048,5216.95,15.54\n",
      "27,2048,5413.27,16.14\n",
      "28,2048,5626.83,16.73\n",
      "29,2048,5895.19,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354f0cbc-031d-46ef-bfc3-fe77c1f48008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:24:58.160134Z",
     "iopub.status.busy": "2023-11-25T18:24:58.158780Z",
     "iopub.status.idle": "2023-11-25T18:24:58.256042Z",
     "shell.execute_reply": "2023-11-25T18:24:58.255606Z",
     "shell.execute_reply.started": "2023-11-25T18:24:58.160096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:04:14 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 2046 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      576        0 MB (  0.00%)\n",
      "GPU:    5,296    5,524 MB (104.29%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,604  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e94de-0941-4598-9d82-16fac43b8f28",
   "metadata": {},
   "source": [
    "## 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1462911-a827-4c0a-a996-bed393c311bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:25:57.106319Z",
     "iopub.status.busy": "2023-11-25T18:25:57.105737Z",
     "iopub.status.idle": "2023-11-25T18:25:57.160960Z",
     "shell.execute_reply": "2023-11-25T18:25:57.160534Z",
     "shell.execute_reply.started": "2023-11-25T18:25:57.106301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,601  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbcbf67-c0e1-4d19-9e75-6743faeb0dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:25.556750Z",
     "iopub.status.busy": "2023-11-25T18:26:25.555375Z",
     "iopub.status.idle": "2023-11-25T18:26:29.675190Z",
     "shell.execute_reply": "2023-11-25T18:26:29.674654Z",
     "shell.execute_reply.started": "2023-11-25T18:26:25.556698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 234.35 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 3867.51 ms\n",
      "Model CPU memory: 0.43 GB\n",
      "Model GPU memory: 3.06 GB\n",
      "Max   GPU memory: 3.07 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b05918-fd0f-4a58-b4d3-77dc494eff25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:59.213176Z",
     "iopub.status.busy": "2023-11-25T18:26:59.212633Z",
     "iopub.status.idle": "2023-11-25T18:36:48.697127Z",
     "shell.execute_reply": "2023-11-25T18:36:48.696617Z",
     "shell.execute_reply.started": "2023-11-25T18:26:59.213156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:26:59.999616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-25 18:27:00.165015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,139.23,0.01\n",
      "2,128,126.06,0.03\n",
      "3,128,150.38,0.04\n",
      "4,128,128.92,0.06\n",
      "5,128,148.06,0.07\n",
      "6,128,99.86,0.09\n",
      "7,128,110.90,0.10\n",
      "8,128,112.90,0.11\n",
      "9,128,146.82,0.13\n",
      "10,128,98.36,0.14\n",
      "11,128,118.09,0.15\n",
      "12,128,162.51,0.17\n",
      "13,128,125.73,0.18\n",
      "14,128,131.17,0.20\n",
      "15,128,181.03,0.21\n",
      "16,128,150.02,0.22\n",
      "17,128,168.77,0.23\n",
      "18,128,170.73,0.25\n",
      "19,128,159.60,0.26\n",
      "20,128,188.03,0.28\n",
      "21,128,192.24,0.29\n",
      "22,128,243.53,0.30\n",
      "23,128,175.19,0.32\n",
      "24,128,192.78,0.33\n",
      "25,128,215.48,0.35\n",
      "26,128,227.16,0.36\n",
      "27,128,235.21,0.37\n",
      "28,128,236.02,0.39\n",
      "29,128,240.61,0.40\n",
      "1,256,119.39,0.03\n",
      "2,256,144.90,0.06\n",
      "3,256,136.77,0.08\n",
      "4,256,112.83,0.11\n",
      "5,256,153.43,0.14\n",
      "6,256,144.35,0.16\n",
      "7,256,158.68,0.20\n",
      "8,256,160.82,0.22\n",
      "9,256,190.68,0.25\n",
      "10,256,199.55,0.28\n",
      "11,256,210.79,0.30\n",
      "12,256,223.55,0.33\n",
      "13,256,232.53,0.36\n",
      "14,256,258.61,0.39\n",
      "15,256,258.73,0.41\n",
      "16,256,283.96,0.44\n",
      "17,256,284.48,0.47\n",
      "18,256,277.84,0.50\n",
      "19,256,302.58,0.53\n",
      "20,256,322.53,0.55\n",
      "21,256,342.52,0.58\n",
      "22,256,335.89,0.61\n",
      "23,256,360.73,0.63\n",
      "24,256,382.35,0.66\n",
      "25,256,388.89,0.69\n",
      "26,256,396.49,0.72\n",
      "27,256,419.27,0.74\n",
      "28,256,433.78,0.77\n",
      "29,256,454.33,0.80\n",
      "1,512,126.69,0.05\n",
      "2,512,110.98,0.11\n",
      "3,512,134.16,0.17\n",
      "4,512,173.40,0.22\n",
      "5,512,194.66,0.28\n",
      "6,512,252.41,0.34\n",
      "7,512,282.69,0.39\n",
      "8,512,298.69,0.45\n",
      "9,512,330.79,0.50\n",
      "10,512,368.96,0.56\n",
      "11,512,387.23,0.61\n",
      "12,512,425.58,0.67\n",
      "13,512,458.24,0.72\n",
      "14,512,486.69,0.78\n",
      "15,512,521.96,0.84\n",
      "16,512,555.36,0.89\n",
      "17,512,595.87,0.95\n",
      "18,512,627.89,1.01\n",
      "19,512,653.96,1.06\n",
      "20,512,688.20,1.11\n",
      "21,512,739.88,1.17\n",
      "22,512,751.76,1.23\n",
      "23,512,783.23,1.28\n",
      "24,512,820.04,1.34\n",
      "25,512,848.24,1.39\n",
      "26,512,886.66,1.45\n",
      "27,512,922.13,1.50\n",
      "28,512,953.99,1.56\n",
      "29,512,989.09,1.62\n",
      "1,1024,143.60,0.17\n",
      "2,1024,211.74,0.35\n",
      "3,1024,296.30,0.53\n",
      "4,1024,363.21,0.70\n",
      "5,1024,427.58,0.87\n",
      "6,1024,512.56,1.04\n",
      "7,1024,594.01,1.22\n",
      "8,1024,672.58,1.39\n",
      "9,1024,754.09,1.57\n",
      "10,1024,831.65,1.74\n",
      "11,1024,911.53,1.92\n",
      "12,1024,994.36,2.09\n",
      "13,1024,1074.88,2.26\n",
      "14,1024,1157.32,2.43\n",
      "15,1024,1244.02,2.61\n",
      "16,1024,1325.28,2.78\n",
      "17,1024,1410.68,2.96\n",
      "18,1024,1508.79,3.13\n",
      "19,1024,1621.96,3.31\n",
      "20,1024,1738.23,3.48\n",
      "21,1024,1822.41,3.65\n",
      "22,1024,1929.39,3.82\n",
      "23,1024,2010.18,4.00\n",
      "24,1024,2113.01,4.17\n",
      "25,1024,2215.16,4.35\n",
      "26,1024,2323.07,4.52\n",
      "27,1024,2448.84,4.70\n",
      "28,1024,2589.33,4.87\n",
      "29,1024,2713.17,5.04\n",
      "1,2048,260.51,0.59\n",
      "2,2048,491.49,1.20\n",
      "3,2048,704.28,1.80\n",
      "4,2048,924.34,2.39\n",
      "5,2048,1152.22,2.99\n",
      "6,2048,1372.37,3.59\n",
      "7,2048,1613.14,4.19\n",
      "8,2048,1833.02,4.78\n",
      "9,2048,2079.61,5.38\n",
      "10,2048,2356.59,5.98\n",
      "11,2048,2605.31,6.58\n",
      "12,2048,2901.83,7.17\n",
      "13,2048,3178.02,7.77\n",
      "14,2048,3458.56,8.37\n",
      "15,2048,3787.02,8.97\n",
      "16,2048,4034.38,9.56\n",
      "17,2048,4298.46,10.16\n",
      "18,2048,4534.19,10.76\n",
      "19,2048,4773.00,11.36\n",
      "20,2048,5028.51,11.95\n",
      "21,2048,5325.63,12.55\n",
      "22,2048,5606.51,13.15\n",
      "23,2048,8651.16,13.75\n",
      "24,2048,12485.91,14.34\n",
      "25,2048,16329.58,14.94\n",
      "26,2048,20278.89,15.54\n",
      "27,2048,104959.78,16.14\n",
      "28,2048,128232.94,16.74\n",
      "29,2048,155784.38,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e078485-b8fc-4898-b121-ca3035410013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.698207Z",
     "iopub.status.busy": "2023-11-25T18:36:48.697921Z",
     "iopub.status.idle": "2023-11-25T18:36:48.867224Z",
     "shell.execute_reply": "2023-11-25T18:36:48.866826Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.698196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:51 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 11457 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      802        0 MB (  0.00%)\n",
      "GPU:    3,400    3,324 MB ( 97.76%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1af3b5-0c94-4981-aae7-ff5db560d54a",
   "metadata": {},
   "source": [
    "## 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5bfc6bf-1eae-49ae-a0d6-c6005f939637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.867829Z",
     "iopub.status.busy": "2023-11-25T18:36:48.867719Z",
     "iopub.status.idle": "2023-11-25T18:36:48.969935Z",
     "shell.execute_reply": "2023-11-25T18:36:48.969501Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.867820Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df1d8cdc-6bb6-4de7-84b2-db67b517f25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.970943Z",
     "iopub.status.busy": "2023-11-25T18:36:48.970653Z",
     "iopub.status.idle": "2023-11-25T18:36:53.462226Z",
     "shell.execute_reply": "2023-11-25T18:36:53.461768Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.970926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 244.97 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 4242.37 ms\n",
      "Model CPU memory: 0.40 GB\n",
      "Model GPU memory: 1.95 GB\n",
      "Max   GPU memory: 1.96 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86db86aa-38c6-46bf-8cad-6f2e80d6f9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:53.462979Z",
     "iopub.status.busy": "2023-11-25T18:36:53.462843Z",
     "iopub.status.idle": "2023-11-25T18:47:10.690379Z",
     "shell.execute_reply": "2023-11-25T18:47:10.689843Z",
     "shell.execute_reply.started": "2023-11-25T18:36:53.462970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,60.92,0.16\n",
      "2,128,71.53,0.17\n",
      "3,128,84.30,0.18\n",
      "4,128,97.68,0.19\n",
      "5,128,118.82,0.20\n",
      "6,128,129.78,0.21\n",
      "7,128,145.19,0.22\n",
      "8,128,155.80,0.23\n",
      "9,128,175.67,0.24\n",
      "10,128,191.66,0.25\n",
      "11,128,203.37,0.26\n",
      "12,128,221.43,0.27\n",
      "13,128,241.67,0.28\n",
      "14,128,254.04,0.29\n",
      "15,128,271.51,0.30\n",
      "16,128,282.05,0.31\n",
      "17,128,298.93,0.32\n",
      "18,128,320.51,0.33\n",
      "19,128,339.63,0.35\n",
      "20,128,353.81,0.36\n",
      "21,128,369.18,0.37\n",
      "22,128,383.35,0.38\n",
      "23,128,405.28,0.39\n",
      "24,128,412.20,0.40\n",
      "25,128,434.94,0.41\n",
      "26,128,453.60,0.42\n",
      "27,128,469.91,0.43\n",
      "28,128,527.36,0.44\n",
      "29,128,499.12,0.45\n",
      "1,256,70.25,0.17\n",
      "2,256,97.60,0.19\n",
      "3,256,133.43,0.21\n",
      "4,256,158.39,0.23\n",
      "5,256,192.77,0.25\n",
      "6,256,223.31,0.27\n",
      "7,256,255.22,0.29\n",
      "8,256,289.06,0.31\n",
      "9,256,324.01,0.33\n",
      "10,256,359.26,0.36\n",
      "11,256,397.50,0.38\n",
      "12,256,422.38,0.40\n",
      "13,256,465.23,0.42\n",
      "14,256,494.10,0.44\n",
      "15,256,524.89,0.46\n",
      "16,256,554.36,0.48\n",
      "17,256,594.87,0.50\n",
      "18,256,625.88,0.52\n",
      "19,256,657.29,0.54\n",
      "20,256,693.34,0.57\n",
      "21,256,734.68,0.59\n",
      "22,256,765.58,0.61\n",
      "23,256,798.60,0.63\n",
      "24,256,865.42,0.66\n",
      "25,256,869.00,0.68\n",
      "26,256,945.38,0.70\n",
      "27,256,938.99,0.73\n",
      "28,256,968.48,0.75\n",
      "29,256,1013.42,0.77\n",
      "1,512,101.13,0.19\n",
      "2,512,162.31,0.23\n",
      "3,512,231.70,0.27\n",
      "4,512,300.92,0.31\n",
      "5,512,378.70,0.36\n",
      "6,512,444.53,0.40\n",
      "7,512,514.73,0.44\n",
      "8,512,577.65,0.48\n",
      "9,512,652.16,0.52\n",
      "10,512,722.41,0.56\n",
      "11,512,801.97,0.61\n",
      "12,512,867.43,0.67\n",
      "13,512,951.10,0.73\n",
      "14,512,1016.43,0.78\n",
      "15,512,1118.21,0.84\n",
      "16,512,1161.92,0.89\n",
      "17,512,1262.12,0.95\n",
      "18,512,1301.05,1.00\n",
      "19,512,1374.66,1.06\n",
      "20,512,1446.17,1.11\n",
      "21,512,1516.33,1.17\n",
      "22,512,1621.12,1.23\n",
      "23,512,1668.60,1.28\n",
      "24,512,1741.53,1.34\n",
      "25,512,1824.29,1.39\n",
      "26,512,1876.20,1.45\n",
      "27,512,1957.67,1.51\n",
      "28,512,2018.63,1.56\n",
      "29,512,2111.34,1.62\n",
      "1,1024,177.56,0.23\n",
      "2,1024,325.82,0.35\n",
      "3,1024,481.74,0.52\n",
      "4,1024,626.40,0.70\n",
      "5,1024,783.28,0.87\n",
      "6,1024,947.58,1.04\n",
      "7,1024,1113.18,1.22\n",
      "8,1024,1252.86,1.39\n",
      "9,1024,1423.26,1.57\n",
      "10,1024,1582.51,1.74\n",
      "11,1024,1743.23,1.92\n",
      "12,1024,1876.38,2.09\n",
      "13,1024,2053.46,2.26\n",
      "14,1024,2214.09,2.43\n",
      "15,1024,2355.47,2.61\n",
      "16,1024,2551.10,2.78\n",
      "17,1024,2707.79,2.96\n",
      "18,1024,2840.95,3.13\n",
      "19,1024,3022.16,3.31\n",
      "20,1024,3155.51,3.48\n",
      "21,1024,3326.08,3.65\n",
      "22,1024,3477.24,3.82\n",
      "23,1024,3649.82,4.00\n",
      "24,1024,3793.07,4.17\n",
      "25,1024,3983.23,4.35\n",
      "26,1024,4103.07,4.52\n",
      "27,1024,4321.68,4.70\n",
      "28,1024,4432.53,4.87\n",
      "29,1024,4618.06,5.04\n",
      "1,2048,380.29,0.59\n",
      "2,2048,735.99,1.20\n",
      "3,2048,1109.80,1.79\n",
      "4,2048,1481.10,2.39\n",
      "5,2048,1857.48,2.99\n",
      "6,2048,2220.60,3.59\n",
      "7,2048,2597.17,4.18\n",
      "8,2048,3008.82,4.78\n",
      "9,2048,3350.06,5.38\n",
      "10,2048,3721.51,5.98\n",
      "11,2048,4100.85,6.57\n",
      "12,2048,4472.83,7.17\n",
      "13,2048,4874.75,7.77\n",
      "14,2048,5250.82,8.37\n",
      "15,2048,5607.23,8.96\n",
      "16,2048,6032.71,9.56\n",
      "17,2048,6416.30,10.16\n",
      "18,2048,6791.63,10.76\n",
      "19,2048,7186.32,11.36\n",
      "20,2048,7609.28,11.95\n",
      "21,2048,8022.64,12.55\n",
      "22,2048,8408.02,13.15\n",
      "23,2048,8836.85,13.75\n",
      "24,2048,13241.47,14.34\n",
      "25,2048,30777.34,14.94\n",
      "26,2048,44202.53,15.54\n",
      "27,2048,61871.65,16.14\n",
      "28,2048,78737.15,16.73\n",
      "29,2048,138651.96,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bf77fa-129e-45a1-b5f9-97bd8bd89eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:47:10.691037Z",
     "iopub.status.busy": "2023-11-25T18:47:10.690918Z",
     "iopub.status.idle": "2023-11-25T18:47:10.827226Z",
     "shell.execute_reply": "2023-11-25T18:47:10.826730Z",
     "shell.execute_reply.started": "2023-11-25T18:47:10.691028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:21 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 13086 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      406      416 MB (102.46%)\n",
      "GPU:    2,130    2,002 MB ( 93.99%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,673  12,018  15,837 MB  16.88% \n",
      "GPU:   1,204  23,359  24,564 MB   4.91% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d779f88-0477-4b7d-9111-3a3bcecc092e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StableLM-3B\n",
    "\n",
    "stabilityai/stablelm-3b-4e1t\n",
    "\n",
    "https://huggingface.co/stabilityai/stablelm-3b-4e1t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae435303-cdac-49c6-89fc-f9a970127f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:24:29.894242Z",
     "iopub.status.busy": "2023-11-29T23:24:29.893754Z",
     "iopub.status.idle": "2023-11-29T23:24:29.896378Z",
     "shell.execute_reply": "2023-11-29T23:24:29.895930Z",
     "shell.execute_reply.started": "2023-11-29T23:24:29.894224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b18a20-0ca7-47e7-9279-e6ee4eb7e323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T20:49:27.416244Z",
     "iopub.status.busy": "2023-11-25T20:49:27.414961Z",
     "iopub.status.idle": "2023-11-25T20:49:41.696725Z",
     "shell.execute_reply": "2023-11-25T20:49:41.695754Z",
     "shell.execute_reply.started": "2023-11-25T20:49:27.415926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     230  14,298  15,837 MB   1.45% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n",
      "Loading model stabilityai/stablelm-3b-4e1t in local cache ...\n",
      "--> model files size   : 5.21 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:12 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -180     -110 MB ( 61.27%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     160  14,382  15,837 MB   1.01% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d606c66-5b9e-4329-8397-1ffebb7d77e9",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9084f350-715a-40b1-92a4-acdd6f002e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:42:58.895791Z",
     "iopub.status.busy": "2023-11-29T23:42:58.895248Z",
     "iopub.status.idle": "2023-11-29T23:42:58.952643Z",
     "shell.execute_reply": "2023-11-29T23:42:58.952154Z",
     "shell.execute_reply.started": "2023-11-29T23:42:58.895769Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     971  29,608  31,785 MB   3.06% \n",
      "GPU:   1,404  23,159  24,564 MB   5.72% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cba0519-c2fb-4945-9c0c-dbca1fc4a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:43:00.831760Z",
     "iopub.status.busy": "2023-11-29T23:43:00.831132Z",
     "iopub.status.idle": "2023-11-29T23:43:02.715793Z",
     "shell.execute_reply": "2023-11-29T23:43:02.715326Z",
     "shell.execute_reply.started": "2023-11-29T23:43:00.831734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.21 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813)\n",
      "\n",
      "Tokenizer load time : 256.93 ms\n",
      "Tokenizer CPU memory: 2.06 MB\n",
      "\n",
      "Model load time : 1622.29 ms\n",
      "Model CPU memory: 0.01 GB\n",
      "Model GPU memory: 5.24 GB\n",
      "Max   GPU memory: 5.25 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06aa726-38a3-4661-a6d3-d0d9e37f689d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(4, 4096)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0574275-ac61-4370-840f-8061454f1ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:39:49.355571Z",
     "iopub.status.busy": "2023-11-29T23:39:49.354598Z",
     "iopub.status.idle": "2023-11-29T23:39:49.632502Z",
     "shell.execute_reply": "2023-11-29T23:39:49.632061Z",
     "shell.execute_reply.started": "2023-11-29T23:39:49.355540Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8519680, 11880278016)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6972b666-9b87-4d7f-a49f-32de8be0e306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T23:42:42.033544Z",
     "iopub.status.busy": "2023-11-29T23:42:42.032614Z",
     "iopub.status.idle": "2023-11-29T23:42:42.091806Z",
     "shell.execute_reply": "2023-11-29T23:42:42.091390Z",
     "shell.execute_reply.started": "2023-11-29T23:42:42.033508Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:05:26 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 551 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:       13        0 MB (  0.00%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     971  29,609  31,785 MB   3.06% \n",
      "GPU:   1,404  23,159  24,564 MB   5.72% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e583cd32-3b5e-47bc-a7be-0678447f84a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T21:22:19.181710Z",
     "iopub.status.busy": "2023-11-27T21:22:19.181264Z",
     "iopub.status.idle": "2023-11-27T21:22:19.206494Z",
     "shell.execute_reply": "2023-11-27T21:22:19.206079Z",
     "shell.execute_reply.started": "2023-11-27T21:22:19.181682Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "StableLMEpochForCausalLM\n",
      "> submodules\n",
      "- model: StableLMEpochModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#StableLMEpochModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [50304, 2560] (245.6 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X DecoderLayer\n",
      "      ---------------------\n",
      "      0..31#DecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: Attention\n",
      "      - mlp: MLP\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "        ---------------------\n",
      "        self_attn#Attention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: RotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 2560] (12.5 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#RotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [10] (0.0 MB)\n",
      "          - cos_cached: bfloat16 [1, 1, 4096, 20] (0.2 MB)\n",
      "          - sin_cached: bfloat16 [1, 1, 4096, 20] (0.2 MB)\n",
      "        ---------------------\n",
      "        mlp#MLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLU\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [6912, 2560] (33.8 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [6912, 2560] (33.8 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2560, 6912] (33.8 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLU\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [2560] (0.0 MB)\n",
      "        - bias: bfloat16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [2560] (0.0 MB)\n",
      "        - bias: bfloat16 [2560] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [2560] (0.0 MB)\n",
      "    - bias: bfloat16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [50304, 2560] (245.6 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "StableLMEpochForCausalLM.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        output_attentions = (\n",
      "            output_attentions\n",
      "            if output_attentions is not None\n",
      "            else self.config.output_attentions\n",
      "        )\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states\n",
      "            if output_hidden_states is not None\n",
      "            else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = (\n",
      "            return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        )\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        logits = self.lm_head(hidden_states).float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "StableLMEpochModel.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # Retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\n",
      "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n",
      "            )\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n",
      "            )\n",
      "\n",
      "        seq_length_with_past = seq_length\n",
      "        past_key_values_length = 0\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length,\n",
      "                seq_length + past_key_values_length,\n",
      "                dtype=torch.long,\n",
      "                device=device,\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        else:\n",
      "            position_ids = position_ids.view(-1, seq_length).long()\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "        # Embed positions\n",
      "        if attention_mask is None:\n",
      "            attention_mask = torch.ones(\n",
      "                (batch_size, seq_length_with_past),\n",
      "                dtype=torch.bool,\n",
      "                device=inputs_embeds.device,\n",
      "            )\n",
      "        attention_mask = self._prepare_decoder_attention_mask(\n",
      "            attention_mask,\n",
      "            (batch_size, seq_length),\n",
      "            inputs_embeds,\n",
      "            past_key_values_length,\n",
      "        )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # Decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = (\n",
      "                past_key_values[idx] if past_key_values is not None else None\n",
      "            )\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "\n",
      "                def create_custom_forward(module):\n",
      "                    def custom_forward(*inputs):\n",
      "                        # None for past_key_value\n",
      "                        return module(*inputs, past_key_value, output_attentions)\n",
      "\n",
      "                    return custom_forward\n",
      "\n",
      "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
      "                    create_custom_forward(decoder_layer),\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # Add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(\n",
      "                v\n",
      "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n",
      "                if v is not None\n",
      "            )\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "DecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "Attention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        query_rot = query_states[..., : self.rotary_ndims]\n",
      "        query_pass = query_states[..., self.rotary_ndims :]\n",
      "        key_rot = key_states[..., : self.rotary_ndims]\n",
      "        key_pass = key_states[..., self.rotary_ndims :]\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "\n",
      "        # [batch_size, num_heads, seq_len, head_dim]\n",
      "        query_states = torch.cat((query_states, query_pass), dim=-1)\n",
      "        key_states = torch.cat((key_states, key_pass), dim=-1)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # Reuse k, v, self_attention\n",
      "            key_states = torch.cat((past_key_value[0], key_states), dim=2)\n",
      "            value_states = torch.cat((past_key_value[1], value_states), dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        # Repeat k/v heads if n_kv_heads < n_heads\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # Upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        # Merge heads\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        # Final linear projection\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "RotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None):\n",
      "        # x: [batch_size, num_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=torch.get_default_dtype())\n",
      "        return (\n",
      "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "---------------------\n",
      "SiLU.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa330f-5df8-4ab8-8c68-cbb5faf8b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bcae39-2ec1-4674-9872-bd07b944499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:28.932661Z",
     "iopub.status.busy": "2023-11-22T23:14:28.931840Z",
     "iopub.status.idle": "2023-11-22T23:14:28.935824Z",
     "shell.execute_reply": "2023-11-22T23:14:28.935080Z",
     "shell.execute_reply.started": "2023-11-22T23:14:28.932634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs['logits']\n",
    "past_key_values = outputs['past_key_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e6b133-7b78-48ac-ba06-938daf8a44f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:59.065051Z",
     "iopub.status.busy": "2023-11-22T23:14:59.064635Z",
     "iopub.status.idle": "2023-11-22T23:14:59.067809Z",
     "shell.execute_reply": "2023-11-22T23:14:59.067428Z",
     "shell.execute_reply.started": "2023-11-22T23:14:59.065034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 50432]), torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size(),logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915d0c6-9bae-47eb-9712-44a9de6892e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:12:26.134495Z",
     "iopub.status.busy": "2023-11-22T23:12:26.133905Z",
     "iopub.status.idle": "2023-11-22T23:12:26.141171Z",
     "shell.execute_reply": "2023-11-22T23:12:26.140688Z",
     "shell.execute_reply.started": "2023-11-22T23:12:26.134464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_key_values), past_key_values[0][0].size(), past_key_values[0][0].dtype, past_key_values[0][1].size(), past_key_values[0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d13c74a-e0c1-449e-b2fa-59943661c78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:56.688325Z",
     "iopub.status.busy": "2023-11-20T21:23:56.687590Z",
     "iopub.status.idle": "2023-11-20T21:23:57.046020Z",
     "shell.execute_reply": "2023-11-20T21:23:57.045602Z",
     "shell.execute_reply.started": "2023-11-20T21:23:56.688294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291d9889-5bb6-4635-8ec1-17dfd0439542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:57.100183Z",
     "iopub.status.busy": "2023-11-20T21:23:57.099421Z",
     "iopub.status.idle": "2023-11-20T21:23:57.109529Z",
     "shell.execute_reply": "2023-11-20T21:23:57.109088Z",
     "shell.execute_reply.started": "2023-11-20T21:23:57.100149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer([\"un test\",\"un deuxième test\"], padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf3dd39-1341-44a7-9119-85b4d1298d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:24:01.082171Z",
     "iopub.status.busy": "2023-11-20T21:24:01.081390Z",
     "iopub.status.idle": "2023-11-20T21:24:01.087945Z",
     "shell.execute_reply": "2023-11-20T21:24:01.087300Z",
     "shell.execute_reply.started": "2023-11-20T21:24:01.082139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  328,  1071,     0,     0,     0],\n",
       "        [  328, 23156,    74, 22722,  1071]]), 'attention_mask': tensor([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fe31a0-9b15-421f-b069-e67546d7ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:32:12.116422Z",
     "iopub.status.busy": "2023-11-20T21:32:12.116190Z",
     "iopub.status.idle": "2023-11-20T21:32:12.120878Z",
     "shell.execute_reply": "2023-11-20T21:32:12.120463Z",
     "shell.execute_reply.started": "2023-11-20T21:32:12.116408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"input_ids\"].size(), encodings[\"input_ids\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef91e00-7d3c-452f-9d93-ec2d707215d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:34:04.936740Z",
     "iopub.status.busy": "2023-11-20T21:34:04.936503Z",
     "iopub.status.idle": "2023-11-20T21:34:04.939242Z",
     "shell.execute_reply": "2023-11-20T21:34:04.938877Z",
     "shell.execute_reply.started": "2023-11-20T21:34:04.936731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"attention_mask\"].size(), encodings[\"attention_mask\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca42a0a-eabb-46a6-aa77-4e925029d168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:22.386686Z",
     "iopub.status.busy": "2023-11-20T21:41:22.385561Z",
     "iopub.status.idle": "2023-11-20T21:41:22.393072Z",
     "shell.execute_reply": "2023-11-20T21:41:22.392631Z",
     "shell.execute_reply.started": "2023-11-20T21:41:22.386648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 50432]), torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"logits\"].size(), output[\"logits\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e887edcc-6554-466d-970e-9a800e7193fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:02.274392Z",
     "iopub.status.busy": "2023-11-20T21:41:02.273974Z",
     "iopub.status.idle": "2023-11-20T21:41:02.281515Z",
     "shell.execute_reply": "2023-11-20T21:41:02.280358Z",
     "shell.execute_reply.started": "2023-11-20T21:41:02.274364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172ffae5-19b4-42e4-a072-57882f130641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:46:28.031823Z",
     "iopub.status.busy": "2023-11-20T21:46:28.031023Z",
     "iopub.status.idle": "2023-11-20T21:46:28.037420Z",
     "shell.execute_reply": "2023-11-20T21:46:28.036755Z",
     "shell.execute_reply.started": "2023-11-20T21:46:28.031790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, torch.Size([2, 32, 5, 80]), torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"past_key_values\"]),output[\"past_key_values\"][0][1].size(),output[\"past_key_values\"][0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67521338-e1f2-4ed2-bb0e-a7d4f77aef74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049ee0b-cff8-4fe7-b39d-8f00ee810cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3025cf6-e7ed-4e21-929a-68809d3ba753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Falcon-7B\n",
    "\n",
    "tiiuae/falcon-7b\n",
    "\n",
    "https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25cd17d-c436-4668-a2c8-055718e0c9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:18:28.584083Z",
     "iopub.status.busy": "2023-12-01T22:18:28.583608Z",
     "iopub.status.idle": "2023-12-01T22:33:00.935728Z",
     "shell.execute_reply": "2023-12-01T22:33:00.934198Z",
     "shell.execute_reply.started": "2023-12-01T22:18:28.584050Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     556  29,651  31,785 MB   1.75% \n",
      "GPU:   1,395  23,168  24,564 MB   5.68% \n",
      "\n",
      "\n",
      "Loading model tiiuae/falcon-7b in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ba02dc34e9451e81173fb58ddae878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc31d3ebc3d4d3ba2ae78bd580e6437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcccc99c9a94fafaf8da7a5e1582035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec3c3a819e45e7bd3425455c744024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c847f873fe5446c1a112fe79b92a5267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7aa8a6f9d9448c81cda3d128cbff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 13.45 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:14:32 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 105 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -408        0 MB (  0.09%)\n",
      "GPU:      -16        0 MB ( -0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     148  30,360  31,785 MB   0.47% \n",
      "GPU:   1,379  23,184  24,564 MB   5.61% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"tiiuae/falcon-7b\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2d554-d0d7-4628-ad06-c752df5a4a9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49eb7f2-bbcf-482d-b6d0-c68d9ca0bf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:20:28.549275Z",
     "iopub.status.busy": "2023-12-02T02:20:28.549092Z",
     "iopub.status.idle": "2023-12-02T02:20:28.815237Z",
     "shell.execute_reply": "2023-12-02T02:20:28.814672Z",
     "shell.execute_reply.started": "2023-12-02T02:20:28.549263Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     559  29,911  31,785 MB   1.76% \n",
      "GPU:     996  23,567  24,564 MB   4.06% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d6f958-0b26-4386-9d07-905c2713e2b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:20:46.318820Z",
     "iopub.status.busy": "2023-12-02T02:20:46.317934Z",
     "iopub.status.idle": "2023-12-02T02:21:12.333519Z",
     "shell.execute_reply": "2023-12-02T02:21:12.332615Z",
     "shell.execute_reply.started": "2023-12-02T02:20:46.318786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba43f2a00f814f10a21b5c95db58b4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 289.48 ms\n",
      "Tokenizer CPU memory: 30.38 MB\n",
      "\n",
      "Model load time : 25717.15 ms\n",
      "Model CPU memory: 0.02 GB\n",
      "Model GPU memory: 12.94 GB\n",
      "Max   GPU memory: 13.49 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bf5255-83c4-4bf8-8f6d-4cd52367b34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:47:00.107180Z",
     "iopub.status.busy": "2023-12-01T22:47:00.106701Z",
     "iopub.status.idle": "2023-12-01T22:47:14.265789Z",
     "shell.execute_reply": "2023-12-01T22:47:14.265044Z",
     "shell.execute_reply.started": "2023-12-01T22:47:00.107163Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 6 and sequence length 2048:\n",
      "transformer.word_embeddings;True;;222.1;;int64[6, 2048];0.1;0.1;106.6;106.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.input_layernorm;True;;178.6;;bfloat16[6, 2048, 4544];106.5;154.6;261.2;261.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.query_key_value;True;;206.6;;bfloat16[6, 2048, 4544];106.5;261.1;370.6;370.6;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.maybe_rotary;True;;407.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;480.1;1016.1;588.1;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention.dense;True;;253.6;;bfloat16[6, 2048, 4544];106.5;586.6;693.1;693.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.self_attention;False;;99356.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;693.1;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.dense_h_to_4h;True;;129.1;;bfloat16[6, 2048, 4544];106.5;367.6;793.6;793.6;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.act;True;;114.9;;bfloat16[6, 2048, 18176];426.0;793.6;1219.6;1219.6;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp.dense_4h_to_h;True;;110.5;;bfloat16[6, 2048, 18176];426.0;793.6;900.1;900.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0.mlp;False;;809.3;;bfloat16[6, 2048, 4544];106.5;367.6;900.1;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.0;False;;100925.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;154.6;900.1;261.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.input_layernorm;True;;136.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.query_key_value;True;;111.2;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.maybe_rotary;True;;665.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention.dense;True;;284.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.self_attention;False;;737624.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.dense_h_to_4h;True;;161.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.act;True;;168.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp.dense_4h_to_h;True;;131.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1.mlp;False;;1130.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.1;False;;739440.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.input_layernorm;True;;140.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.query_key_value;True;;272.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.maybe_rotary;True;;498.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention.dense;True;;179.7;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.self_attention;False;;356670.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.dense_h_to_4h;True;;123.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.act;True;;119.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp.dense_4h_to_h;True;;126.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2.mlp;False;;923.3;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.2;False;;358189.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.input_layernorm;True;;134.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.query_key_value;True;;118.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.maybe_rotary;True;;688.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention.dense;True;;167.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.self_attention;False;;358679.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.dense_h_to_4h;True;;121.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.act;True;;121.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp.dense_4h_to_h;True;;125.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3.mlp;False;;1038.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.3;False;;360299.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.input_layernorm;True;;135.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.query_key_value;True;;120.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.maybe_rotary;True;;674.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention.dense;True;;449.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.self_attention;False;;358821.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.dense_h_to_4h;True;;163.2;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.act;True;;117.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp.dense_4h_to_h;True;;118.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4.mlp;False;;766.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.4;False;;360149.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.input_layernorm;True;;251.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.query_key_value;True;;235.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.maybe_rotary;True;;511.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention.dense;True;;257.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.self_attention;False;;357363.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.dense_h_to_4h;True;;203.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.act;True;;169.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp.dense_4h_to_h;True;;149.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5.mlp;False;;912.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.5;False;;359411.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.input_layernorm;True;;311.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.query_key_value;True;;116.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.maybe_rotary;True;;573.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention.dense;True;;167.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.self_attention;False;;358488.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.dense_h_to_4h;True;;358.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.act;True;;184.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp.dense_4h_to_h;True;;164.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6.mlp;False;;1109.0;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.6;False;;360435.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.input_layernorm;True;;171.2;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.query_key_value;True;;119.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.maybe_rotary;True;;608.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention.dense;True;;204.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.self_attention;False;;358691.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.dense_h_to_4h;True;;127.2;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.act;True;;268.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp.dense_4h_to_h;True;;115.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7.mlp;False;;895.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.7;False;;360196.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.input_layernorm;True;;146.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.query_key_value;True;;116.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.maybe_rotary;True;;556.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention.dense;True;;231.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.self_attention;False;;358691.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.dense_h_to_4h;True;;193.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.act;True;;129.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp.dense_4h_to_h;True;;326.5;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8.mlp;False;;1154.4;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.8;False;;360450.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.input_layernorm;True;;140.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.query_key_value;True;;118.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.maybe_rotary;True;;537.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention.dense;True;;192.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.self_attention;False;;358632.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.dense_h_to_4h;True;;113.3;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.act;True;;106.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp.dense_4h_to_h;True;;122.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9.mlp;False;;722.6;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.9;False;;360076.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.input_layernorm;True;;118.6;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.query_key_value;True;;131.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.maybe_rotary;True;;571.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention.dense;True;;246.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.self_attention;False;;357819.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.dense_h_to_4h;True;;147.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.act;True;;129.5;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp.dense_4h_to_h;True;;147.1;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10.mlp;False;;835.3;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.10;False;;359200.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.input_layernorm;True;;143.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.query_key_value;True;;132.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.maybe_rotary;True;;679.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention.dense;True;;183.2;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.self_attention;False;;359142.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.dense_h_to_4h;True;;215.4;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.act;True;;154.3;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp.dense_4h_to_h;True;;120.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11.mlp;False;;1088.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.11;False;;361003.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.input_layernorm;True;;138.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.query_key_value;True;;119.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.maybe_rotary;True;;726.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention.dense;True;;195.9;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.self_attention;False;;358382.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.dense_h_to_4h;True;;374.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.act;True;;153.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp.dense_4h_to_h;True;;127.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12.mlp;False;;1097.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.12;False;;360219.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.input_layernorm;True;;142.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.query_key_value;True;;140.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.maybe_rotary;True;;436.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention.dense;True;;173.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.self_attention;False;;358183.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.dense_h_to_4h;True;;117.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.act;True;;263.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp.dense_4h_to_h;True;;109.0;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13.mlp;False;;842.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.13;False;;359602.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.input_layernorm;True;;121.4;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.query_key_value;True;;110.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.maybe_rotary;True;;474.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention.dense;True;;165.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.self_attention;False;;358976.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.dense_h_to_4h;True;;120.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.act;True;;115.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp.dense_4h_to_h;True;;295.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14.mlp;False;;917.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.14;False;;360421.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.input_layernorm;True;;165.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.query_key_value;True;;118.5;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.maybe_rotary;True;;472.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention.dense;True;;179.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.self_attention;False;;359023.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.dense_h_to_4h;True;;158.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.act;True;;119.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp.dense_4h_to_h;True;;120.6;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15.mlp;False;;792.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.15;False;;360556.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.input_layernorm;True;;129.0;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.query_key_value;True;;115.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.maybe_rotary;True;;513.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention.dense;True;;176.5;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.self_attention;False;;359326.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.dense_h_to_4h;True;;125.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.act;True;;108.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp.dense_4h_to_h;True;;109.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16.mlp;False;;694.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.16;False;;360569.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.input_layernorm;True;;133.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.query_key_value;True;;119.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.maybe_rotary;True;;627.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention.dense;True;;239.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.self_attention;False;;357194.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.dense_h_to_4h;True;;140.5;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.act;True;;121.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp.dense_4h_to_h;True;;122.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17.mlp;False;;1026.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.17;False;;358972.7;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.input_layernorm;True;;169.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.query_key_value;True;;143.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.maybe_rotary;True;;644.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention.dense;True;;255.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.self_attention;False;;353644.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.dense_h_to_4h;True;;130.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.act;True;;117.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp.dense_4h_to_h;True;;128.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18.mlp;False;;1058.5;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.18;False;;355437.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.input_layernorm;True;;128.4;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.query_key_value;True;;322.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.maybe_rotary;True;;534.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention.dense;True;;194.1;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.self_attention;False;;353359.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.dense_h_to_4h;True;;137.6;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.act;True;;130.1;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp.dense_4h_to_h;True;;122.3;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19.mlp;False;;962.0;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.19;False;;354908.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.input_layernorm;True;;135.2;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.query_key_value;True;;136.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.maybe_rotary;True;;951.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention.dense;True;;188.3;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.self_attention;False;;354370.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.dense_h_to_4h;True;;169.4;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.act;True;;168.4;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp.dense_4h_to_h;True;;253.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20.mlp;False;;1204.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.20;False;;356174.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.input_layernorm;True;;129.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.query_key_value;True;;202.6;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.maybe_rotary;True;;576.0;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention.dense;True;;473.6;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.self_attention;False;;353574.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.dense_h_to_4h;True;;126.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.act;True;;164.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp.dense_4h_to_h;True;;148.4;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21.mlp;False;;845.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.21;False;;354982.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.input_layernorm;True;;162.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.query_key_value;True;;150.7;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.maybe_rotary;True;;502.1;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention.dense;True;;201.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.self_attention;False;;353301.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.dense_h_to_4h;True;;144.3;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.act;True;;124.6;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp.dense_4h_to_h;True;;119.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22.mlp;False;;790.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.22;False;;354975.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.input_layernorm;True;;320.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.query_key_value;True;;152.2;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.maybe_rotary;True;;662.7;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention.dense;True;;230.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.self_attention;False;;353630.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.dense_h_to_4h;True;;461.9;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.act;True;;188.7;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp.dense_4h_to_h;True;;124.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23.mlp;False;;1227.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.23;False;;355654.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.input_layernorm;True;;134.1;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.query_key_value;True;;170.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.maybe_rotary;True;;520.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention.dense;True;;185.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.self_attention;False;;352807.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.dense_h_to_4h;True;;138.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.act;True;;126.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp.dense_4h_to_h;True;;122.9;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24.mlp;False;;963.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.24;False;;354358.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.input_layernorm;True;;131.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.query_key_value;True;;292.3;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.maybe_rotary;True;;642.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention.dense;True;;189.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.self_attention;False;;353775.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.dense_h_to_4h;True;;116.0;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.act;True;;127.2;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp.dense_4h_to_h;True;;120.3;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25.mlp;False;;883.7;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.25;False;;355231.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.input_layernorm;True;;120.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.query_key_value;True;;112.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.maybe_rotary;True;;963.2;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention.dense;True;;165.3;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.self_attention;False;;354135.6;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.dense_h_to_4h;True;;172.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.act;True;;112.7;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp.dense_4h_to_h;True;;152.2;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26.mlp;False;;1047.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.26;False;;355727.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.input_layernorm;True;;165.8;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.query_key_value;True;;115.8;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.maybe_rotary;True;;640.5;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention.dense;True;;535.4;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.self_attention;False;;354158.4;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.dense_h_to_4h;True;;222.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.act;True;;158.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp.dense_4h_to_h;True;;130.7;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27.mlp;False;;954.8;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.27;False;;355794.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.input_layernorm;True;;142.6;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.query_key_value;True;;188.0;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.maybe_rotary;True;;535.4;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention.dense;True;;202.0;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.self_attention;False;;353124.8;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.dense_h_to_4h;True;;154.1;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.act;True;;161.9;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp.dense_4h_to_h;True;;133.0;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28.mlp;False;;912.9;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.28;False;;354834.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.input_layernorm;True;;291.7;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.query_key_value;True;;114.1;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.maybe_rotary;True;;542.6;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention.dense;True;;182.2;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.self_attention;False;;353142.5;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.dense_h_to_4h;True;;330.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.act;True;;121.0;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp.dense_4h_to_h;True;;143.2;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29.mlp;False;;1034.1;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.29;False;;354932.2;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.input_layernorm;True;;138.9;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.query_key_value;True;;137.4;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.maybe_rotary;True;;502.9;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention.dense;True;;195.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.self_attention;False;;353290.0;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.dense_h_to_4h;True;;141.8;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.act;True;;297.4;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp.dense_4h_to_h;True;;119.8;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30.mlp;False;;960.2;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.30;False;;354913.1;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.input_layernorm;True;;133.3;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.query_key_value;True;;240.0;;bfloat16[6, 2048, 4544];106.5;367.6;477.1;477.1;bfloat16[6, 2048, 4672];109.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.maybe_rotary;True;;659.8;;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64]int64[1, 2048];108.0;586.6;1122.6;694.6;bfloat16[426, 2048, 64]bfloat16[6, 2048, 64];108.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention.dense;True;;222.8;;bfloat16[6, 2048, 4544];106.5;693.1;799.6;799.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.self_attention;False;;353482.3;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;367.6;799.6;474.1;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.dense_h_to_4h;True;;190.7;;bfloat16[6, 2048, 4544];106.5;474.1;900.1;900.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.act;True;;124.8;;bfloat16[6, 2048, 18176];426.0;900.1;1326.1;1326.1;bfloat16[6, 2048, 18176];426.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp.dense_4h_to_h;True;;281.1;;bfloat16[6, 2048, 18176];426.0;900.1;1006.6;1006.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31.mlp;False;;1003.4;;bfloat16[6, 2048, 4544];106.5;474.1;1006.6;580.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.h.31;False;;355158.9;;bfloat16[6, 2048, 4544]bfloat16[6, 1, 2048, 2048]int64[1, 2048];154.5;261.1;1006.6;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer.ln_f;True;;190.5;;bfloat16[6, 2048, 4544];106.5;261.1;367.7;367.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "transformer;False;;11578242.6;;int64[6, 2048]float32[6, 2048];0.1;0.1;367.7;106.6;bfloat16[6, 2048, 4544];106.5;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "lm_head;True;;345.3;;bfloat16[6, 2048, 4544];106.5;106.6;1630.6;1630.6;bfloat16[6, 2048, 65024];1524.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n",
      "<model>;False;;11579376.7;;int64[6, 2048]float32[6, 2048];0.1;0.1;1630.6;1524.1;bfloat16[6, 2048, 65024];1524.0;;0.0;;;-14800.5;-14800.5;-14800.5;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(6, 2048)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502ae39c-f50f-4e84-8e8e-f400888731b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:17.936510Z",
     "iopub.status.busy": "2023-12-01T22:48:17.935101Z",
     "iopub.status.idle": "2023-12-01T22:48:18.191556Z",
     "shell.execute_reply": "2023-12-01T22:48:18.191105Z",
     "shell.execute_reply.started": "2023-12-01T22:48:17.936461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8519680, 17229268992)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3e674d-af67-42f5-803e-e850768437a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:22.153377Z",
     "iopub.status.busy": "2023-12-01T22:48:22.152979Z",
     "iopub.status.idle": "2023-12-01T22:48:22.213279Z",
     "shell.execute_reply": "2023-12-01T22:48:22.212691Z",
     "shell.execute_reply.started": "2023-12-01T22:48:22.153364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:01:46 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 2714 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      417        0 MB (  0.00%)\n",
      "GPU:      578        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     969  29,606  31,785 MB   3.05% \n",
      "GPU:   1,906  22,657  24,564 MB   7.76% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929970ff-2c7c-422b-a708-f0133021e035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:48:40.028266Z",
     "iopub.status.busy": "2023-12-01T22:48:40.027866Z",
     "iopub.status.idle": "2023-12-01T22:48:40.066791Z",
     "shell.execute_reply": "2023-12-01T22:48:40.066310Z",
     "shell.execute_reply.started": "2023-12-01T22:48:40.028244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "FalconForCausalLM\n",
      "> submodules\n",
      "- transformer: FalconModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  transformer#FalconModel\n",
      "  > submodules\n",
      "  - word_embeddings: Embedding\n",
      "  - h: ModuleList\n",
      "  - ln_f: LayerNorm\n",
      "    ---------------------\n",
      "    word_embeddings#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [65024, 4544] (563.6 MB)\n",
      "    ---------------------\n",
      "    h#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X FalconDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#FalconDecoderLayer\n",
      "      > submodules\n",
      "      - self_attention: FalconAttention\n",
      "      - mlp: FalconMLP\n",
      "      - input_layernorm: LayerNorm\n",
      "        ---------------------\n",
      "        self_attention#FalconAttention\n",
      "        > submodules\n",
      "        - maybe_rotary: FalconRotaryEmbedding\n",
      "        - query_key_value: FalconLinear\n",
      "        - dense: FalconLinear\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          maybe_rotary#FalconRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [32] (0.0 MB)\n",
      "          ---------------------\n",
      "          query_key_value#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4672, 4544] (40.5 MB)\n",
      "          ---------------------\n",
      "          dense#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4544, 4544] (39.4 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#FalconMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: FalconLinear\n",
      "        - act: GELU\n",
      "        - dense_4h_to_h: FalconLinear\n",
      "          ---------------------\n",
      "          dense_h_to_4h#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [18176, 4544] (157.5 MB)\n",
      "          ---------------------\n",
      "          act#GELU\n",
      "          ---------------------\n",
      "          dense_4h_to_h#FalconLinear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4544, 18176] (157.5 MB)\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4544] (0.0 MB)\n",
      "        - bias: bfloat16 [4544] (0.0 MB)\n",
      "    ---------------------\n",
      "    ln_f#LayerNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [4544] (0.0 MB)\n",
      "    - bias: bfloat16 [4544] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [65024, 4544] (563.6 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "FalconForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=CausalLMOutputWithCrossAttentions,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        inputs_embeds: Optional[torch.Tensor] = None,\n",
      "        labels: Optional[torch.Tensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
      "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
      "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
      "        \"\"\"\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        transformer_outputs = self.transformer(\n",
      "            input_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "        hidden_states = transformer_outputs[0]\n",
      "\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            batch_size, seq_length, vocab_size = shift_logits.shape\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            loss = loss_fct(\n",
      "                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n",
      "            )\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + transformer_outputs[1:]\n",
      "            return ((loss,) + output) if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithCrossAttentions(\n",
      "            loss=loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=transformer_outputs.past_key_values,\n",
      "            hidden_states=transformer_outputs.hidden_states,\n",
      "            attentions=transformer_outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "FalconModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_key_values = tuple([None] * len(self.h))\n",
      "        else:\n",
      "            past_key_values = self._convert_to_rw_cache(past_key_values)\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape batch_size x num_heads x N x N\n",
      "        # head_mask has shape n_layer x batch x num_heads x N x N\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.word_embeddings(input_ids)\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "        presents = () if use_cache else None\n",
      "        all_self_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "\n",
      "        # Compute alibi tensor: check build_alibi_tensor documentation\n",
      "        past_key_values_length = 0\n",
      "        if past_key_values[0] is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[1]  # 1 because RW-cache, not standard format\n",
      "\n",
      "        if self.use_alibi:\n",
      "            mask = (\n",
      "                torch.ones(\n",
      "                    (batch_size, seq_length + past_key_values_length), device=inputs_embeds.device, dtype=torch.long\n",
      "                )\n",
      "                if attention_mask is None\n",
      "                else attention_mask\n",
      "            )\n",
      "            alibi = build_alibi_tensor(mask, self.num_heads, dtype=hidden_states.dtype)\n",
      "        else:\n",
      "            alibi = None\n",
      "            if position_ids is None:\n",
      "                device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "                position_ids = torch.arange(\n",
      "                    past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "                )\n",
      "                position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
      "            )\n",
      "\n",
      "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    block.__call__,\n",
      "                    hidden_states,\n",
      "                    alibi,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    layer_past,\n",
      "                    use_cache,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = block(\n",
      "                    hidden_states,\n",
      "                    layer_past=layer_past,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                    alibi=alibi,\n",
      "                )\n",
      "\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        # Add last hidden state\n",
      "        hidden_states = self.ln_f(hidden_states)\n",
      "\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if presents is not None:\n",
      "            presents = self._convert_cache_to_standard_format(presents, batch_size)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPastAndCrossAttentions(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "FalconDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        alibi: Optional[torch.Tensor],\n",
      "        attention_mask: torch.Tensor,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        use_cache: bool = False,\n",
      "        output_attentions: bool = False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        if self.config.new_decoder_architecture:\n",
      "            attention_layernorm_out = self.ln_attn(hidden_states)\n",
      "            mlp_layernorm_out = self.ln_mlp(hidden_states)\n",
      "        else:\n",
      "            attention_layernorm_out = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self attention.\n",
      "        attn_outputs = self.self_attention(\n",
      "            attention_layernorm_out,\n",
      "            layer_past=layer_past,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            alibi=alibi,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attention_output = attn_outputs[0]\n",
      "\n",
      "        if not self.config.new_decoder_architecture:\n",
      "            if self.config.parallel_attn:\n",
      "                mlp_layernorm_out = attention_layernorm_out\n",
      "            else:\n",
      "                residual = dropout_add(\n",
      "                    attention_output, residual, self.config.attention_dropout, training=self.training\n",
      "                )\n",
      "                mlp_layernorm_out = self.post_attention_layernorm(residual)\n",
      "\n",
      "        outputs = attn_outputs[1:]\n",
      "\n",
      "        # MLP.\n",
      "        mlp_output = self.mlp(mlp_layernorm_out)\n",
      "\n",
      "        if self.config.new_decoder_architecture or self.config.parallel_attn:\n",
      "            mlp_output += attention_output\n",
      "\n",
      "        output = dropout_add(mlp_output, residual, self.config.hidden_dropout, training=self.training)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (output,) + outputs\n",
      "        else:\n",
      "            outputs = (output,) + outputs[1:]\n",
      "\n",
      "        return outputs  # hidden_states, present, attentions\n",
      "\n",
      "---------------------\n",
      "FalconAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        alibi: Optional[torch.Tensor],\n",
      "        attention_mask: torch.Tensor,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        use_cache: bool = False,\n",
      "        output_attentions: bool = False,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
      "        num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n",
      "        # 3 x [batch_size, seq_length, num_heads, head_dim]\n",
      "        (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n",
      "\n",
      "        batch_size, query_length, _, _ = query_layer.shape\n",
      "\n",
      "        query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n",
      "        key_layer = key_layer.transpose(1, 2).reshape(\n",
      "            batch_size * num_kv_heads,\n",
      "            query_length,\n",
      "            self.head_dim,\n",
      "        )\n",
      "        value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n",
      "\n",
      "        past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n",
      "        query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, past_kv_length, position_ids)\n",
      "\n",
      "        if layer_past is not None:\n",
      "            past_key, past_value = layer_past\n",
      "            # concatenate along seq_length dimension:\n",
      "            #  - key: [batch_size * self.num_heads, kv_length, head_dim]\n",
      "            #  - value: [batch_size * self.num_heads, kv_length, head_dim]\n",
      "            key_layer = torch.cat((past_key, key_layer), dim=1)\n",
      "            value_layer = torch.cat((past_value, value_layer), dim=1)\n",
      "\n",
      "        _, kv_length, _ = key_layer.shape\n",
      "        if use_cache:\n",
      "            present = (key_layer, value_layer)\n",
      "        else:\n",
      "            present = None\n",
      "\n",
      "        query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n",
      "        key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n",
      "        value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n",
      "\n",
      "        if alibi is None:\n",
      "            if hasattr(F, \"scaled_dot_product_attention\") and not output_attentions:\n",
      "                # TODO: deprecate this once we add FA2 support in Falcon\n",
      "                logger.warning_once(\n",
      "                    \"The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the\"\n",
      "                    \" future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call \"\n",
      "                    \"`model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\"\n",
      "                )\n",
      "\n",
      "                attn_output = F.scaled_dot_product_attention(\n",
      "                    query_layer_, key_layer_, value_layer_, attention_mask, 0.0, is_causal=False\n",
      "                )\n",
      "                attention_scores = None\n",
      "            else:\n",
      "                attention_scores = query_layer_ @ key_layer_.transpose(-1, -2)\n",
      "                attention_scores /= math.sqrt(self.head_dim)\n",
      "\n",
      "                attention_scores = F.softmax(attention_scores + attention_mask, dim=-1, dtype=hidden_states.dtype)\n",
      "                attn_output = attention_scores @ value_layer_\n",
      "\n",
      "            attn_output = attn_output.view(batch_size, self.num_heads, query_length, self.head_dim)\n",
      "            attn_output = attn_output.permute(0, 2, 1, 3)\n",
      "            attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n",
      "\n",
      "            output_tensor = self.dense(attn_output)\n",
      "\n",
      "            if output_attentions:\n",
      "                return output_tensor, present, attention_scores\n",
      "            else:\n",
      "                return output_tensor, present\n",
      "\n",
      "        else:\n",
      "            matmul_result = query_layer_ @ key_layer_.transpose(-1, -2)\n",
      "\n",
      "            # change view to [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_scores = matmul_result.view(batch_size, self.num_heads, query_length, kv_length)\n",
      "\n",
      "            # cast attention scores to fp32, compute scaled softmax and cast back to initial dtype - [batch_size, num_heads, q_length, kv_length]\n",
      "            input_dtype = attention_scores.dtype\n",
      "            # `float16` has a minimum value of -65504.0, whereas `bfloat16` and `float32` have a minimum value of `-3.4e+38`\n",
      "            if input_dtype == torch.float16 or input_dtype == torch.bfloat16:\n",
      "                attention_scores = attention_scores.to(torch.float32)\n",
      "            # Matt (HF) note: We could possibly use F.scaled_dot_product_attention here too, by\n",
      "            # adding (alibi * self.inv_norm_factor) to attention_mask. I think this would be mathematically\n",
      "            # equivalent and more performant, but there might be a numerical difference. If you're reading this\n",
      "            # and you'd like to experiment and maybe file a PR, feel free!\n",
      "            attention_logits = attention_scores + alibi.view(batch_size, self.num_heads, 1, -1)\n",
      "            attention_logits *= self.inv_norm_factor\n",
      "            attention_probs = F.softmax(attention_logits + attention_mask, dim=-1, dtype=hidden_states.dtype)\n",
      "            # [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_probs = self.attention_dropout(attention_probs)\n",
      "\n",
      "            if head_mask is not None:\n",
      "                attention_probs = attention_probs * head_mask\n",
      "\n",
      "            # change view [batch_size, num_heads, q_length, kv_length]\n",
      "            attention_probs_reshaped = attention_probs.view(batch_size, self.num_heads, query_length, kv_length)\n",
      "\n",
      "            # matmul: [batch_size * num_heads, q_length, head_dim]\n",
      "            context_layer = (attention_probs_reshaped @ value_layer_).flatten(0, 1)\n",
      "\n",
      "            # change view [batch_size, q_length, num_heads * head_dim]\n",
      "            context_layer = self._merge_heads(context_layer)\n",
      "\n",
      "            output_tensor = self.dense(context_layer)\n",
      "\n",
      "            if output_attentions:\n",
      "                return output_tensor, present, attention_probs\n",
      "            else:\n",
      "                return output_tensor, present\n",
      "\n",
      "---------------------\n",
      "FalconRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, query, key, past_key_values_length, position_ids):\n",
      "        _, seq_len, _ = query.shape\n",
      "        cos, sin = self.cos_sin(seq_len, past_key_values_length, position_ids, query.device, query.dtype)\n",
      "        # Query and key's shapes are [bs * num_heads, seq_len, dim], might need manual expansion. Ifs and elses used to\n",
      "        # avoid unnecessary repeat_interleave operations.\n",
      "        query_expansion_factor = int(query.shape[0] / cos.shape[0])\n",
      "        if query_expansion_factor > 1:\n",
      "            query_cos = torch.repeat_interleave(cos, query_expansion_factor, dim=0)\n",
      "            query_sin = torch.repeat_interleave(sin, query_expansion_factor, dim=0)\n",
      "        else:\n",
      "            query_cos, query_sin = cos, sin\n",
      "\n",
      "        key_expansion_factor = int(key.shape[0] / cos.shape[0])\n",
      "        if key_expansion_factor > 1:\n",
      "            if key_expansion_factor != query_expansion_factor:\n",
      "                key_cos = torch.repeat_interleave(cos, key_expansion_factor, dim=0)\n",
      "                key_sin = torch.repeat_interleave(sin, key_expansion_factor, dim=0)\n",
      "            else:\n",
      "                key_cos, key_sin = query_cos, query_sin\n",
      "        else:\n",
      "            key_cos, key_sin = cos, sin\n",
      "\n",
      "        return (query * query_cos) + (rotate_half(query) * query_sin), (key * key_cos) + (rotate_half(key) * key_sin)\n",
      "\n",
      "---------------------\n",
      "FalconLinear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
      "        hidden_states = input @ self.weight.T\n",
      "        if self.bias is None:\n",
      "            return hidden_states\n",
      "        return hidden_states + self.bias\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "FalconMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.act(self.dense_h_to_4h(x))\n",
      "        x = self.dense_4h_to_h(x)\n",
      "        return x\n",
      "\n",
      "---------------------\n",
      "GELU.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.gelu(input, approximate=self.approximate)\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ef91d-3958-474e-80a9-3ba6ce44c061",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fde57-da03-4f24-8a61-fce23eb0f3a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Native HF version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8154d8a-cad8-420e-811d-5c3d1c55446f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:56:03.169170Z",
     "iopub.status.busy": "2023-12-05T21:56:03.168508Z",
     "iopub.status.idle": "2023-12-05T21:56:03.224493Z",
     "shell.execute_reply": "2023-12-05T21:56:03.224048Z",
     "shell.execute_reply.started": "2023-12-05T21:56:03.169135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     954  29,435  31,785 MB   3.00% \n",
      "GPU:   1,789  22,774  24,564 MB   7.28% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfac6f3-ee66-4321-884e-020bde703c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:56:04.156093Z",
     "iopub.status.busy": "2023-12-05T21:56:04.155621Z",
     "iopub.status.idle": "2023-12-05T21:56:19.001204Z",
     "shell.execute_reply": "2023-12-05T21:56:19.000775Z",
     "shell.execute_reply.started": "2023-12-05T21:56:04.156062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40913f850738461685183a46a65777db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 277.75 ms\n",
      "Tokenizer CPU memory: 15.63 MB\n",
      "\n",
      "Model load time : 14560.34 ms\n",
      "Model CPU memory: 0.00 GB\n",
      "Model GPU memory: 12.94 GB\n",
      "Max   GPU memory: 13.50 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "019f8df2-b15e-49d8-b542-e5aef67abf64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:56:47.799367Z",
     "iopub.status.busy": "2023-12-05T21:56:47.798462Z",
     "iopub.status.idle": "2023-12-05T21:57:14.158917Z",
     "shell.execute_reply": "2023-12-05T21:57:14.158471Z",
     "shell.execute_reply.started": "2023-12-05T21:56:47.799328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,3156.07,40.56,12.96,0.02,12.98\n",
      "2,128,5141.33,49.79,12.96,0.03,13.00\n",
      "3,128,7377.56,52.05,12.96,0.05,13.01\n",
      "4,128,7012.45,73.01,12.96,0.07,13.03\n",
      "5,128,8208.17,77.97,12.96,0.08,13.05\n",
      "6,128,8003.24,95.96,12.96,0.10,13.06\n",
      "7,128,7595.15,117.97,12.96,0.12,13.08\n",
      "8,128,9246.43,110.75,12.96,0.13,13.10\n",
      "1,256,5730.10,44.68,12.96,0.03,13.00\n",
      "2,256,7924.44,64.61,12.96,0.07,13.03\n",
      "3,256,8084.97,94.99,12.96,0.10,13.06\n",
      "4,256,8844.45,115.78,12.96,0.13,13.10\n",
      "5,256,8469.60,151.13,12.96,0.17,13.13\n",
      "6,256,9299.06,165.18,12.96,0.20,13.16\n",
      "7,256,9068.66,197.60,12.96,0.23,13.20\n",
      "8,256,8660.36,236.48,12.96,0.27,13.23\n",
      "1,512,7660.64,66.84,12.96,0.10,13.06\n",
      "2,512,7794.13,131.38,12.96,0.19,13.16\n",
      "3,512,8482.23,181.08,12.96,0.29,13.25\n",
      "4,512,8616.79,237.68,12.96,0.39,13.35\n",
      "5,512,8790.54,291.22,12.96,0.48,13.44\n",
      "6,512,8826.76,348.03,12.96,0.58,13.54\n",
      "7,512,9029.24,396.93,12.96,0.68,13.64\n",
      "8,512,9099.31,450.14,12.96,0.77,13.73\n",
      "1,1024,7076.54,144.70,12.96,0.33,13.30\n",
      "2,1024,7520.88,272.31,12.96,0.66,13.63\n",
      "3,1024,7609.60,403.70,12.96,1.00,13.96\n",
      "4,1024,7816.14,524.04,12.96,1.33,14.29\n",
      "5,1024,7748.00,660.82,12.96,1.66,14.62\n",
      "6,1024,7783.55,789.36,12.96,1.99,14.96\n",
      "7,1024,7633.06,939.07,12.96,2.32,15.29\n",
      "8,1024,7905.83,1036.20,12.96,2.66,15.62\n",
      "1,2048,6092.94,336.13,12.96,1.22,14.19\n",
      "2,2048,6076.67,674.05,12.96,2.45,15.41\n",
      "3,2048,6081.46,1010.28,12.96,3.67,16.63\n",
      "4,2048,6155.05,1330.94,12.96,4.89,17.86\n",
      "5,2048,6091.09,1681.14,12.96,6.11,19.07\n",
      "6,2048,6094.70,2016.18,12.96,7.33,20.30\n",
      "7,2048,6156.02,2328.78,12.96,8.56,21.52\n",
      "8,2048,2676.68,6121.01,12.96,9.78,22.74\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb4a2800-c7cb-4932-ac50-fa67c1281a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T21:59:32.667501Z",
     "iopub.status.busy": "2023-12-05T21:59:32.667218Z",
     "iopub.status.idle": "2023-12-05T21:59:32.825046Z",
     "shell.execute_reply": "2023-12-05T21:59:32.824635Z",
     "shell.execute_reply.started": "2023-12-05T21:59:32.667482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:03:29 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 755 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:       15       15 MB ( 99.53%)\n",
      "GPU:   12,798   12,798 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     954  29,425  31,785 MB   3.00% \n",
      "GPU:   1,789  22,774  24,564 MB   7.28% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990664c-0612-44c6-975c-d02f0a5b3692",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compiled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9441997e-e742-40c5-a033-695b40a92b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:33:49.027147Z",
     "iopub.status.busy": "2023-12-05T22:33:49.026185Z",
     "iopub.status.idle": "2023-12-05T22:33:49.257410Z",
     "shell.execute_reply": "2023-12-05T22:33:49.256952Z",
     "shell.execute_reply.started": "2023-12-05T22:33:49.027093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     554  29,776  31,785 MB   1.74% \n",
      "GPU:   1,146  23,417  24,564 MB   4.67% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a969b399-fa58-4fed-8c97-597ad1ec1005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:33:49.676254Z",
     "iopub.status.busy": "2023-12-05T22:33:49.675815Z",
     "iopub.status.idle": "2023-12-05T22:34:01.576206Z",
     "shell.execute_reply": "2023-12-05T22:34:01.575719Z",
     "shell.execute_reply.started": "2023-12-05T22:33:49.676226Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61723fa6494f40aa86f39ed518a97317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 280.54 ms\n",
      "Tokenizer CPU memory: 30.21 MB\n",
      "\n",
      "Model load time : 11613.38 ms\n",
      "Model CPU memory: 0.02 GB\n",
      "Model GPU memory: 12.94 GB\n",
      "Max   GPU memory: 13.49 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7623137-54d1-44f7-a93b-1bff3e50fd49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:34:01.577150Z",
     "iopub.status.busy": "2023-12-05T22:34:01.576937Z",
     "iopub.status.idle": "2023-12-05T22:34:02.249587Z",
     "shell.execute_reply": "2023-12-05T22:34:02.248789Z",
     "shell.execute_reply.started": "2023-12-05T22:34:01.577141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.model = torch.compile(model_benchmark.model) #, fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe2b30-ea17-402c-a7d2-b46816d33402",
   "metadata": {},
   "source": [
    "- ERROR with fulllgraph=True: https://github.com/huggingface/transformers/issues/27789\n",
    "- kernel dies for unknown reason with: mode=\"max-autotune\"\n",
    "- AssertionError: anyway without parameters\n",
    "\n",
    "Giving up for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7f8ba-056c-4013-8193-1dfd59459a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9aa020-c03b-402f-a805-35fb4e47ac6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:47:26.962804Z",
     "iopub.status.busy": "2023-12-05T22:47:26.962000Z",
     "iopub.status.idle": "2023-12-05T22:47:27.133784Z",
     "shell.execute_reply": "2023-12-05T22:47:27.133306Z",
     "shell.execute_reply.started": "2023-12-05T22:47:26.962764Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:13:37 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 29148 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      381        0 MB (  0.00%)\n",
      "GPU:   13,960        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     936  28,589  31,785 MB   2.95% \n",
      "GPU:  15,106   9,457  24,564 MB  61.50% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fed9f0-fff2-4390-98c5-5d662177d914",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Flash attention 2 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49786bee-1423-48d8-a38c-2f1a1fa00d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:52:06.017753Z",
     "iopub.status.busy": "2023-12-05T22:52:06.017393Z",
     "iopub.status.idle": "2023-12-05T22:52:06.078626Z",
     "shell.execute_reply": "2023-12-05T22:52:06.077557Z",
     "shell.execute_reply.started": "2023-12-05T22:52:06.017724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     933  29,415  31,785 MB   2.94% \n",
      "GPU:   1,788  22,775  24,564 MB   7.28% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7380254-a759-4d0a-b0ee-665ddace379a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:52:06.892326Z",
     "iopub.status.busy": "2023-12-05T22:52:06.891878Z",
     "iopub.status.idle": "2023-12-05T22:52:11.849486Z",
     "shell.execute_reply": "2023-12-05T22:52:11.848914Z",
     "shell.execute_reply.started": "2023-12-05T22:52:06.892296Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f00826a7784422bb155445cc3275a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 153.23 ms\n",
      "Tokenizer CPU memory: 27.83 MB\n",
      "\n",
      "Model load time : 4795.27 ms\n",
      "Model CPU memory: 0.00 GB\n",
      "Model GPU memory: 12.94 GB\n",
      "Max   GPU memory: 13.50 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", use_flash_attention_2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea9515fb-2d72-4676-9f97-730c54969fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:52:20.343785Z",
     "iopub.status.busy": "2023-12-05T22:52:20.343492Z",
     "iopub.status.idle": "2023-12-05T22:59:07.476785Z",
     "shell.execute_reply": "2023-12-05T22:59:07.476211Z",
     "shell.execute_reply.started": "2023-12-05T22:52:20.343773Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,3619.80,35.36,12.96,0.02,12.98\n",
      "2,128,6410.97,39.93,12.96,0.03,13.00\n",
      "3,128,7467.92,51.42,12.96,0.05,13.01\n",
      "4,128,7616.50,67.22,12.96,0.07,13.03\n",
      "5,128,9064.32,70.61,12.96,0.08,13.05\n",
      "6,128,8408.11,91.34,12.96,0.10,13.06\n",
      "7,128,9773.70,91.67,12.96,0.12,13.08\n",
      "8,128,9364.34,109.35,12.96,0.13,13.10\n",
      "9,128,9504.79,121.20,12.96,0.15,13.11\n",
      "10,128,9686.05,132.15,12.96,0.17,13.13\n",
      "11,128,9831.35,143.22,12.96,0.18,13.15\n",
      "12,128,10351.43,148.39,12.96,0.20,13.16\n",
      "13,128,10337.61,160.97,12.96,0.22,13.18\n",
      "14,128,10499.85,170.67,12.96,0.23,13.20\n",
      "15,128,10066.29,190.74,12.96,0.25,13.21\n",
      "16,128,10321.34,198.42,12.96,0.27,13.23\n",
      "17,128,10607.14,205.14,12.96,0.28,13.25\n",
      "18,128,10735.19,214.62,12.96,0.30,13.26\n",
      "19,128,10687.54,227.55,12.96,0.32,13.28\n",
      "20,128,10581.16,241.94,12.96,0.33,13.30\n",
      "21,128,10751.49,250.01,12.96,0.35,13.31\n",
      "22,128,10329.15,272.63,12.96,0.37,13.33\n",
      "23,128,10377.48,283.69,12.96,0.38,13.35\n",
      "24,128,10555.01,291.05,12.96,0.40,13.36\n",
      "25,128,10853.33,294.84,12.96,0.41,13.38\n",
      "26,128,10693.20,311.23,12.96,0.43,13.39\n",
      "27,128,10761.76,321.14,12.96,0.45,13.41\n",
      "28,128,10883.59,329.30,12.96,0.47,13.43\n",
      "29,128,10490.76,353.84,12.96,0.48,13.44\n",
      "30,128,10794.57,355.73,12.96,0.50,13.46\n",
      "31,128,10682.19,371.46,12.96,0.51,13.48\n",
      "32,128,10686.19,383.30,12.96,0.53,13.49\n",
      "33,128,10772.36,392.11,12.96,0.55,13.51\n",
      "34,128,10851.90,401.04,12.96,0.56,13.53\n",
      "35,128,10696.55,418.83,12.96,0.58,13.54\n",
      "36,128,10722.86,429.74,12.96,0.60,13.56\n",
      "37,128,10430.31,454.06,12.96,0.62,13.58\n",
      "38,128,10527.73,462.02,12.96,0.63,13.59\n",
      "39,128,10649.73,468.74,12.96,0.65,13.61\n",
      "40,128,10824.41,473.01,12.96,0.66,13.63\n",
      "1,256,6590.93,38.84,12.96,0.03,13.00\n",
      "2,256,7937.88,64.50,12.96,0.07,13.03\n",
      "3,256,8737.87,87.89,12.96,0.10,13.06\n",
      "4,256,8270.70,123.81,12.96,0.13,13.10\n",
      "5,256,9453.91,135.39,12.96,0.17,13.13\n",
      "6,256,10196.42,150.64,12.96,0.20,13.16\n",
      "7,256,10323.37,173.59,12.96,0.23,13.20\n",
      "8,256,10482.99,195.36,12.96,0.27,13.23\n",
      "9,256,10633.12,216.68,12.96,0.30,13.26\n",
      "10,256,10676.18,239.79,12.96,0.33,13.30\n",
      "11,256,10352.49,272.01,12.96,0.37,13.33\n",
      "12,256,10499.02,292.60,12.96,0.40,13.36\n",
      "13,256,10732.13,310.10,12.96,0.43,13.39\n",
      "14,256,10761.21,333.05,12.96,0.47,13.43\n",
      "15,256,10760.21,356.87,12.96,0.50,13.46\n",
      "16,256,10867.87,376.89,12.96,0.53,13.49\n",
      "17,256,10820.35,402.20,12.96,0.56,13.53\n",
      "18,256,10622.15,433.81,12.96,0.60,13.56\n",
      "19,256,10458.10,465.09,12.96,0.63,13.59\n",
      "20,256,10717.87,477.71,12.96,0.66,13.63\n",
      "21,256,10930.59,491.83,12.96,0.70,13.66\n",
      "22,256,10425.23,540.23,12.96,0.73,13.69\n",
      "23,256,10746.91,547.88,12.96,0.76,13.73\n",
      "24,256,10858.93,565.80,12.96,0.80,13.76\n",
      "25,256,10350.54,618.32,12.96,0.83,13.79\n",
      "26,256,10738.66,619.82,12.96,0.86,13.83\n",
      "27,256,10853.49,636.85,12.96,0.90,13.86\n",
      "28,256,10938.63,655.29,12.96,0.93,13.89\n",
      "29,256,10614.20,699.44,12.96,0.96,13.93\n",
      "30,256,10825.33,709.45,12.96,1.00,13.96\n",
      "31,256,10823.77,733.20,12.96,1.03,13.99\n",
      "32,256,11128.37,736.14,12.96,1.06,14.03\n",
      "33,256,10813.19,781.27,12.96,1.10,14.06\n",
      "34,256,10897.45,798.72,12.96,1.13,14.09\n",
      "35,256,11087.36,808.13,12.96,1.16,14.13\n",
      "36,256,10808.54,852.66,12.96,1.20,14.16\n",
      "37,256,10901.31,868.89,12.96,1.23,14.19\n",
      "38,256,10972.28,886.60,12.96,1.26,14.22\n",
      "39,256,11103.39,899.18,12.96,1.29,14.26\n",
      "40,256,10889.78,940.33,12.96,1.33,14.29\n",
      "1,512,8581.00,59.67,12.96,0.07,13.03\n",
      "2,512,9529.43,107.46,12.96,0.13,13.10\n",
      "3,512,9902.86,155.11,12.96,0.20,13.16\n",
      "4,512,10218.65,200.42,12.96,0.27,13.23\n",
      "5,512,10497.16,243.88,12.96,0.33,13.30\n",
      "6,512,10446.40,294.07,12.96,0.40,13.36\n",
      "7,512,10734.99,333.86,12.96,0.47,13.43\n",
      "8,512,10838.84,377.90,12.96,0.53,13.49\n",
      "9,512,10689.19,431.09,12.96,0.60,13.56\n",
      "10,512,10729.84,477.17,12.96,0.66,13.63\n",
      "11,512,10459.12,538.48,12.96,0.73,13.69\n",
      "12,512,10767.48,570.61,12.96,0.80,13.76\n",
      "13,512,10668.20,623.91,12.96,0.86,13.83\n",
      "14,512,10904.43,657.35,12.96,0.93,13.89\n",
      "15,512,10735.97,715.35,12.96,1.00,13.96\n",
      "16,512,11029.47,742.74,12.96,1.06,14.03\n",
      "17,512,10817.54,804.62,12.96,1.13,14.09\n",
      "18,512,10762.06,856.34,12.96,1.20,14.16\n",
      "19,512,10907.30,891.88,12.96,1.26,14.22\n",
      "20,512,10832.60,945.30,12.96,1.33,14.29\n",
      "21,512,11032.53,974.57,12.96,1.39,14.36\n",
      "22,512,10888.65,1034.47,12.96,1.46,14.42\n",
      "23,512,11086.04,1062.24,12.96,1.53,14.49\n",
      "24,512,10955.50,1121.63,12.96,1.59,14.56\n",
      "25,512,10887.21,1175.69,12.96,1.66,14.62\n",
      "26,512,10950.92,1215.61,12.96,1.73,14.69\n",
      "27,512,10893.96,1268.96,12.96,1.79,14.75\n",
      "28,512,11051.00,1297.26,12.96,1.86,14.82\n",
      "29,512,10936.14,1357.70,12.96,1.92,14.89\n",
      "30,512,11111.71,1382.32,12.96,1.99,14.96\n",
      "31,512,11013.36,1441.16,12.96,2.06,15.02\n",
      "32,512,11206.95,1461.95,12.96,2.12,15.09\n",
      "33,512,11024.66,1532.56,12.96,2.19,15.15\n",
      "34,512,10736.29,1621.42,12.96,2.26,15.22\n",
      "35,512,11124.21,1610.90,12.96,2.32,15.29\n",
      "36,512,11009.04,1674.26,12.96,2.39,15.35\n",
      "37,512,11128.49,1702.30,12.96,2.46,15.42\n",
      "38,512,11055.32,1759.88,12.96,2.52,15.49\n",
      "39,512,11154.33,1790.16,12.96,2.59,15.55\n",
      "40,512,11072.53,1849.62,12.96,2.65,15.62\n",
      "1,1024,9104.22,112.48,12.96,0.13,13.10\n",
      "2,1024,10011.95,204.56,12.96,0.27,13.23\n",
      "3,1024,10295.91,298.37,12.96,0.40,13.36\n",
      "4,1024,10679.69,383.53,12.96,0.53,13.49\n",
      "5,1024,10606.10,482.74,12.96,0.66,13.63\n",
      "6,1024,10552.58,582.23,12.96,0.80,13.76\n",
      "7,1024,10764.64,665.88,12.96,0.93,13.89\n",
      "8,1024,10872.02,753.49,12.96,1.06,14.03\n",
      "9,1024,10642.25,865.98,12.96,1.20,14.16\n",
      "10,1024,10695.02,957.46,12.96,1.33,14.29\n",
      "11,1024,10764.87,1046.37,12.96,1.46,14.42\n",
      "12,1024,10811.53,1136.56,12.96,1.59,14.56\n",
      "13,1024,10897.73,1221.54,12.96,1.73,14.69\n",
      "14,1024,10655.13,1345.46,12.96,1.86,14.82\n",
      "15,1024,10996.64,1396.79,12.96,1.99,14.96\n",
      "16,1024,11065.05,1480.70,12.96,2.12,15.09\n",
      "17,1024,10818.80,1609.05,12.96,2.26,15.22\n",
      "18,1024,10911.06,1689.29,12.96,2.39,15.35\n",
      "19,1024,10927.51,1780.46,12.96,2.52,15.49\n",
      "20,1024,10806.09,1895.23,12.96,2.65,15.62\n",
      "21,1024,10918.40,1969.52,12.96,2.79,15.75\n",
      "22,1024,10921.23,2062.77,12.96,2.92,15.88\n",
      "23,1024,10886.50,2163.41,12.96,3.05,16.02\n",
      "24,1024,11052.32,2223.61,12.96,3.19,16.15\n",
      "25,1024,10940.10,2340.01,12.96,3.32,16.28\n",
      "26,1024,10994.78,2421.51,12.96,3.45,16.41\n",
      "27,1024,10826.09,2553.83,12.96,3.58,16.55\n",
      "28,1024,11027.68,2600.00,12.96,3.72,16.68\n",
      "29,1024,11053.66,2686.53,12.96,3.85,16.81\n",
      "30,1024,10937.91,2808.58,12.96,3.98,16.94\n",
      "31,1024,11063.14,2869.35,12.96,4.12,17.08\n",
      "32,1024,11048.74,2965.77,12.96,4.25,17.21\n",
      "33,1024,10974.82,3079.05,12.96,4.38,17.34\n",
      "34,1024,10896.93,3195.03,12.96,4.51,17.48\n",
      "35,1024,10990.92,3260.87,12.96,4.65,17.61\n",
      "36,1024,11037.85,3339.78,12.96,4.78,17.74\n",
      "37,1024,7258.17,5220.05,12.96,4.91,17.87\n",
      "38,1024,10978.98,3544.23,12.96,5.04,18.01\n",
      "39,1024,11054.35,3612.70,12.96,5.18,18.14\n",
      "40,1024,11068.81,3700.49,12.96,5.31,18.27\n",
      "1,2048,9561.27,214.20,12.96,0.27,13.23\n",
      "2,2048,10378.65,394.66,12.96,0.53,13.49\n",
      "3,2048,10455.49,587.63,12.96,0.80,13.76\n",
      "4,2048,10656.92,768.70,12.96,1.06,14.03\n",
      "5,2048,10466.21,978.39,12.96,1.33,14.29\n",
      "6,2048,10605.94,1158.60,12.96,1.59,14.56\n",
      "7,2048,10717.90,1337.58,12.96,1.86,14.82\n",
      "8,2048,10855.97,1509.22,12.96,2.12,15.09\n",
      "9,2048,10545.46,1747.86,12.96,2.39,15.35\n",
      "10,2048,10784.03,1899.10,12.96,2.65,15.62\n",
      "11,2048,10771.15,2091.51,12.96,2.92,15.88\n",
      "12,2048,10804.60,2274.59,12.96,3.19,16.15\n",
      "13,2048,10783.73,2468.90,12.96,3.45,16.41\n",
      "14,2048,10813.42,2651.52,12.96,3.72,16.68\n",
      "15,2048,10840.39,2833.85,12.96,3.98,16.94\n",
      "16,2048,10808.61,3031.66,12.96,4.25,17.21\n",
      "17,2048,10799.93,3223.72,12.96,4.51,17.48\n",
      "18,2048,10821.18,3406.65,12.96,4.78,17.74\n",
      "19,2048,10821.53,3595.79,12.96,5.04,18.01\n",
      "20,2048,10851.83,3774.48,12.96,5.31,18.27\n",
      "21,2048,10818.27,3975.50,12.96,5.57,18.54\n",
      "22,2048,10806.33,4169.41,12.96,5.84,18.80\n",
      "23,2048,10867.14,4334.54,12.96,6.10,19.07\n",
      "24,2048,10866.56,4523.23,12.96,6.37,19.33\n",
      "25,2048,10775.71,4751.43,12.96,6.63,19.60\n",
      "26,2048,10815.49,4923.31,12.96,6.90,19.86\n",
      "27,2048,10847.23,5097.70,12.96,7.17,20.13\n",
      "28,2048,10868.75,5276.04,12.96,7.43,20.40\n",
      "29,2048,10834.11,5481.95,12.96,7.70,20.66\n",
      "30,2048,10806.48,5685.48,12.96,7.96,20.93\n",
      "31,2048,10830.61,5861.90,12.96,8.23,21.19\n",
      "32,2048,6124.96,10699.83,12.96,8.49,21.46\n",
      "33,2048,2417.13,27960.40,12.96,8.76,21.72\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2910/3608854491.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prefill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2910/3944739197.py\u001b[0m in \u001b[0;36mcheck_prefill\u001b[0;34m(self, max_batch_size)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# see \"expandable_segments\": Pytorch allocator doesn't work when we gradually increase batch size !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# when inferencing with a constant batch size, this should not be needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mrelease_cached_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mafter_release\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mgpu_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gpu_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_used_and_max_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2910/344926584.py\u001b[0m in \u001b[0;36mrelease_cached_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelease_cached_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecord_memory_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25744af6-e351-4b7e-ac56-001d3f9bd4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:59:25.707894Z",
     "iopub.status.busy": "2023-12-05T22:59:25.707564Z",
     "iopub.status.idle": "2023-12-05T22:59:25.767164Z",
     "shell.execute_reply": "2023-12-05T22:59:25.766753Z",
     "shell.execute_reply.started": "2023-12-05T22:59:25.707879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:07:19 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1537 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:       28        0 MB (  0.00%)\n",
      "GPU:   12,653        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     962  29,387  31,785 MB   3.03% \n",
      "GPU:  14,442  10,121  24,564 MB  58.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd660e3-094a-4009-85f0-c7de3857c873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Flash attention 2 / 8 bits version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2e5d74-0328-48ce-bacf-1c62d9c58726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:03:57.557119Z",
     "iopub.status.busy": "2023-12-05T23:03:57.556941Z",
     "iopub.status.idle": "2023-12-05T23:03:57.792075Z",
     "shell.execute_reply": "2023-12-05T23:03:57.791465Z",
     "shell.execute_reply.started": "2023-12-05T23:03:57.557109Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     554  29,752  31,785 MB   1.74% \n",
      "GPU:   1,130  23,433  24,564 MB   4.60% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4702fa-af86-4676-b971-e5fb30870542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:03:58.257352Z",
     "iopub.status.busy": "2023-12-05T23:03:58.256547Z",
     "iopub.status.idle": "2023-12-05T23:04:05.940448Z",
     "shell.execute_reply": "2023-12-05T23:04:05.940003Z",
     "shell.execute_reply.started": "2023-12-05T23:03:58.257316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae12218d33a340739f4c48c7cb60e8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 259.53 ms\n",
      "Tokenizer CPU memory: 29.71 MB\n",
      "\n",
      "Model load time : 7416.93 ms\n",
      "Model CPU memory: 0.09 GB\n",
      "Model GPU memory: 6.73 GB\n",
      "Max   GPU memory: 7.28 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", use_flash_attention_2=True, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b170d61-5cc3-452c-a20f-540094c902e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:04:23.033121Z",
     "iopub.status.busy": "2023-12-05T23:04:23.032515Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "2023-12-05 23:04:23.953951: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 23:04:23.980645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,1148.55,111.44,6.76,0.02,6.79\n",
      "2,128,2320.54,110.32,6.76,0.05,6.81\n",
      "3,128,3716.02,103.34,6.76,0.07,6.83\n",
      "4,128,4417.32,115.91,6.76,0.10,6.86\n",
      "5,128,6345.02,100.87,6.76,0.12,6.88\n",
      "6,128,7732.24,99.32,6.76,0.14,6.90\n",
      "7,128,6916.41,129.55,6.76,0.16,6.93\n",
      "8,128,8820.65,116.09,6.76,0.19,6.95\n",
      "9,128,8228.79,140.00,6.76,0.21,6.97\n",
      "10,128,8684.90,147.38,6.76,0.23,7.00\n",
      "11,128,9222.18,152.68,6.76,0.26,7.02\n",
      "12,128,9365.54,164.01,6.76,0.28,7.05\n",
      "13,128,9169.83,181.46,6.77,0.30,7.07\n",
      "14,128,9362.42,191.40,6.77,0.33,7.09\n",
      "15,128,9895.10,194.04,6.77,0.35,7.12\n",
      "16,128,9190.62,222.84,6.77,0.38,7.14\n",
      "17,128,9376.95,232.06,6.77,0.40,7.16\n",
      "18,128,10648.52,216.37,6.77,0.42,7.19\n",
      "19,128,9683.60,251.15,6.77,0.45,7.21\n",
      "20,128,9377.15,273.00,6.77,0.47,7.23\n",
      "21,128,10274.52,261.62,6.77,0.49,7.26\n",
      "22,128,10311.49,273.09,6.77,0.51,7.28\n",
      "23,128,9760.48,301.62,6.77,0.54,7.30\n",
      "24,128,10799.48,284.46,6.77,0.56,7.33\n",
      "25,128,10819.12,295.77,6.77,0.58,7.35\n",
      "26,128,10748.89,309.61,6.77,0.61,7.37\n",
      "27,128,11067.99,312.25,6.77,0.63,7.40\n",
      "28,128,11007.27,325.60,6.77,0.66,7.42\n",
      "29,128,9538.01,389.18,6.77,0.68,7.44\n",
      "30,128,11084.22,346.44,6.77,0.70,7.47\n",
      "31,128,11144.66,356.05,6.77,0.72,7.49\n",
      "32,128,11209.97,365.39,6.77,0.75,7.52\n",
      "33,128,10879.50,388.25,6.77,0.77,7.54\n",
      "34,128,10945.62,397.60,6.77,0.79,7.56\n",
      "35,128,10992.84,407.54,6.77,0.82,7.59\n",
      "36,128,10609.82,434.31,6.77,0.84,7.61\n",
      "37,128,10943.05,432.79,6.77,0.86,7.63\n",
      "38,128,11090.73,438.56,6.77,0.89,7.66\n",
      "39,128,11007.79,453.50,6.77,0.92,7.68\n",
      "40,128,10944.34,467.82,6.77,0.93,7.70\n",
      "41,128,10939.46,479.73,6.77,0.96,7.73\n",
      "42,128,11229.63,478.73,6.77,0.98,7.75\n",
      "43,128,11023.03,499.32,6.77,1.01,7.77\n",
      "44,128,10083.34,558.54,6.77,1.03,7.80\n",
      "45,128,11117.25,518.11,6.77,1.05,7.82\n",
      "46,128,10699.22,550.32,6.77,1.08,7.85\n",
      "47,128,10929.80,550.42,6.77,1.10,7.87\n",
      "48,128,11087.21,554.15,6.77,1.13,7.89\n",
      "49,128,10932.52,573.70,6.77,1.14,7.92\n",
      "50,128,10686.05,598.91,6.77,1.17,7.94\n",
      "51,128,10745.01,607.54,6.77,1.19,7.96\n",
      "52,128,10887.18,611.36,6.77,1.21,7.98\n",
      "53,128,10699.38,634.06,6.77,1.24,8.01\n",
      "54,128,10441.25,661.99,6.77,1.26,8.03\n",
      "55,128,10550.23,667.28,6.77,1.29,8.06\n",
      "56,128,10791.74,664.21,6.77,1.31,8.08\n",
      "57,128,10947.40,666.46,6.77,1.33,8.10\n",
      "58,128,10817.04,686.32,6.77,1.35,8.12\n",
      "59,128,10787.99,700.04,6.77,1.38,8.15\n",
      "60,128,10675.75,719.39,6.77,1.40,8.17\n",
      "61,128,10717.02,728.56,6.77,1.42,8.20\n",
      "62,128,10761.75,737.43,6.77,1.45,8.22\n",
      "63,128,10761.18,749.36,6.77,1.47,8.24\n",
      "64,128,10517.03,778.93,6.77,1.49,8.27\n",
      "1,256,2182.23,117.31,6.77,0.05,6.82\n",
      "2,256,4031.50,127.00,6.76,0.10,6.86\n",
      "3,256,7572.86,101.41,6.76,0.14,6.90\n",
      "4,256,7479.72,136.90,6.76,0.19,6.95\n",
      "5,256,8187.42,156.34,6.76,0.23,7.00\n",
      "6,256,9204.13,166.88,6.76,0.28,7.05\n",
      "7,256,9101.95,196.88,6.77,0.33,7.09\n",
      "8,256,9213.75,222.28,6.77,0.38,7.14\n",
      "9,256,10654.88,216.24,6.77,0.42,7.19\n",
      "10,256,10078.30,254.01,6.77,0.47,7.23\n",
      "11,256,10095.45,278.94,6.77,0.51,7.28\n",
      "12,256,10896.96,281.91,6.77,0.56,7.33\n",
      "13,256,10685.80,311.44,6.77,0.61,7.37\n",
      "14,256,11172.06,320.80,6.77,0.66,7.42\n",
      "15,256,10713.06,358.44,6.77,0.70,7.47\n",
      "16,256,10712.87,382.34,6.77,0.75,7.52\n",
      "17,256,10950.43,397.43,6.77,0.79,7.56\n",
      "18,256,10765.98,428.02,6.77,0.84,7.61\n",
      "19,256,10784.43,451.02,6.77,0.89,7.66\n",
      "20,256,10825.56,472.95,6.77,0.93,7.70\n",
      "21,256,10824.88,496.63,6.77,0.98,7.75\n",
      "22,256,10845.22,519.31,6.77,1.03,7.80\n",
      "23,256,10843.33,543.01,6.77,1.08,7.84\n",
      "24,256,10802.38,568.76,6.77,1.12,7.89\n",
      "25,256,10811.29,591.97,6.77,1.17,7.94\n",
      "26,256,10764.11,618.35,6.77,1.21,7.98\n",
      "27,256,10769.29,641.83,6.77,1.26,8.03\n",
      "28,256,10677.17,671.34,6.77,1.31,8.08\n",
      "29,256,10858.20,683.72,6.77,1.35,8.12\n",
      "30,256,10747.80,714.56,6.77,1.40,8.17\n",
      "31,256,10571.42,750.70,6.77,1.45,8.22\n",
      "32,256,10645.84,769.50,6.77,1.49,8.27\n",
      "33,256,10424.79,810.38,6.77,1.54,8.31\n",
      "34,256,10518.52,827.49,6.77,1.59,8.36\n",
      "35,256,10620.39,843.66,6.77,1.63,8.40\n",
      "36,256,10290.17,895.61,6.77,1.68,8.45\n",
      "37,256,10606.85,893.01,6.77,1.73,8.50\n",
      "38,256,10572.20,920.15,6.77,1.77,8.55\n",
      "39,256,10422.53,957.93,6.77,1.82,8.59\n",
      "40,256,10412.70,983.41,6.77,1.87,8.64\n",
      "41,256,10378.13,1011.36,6.77,1.91,8.69\n",
      "42,256,10345.68,1039.27,6.78,1.96,8.74\n",
      "43,256,10405.51,1057.90,6.77,2.01,8.78\n",
      "44,256,10416.94,1081.32,6.77,2.05,8.83\n",
      "45,256,10672.63,1079.40,6.77,2.10,8.87\n",
      "46,256,10764.12,1094.00,6.77,2.15,8.92\n",
      "47,256,10782.64,1115.87,6.77,2.19,8.97\n",
      "48,256,10623.52,1156.68,6.78,2.24,9.01\n",
      "49,256,10853.51,1155.75,6.78,2.28,9.06\n",
      "50,256,10904.34,1173.84,6.78,2.33,9.11\n",
      "51,256,10780.56,1211.07,6.77,2.38,9.15\n",
      "52,256,10527.34,1264.52,6.77,2.43,9.20\n",
      "53,256,10761.27,1260.82,6.77,2.47,9.25\n",
      "54,256,10672.87,1295.25,6.78,2.52,9.29\n",
      "55,256,10619.93,1325.81,6.77,2.57,9.34\n",
      "56,256,10784.73,1329.29,6.78,2.61,9.39\n",
      "57,256,10439.99,1397.70,6.78,2.66,9.43\n",
      "58,256,10633.23,1396.38,6.78,2.70,9.48\n",
      "59,256,10531.92,1434.12,6.77,2.75,9.53\n",
      "60,256,10037.00,1530.34,6.77,2.80,9.58\n",
      "61,256,10399.87,1501.56,6.78,2.85,9.62\n",
      "62,256,10238.87,1550.17,6.78,2.89,9.67\n",
      "63,256,10162.06,1587.08,6.78,2.94,9.71\n",
      "64,256,10084.76,1624.63,6.78,2.98,9.76\n",
      "1,512,5419.16,94.48,6.78,0.09,6.87\n",
      "2,512,8247.84,124.15,6.76,0.19,6.95\n",
      "3,512,9418.28,163.09,6.76,0.28,7.04\n",
      "4,512,10440.40,196.16,6.76,0.38,7.14\n",
      "5,512,10495.48,243.91,6.77,0.47,7.23\n",
      "6,512,10812.67,284.11,6.77,0.56,7.33\n",
      "7,512,10885.91,329.23,6.77,0.66,7.42\n",
      "8,512,10738.07,381.45,6.77,0.75,7.52\n",
      "9,512,11012.59,418.43,6.77,0.84,7.61\n",
      "10,512,10830.67,472.73,6.77,0.93,7.70\n",
      "11,512,10895.90,516.89,6.77,1.03,7.80\n",
      "12,512,10707.27,573.82,6.77,1.12,7.89\n",
      "13,512,10869.71,612.34,6.77,1.21,7.98\n",
      "14,512,10614.30,675.32,6.77,1.31,8.08\n",
      "15,512,10839.35,708.53,6.77,1.40,8.17\n",
      "16,512,10550.85,776.43,6.77,1.50,8.27\n",
      "17,512,10677.52,815.17,6.77,1.59,8.36\n",
      "18,512,10300.67,894.70,6.77,1.68,8.45\n",
      "19,512,10397.26,935.63,6.77,1.77,8.55\n",
      "20,512,10441.56,980.70,6.77,1.87,8.64\n",
      "21,512,10262.05,1047.74,6.77,1.96,8.73\n",
      "22,512,10470.22,1075.81,6.77,2.05,8.83\n",
      "23,512,10772.27,1093.18,6.77,2.15,8.92\n",
      "24,512,10587.21,1160.65,6.78,2.24,9.01\n",
      "25,512,10541.40,1214.26,6.78,2.33,9.11\n",
      "26,512,10627.03,1252.66,6.78,2.43,9.20\n",
      "27,512,10618.26,1301.91,6.78,2.52,9.29\n",
      "28,512,10349.92,1385.13,6.78,2.61,9.39\n",
      "29,512,10628.69,1396.97,6.78,2.70,9.48\n",
      "30,512,10058.98,1526.99,6.77,2.80,9.58\n",
      "31,512,10314.04,1538.87,6.78,2.89,9.67\n",
      "32,512,9844.97,1664.20,6.78,2.98,9.76\n",
      "33,512,9820.00,1720.57,6.78,3.08,9.86\n",
      "34,512,9853.65,1766.65,6.78,3.17,9.95\n",
      "35,512,9795.78,1829.36,6.78,3.27,10.04\n",
      "36,512,9097.78,2025.99,6.78,3.36,10.14\n",
      "37,512,9596.66,1974.02,6.78,3.45,10.23\n",
      "38,512,9431.10,2062.96,6.78,3.54,10.32\n",
      "39,512,9242.20,2160.52,6.78,3.64,10.42\n",
      "40,512,9103.85,2249.60,6.78,3.73,10.51\n",
      "41,512,8999.71,2332.52,6.78,3.82,10.60\n",
      "42,512,8646.79,2486.93,6.78,3.92,10.70\n",
      "43,512,8656.58,2543.27,6.78,4.01,10.79\n",
      "44,512,8411.39,2678.27,6.78,4.11,10.89\n",
      "45,512,8318.41,2769.76,6.78,4.20,10.98\n",
      "46,512,8278.77,2844.87,6.78,4.29,11.07\n",
      "47,512,8322.50,2891.44,6.78,4.38,11.16\n",
      "48,512,8148.27,3016.10,6.78,4.48,11.26\n",
      "49,512,8151.37,3077.76,6.78,4.57,11.35\n",
      "50,512,8171.93,3132.67,6.78,4.66,11.44\n",
      "51,512,8142.41,3206.91,6.78,4.75,11.54\n",
      "52,512,8159.20,3263.06,6.78,4.85,11.63\n",
      "53,512,8162.97,3324.28,6.78,4.94,11.72\n",
      "54,512,8106.72,3410.50,6.78,5.03,11.82\n",
      "55,512,8133.63,3462.17,6.78,5.12,11.90\n",
      "56,512,8139.73,3522.48,6.78,5.22,12.00\n",
      "57,512,8132.31,3588.65,6.78,5.31,12.09\n",
      "58,512,8148.26,3644.46,6.78,5.41,12.19\n",
      "59,512,8093.51,3732.37,6.78,5.50,12.28\n",
      "60,512,8146.45,3770.97,6.78,5.59,12.37\n",
      "61,512,8062.93,3873.53,6.78,5.69,12.47\n",
      "62,512,8119.25,3909.72,6.78,5.78,12.57\n",
      "63,512,8214.42,3926.75,6.78,5.87,12.66\n",
      "64,512,8148.10,4021.55,6.78,5.96,12.75\n",
      "1,1024,8482.00,120.73,6.79,0.18,6.97\n",
      "2,1024,9667.55,211.84,6.76,0.38,7.14\n",
      "3,1024,10772.48,285.17,6.77,0.56,7.33\n",
      "4,1024,9748.18,420.18,6.77,0.75,7.52\n",
      "5,1024,10796.18,474.24,6.77,0.93,7.70\n",
      "6,1024,10804.49,568.65,6.77,1.12,7.89\n",
      "7,1024,10697.99,670.03,6.77,1.31,8.08\n",
      "8,1024,10663.92,768.20,6.77,1.49,8.26\n",
      "9,1024,10164.16,906.72,6.77,1.68,8.45\n",
      "10,1024,10158.60,1008.01,6.77,1.87,8.64\n",
      "11,1024,10299.67,1093.63,6.77,2.05,8.83\n",
      "12,1024,10519.28,1168.14,6.77,2.24,9.01\n",
      "13,1024,10515.79,1265.91,6.78,2.43,9.20\n",
      "14,1024,10460.33,1370.51,6.77,2.61,9.39\n",
      "15,1024,9938.75,1545.47,6.78,2.80,9.58\n",
      "16,1024,9705.48,1688.12,6.78,2.98,9.76\n",
      "17,1024,9575.57,1817.96,6.78,3.17,9.95\n",
      "18,1024,9229.82,1997.01,6.78,3.36,10.14\n",
      "19,1024,9360.30,2078.57,6.78,3.54,10.32\n",
      "20,1024,8883.31,2305.45,6.78,3.73,10.51\n",
      "21,1024,8531.82,2520.45,6.78,3.92,10.70\n",
      "22,1024,8315.10,2709.29,6.78,4.11,10.89\n",
      "23,1024,8169.10,2883.06,6.78,4.29,11.07\n",
      "24,1024,8120.72,3026.33,6.78,4.47,11.26\n",
      "25,1024,8054.92,3178.18,6.78,4.67,11.44\n",
      "26,1024,8074.78,3297.18,6.78,4.85,11.63\n",
      "27,1024,8068.57,3426.63,6.78,5.03,11.82\n",
      "28,1024,8081.64,3547.79,6.78,5.22,12.00\n",
      "29,1024,8039.23,3693.89,6.78,5.41,12.19\n",
      "30,1024,8016.74,3831.98,6.78,5.59,12.38\n",
      "31,1024,8030.73,3952.82,6.78,5.78,12.57\n",
      "32,1024,8099.38,4045.74,6.78,5.96,12.75\n",
      "33,1024,8073.45,4185.57,6.78,6.15,12.94\n",
      "34,1024,8056.10,4321.69,6.78,6.34,13.12\n",
      "35,1024,8011.44,4473.60,6.79,6.53,13.31\n",
      "36,1024,8052.30,4578.07,6.79,6.71,13.50\n",
      "37,1024,7974.29,4751.27,6.79,6.90,13.68\n",
      "38,1024,8052.30,4832.41,6.79,7.09,13.87\n",
      "39,1024,7957.53,5018.64,6.79,7.27,14.06\n",
      "40,1024,7980.65,5132.41,6.78,7.46,14.24\n",
      "41,1024,8039.53,5222.19,6.79,7.64,14.43\n",
      "42,1024,8072.88,5327.47,6.79,7.83,14.62\n",
      "43,1024,7589.75,5801.51,6.79,8.02,14.80\n",
      "44,1024,6457.26,6977.58,6.78,8.20,14.99\n",
      "45,1024,8040.71,5730.84,6.79,8.39,15.18\n",
      "46,1024,8056.86,5846.44,6.79,8.57,15.36\n",
      "47,1024,8037.99,5987.56,6.79,8.77,15.55\n",
      "48,1024,8080.01,6083.16,6.79,8.95,15.74\n",
      "49,1024,8048.15,6234.48,6.79,9.13,15.93\n",
      "50,1024,8047.59,6362.15,6.79,9.32,16.11\n",
      "51,1024,8049.94,6487.50,6.79,9.51,16.29\n",
      "52,1024,8107.32,6567.89,6.79,9.70,16.48\n",
      "53,1024,8071.50,6723.91,6.79,9.88,16.67\n",
      "54,1024,8080.07,6843.51,6.79,10.07,16.86\n",
      "55,1024,8059.07,6988.40,6.79,10.25,17.04\n",
      "56,1024,8111.70,7069.29,6.79,10.44,17.23\n",
      "57,1024,8076.61,7226.80,6.79,10.63,17.42\n",
      "58,1024,8051.20,7376.79,6.79,10.81,17.60\n",
      "59,1024,8056.22,7499.30,6.79,11.00,17.79\n",
      "60,1024,8105.53,7580.01,6.79,11.18,17.97\n",
      "61,1024,2343.57,26653.35,6.79,11.37,18.17\n",
      "62,1024,7245.03,8762.97,6.79,11.56,18.35\n",
      "63,1024,3540.17,18222.84,6.79,11.75,18.54\n",
      "64,1024,665.14,98530.06,6.79,11.93,18.72\n",
      "1,2048,5255.29,389.70,6.79,0.37,7.17\n",
      "2,2048,6021.20,680.26,6.77,0.75,7.52\n",
      "3,2048,5920.03,1037.83,6.77,1.13,7.89\n",
      "4,2048,5910.82,1385.93,6.77,1.50,8.27\n",
      "5,2048,6181.00,1656.69,6.77,1.87,8.64\n",
      "6,2048,10172.59,1207.95,6.77,2.24,9.01\n",
      "7,2048,10340.18,1386.44,6.78,2.61,9.39\n",
      "8,2048,9675.27,1693.39,6.78,2.99,9.76\n",
      "9,2048,8949.55,2059.55,6.78,3.36,10.14\n",
      "10,2048,8663.13,2364.04,6.78,3.73,10.51\n",
      "11,2048,8255.81,2728.75,6.78,4.11,10.89\n",
      "12,2048,8009.91,3068.20,6.78,4.48,11.26\n",
      "13,2048,7911.06,3365.41,6.78,4.85,11.63\n",
      "14,2048,7935.03,3613.35,6.78,5.22,12.00\n",
      "15,2048,7922.53,3877.55,6.78,5.59,12.38\n",
      "16,2048,7965.50,4113.74,6.78,5.96,12.75\n",
      "17,2048,7922.39,4394.63,6.79,6.34,13.13\n",
      "18,2048,7870.85,4683.61,6.79,6.71,13.50\n",
      "19,2048,7905.19,4922.34,6.79,7.09,13.87\n",
      "20,2048,7889.02,5192.03,6.79,7.46,14.25\n",
      "21,2048,7940.57,5416.24,6.78,7.83,14.62\n",
      "22,2048,7746.04,5816.65,6.79,8.20,14.99\n",
      "23,2048,7923.29,5945.00,6.78,8.58,15.36\n",
      "24,2048,7959.13,6175.55,6.79,8.95,15.74\n",
      "25,2048,7939.13,6449.07,6.79,9.32,16.11\n",
      "26,2048,7947.66,6699.83,6.79,9.69,16.48\n",
      "27,2048,7929.32,6973.61,6.79,10.07,16.86\n",
      "28,2048,7957.34,7206.43,6.79,10.44,17.23\n",
      "29,2048,7969.90,7452.04,6.79,10.81,17.60\n",
      "30,2048,7963.17,7715.52,6.79,11.18,17.97\n",
      "31,2048,7145.37,8885.19,6.79,11.56,18.35\n",
      "32,2048,7525.88,8708.08,6.79,11.93,18.72\n",
      "33,2048,5511.08,12263.28,6.79,12.30,19.10\n",
      "34,2048,4207.37,16550.01,6.79,12.67,19.47\n",
      "35,2048,3174.46,22580.25,6.79,13.05,19.85\n",
      "36,2048,2934.24,25126.74,6.80,13.42,20.22\n",
      "37,2048,2749.91,27555.82,6.79,13.79,20.59\n",
      "38,2048,1803.29,43156.78,6.79,14.17,20.96\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e8940a-4816-4f17-8c74-8575fd4c9baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:02:31.179837Z",
     "iopub.status.busy": "2023-12-05T23:02:31.179084Z",
     "iopub.status.idle": "2023-12-05T23:02:31.317409Z",
     "shell.execute_reply": "2023-12-05T23:02:31.316945Z",
     "shell.execute_reply.started": "2023-12-05T23:02:31.179807Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:01:48 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1929 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:    1,037        0 MB (  0.00%)\n",
      "GPU:    8,976        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,595  29,032  31,785 MB   5.02% \n",
      "GPU:  10,106  14,457  24,564 MB  41.14% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0a65a-3ed2-40ea-a4c6-d0fa795a7159",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Flash attention 2 / 4 bits version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24fc3f25-1348-4aa4-b331-80d1e5587792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:22:08.291238Z",
     "iopub.status.busy": "2023-12-05T23:22:08.290385Z",
     "iopub.status.idle": "2023-12-05T23:22:08.624630Z",
     "shell.execute_reply": "2023-12-05T23:22:08.624024Z",
     "shell.execute_reply.started": "2023-12-05T23:22:08.291189Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     567  29,722  31,785 MB   1.79% \n",
      "GPU:   1,109  23,454  24,564 MB   4.52% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719891d9-bfd2-461a-b256-369f136d7af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:22:09.293162Z",
     "iopub.status.busy": "2023-12-05T23:22:09.292719Z",
     "iopub.status.idle": "2023-12-05T23:22:30.964357Z",
     "shell.execute_reply": "2023-12-05T23:22:30.963665Z",
     "shell.execute_reply.started": "2023-12-05T23:22:09.293132Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373cf1fee2554e3e867712d1f2c50457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.45 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36)\n",
      "\n",
      "Tokenizer load time : 329.99 ms\n",
      "Tokenizer CPU memory: 30.09 MB\n",
      "\n",
      "Model load time : 21322.12 ms\n",
      "Model CPU memory: 0.09 GB\n",
      "Model GPU memory: 4.06 GB\n",
      "Max   GPU memory: 4.61 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", use_flash_attention_2=True, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b058275-c10a-484f-acd0-45b1c97b9f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T23:22:40.288025Z",
     "iopub.status.busy": "2023-12-05T23:22:40.287750Z",
     "iopub.status.idle": "2023-12-05T23:40:43.488378Z",
     "shell.execute_reply": "2023-12-05T23:40:43.486269Z",
     "shell.execute_reply.started": "2023-12-05T23:22:40.288013Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 23:22:41.297748: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 23:22:41.350516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,1495.51,85.59,4.08,0.32,4.40\n",
      "2,128,2775.95,92.22,4.08,0.33,4.41\n",
      "3,128,3707.59,103.57,4.08,0.33,4.42\n",
      "4,128,4313.38,118.70,4.08,0.34,4.43\n",
      "5,128,4922.25,130.02,4.08,0.35,4.43\n",
      "6,128,5392.55,142.42,4.08,0.36,4.44\n",
      "7,128,6129.16,146.19,4.08,0.37,4.45\n",
      "8,128,6297.46,162.61,4.08,0.38,4.46\n",
      "9,128,6543.13,176.06,4.08,0.39,4.47\n",
      "10,128,5980.11,214.04,4.08,0.40,4.48\n",
      "11,128,7045.90,199.83,4.08,0.40,4.49\n",
      "12,128,7336.65,209.36,4.08,0.41,4.50\n",
      "13,128,7563.31,220.01,4.08,0.42,4.50\n",
      "14,128,8080.85,221.76,4.08,0.43,4.51\n",
      "15,128,7720.86,248.68,4.08,0.44,4.52\n",
      "16,128,8246.93,248.33,4.08,0.45,4.53\n",
      "17,128,8415.62,258.57,4.08,0.46,4.54\n",
      "18,128,8558.20,269.22,4.08,0.46,4.55\n",
      "19,128,8535.33,284.93,4.08,0.48,4.56\n",
      "20,128,8816.31,290.37,4.08,0.48,4.56\n",
      "21,128,8934.01,300.87,4.08,0.49,4.58\n",
      "22,128,8659.50,325.19,4.08,0.50,4.58\n",
      "23,128,8808.82,334.21,4.08,0.51,4.59\n",
      "24,128,8945.24,343.42,4.08,0.52,4.60\n",
      "25,128,9292.41,344.37,4.08,0.53,4.61\n",
      "26,128,9410.15,353.66,4.08,0.53,4.62\n",
      "27,128,9312.89,371.10,4.08,0.54,4.63\n",
      "28,128,9347.76,383.41,4.08,0.55,4.64\n",
      "29,128,9173.74,404.63,4.08,0.56,4.64\n",
      "30,128,9496.02,404.38,4.08,0.57,4.65\n",
      "31,128,9403.78,421.96,4.08,0.58,4.66\n",
      "32,128,9654.54,424.26,4.08,0.59,4.67\n",
      "33,128,9575.75,441.11,4.08,0.59,4.68\n",
      "34,128,9646.98,451.13,4.08,0.60,4.69\n",
      "35,128,9552.17,469.00,4.08,0.61,4.69\n",
      "36,128,9640.34,477.99,4.08,0.62,4.70\n",
      "37,128,9409.95,503.30,4.08,0.63,4.72\n",
      "38,128,9483.60,512.89,4.08,0.64,4.72\n",
      "39,128,9685.14,515.43,4.08,0.65,4.73\n",
      "40,128,9768.08,524.16,4.08,0.66,4.75\n",
      "41,128,9864.31,532.02,4.08,0.68,4.76\n",
      "42,128,9929.22,541.43,4.08,0.70,4.78\n",
      "43,128,9550.26,576.32,4.08,0.71,4.80\n",
      "44,128,9612.99,585.87,4.08,0.73,4.81\n",
      "45,128,9785.71,588.61,4.08,0.75,4.83\n",
      "46,128,9885.40,595.63,4.08,0.76,4.85\n",
      "47,128,9841.30,611.30,4.08,0.78,4.86\n",
      "48,128,9994.57,614.73,4.08,0.80,4.88\n",
      "49,128,10064.07,623.21,4.08,0.81,4.90\n",
      "50,128,9533.13,671.34,4.08,0.83,4.91\n",
      "51,128,9807.83,665.59,4.08,0.85,4.93\n",
      "52,128,9960.31,668.25,4.08,0.86,4.95\n",
      "53,128,10024.28,676.76,4.08,0.88,4.96\n",
      "54,128,9957.08,694.18,4.08,0.90,4.98\n",
      "55,128,10128.99,695.03,4.08,0.91,5.00\n",
      "56,128,10188.42,703.54,4.08,0.93,5.01\n",
      "57,128,9894.35,737.39,4.08,0.95,5.03\n",
      "58,128,9945.95,746.43,4.08,0.96,5.04\n",
      "59,128,10046.18,751.73,4.08,0.98,5.06\n",
      "60,128,9776.93,785.52,4.08,1.00,5.08\n",
      "61,128,9972.82,782.93,4.08,1.01,5.09\n",
      "62,128,10210.30,777.25,4.08,1.03,5.11\n",
      "63,128,10379.16,776.94,4.08,1.05,5.13\n",
      "64,128,10427.65,785.60,4.08,1.06,5.15\n",
      "1,256,2827.85,90.53,4.08,0.33,4.41\n",
      "2,256,4289.08,119.37,4.08,0.34,4.43\n",
      "3,256,5275.82,145.57,4.08,0.36,4.44\n",
      "4,256,6299.47,162.55,4.08,0.38,4.46\n",
      "5,256,6982.96,183.30,4.08,0.40,4.48\n",
      "6,256,7618.38,201.62,4.08,0.41,4.50\n",
      "7,256,7837.78,228.64,4.08,0.43,4.51\n",
      "8,256,8203.80,249.64,4.08,0.45,4.53\n",
      "9,256,8640.02,266.67,4.08,0.46,4.55\n",
      "10,256,8649.00,295.99,4.08,0.48,4.56\n",
      "11,256,8622.88,326.57,4.08,0.50,4.58\n",
      "12,256,8836.29,347.66,4.08,0.52,4.60\n",
      "13,256,9355.82,355.71,4.08,0.53,4.62\n",
      "14,256,9375.89,382.26,4.08,0.55,4.64\n",
      "15,256,9464.38,405.73,4.08,0.57,4.65\n",
      "16,256,9640.40,424.88,4.08,0.59,4.67\n",
      "17,256,9608.52,452.93,4.08,0.60,4.69\n",
      "18,256,9570.85,481.46,4.08,0.62,4.70\n",
      "19,256,9460.39,514.14,4.08,0.64,4.72\n",
      "20,256,9752.72,524.98,4.08,0.66,4.75\n",
      "21,256,9900.73,542.99,4.08,0.70,4.78\n",
      "22,256,9238.84,609.60,4.08,0.73,4.81\n",
      "23,256,9744.50,604.24,4.08,0.76,4.85\n",
      "24,256,9946.56,617.70,4.08,0.80,4.88\n",
      "25,256,9680.15,661.15,4.08,0.83,4.91\n",
      "26,256,9912.27,671.49,4.08,0.86,4.95\n",
      "27,256,10026.21,689.39,4.08,0.90,4.98\n",
      "28,256,10143.09,706.69,4.08,0.93,5.01\n",
      "29,256,9846.45,753.98,4.08,0.96,5.04\n",
      "30,256,10080.39,761.88,4.08,1.00,5.08\n",
      "31,256,10192.49,778.61,4.08,1.03,5.11\n",
      "32,256,10356.73,790.98,4.08,1.06,5.15\n",
      "33,256,10140.24,833.12,4.08,1.10,5.18\n",
      "34,256,10233.82,850.51,4.08,1.13,5.21\n",
      "35,256,10420.59,859.84,4.08,1.16,5.24\n",
      "36,256,10193.35,904.12,4.08,1.20,5.28\n",
      "37,256,10252.91,923.83,4.08,1.23,5.31\n",
      "38,256,10363.52,938.68,4.08,1.26,5.34\n",
      "39,256,10416.39,958.49,4.08,1.29,5.38\n",
      "40,256,10313.13,992.91,4.08,1.33,5.41\n",
      "41,256,10377.49,1011.42,4.08,1.36,5.44\n",
      "42,256,10532.20,1020.87,4.08,1.39,5.48\n",
      "43,256,10335.72,1065.04,4.08,1.43,5.51\n",
      "44,256,10409.07,1082.13,4.08,1.46,5.54\n",
      "45,256,10537.92,1093.20,4.08,1.49,5.58\n",
      "46,256,10607.36,1110.17,4.08,1.53,5.61\n",
      "47,256,10415.01,1155.26,4.08,1.56,5.64\n",
      "48,256,10305.65,1192.36,4.08,1.59,5.67\n",
      "49,256,10582.73,1185.33,4.08,1.63,5.71\n",
      "50,256,10459.44,1223.77,4.08,1.66,5.74\n",
      "51,256,10508.23,1242.45,4.08,1.69,5.78\n",
      "52,256,10627.10,1252.65,4.08,1.73,5.81\n",
      "53,256,10729.00,1264.61,4.08,1.76,5.84\n",
      "54,256,10527.53,1313.13,4.08,1.79,5.87\n",
      "55,256,10584.25,1330.28,4.08,1.82,5.91\n",
      "56,256,10666.10,1344.07,4.08,1.86,5.94\n",
      "57,256,10519.57,1387.13,4.08,1.89,5.97\n",
      "58,256,10575.66,1403.98,4.08,1.92,6.01\n",
      "59,256,10669.90,1415.57,4.08,1.96,6.04\n",
      "60,256,10517.77,1460.39,4.08,1.99,6.07\n",
      "61,256,10605.51,1472.44,4.08,2.02,6.11\n",
      "62,256,10609.33,1496.04,4.08,2.06,6.14\n",
      "63,256,10570.11,1525.81,4.08,2.09,6.17\n",
      "64,256,10877.80,1506.19,4.08,2.12,6.21\n",
      "1,512,4493.30,113.95,4.08,0.34,4.43\n",
      "2,512,6384.85,160.38,4.08,0.38,4.46\n",
      "3,512,7593.27,202.28,4.08,0.41,4.50\n",
      "4,512,8253.66,248.13,4.08,0.45,4.53\n",
      "5,512,8587.11,298.12,4.08,0.48,4.56\n",
      "6,512,8887.29,345.66,4.08,0.52,4.60\n",
      "7,512,9291.36,385.73,4.08,0.55,4.64\n",
      "8,512,9559.51,428.47,4.08,0.59,4.67\n",
      "9,512,9467.59,486.71,4.08,0.62,4.70\n",
      "10,512,9637.67,531.25,4.08,0.66,4.75\n",
      "11,512,9477.48,594.25,4.08,0.73,4.81\n",
      "12,512,9882.29,621.72,4.08,0.80,4.88\n",
      "13,512,9857.38,675.23,4.08,0.86,4.95\n",
      "14,512,9949.93,720.41,4.08,0.93,5.01\n",
      "15,512,10016.01,766.77,4.08,1.00,5.08\n",
      "16,512,10271.41,797.55,4.08,1.06,5.15\n",
      "17,512,10093.77,862.31,4.08,1.13,5.21\n",
      "18,512,10126.27,910.11,4.08,1.20,5.28\n",
      "19,512,9907.70,981.86,4.08,1.26,5.34\n",
      "20,512,10026.16,1021.33,4.08,1.33,5.41\n",
      "21,512,10422.09,1031.65,4.08,1.39,5.48\n",
      "22,512,10344.02,1088.94,4.08,1.46,5.54\n",
      "23,512,10531.74,1118.14,4.08,1.53,5.61\n",
      "24,512,10401.69,1181.35,4.08,1.59,5.67\n",
      "25,512,10107.60,1266.37,4.08,1.66,5.74\n",
      "26,512,10538.03,1263.23,4.08,1.73,5.81\n",
      "27,512,10455.63,1322.16,4.08,1.79,5.87\n",
      "28,512,10404.96,1377.80,4.08,1.86,5.94\n",
      "29,512,10506.34,1413.24,4.08,1.92,6.01\n",
      "30,512,10693.21,1436.43,4.08,1.99,6.07\n",
      "31,512,10614.21,1495.35,4.08,2.06,6.14\n",
      "32,512,10757.72,1523.00,4.08,2.12,6.21\n",
      "33,512,10412.10,1622.73,4.08,2.19,6.27\n",
      "34,512,10603.90,1641.66,4.08,2.26,6.34\n",
      "35,512,10776.19,1662.93,4.08,2.32,6.41\n",
      "36,512,10716.49,1719.97,4.08,2.39,6.47\n",
      "37,512,10816.89,1751.33,4.08,2.46,6.54\n",
      "38,512,10744.58,1810.77,4.08,2.52,6.60\n",
      "39,512,10804.52,1848.12,4.08,2.59,6.67\n",
      "40,512,10813.91,1893.86,4.08,2.65,6.74\n",
      "41,512,10674.17,1966.62,4.08,2.72,6.80\n",
      "42,512,10851.33,1981.69,4.08,2.79,6.87\n",
      "43,512,10643.58,2068.48,4.08,2.85,6.94\n",
      "44,512,10871.96,2072.12,4.08,2.92,7.00\n",
      "45,512,10780.68,2137.16,4.08,2.99,7.07\n",
      "46,512,10776.39,2185.52,4.08,3.05,7.14\n",
      "47,512,10874.08,2212.97,4.08,3.12,7.20\n",
      "48,512,10921.58,2250.22,4.08,3.19,7.27\n",
      "49,512,10793.10,2324.45,4.08,3.25,7.33\n",
      "50,512,10822.61,2365.42,4.08,3.32,7.40\n",
      "51,512,10919.39,2391.34,4.08,3.38,7.47\n",
      "52,512,10785.95,2468.40,4.08,3.45,7.53\n",
      "53,512,10942.21,2479.94,4.08,3.52,7.60\n",
      "54,512,10868.93,2543.76,4.08,3.58,7.67\n",
      "55,512,10971.91,2566.55,4.08,3.65,7.73\n",
      "56,512,10918.77,2625.94,4.08,3.72,7.80\n",
      "57,512,10845.72,2690.83,4.08,3.78,7.86\n",
      "58,512,10901.88,2723.94,4.08,3.85,7.93\n",
      "59,512,10836.51,2787.61,4.08,3.91,8.00\n",
      "60,512,10833.92,2835.54,4.08,3.98,8.06\n",
      "61,512,10901.63,2864.89,4.08,4.05,8.13\n",
      "62,512,10856.81,2923.88,4.08,4.12,8.20\n",
      "63,512,10897.13,2960.04,4.08,4.18,8.26\n",
      "64,512,10969.88,2987.09,4.08,4.25,8.33\n",
      "1,1024,6339.72,161.52,4.08,0.38,4.46\n",
      "2,1024,8208.58,249.49,4.08,0.45,4.53\n",
      "3,1024,8748.13,351.16,4.08,0.52,4.60\n",
      "4,1024,9358.70,437.67,4.08,0.59,4.67\n",
      "5,1024,9591.99,533.78,4.08,0.66,4.75\n",
      "6,1024,9793.62,627.35,4.08,0.80,4.88\n",
      "7,1024,9940.28,721.11,4.08,0.93,5.01\n",
      "8,1024,10209.93,802.36,4.08,1.06,5.15\n",
      "9,1024,10032.37,918.63,4.08,1.20,5.28\n",
      "10,1024,10154.34,1008.44,4.08,1.33,5.41\n",
      "11,1024,10161.73,1108.47,4.08,1.46,5.54\n",
      "12,1024,10334.18,1189.06,4.08,1.59,5.67\n",
      "13,1024,10276.08,1295.44,4.08,1.73,5.81\n",
      "14,1024,10511.62,1363.82,4.08,1.86,5.94\n",
      "15,1024,10596.93,1449.48,4.08,1.99,6.07\n",
      "16,1024,10656.53,1537.46,4.08,2.12,6.21\n",
      "17,1024,10496.26,1658.50,4.08,2.26,6.34\n",
      "18,1024,10615.44,1736.34,4.08,2.39,6.47\n",
      "19,1024,10655.40,1825.93,4.08,2.52,6.60\n",
      "20,1024,10717.10,1910.96,4.08,2.65,6.74\n",
      "21,1024,10572.88,2033.88,4.08,2.79,6.87\n",
      "22,1024,10767.39,2092.24,4.08,2.92,7.00\n",
      "23,1024,10789.76,2182.81,4.08,3.05,7.14\n",
      "24,1024,10678.10,2301.53,4.08,3.19,7.27\n",
      "25,1024,10739.90,2383.64,4.08,3.32,7.40\n",
      "26,1024,10691.82,2490.13,4.08,3.45,7.53\n",
      "27,1024,10644.19,2597.47,4.08,3.58,7.67\n",
      "28,1024,10713.90,2676.15,4.08,3.72,7.80\n",
      "29,1024,10728.76,2767.89,4.08,3.85,7.93\n",
      "30,1024,10819.53,2839.31,4.08,3.98,8.06\n",
      "31,1024,10905.70,2910.77,4.08,4.12,8.20\n",
      "32,1024,10901.95,3005.70,4.08,4.25,8.33\n",
      "33,1024,10760.03,3140.51,4.08,4.38,8.46\n",
      "34,1024,10839.44,3211.97,4.08,4.51,8.59\n",
      "35,1024,10848.11,3303.80,4.08,4.65,8.73\n",
      "36,1024,10861.97,3393.86,4.08,4.78,8.86\n",
      "37,1024,10787.69,3512.15,4.08,4.91,8.99\n",
      "38,1024,10908.59,3567.10,4.08,5.04,9.13\n",
      "39,1024,10927.18,3654.74,4.08,5.18,9.26\n",
      "40,1024,10873.62,3766.91,4.08,5.31,9.39\n",
      "41,1024,10882.35,3857.99,4.08,5.44,9.52\n",
      "42,1024,10874.00,3955.12,4.08,5.57,9.66\n",
      "43,1024,10849.24,4058.53,4.08,5.71,9.79\n",
      "44,1024,10915.96,4127.53,4.08,5.84,9.92\n",
      "45,1024,10949.59,4208.38,4.08,5.97,10.06\n",
      "46,1024,10929.41,4309.84,4.08,6.10,10.19\n",
      "47,1024,10957.88,4392.09,4.08,6.24,10.32\n",
      "48,1024,10952.34,4487.81,4.08,6.37,10.45\n",
      "49,1024,10845.89,4626.27,4.08,6.50,10.59\n",
      "50,1024,10920.06,4688.62,4.08,6.63,10.72\n",
      "51,1024,10956.91,4766.31,4.08,6.77,10.85\n",
      "52,1024,10949.03,4863.26,4.08,6.90,10.98\n",
      "53,1024,10971.10,4946.81,4.08,7.03,11.12\n",
      "54,1024,10949.45,5050.12,4.08,7.17,11.25\n",
      "55,1024,10948.94,5143.87,4.08,7.30,11.38\n",
      "56,1024,10988.34,5218.62,4.08,7.43,11.51\n",
      "57,1024,10940.57,5335.01,4.08,7.56,11.65\n",
      "58,1024,10964.08,5416.96,4.08,7.70,11.78\n",
      "59,1024,10936.98,5524.01,4.08,7.83,11.91\n",
      "60,1024,10959.44,5606.13,4.08,7.96,12.04\n",
      "61,1024,10994.65,5681.31,4.08,8.10,12.18\n",
      "62,1024,10958.15,5793.68,4.08,8.23,12.31\n",
      "63,1024,10889.90,5924.02,4.08,8.36,12.44\n",
      "64,1024,10992.97,5961.63,4.08,8.49,12.58\n",
      "1,2048,7355.43,278.43,4.08,0.45,4.53\n",
      "2,2048,9198.77,445.28,4.08,0.59,4.67\n",
      "3,2048,9630.60,637.97,4.08,0.80,4.88\n",
      "4,2048,10019.10,817.64,4.08,1.06,5.15\n",
      "5,2048,9977.93,1026.27,4.08,1.33,5.41\n",
      "6,2048,10160.62,1209.38,4.08,1.59,5.67\n",
      "7,2048,10341.32,1386.28,4.08,1.86,5.94\n",
      "8,2048,10505.93,1559.50,4.08,2.12,6.21\n",
      "9,2048,10431.90,1766.89,4.08,2.39,6.47\n",
      "10,2048,10377.13,1973.57,4.08,2.65,6.74\n",
      "11,2048,10486.05,2148.38,4.08,2.92,7.00\n",
      "12,2048,10547.27,2330.08,4.08,3.19,7.27\n",
      "13,2048,10467.98,2543.37,4.08,3.45,7.53\n",
      "14,2048,10548.36,2718.15,4.08,3.72,7.80\n",
      "15,2048,10533.18,2916.50,4.08,3.98,8.06\n",
      "16,2048,10728.55,3054.28,4.08,4.25,8.33\n",
      "17,2048,10547.05,3301.02,4.08,4.51,8.59\n",
      "18,2048,10710.35,3441.91,4.08,4.78,8.86\n",
      "19,2048,10536.87,3692.94,4.08,5.04,9.13\n",
      "20,2048,10642.83,3848.60,4.08,5.31,9.39\n",
      "21,2048,10613.27,4052.29,4.08,5.57,9.66\n",
      "22,2048,10733.15,4197.84,4.08,5.84,9.92\n",
      "23,2048,10693.01,4405.12,4.08,6.10,10.19\n",
      "24,2048,10784.84,4557.51,4.08,6.37,10.45\n",
      "25,2048,10666.78,4799.95,4.08,6.63,10.72\n",
      "26,2048,10753.04,4951.90,4.08,6.90,10.98\n",
      "27,2048,10793.18,5123.23,4.08,7.17,11.25\n",
      "28,2048,10804.01,5307.66,4.08,7.43,11.51\n",
      "29,2048,10770.26,5514.44,4.08,7.70,11.78\n",
      "30,2048,10760.20,5709.93,4.08,7.96,12.04\n",
      "31,2048,10736.10,5913.51,4.08,8.23,12.31\n",
      "32,2048,10799.42,6068.47,4.08,8.49,12.58\n",
      "33,2048,10681.86,6326.99,4.08,8.76,12.84\n",
      "34,2048,10807.53,6442.92,4.08,9.02,13.11\n",
      "35,2048,7509.49,9545.25,4.08,9.29,13.37\n",
      "36,2048,10804.29,6823.95,4.08,9.55,13.64\n",
      "37,2048,10791.01,7022.14,4.08,9.82,13.90\n",
      "38,2048,10785.23,7215.79,4.08,10.08,14.17\n",
      "39,2048,10811.20,7387.90,4.08,10.35,14.43\n",
      "40,2048,10813.38,7575.80,4.08,10.62,14.70\n",
      "41,2048,10811.95,7766.22,4.08,10.88,14.96\n",
      "42,2048,10811.88,7955.69,4.08,11.15,15.23\n",
      "43,2048,10815.59,8142.32,4.08,11.41,15.50\n",
      "44,2048,10836.93,8315.27,4.08,11.68,15.76\n",
      "45,2048,10814.66,8521.77,4.08,11.94,16.03\n",
      "46,2048,10823.86,8703.73,4.08,12.21,16.29\n",
      "47,2048,10830.08,8887.84,4.08,12.47,16.56\n",
      "48,2048,10839.25,9069.26,4.08,12.74,16.82\n",
      "49,2048,8697.66,11537.81,4.08,13.00,17.09\n",
      "50,2048,10858.42,9430.47,4.08,13.27,17.35\n",
      "51,2048,10852.20,9624.59,4.08,13.54,17.62\n",
      "52,2048,10854.49,9811.24,4.08,13.80,17.88\n",
      "53,2048,10846.71,10007.09,4.08,14.07,18.15\n",
      "54,2048,10850.76,10192.10,4.08,14.33,18.41\n",
      "55,2048,10839.37,10391.75,4.08,14.60,18.68\n",
      "56,2048,10842.12,10578.00,4.08,14.86,18.95\n",
      "57,2048,10822.16,10786.75,4.08,15.13,19.21\n",
      "58,2048,10850.82,10947.01,4.08,15.39,19.48\n",
      "59,2048,8647.00,13973.86,4.08,15.66,19.74\n",
      "60,2048,9538.99,12881.86,4.08,15.92,20.01\n",
      "61,2048,5732.78,21791.86,4.08,16.19,20.27\n",
      "62,2048,2270.22,55931.08,4.08,16.45,20.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3399/3243844350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_prefill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3399/3944739197.py\u001b[0m in \u001b[0;36mcheck_prefill\u001b[0;34m(self, max_batch_size)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mbefore_forward_time_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mafter_forward_time_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mbefore_release\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 )\n\u001b[1;32m   1121\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m   1123\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, alibi, attention_mask, position_ids, layer_past, head_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;31m# Self attention.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         attn_outputs = self.self_attention(\n\u001b[0m\u001b[1;32m    797\u001b[0m             \u001b[0mattention_layernorm_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, alibi, attention_mask, position_ids, layer_past, head_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mnum_kv_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_decoder_architecture\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_kv_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# 3 x [batch_size, seq_length, num_heads, head_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused_qkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac21d86-594c-496e-aed6-92d4497b1c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T22:59:25.707894Z",
     "iopub.status.busy": "2023-12-05T22:59:25.707564Z",
     "iopub.status.idle": "2023-12-05T22:59:25.767164Z",
     "shell.execute_reply": "2023-12-05T22:59:25.766753Z",
     "shell.execute_reply.started": "2023-12-05T22:59:25.707879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:07:19 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1537 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:       28        0 MB (  0.00%)\n",
      "GPU:   12,653        0 MB (  0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     962  29,387  31,785 MB   3.03% \n",
      "GPU:  14,442  10,121  24,564 MB  58.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0e1e-67c1-4d9d-8d65-7beb8c0ffb63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Falcon-40B / 4 bits instruct models\n",
    "\n",
    "https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ\n",
    "\n",
    "https://huggingface.co/TheBloke/alfred-40B-1023-GPTQ\n",
    "\n",
    "https://huggingface.co/TheBloke/alfred-40B-1023-AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5a4ad-4217-45b9-b88f-605edd59547c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c982e3-934e-48ef-86e1-41d304159ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929196d-2fa7-49a0-93d6-5c90e26bdfd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TheBloke/falcon-40b-instruct-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1eabe-d534-48b7-9412-2ccf3205db70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"TheBloke/falcon-40b-instruct-GPTQ\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a9b2f4-b4ce-41dc-bfc1-4bbb077fc4ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T11:06:42.823445Z",
     "iopub.status.busy": "2023-12-09T11:06:42.822339Z",
     "iopub.status.idle": "2023-12-09T11:06:43.062626Z",
     "shell.execute_reply": "2023-12-09T11:06:43.061980Z",
     "shell.execute_reply.started": "2023-12-09T11:06:42.823419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     551  29,849  31,785 MB   1.73% \n",
      "GPU:   1,105  23,458  24,564 MB   4.50% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88610d58-a013-454c-8cca-893911c02ddf",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Need to fix : /models/huggingface/modules/transformers_modules/TheBloke/falcon-40b-instruct-GPTQ/57ac6eae1469d42d37781df19576896490023ec2/modelling_RW.py\n",
    "\n",
    "With the changes in : https://huggingface.co/tiiuae/falcon-40b/discussions/13/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46cbff3-d6f7-4973-aba6-e530d54c1d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T11:06:52.085414Z",
     "iopub.status.busy": "2023-12-09T11:06:52.084910Z",
     "iopub.status.idle": "2023-12-09T11:07:10.688918Z",
     "shell.execute_reply": "2023-12-09T11:07:10.688408Z",
     "shell.execute_reply.started": "2023-12-09T11:06:52.085383Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:06:53.016213: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-09 11:06:53.052381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 21.00 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--TheBloke--falcon-40b-instruct-GPTQ/snapshots/57ac6eae1469d42d37781df19576896490023ec2)\n",
      "\n",
      "Tokenizer load time : 231.64 ms\n",
      "Tokenizer CPU memory: 30.17 MB\n",
      "\n",
      "Model load time : 18364.91 ms\n",
      "Model CPU memory: 0.54 GB\n",
      "Model GPU memory: 20.63 GB\n",
      "Max   GPU memory: 21.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"TheBloke/falcon-40b-instruct-GPTQ\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d633542-29ae-452a-9643-72409e53c578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eab50-1a4b-49a3-b415-dafe6459f449",
   "metadata": {},
   "source": [
    "**ERROR** the fix above was not enough, the repository is tooo old, giving up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c3c3e-0551-43e6-9380-cbf0aeea3cd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TheBloke/alfred-40B-1023-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedeb873-f330-4dff-93a1-e932f27ffa67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T11:46:39.637063Z",
     "iopub.status.busy": "2023-12-09T11:46:39.636171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     562  29,800  31,785 MB   1.77% \n",
      "GPU:   1,105  23,458  24,564 MB   4.50% \n",
      "\n",
      "\n",
      "Loading model TheBloke/alfred-40B-1023-GPTQ in local cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 11:46:40.902901: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-09 11:46:40.940174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"TheBloke/alfred-40B-1023-GPTQ\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7745de12-8391-4b58-a7d4-2b3f8d62c294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T11:56:45.544759Z",
     "iopub.status.busy": "2023-12-09T11:56:45.544333Z",
     "iopub.status.idle": "2023-12-09T11:56:45.795717Z",
     "shell.execute_reply": "2023-12-09T11:56:45.795166Z",
     "shell.execute_reply.started": "2023-12-09T11:56:45.544735Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     563  29,786  31,785 MB   1.77% \n",
      "GPU:   1,105  23,458  24,564 MB   4.50% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7094962d-45ba-4df1-9b13-bd6426824ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:14:56.104184Z",
     "iopub.status.busy": "2023-12-09T12:14:56.103249Z",
     "iopub.status.idle": "2023-12-09T12:15:05.216667Z",
     "shell.execute_reply": "2023-12-09T12:15:05.216207Z",
     "shell.execute_reply.started": "2023-12-09T12:14:56.104143Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 21.00 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--TheBloke--alfred-40B-1023-GPTQ/snapshots/f8e310f64befd66c681321b01f7b89043a3a7ee3)\n",
      "\n",
      "Tokenizer load time : 239.32 ms\n",
      "Tokenizer CPU memory: 14.94 MB\n",
      "\n",
      "Model load time : 8866.06 ms\n",
      "Model CPU memory: 0.00 GB\n",
      "Model GPU memory: 20.63 GB\n",
      "Max   GPU memory: 21.01 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"TheBloke/alfred-40B-1023-GPTQ\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df06a84-56b3-4536-97d1-08e09cba404e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:15:15.648871Z",
     "iopub.status.busy": "2023-12-09T12:15:15.647996Z",
     "iopub.status.idle": "2023-12-09T12:15:15.773576Z",
     "shell.execute_reply": "2023-12-09T12:15:15.773083Z",
     "shell.execute_reply.started": "2023-12-09T12:15:15.648830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from auto_gptq import exllama_set_max_input_length\n",
    "model = exllama_set_max_input_length(model_benchmark.model, max_input_length=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8486e6c-87f7-4a99-bddb-c5ba1c8fe696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T11:57:12.082008Z",
     "iopub.status.busy": "2023-12-09T11:57:12.081332Z",
     "iopub.status.idle": "2023-12-09T11:59:25.510883Z",
     "shell.execute_reply": "2023-12-09T11:59:25.510264Z",
     "shell.execute_reply.started": "2023-12-09T11:57:12.081972Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,456.85,280.18,21.13,0.02,21.15\n",
      "1,256,949.31,269.67,21.02,0.05,21.07\n",
      "1,512,1515.01,337.95,21.02,0.10,21.12\n",
      "1,1024,1974.00,518.74,21.02,0.20,21.22\n",
      "1,2048,2287.64,895.24,21.03,0.39,21.42\n",
      "1,4096,2243.64,1825.60,21.05,0.79,21.84\n",
      "1,8192,168.28,48681.63,21.07,1.62,22.70\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58101c5-7066-4193-9083-93558d8af6e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:02:36.138501Z",
     "iopub.status.busy": "2023-12-09T12:02:36.137540Z",
     "iopub.status.idle": "2023-12-09T12:04:13.647127Z",
     "shell.execute_reply": "2023-12-09T12:04:13.646530Z",
     "shell.execute_reply.started": "2023-12-09T12:02:36.138459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,520.69,245.83,21.13,0.02,21.15\n",
      "1,256,961.42,266.27,21.02,0.05,21.07\n",
      "1,512,1559.34,328.34,21.02,0.10,21.12\n",
      "1,1024,1991.83,514.10,21.02,0.20,21.22\n",
      "1,2048,2319.98,882.76,21.03,0.39,21.42\n",
      "1,4096,2304.45,1777.43,21.05,0.79,21.84\n",
      "1,8192,111.78,73286.95,21.07,1.62,22.70\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(max_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b296b235-3e21-4b60-9eaa-05384b39b195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:18:25.021980Z",
     "iopub.status.busy": "2023-12-09T12:18:25.021318Z",
     "iopub.status.idle": "2023-12-09T12:20:22.538133Z",
     "shell.execute_reply": "2023-12-09T12:20:22.537632Z",
     "shell.execute_reply.started": "2023-12-09T12:18:25.021952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: <start_system>You are Alfred, a helpful assistant trained by LightOn. Knowledge cutoff: November 2022. Current date: 16 November, 2023<end_message><start_user>Write me an email to my boss, explaining how the company could benefit by using LightOns platform for Large Language Models, Paradigm.<end_message><start_assistant>Subject: Benefits of using LightOn's Paradigm platform for Large Language Models\n",
      "\n",
      "Dear [Boss's Name],\n",
      "\n",
      "I would like to bring to your attention the benefits of using LightOn's platform, Paradigm, for Large Language Models. As you are aware, Large Language Models are becoming increasingly important in various fields such as natural language processing, conversational AI, and machine translation.\n",
      "\n",
      "By using LightOn's platform, our company can benefit in several ways. First and foremost, it can help us save costs. The platform provides a scalable and cost-effective solution for training and deploying Large Language Models. This means that we can reduce the amount of money spent on infrastructure, while still being able to leverage the power of these models.\n",
      "\n",
      "Moreover, using LightOn's platform can help us improve the quality of our models. The platform provides access to a vast amount of data, which can be used to train models that are more accurate and effective. This can lead to better customer satisfaction and increased revenue.\n",
      "\n",
      "In addition, the platform offers a user-friendly interface that allows us to easily manage and monitor our models. We can track performance metrics and make necessary adjustments quickly. This can save us time and resources, while also ensuring that our models are always up-to-date and performing optimally.\n",
      "\n",
      "In conclusion, I believe that adopting LightOn's platform for Large Language Models can greatly benefit our company. It can help us save costs, improve the quality of our models, and provide a user-friendly interface for managing and monitoring them. I highly recommend considering this platform as we continue to explore ways to improve our business processes and stay ahead of the competition.\n",
      "\n",
      "Thank you for your time and I look forward to hearing your thoughts on this matter.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_benchmark.model,\n",
    "    tokenizer=model_benchmark.tokenizer\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "   \"<start_system>You are Alfred, a helpful assistant trained by LightOn. Knowledge cutoff: November 2022. Current date: 16 November, 2023<end_message><start_user>Write me an email to my boss, explaining how the company could benefit by using LightOns platform for Large Language Models, Paradigm.<end_message><start_assistant>\",\n",
    "    max_length=1000,\n",
    "    do_sample=True,\n",
    "    top_k=3,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=model_benchmark.tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d95d7-ee59-4839-a2d9-a96011ef8abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1e1b9-2526-478f-845e-d0c87be77edf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TheBloke/alfred-40B-1023-AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749471e9-308a-416d-a201-9a679600db8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d5181d-c420-4947-b4b7-b09c82bb53ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T12:52:00.744278Z",
     "iopub.status.busy": "2023-12-09T12:52:00.743265Z",
     "iopub.status.idle": "2023-12-09T13:20:28.083907Z",
     "shell.execute_reply": "2023-12-09T13:20:28.083397Z",
     "shell.execute_reply.started": "2023-12-09T12:52:00.744235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     562  29,687  31,785 MB   1.77% \n",
      "GPU:   1,094  23,469  24,564 MB   4.46% \n",
      "\n",
      "\n",
      "Loading model TheBloke/alfred-40B-1023-AWQ in local cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312035d88df24a0da78bb8d3c85b3b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/86.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61708ec675c54ec5a546eb8ede694a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e263a40bb5814421910555290999db18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/11.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0863e62b634f41ea824f5feb56bdbf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e855813032714c18aefd88999f228bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/2.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c1eea2c65a40e488728e6c96f411ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66d30b929cd4dda9ba5185e8d58dad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 21.72 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--TheBloke--alfred-40B-1023-AWQ/snapshots/159a50a0b23df16b1955eb6b1afc34152c345430\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:28:27 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 105 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      254       78 MB ( 31.01%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     737  29,559  31,785 MB   2.32% \n",
      "GPU:   1,094  23,469  24,564 MB   4.46% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"TheBloke/alfred-40B-1023-AWQ\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a64f0e-0431-4a9f-9a4d-a5872c1494f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5dc7e-ffd4-408a-9609-8eec4701673b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T15:14:32.824051Z",
     "iopub.status.busy": "2023-12-09T15:14:32.823275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type RefinedWeb to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-09 15:14:33 config.py:140] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-09 15:14:33 llm_engine.py:73] Initializing an LLM engine with config: model='TheBloke/alfred-40B-1023-AWQ', tokenizer='TheBloke/alfred-40B-1023-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir='/models/huggingface/transformers', load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = \"TheBloke/alfred-40B-1023-AWQ\"\n",
    "llm = LLM(model=model_id, trust_remote_code=True, max_model_len=8192, download_dir=\"/models/huggingface/transformers\", quantization=\"AWQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2bedb-a0dd-49b9-8567-12c24ab0ff2c",
   "metadata": {},
   "source": [
    "**too slow** the function call above was interrupted after more than 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d466860-fad9-44d6-8894-4a61ff1d9f3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Llama-2-7B\n",
    "\n",
    "meta-llama/Llama-2-7b-hf\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d0c1ba-2b8b-450a-b0e0-2695329f10b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:52:46.130649Z",
     "iopub.status.busy": "2023-12-01T22:52:46.129955Z",
     "iopub.status.idle": "2023-12-01T22:52:46.135177Z",
     "shell.execute_reply": "2023-12-01T22:52:46.134573Z",
     "shell.execute_reply.started": "2023-12-01T22:52:46.130619Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc44f39-4b42-42f3-bee5-1b3f544f9335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T22:54:49.136597Z",
     "iopub.status.busy": "2023-12-01T22:54:49.136126Z",
     "iopub.status.idle": "2023-12-01T23:09:14.106916Z",
     "shell.execute_reply": "2023-12-01T23:09:14.105899Z",
     "shell.execute_reply.started": "2023-12-01T22:54:49.136580Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     554  29,977  31,785 MB   1.74% \n",
      "GPU:   1,328  23,235  24,564 MB   5.41% \n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-2-7b-hf in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd18ab1617f948a08a52824a211c77f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b53a3bf946473b8bf2bdd49ffa0034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d2784ec2a45f7a3097ddf3ef73535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d05dc267e464cad36b327b6b526f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ace1bdb8d34d0bbba4e58eb793a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0518d53b6eb94ca5a9045061cc65c711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a6e51c27a64b65ad22236202d3a4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7bf6e4271b4b14b237277d8cdfb61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3168681d74ac4b77a74c4849a96aeba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0f43f9ff664b5598c38e28221e97cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7923c2d385964cb3ae6884eea5c81831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 12.55 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:14:24 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 105 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -344        0 MB (  0.18%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     210  30,201  31,785 MB   0.66% \n",
      "GPU:   1,328  23,235  24,564 MB   5.41% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, token=myhftoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99505cc-b7f5-4835-9a51-a229ae5d570a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4596b37f-9579-43d7-a4a7-3be9827fbf49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:36.322556Z",
     "iopub.status.busy": "2023-12-01T23:16:36.321795Z",
     "iopub.status.idle": "2023-12-01T23:16:36.382581Z",
     "shell.execute_reply": "2023-12-01T23:16:36.381987Z",
     "shell.execute_reply.started": "2023-12-01T23:16:36.322524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     739  29,709  31,785 MB   2.33% \n",
      "GPU:   1,422  23,141  24,564 MB   5.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9bcadc5-46a3-458c-a753-ba1a4bb6b9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:37.130632Z",
     "iopub.status.busy": "2023-12-01T23:16:37.129696Z",
     "iopub.status.idle": "2023-12-01T23:16:44.269397Z",
     "shell.execute_reply": "2023-12-01T23:16:44.268949Z",
     "shell.execute_reply.started": "2023-12-01T23:16:37.130577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c040b5a334dadafecadcccfd4428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 12.55 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--meta-llama--Llama-2-7b-hf/snapshots/8cca527612d856d7d32bd94f8103728d614eb852)\n",
      "\n",
      "Tokenizer load time : 160.45 ms\n",
      "Tokenizer CPU memory: 13.38 MB\n",
      "\n",
      "Model load time : 6971.62 ms\n",
      "Model CPU memory: 0.14 GB\n",
      "Model GPU memory: 12.61 GB\n",
      "Max   GPU memory: 12.62 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d8db7a8-092b-44ae-9337-87a60de1daf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:09.070057Z",
     "iopub.status.busy": "2023-12-01T23:17:09.069011Z",
     "iopub.status.idle": "2023-12-01T23:17:09.108852Z",
     "shell.execute_reply": "2023-12-01T23:17:09.108415Z",
     "shell.execute_reply.started": "2023-12-01T23:17:09.070027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "LlamaForCausalLM\n",
      "> submodules\n",
      "- model: LlamaModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#LlamaModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: LlamaRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [32000, 4096] (250.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X LlamaDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#LlamaDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: LlamaAttention\n",
      "      - mlp: LlamaMLP\n",
      "      - input_layernorm: LlamaRMSNorm\n",
      "      - post_attention_layernorm: LlamaRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#LlamaAttention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: LlamaRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#LlamaRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "          - cos_cached: float16 [4096, 128] (1.0 MB)\n",
      "          - sin_cached: float16 [4096, 128] (1.0 MB)\n",
      "        ---------------------\n",
      "        mlp#LlamaMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [11008, 4096] (86.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [11008, 4096] (86.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [4096, 11008] (86.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [4096] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [4096] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#LlamaRMSNorm\n",
      "    > parameters\n",
      "    - weight: float16 [4096] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [32000, 4096] (250.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "LlamaForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
      "\n",
      "        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
      "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            logits = torch.cat(logits, dim=-1)\n",
      "        else:\n",
      "            logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "LlamaModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape[:2]\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length = inputs_embeds.shape[:2]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        past_key_values_length = 0\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
      "            )\n",
      "\n",
      "        # embed positions\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_value,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "LlamaDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*):\n",
      "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
      "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LlamaAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
      "            query_slices = self.q_proj.weight.split(\n",
      "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
      "            )\n",
      "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
      "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
      "\n",
      "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            query_states = torch.cat(query_states, dim=-1)\n",
      "\n",
      "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            key_states = torch.cat(key_states, dim=-1)\n",
      "\n",
      "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            value_states = torch.cat(value_states, dim=-1)\n",
      "\n",
      "        else:\n",
      "            query_states = self.q_proj(hidden_states)\n",
      "            key_states = self.k_proj(hidden_states)\n",
      "            value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
      "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
      "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
      "        else:\n",
      "            attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "LlamaRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "LlamaMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            slice = self.intermediate_size // self.config.pretraining_tp\n",
      "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
      "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
      "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
      "\n",
      "            gate_proj = torch.cat(\n",
      "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
      "            )\n",
      "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
      "\n",
      "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
      "            down_proj = [\n",
      "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
      "            ]\n",
      "            down_proj = sum(down_proj)\n",
      "        else:\n",
      "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "        return down_proj\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "LlamaRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a609566c-7f04-4116-9620-4864bc520031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:16:09.033747Z",
     "iopub.status.busy": "2023-12-01T23:16:09.032559Z",
     "iopub.status.idle": "2023-12-01T23:16:12.686941Z",
     "shell.execute_reply": "2023-12-01T23:16:12.686415Z",
     "shell.execute_reply.started": "2023-12-01T23:16:09.033700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 6 and sequence length 2048:\n",
      "model.embed_tokens;True;;502.3;;int64[6, 2048];0.1;0.1;96.1;96.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;295.4;;float16[6, 2048, 4096];96.0;144.1;528.2;240.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;192.4;;float16[6, 2048, 4096];96.0;240.1;336.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;130.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;127.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;126.5;;float16[6, 32, 2048, 128];96.0;528.1;528.1;528.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;258.9;;float16[6, 2048, 4096];96.0;2160.1;2256.1;2256.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;10618.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;2256.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;252.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;171.7;;float16[6, 2048, 4096];96.0;336.1;594.1;594.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;274.0;;float16[6, 2048, 11008];258.0;594.1;852.1;852.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;128.0;;float16[6, 2048, 4096];96.0;594.1;852.1;852.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;181.8;;float16[6, 2048, 11008];258.0;594.1;690.1;690.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1419.0;;float16[6, 2048, 4096];96.0;336.1;690.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.0;False;;13299.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;144.1;690.1;240.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;240.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;132.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;108.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;104.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;97.3;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;129.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;1605.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;193.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;114.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;101.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;105.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;112.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1036.9;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.1;False;;3721.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;192.7;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;113.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;106.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;419.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;100.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;122.5;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;1727.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;188.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;112.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;106.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;107.2;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;108.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2.mlp;False;;1010.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.2;False;;3595.2;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;240.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;124.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;103.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;102.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;97.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;134.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;1568.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;194.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;113.3;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;101.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;106.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;247.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1014.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.3;False;;3634.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;183.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;117.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;106.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;129.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;97.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;133.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;1576.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;195.8;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;255.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;161.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;133.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;119.8;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4.mlp;False;;1316.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.4;False;;3762.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;355.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;124.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;107.2;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;102.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;101.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;284.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;1615.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;193.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;128.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;103.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;191.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;120.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1178.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.5;False;;3896.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;192.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;119.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;248.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;102.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;96.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;138.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1569.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;214.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;122.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;101.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;108.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;109.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6.mlp;False;;1034.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.6;False;;3511.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;191.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;114.9;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;104.4;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;122.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;228.3;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;132.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1576.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;188.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;113.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;101.3;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;109.6;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;107.9;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1019.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.7;False;;3611.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;187.0;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;276.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;112.6;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;103.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;101.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;134.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;1605.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;335.5;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;115.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;98.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;106.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;107.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8.mlp;False;;874.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.8;False;;3638.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;180.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;114.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;108.4;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;103.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;102.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;131.2;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;1540.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;178.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;112.0;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;241.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;105.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;235.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9.mlp;False;;1181.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.9;False;;3576.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;200.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;226.0;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;144.6;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;116.8;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;115.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;168.7;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;1993.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;191.0;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;117.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;102.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;116.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;110.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10.mlp;False;;1079.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.10;False;;4149.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;195.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;116.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;108.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;254.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;98.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;163.2;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;1570.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;180.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;121.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;114.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;114.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;114.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1274.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.11;False;;3699.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;178.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;121.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;113.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;106.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;99.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;124.3;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1558.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;184.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;113.5;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;97.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;110.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;253.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12.mlp;False;;1038.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.12;False;;3603.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;190.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;114.5;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;103.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;104.0;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;117.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;138.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1540.2;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;194.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;261.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;98.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;105.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;120.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13.mlp;False;;1036.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.13;False;;3453.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;321.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;128.6;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;104.9;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;100.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;99.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;152.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1612.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;183.0;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;121.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;100.3;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;254.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;109.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1033.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.14;False;;3624.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;175.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;114.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;109.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;107.4;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;98.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;137.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;1558.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;188.6;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;129.7;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;100.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;116.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;107.6;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15.mlp;False;;926.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.15;False;;3492.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;215.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;118.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;104.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;103.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;312.5;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;133.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;1639.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;203.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;113.5;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;101.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;111.8;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;108.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16.mlp;False;;1027.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.16;False;;3565.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;177.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;257.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;102.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;110.8;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;96.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;124.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;1494.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;347.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;113.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;97.2;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;103.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;119.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17.mlp;False;;917.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.17;False;;3568.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;174.0;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;124.0;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;109.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;104.0;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;99.8;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;121.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;1561.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;174.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;164.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;248.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;111.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;106.1;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1088.7;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.18;False;;3494.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;187.7;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;119.5;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;127.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;102.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;96.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;124.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;1553.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;181.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;114.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;98.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;124.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;117.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19.mlp;False;;1064.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.19;False;;3605.3;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;198.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;116.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;106.7;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;251.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;100.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;137.9;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;1560.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;189.4;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;121.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;107.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;110.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;112.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20.mlp;False;;1079.3;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.20;False;;3507.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;187.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;129.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;105.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;102.6;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;98.0;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;131.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;1566.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;186.6;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;116.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;193.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;141.7;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;3535.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21.mlp;False;;6609.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.21;False;;14349.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;10380.3;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;246.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;312.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;109.1;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;102.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;182.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;31693.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;1868.1;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;121.9;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;289.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;1984.9;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;3551.5;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22.mlp;False;;9963.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.22;False;;59548.7;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;10225.1;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;191.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;169.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;104.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;181.4;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;199.4;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;31801.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;1781.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;172.8;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;318.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;2015.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;3551.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23.mlp;False;;9976.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.23;False;;59607.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;10381.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;216.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;128.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;112.2;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;147.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;179.8;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;31766.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;1879.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;127.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;257.7;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;2000.0;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;3542.0;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24.mlp;False;;9812.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.24;False;;59668.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;10160.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;236.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;178.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;113.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;256.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;178.5;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;31767.6;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;1895.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;117.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;450.5;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;1812.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;3540.6;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25.mlp;False;;9990.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.25;False;;59413.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;10385.6;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;290.1;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;118.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;104.7;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;100.7;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;171.0;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;31778.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;1860.9;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;121.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;422.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;1991.1;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;3552.3;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26.mlp;False;;9952.1;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.26;False;;59646.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;10416.9;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;214.3;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;197.3;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;118.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;108.6;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;168.3;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;31734.0;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;1856.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;131.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;418.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;1982.4;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;3542.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27.mlp;False;;10021.5;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.27;False;;59667.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;10326.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;159.8;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;162.8;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;109.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;104.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;373.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;32014.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;1341.2;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;127.6;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;428.8;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;1991.2;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;3549.7;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28.mlp;False;;10111.6;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.28;False;;59589.1;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;10373.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;239.7;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;178.0;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;268.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;104.9;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;206.9;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;31913.5;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;1755.8;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;147.1;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;222.4;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;1950.5;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;3547.4;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29.mlp;False;;9954.4;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.29;False;;59642.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;10353.5;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;145.2;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;117.1;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;109.9;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;105.1;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;387.6;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;31808.8;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;1602.7;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;119.2;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;442.1;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;1980.9;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;3546.2;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30.mlp;False;;9949.0;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.30;False;;59614.4;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;9947.2;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;223.4;;float16[6, 2048, 4096];96.0;336.1;432.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;273.5;;float16[6, 2048, 4096];96.0;432.1;528.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;124.5;;float16[6, 2048, 4096];96.0;528.1;624.1;624.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;106.2;;float16[6, 32, 2048, 128];96.0;624.1;624.1;624.1;float16[2048, 128]float16[2048, 128];1.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;164.1;;float16[6, 2048, 4096];96.0;2256.1;2352.1;2352.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;31770.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;336.1;2352.1;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;1878.3;;float16[6, 2048, 4096];96.0;336.1;720.2;432.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;127.4;;float16[6, 2048, 4096];96.0;432.1;690.1;690.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;273.6;;float16[6, 2048, 11008];258.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;1993.3;;float16[6, 2048, 4096];96.0;690.1;948.1;948.1;float16[6, 2048, 11008];258.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;3561.3;;float16[6, 2048, 11008];258.0;690.1;786.1;786.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31.mlp;False;;9996.2;;float16[6, 2048, 4096];96.0;432.1;786.1;528.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.layers.31;False;;59380.9;;float16[6, 2048, 4096]float16[6, 1, 2048, 2048]int64[1, 2048];144.0;240.1;786.1;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model.norm;True;;10446.8;;float16[6, 2048, 4096];96.0;240.1;624.2;336.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "model;False;;711749.7;;int64[6, 2048]float32[6, 2048];0.1;0.1;624.2;96.1;float16[6, 2048, 4096];96.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "lm_head;True;;326.5;;float16[6, 2048, 4096];96.0;96.1;846.1;846.1;float16[6, 2048, 32000];750.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n",
      "<model>;False;;715227.4;;int64[6, 2048]float32[6, 2048];0.1;0.1;2346.1;1500.1;float32[6, 2048, 32000];1500.0;;0.0;;;-14424.7;-14424.7;-14424.7;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(6, 2048)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66407881-f833-4fcd-acdf-7de603f72ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:34.264599Z",
     "iopub.status.busy": "2023-12-01T23:17:34.264241Z",
     "iopub.status.idle": "2023-12-01T23:17:34.268467Z",
     "shell.execute_reply": "2023-12-01T23:17:34.267804Z",
     "shell.execute_reply.started": "2023-12-01T23:17:34.264586Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13552476160, 13552476160)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ccc7692-dda4-442d-8d61-66865458177c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:17:34.621465Z",
     "iopub.status.busy": "2023-12-01T23:17:34.620551Z",
     "iopub.status.idle": "2023-12-01T23:17:34.788039Z",
     "shell.execute_reply": "2023-12-01T23:17:34.787598Z",
     "shell.execute_reply.started": "2023-12-01T23:17:34.621426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:58 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 517 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      158      145 MB ( 92.39%)\n",
      "GPU:   12,918   12,918 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     751  29,685  31,785 MB   2.36% \n",
      "GPU:   1,422  23,141  24,564 MB   5.79% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcce30-4544-48cc-a375-262dfe1d724b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Mistral-7B\n",
    "\n",
    "mistralai/Mistral-7B-v0.1\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d211ba3-af65-431a-9c5b-8e5e15f9cbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:34:12.402566Z",
     "iopub.status.busy": "2023-12-01T23:34:12.402085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     577  30,026  31,785 MB   1.82% \n",
      "GPU:   1,344  23,219  24,564 MB   5.47% \n",
      "\n",
      "\n",
      "Loading model mistralai/Mistral-7B-v0.1 in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482c325420f4a199b71ca5ac96e66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015924f-8c33-4fcc-adfa-6414574c536a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1596c51f-885d-49bd-9a5c-b3eb46fa9424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:22:28.756839Z",
     "iopub.status.busy": "2023-12-02T02:22:28.756296Z",
     "iopub.status.idle": "2023-12-02T02:22:29.072065Z",
     "shell.execute_reply": "2023-12-02T02:22:29.071569Z",
     "shell.execute_reply.started": "2023-12-02T02:22:28.756802Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     551  29,953  31,785 MB   1.74% \n",
      "GPU:     996  23,567  24,564 MB   4.06% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aba89c-5812-42b0-8a7c-5596ab162f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:32:04.300537Z",
     "iopub.status.busy": "2023-12-02T02:32:04.299222Z",
     "iopub.status.idle": "2023-12-02T02:32:04.362187Z",
     "shell.execute_reply": "2023-12-02T02:32:04.361654Z",
     "shell.execute_reply.started": "2023-12-02T02:32:04.300497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8390e6ef-d7b5-44dc-8f4e-0fe3e653c0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:32:06.102311Z",
     "iopub.status.busy": "2023-12-02T02:32:06.101200Z",
     "iopub.status.idle": "2023-12-02T02:33:01.449434Z",
     "shell.execute_reply": "2023-12-02T02:33:01.448758Z",
     "shell.execute_reply.started": "2023-12-02T02:32:06.102272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31cacce670449659f79104cfecc1184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.98 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658)\n",
      "\n",
      "Tokenizer load time : 301.49 ms\n",
      "Tokenizer CPU memory: 42.26 MB\n",
      "\n",
      "Model load time : 55007.52 ms\n",
      "Model CPU memory: 1.87 GB\n",
      "Model GPU memory: 13.99 GB\n",
      "Max   GPU memory: 13.99 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffbb07e-8651-46c8-ac7e-1bd0dbbc3558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:33:05.208756Z",
     "iopub.status.busy": "2023-12-02T02:33:05.208360Z",
     "iopub.status.idle": "2023-12-02T02:33:05.213484Z",
     "shell.execute_reply": "2023-12-02T02:33:05.212951Z",
     "shell.execute_reply.started": "2023-12-02T02:33:05.208734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231202_023305.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46631b-fa20-4334-8f02-5c92a7bf4060",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb05fc9f-ce25-49b8-b300-7c499f3bd7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:37:12.632791Z",
     "iopub.status.busy": "2023-12-01T23:37:12.631277Z",
     "iopub.status.idle": "2023-12-01T23:37:12.669446Z",
     "shell.execute_reply": "2023-12-01T23:37:12.668853Z",
     "shell.execute_reply.started": "2023-12-01T23:37:12.632688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "MistralForCausalLM\n",
      "> submodules\n",
      "- model: MistralModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#MistralModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: MistralRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X MistralDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#MistralDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: MistralAttention\n",
      "      - mlp: MistralMLP\n",
      "      - input_layernorm: MistralRMSNorm\n",
      "      - post_attention_layernorm: MistralRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#MistralAttention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: MistralRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#MistralRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "          - cos_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "          - sin_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "        ---------------------\n",
      "        mlp#MistralMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 14336] (112.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#MistralRMSNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [4096] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "MistralForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, MistralForCausalLM\n",
      "\n",
      "        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
      "\n",
      "        seq_length_with_past = seq_length\n",
      "        past_key_values_length = 0\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        else:\n",
      "            position_ids = position_ids.view(-1, seq_length).long()\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if (\n",
      "            attention_mask is not None\n",
      "            and hasattr(self.config, \"_flash_attn_2_enabled\")\n",
      "            and self.config._flash_attn_2_enabled\n",
      "            and past_key_values is not None\n",
      "        ):\n",
      "            is_padding_right = attention_mask[:, -1].sum().item() != batch_size\n",
      "            if is_padding_right:\n",
      "                raise ValueError(\n",
      "                    \"You are attempting to perform batched generation with padding_side='right'\"\n",
      "                    \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n",
      "                    \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n",
      "                )\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask,\n",
      "                (batch_size, seq_length),\n",
      "                inputs_embeds,\n",
      "                past_key_values_length,\n",
      "                sliding_window=self.config.sliding_window,\n",
      "            )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_value,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "MistralDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
      "                `(batch, sequence_length)` where padding elements are indicated by 0.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "MistralAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        # repeat k/v heads if n_kv_heads < n_heads\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "MistralRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "MistralRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715f476-0c3a-4547-a4be-da5de9c28db8",
   "metadata": {},
   "source": [
    "#### Perf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533074ac-3463-4399-bc05-7a826a489d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:24:26.563700Z",
     "iopub.status.busy": "2023-12-02T02:24:26.563385Z",
     "iopub.status.idle": "2023-12-02T02:24:26.569114Z",
     "shell.execute_reply": "2023-12-02T02:24:26.568523Z",
     "shell.execute_reply.started": "2023-12-02T02:24:26.563667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15020351488, 15020351488)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0e7ccc-6181-402a-b1c9-50a570cf1d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:24:28.513196Z",
     "iopub.status.busy": "2023-12-02T02:24:28.512688Z",
     "iopub.status.idle": "2023-12-02T02:25:14.219512Z",
     "shell.execute_reply": "2023-12-02T02:25:14.218800Z",
     "shell.execute_reply.started": "2023-12-02T02:24:28.513177Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 2 and sequence length 4096:\n",
      "model.embed_tokens;True;;516.3;;int64[2, 4096];0.1;0.1;64.1;64.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;487.2;;bfloat16[2, 4096, 4096];64.0;128.1;384.1;192.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;381.7;;bfloat16[2, 4096, 4096];64.0;192.1;256.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;251.0;;bfloat16[2, 4096, 4096];64.0;256.1;272.1;272.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;148.7;;bfloat16[2, 4096, 4096];64.0;272.1;288.1;288.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;236.7;;bfloat16[2, 8, 4096, 128];16.0;288.1;288.1;288.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;168.4;;bfloat16[2, 4096, 4096];64.0;2496.1;2560.1;2560.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;2977.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;2560.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;746.9;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;189.4;;bfloat16[2, 4096, 4096];64.0;256.1;480.1;480.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;267.5;;bfloat16[2, 4096, 14336];224.0;480.1;704.1;704.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;135.3;;bfloat16[2, 4096, 4096];64.0;480.1;704.1;704.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;139.2;;bfloat16[2, 4096, 14336];224.0;480.1;544.1;544.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1332.0;;bfloat16[2, 4096, 4096];64.0;256.1;544.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0;False;;6459.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;128.1;544.1;192.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;290.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;135.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;125.3;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;129.2;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;123.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;163.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;1939.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;228.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;141.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;114.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;120.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;129.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1147.0;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1;False;;4336.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;247.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;159.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;187.3;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;193.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;121.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;168.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;2892.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;342.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;545.0;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;133.8;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;149.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;176.6;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp;False;;1543.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2;False;;5589.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;855.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;141.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;117.1;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;107.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;113.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;301.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;1840.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;201.3;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;142.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;104.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;114.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;116.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1098.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3;False;;4611.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;191.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;143.1;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;269.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;107.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;100.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;150.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;1755.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;201.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;147.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;118.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;113.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;114.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp;False;;1118.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4;False;;3761.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;201.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;116.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;108.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;106.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;139.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;145.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;1749.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;199.6;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;118.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;105.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;270.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;114.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1079.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5;False;;3877.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;232.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;128.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;114.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;107.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;103.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;136.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1700.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;216.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;121.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;103.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;111.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;130.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp;False;;939.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6;False;;3776.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;206.3;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;118.6;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;108.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;107.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;284.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;155.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1760.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;210.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;122.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;108.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;114.2;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;114.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1073.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7;False;;3765.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;195.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;278.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;110.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;121.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;100.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;146.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;1759.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;353.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;118.0;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;103.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;123.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;117.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp;False;;932.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8;False;;3901.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;196.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;115.9;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;109.2;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;106.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;102.2;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;151.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;1684.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;201.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;122.4;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;250.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;124.1;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;124.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp;False;;1098.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9;False;;3666.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;222.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;129.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;127.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;113.4;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;119.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;147.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;1791.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;216.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;122.4;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;103.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;112.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;257.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp;False;;1064.3;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10;False;;3985.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;197.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;143.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;121.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;118.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;109.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;133.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;1775.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;198.8;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;317.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;109.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;114.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;111.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1127.3;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11;False;;3831.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;362.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;122.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;107.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;121.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;102.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;315.7;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1763.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;202.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;119.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;105.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;114.2;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;118.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp;False;;1071.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12;False;;3928.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;202.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;124.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;285.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;111.4;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;102.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;147.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1690.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;193.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;135.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;106.9;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;117.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;113.4;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp;False;;1115.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13;False;;3743.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;199.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;120.8;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;108.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;107.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;119.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;146.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1702.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;183.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;117.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;106.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;260.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;124.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1099.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14;False;;3856.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;183.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;127.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;114.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;107.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;127.9;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;151.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;1702.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;195.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;123.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;106.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;115.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;116.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp;False;;936.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15;False;;3687.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;190.0;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;110.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;111.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;289.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;135.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;1705.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;181.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;114.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;102.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;119.5;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;110.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp;False;;1062.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16;False;;3643.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;188.3;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;280.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;110.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;107.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;110.9;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;146.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;1682.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;388.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;128.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;273.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;152.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;131.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp;False;;1213.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17;False;;4201.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;286.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;139.1;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;118.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;116.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;104.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;149.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;1756.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;190.8;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;118.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;323.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;125.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;113.3;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1174.4;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18;False;;3959.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;193.0;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;758.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;114.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;110.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;103.0;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;146.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;2513.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;201.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;134.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;105.8;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;117.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;276.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp;False;;1142.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19;False;;4555.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;199.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;118.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;116.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;115.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;102.0;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;272.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;745463.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;2680.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;352.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;126.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;124.3;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp;False;;1250.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20;False;;750152.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;2330.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;4847.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;7406.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;3620.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;188.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;593.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;821322.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;2357.6;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;186.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;166.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;259.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;138.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp;False;;1675.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21;False;;828301.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;1866.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;4823.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;7366.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;3651.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;149.2;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;267.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;821560.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;2673.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;139.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;120.9;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;118.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;120.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp;False;;1169.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22;False;;827862.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;2298.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;4416.8;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;7390.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;3673.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;138.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;282.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;820502.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;2693.0;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;130.2;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;132.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;280.5;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;117.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp;False;;1177.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23;False;;827607.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;2441.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;4791.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;7093.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;3680.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;111.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;275.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;821008.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;2748.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;290.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;171.0;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;133.9;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;126.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp;False;;1358.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24;False;;828507.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;1768.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;4690.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;7408.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;3647.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;290.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;270.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;821681.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;2709.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;137.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;136.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;120.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;115.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp;False;;1224.4;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25;False;;827969.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;2366.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;4847.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;7357.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;3676.2;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;112.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;263.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;823341.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;2711.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;130.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;113.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;118.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;120.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp;False;;983.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26;False;;830203.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;2320.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;4621.9;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;7428.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;3607.8;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;116.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;258.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;821668.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;2709.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;152.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;303.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;125.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;120.6;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp;False;;1296.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27;False;;828694.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;2255.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;4698.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;7383.1;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;3681.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;165.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;320.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;820609.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;2272.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;155.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;125.0;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;123.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;462.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp;False;;1465.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28;False;;827377.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;2059.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;4847.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;7392.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;3422.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;127.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;255.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;820669.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;2712.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;324.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;112.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;112.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;113.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp;False;;1142.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29;False;;827138.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;2453.7;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;4843.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;7377.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;3694.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;141.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;561.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;822634.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;2594.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;194.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;147.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;213.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;159.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp;False;;1584.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30;False;;829909.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;1737.7;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;4837.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;7332.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;3682.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;117.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;306.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;822358.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;2644.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;162.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;125.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;174.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp;False;;1560.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31;False;;828913.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.norm;True;;2098.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model;False;;9953890.9;;int64[2, 4096]float32[2, 4096];0.1;0.0;448.1;64.0;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "lm_head;True;;620.5;;bfloat16[2, 4096, 4096];64.0;64.0;564.0;564.0;bfloat16[2, 4096, 32000];500.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "<model>;False;;9975285.6;;int64[2, 4096]float32[2, 4096];0.1;0.0;1564.0;1000.0;float32[2, 4096, 32000];1000.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(2, 4096)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6c2ca2-2608-4c5e-b47c-ac5020cb72e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:27:10.396037Z",
     "iopub.status.busy": "2023-12-02T02:27:10.394176Z",
     "iopub.status.idle": "2023-12-02T02:27:10.425785Z",
     "shell.execute_reply": "2023-12-02T02:27:10.425269Z",
     "shell.execute_reply.started": "2023-12-02T02:27:10.395966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231202_022710.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d09ad0-f801-4e1b-b2f1-89896d9c5b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:27:41.071205Z",
     "iopub.status.busy": "2023-12-02T02:27:41.070266Z",
     "iopub.status.idle": "2023-12-02T02:27:41.081821Z",
     "shell.execute_reply": "2023-12-02T02:27:41.081050Z",
     "shell.execute_reply.started": "2023-12-02T02:27:41.071169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9c0212-735d-4fe8-8757-c75c22fdf5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T00:25:29.632043Z",
     "iopub.status.busy": "2023-12-02T00:25:29.631533Z",
     "iopub.status.idle": "2023-12-02T00:25:30.156694Z",
     "shell.execute_reply": "2023-12-02T00:25:30.156119Z",
     "shell.execute_reply.started": "2023-12-02T00:25:29.632027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:24:30 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1049 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:    2,002        0 MB (  0.00%)\n",
      "GPU:     -162        0 MB ( -0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,571  27,941  31,785 MB   8.09% \n",
      "GPU:     910  23,653  24,564 MB   3.71% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc22d5-b937-4a3f-b940-a24db29be5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
