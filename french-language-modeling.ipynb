{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {},
   "source": [
    "# Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:14:03.286861Z",
     "iopub.status.busy": "2023-11-17T20:14:03.286304Z",
     "iopub.status.idle": "2023-11-17T20:14:03.804335Z",
     "shell.execute_reply": "2023-11-17T20:14:03.803869Z",
     "shell.execute_reply.started": "2023-11-17T20:14:03.286844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {},
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T23:38:05.973188Z",
     "iopub.status.busy": "2023-11-16T23:38:05.972336Z",
     "iopub.status.idle": "2023-11-16T23:38:05.977439Z",
     "shell.execute_reply": "2023-11-16T23:38:05.976555Z",
     "shell.execute_reply.started": "2023-11-16T23:38:05.973159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:22:18.988139Z",
     "iopub.status.busy": "2023-11-15T06:22:18.987564Z",
     "iopub.status.idle": "2023-11-15T06:22:25.849906Z",
     "shell.execute_reply": "2023-11-15T06:22:25.849209Z",
     "shell.execute_reply.started": "2023-11-15T06:22:18.988115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-6B.0b745f4b791d153df9fbd0062e81c45728868b56.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.f7bc352f27bb1c02ee371a4576942a7d96c8bb97.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.d08b22ae22ee52b7fd762bf6e44711d90cc96794.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "{'type': <class 'transformers_modules.01-ai.Yi-34B.3fac38b00c086fb2420a731d53517582d9303843.tokenization_yi.YiTokenizer'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'model': <class 'sentencepiece.SentencePieceProcessor'>}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {},
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T06:25:16.477085Z",
     "iopub.status.busy": "2023-11-15T06:25:16.476437Z",
     "iopub.status.idle": "2023-11-15T20:18:05.373331Z",
     "shell.execute_reply": "2023-11-15T20:18:05.361097Z",
     "shell.execute_reply.started": "2023-11-15T06:25:16.477066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466e7dfdc62a4073bf32fd1beb3b099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fad18c8c64ca2bc8f09635d0d7d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26e5a4d1f744dcb6933fe85293ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6edff8851f4abdb00d256bd38919f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fdd7631cc4e2a89580fabf990cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1636c2841b24452b9dd3a0b609300911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-webscraper/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b9087acf2b42248cf5af8a981024df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3407c0b6940c09a7f929cab3bc22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e252a7ffff74a4d909a8bf7b5d4aef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffde3291ccd43aebed0045a6bc157e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efbb66c793e4f02bfa39c35e3d6a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c569195cc44a782e98dbd953ccb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6aefa8c53c42a69a223d33daa7e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cc7b0e0e94dfd9080fc4a7faf9848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff776e00b6cf42e58bd0caeff74a8a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c009ec2f1df34c0c915013bdc3c3ffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f3b8d94964fb18fbc66d9745d21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d622690413b4bd8a9257d48b7a34f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becd40479c0b4e4a8bdf4d2d0e850932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d9471d7744e16b18245d0bf57d7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad3b977791546c985708da95bde3fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710cca87f5c34722a8e52d6f0a0f3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c334be2f31e4d34ac6d1c5de47f91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15c1cd12cf4e73b48aa58c95609a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b096d674c8e429ea2383ee3e4d322d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07b3f0c-fa88-49c0-a2a9-74da94b0181d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:36.772930Z",
     "iopub.status.busy": "2023-11-15T22:28:36.772616Z",
     "iopub.status.idle": "2023-11-15T22:28:36.782697Z",
     "shell.execute_reply": "2023-11-15T22:28:36.781762Z",
     "shell.execute_reply.started": "2023-11-15T22:28:36.772887Z"
    }
   },
   "outputs": [],
   "source": [
    "other_models = { \n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b093a3-5c98-4506-8ff6-fd4d3fcf4dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T22:28:54.337937Z",
     "iopub.status.busy": "2023-11-15T22:28:54.337640Z",
     "iopub.status.idle": "2023-11-15T22:47:11.856419Z",
     "shell.execute_reply": "2023-11-15T22:47:11.855564Z",
     "shell.execute_reply.started": "2023-11-15T22:28:54.337905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95363c4b2ecb4817aefe35f5efd8e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff40d5c48894470a78b4613e83e09a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670c247f14504a7499e23e49e7e90f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612755630066205 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf5ed54abf3424580c210b51b182742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "for model in other_models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {},
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {},
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef2623-1568-44c9-b0e1-22d434c8d673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:11.007140Z",
     "iopub.status.busy": "2023-11-18T12:50:11.006435Z",
     "iopub.status.idle": "2023-11-18T12:50:11.012078Z",
     "shell.execute_reply": "2023-11-18T12:50:11.011446Z",
     "shell.execute_reply.started": "2023-11-18T12:50:11.007116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",\n",
    "\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "\n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ea611a-0c97-45f6-aa47-4fe8474954b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:50:16.865613Z",
     "iopub.status.busy": "2023-11-18T12:50:16.864880Z",
     "iopub.status.idle": "2023-11-18T12:50:16.870681Z",
     "shell.execute_reply": "2023-11-18T12:50:16.869546Z",
     "shell.execute_reply.started": "2023-11-18T12:50:16.865578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = models[\"redpajama_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f28d43-e44b-49b2-a29f-20b103d1d1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:51.109646Z",
     "iopub.status.busy": "2023-11-17T23:49:51.109274Z",
     "iopub.status.idle": "2023-11-17T23:49:56.330791Z",
     "shell.execute_reply": "2023-11-17T23:49:56.330376Z",
     "shell.execute_reply.started": "2023-11-17T23:49:51.109630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dc4ac7-3333-4310-bb4d-bc555b41ebf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:49:56.331914Z",
     "iopub.status.busy": "2023-11-17T23:49:56.331453Z",
     "iopub.status.idle": "2023-11-17T23:50:06.822516Z",
     "shell.execute_reply": "2023-11-17T23:50:06.821911Z",
     "shell.execute_reply.started": "2023-11-17T23:49:56.331902Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb64419b9e54f1d9271e6a7034fe838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8817b2eeb74d83a7ac4fc16bd30f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1290ddeb18645fa93e0fd5058c7949c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407a3a7bc743445fbe9feaef5900fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda35c0f10d4d15a9a0fbb3aa408b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7eb0a738c40e991f2de95404a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"frenchtext/banque-fr-2311\"\n",
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dbb014-c670-410e-88a6-219a3466770f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.823168Z",
     "iopub.status.busy": "2023-11-17T23:50:06.822994Z",
     "iopub.status.idle": "2023-11-17T23:50:06.826497Z",
     "shell.execute_reply": "2023-11-17T23:50:06.825982Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.823160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example = dataset[0][\"Text\"]\n",
    "len(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896de44a-2e1c-45d3-87ce-527e6b3f38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.827305Z",
     "iopub.status.busy": "2023-11-17T23:50:06.827192Z",
     "iopub.status.idle": "2023-11-17T23:50:06.848030Z",
     "shell.execute_reply": "2023-11-17T23:50:06.847618Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.827297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text_example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fa350c-36ef-4615-9c2b-23b845880794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:06.848944Z",
     "iopub.status.busy": "2023-11-17T23:50:06.848554Z",
     "iopub.status.idle": "2023-11-17T23:50:08.534591Z",
     "shell.execute_reply": "2023-11-17T23:50:08.534182Z",
     "shell.execute_reply.started": "2023-11-17T23:50:06.848929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 23:50:07.766211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-17 23:50:07.786800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.375988006591797"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenize_input = tokenizer.encode(text_example[:2048], return_tensors=\"pt\")\n",
    "    loss = model(tokenize_input, labels=tokenize_input)[0]\n",
    "torch.exp(loss.float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c21247-3ea4-450f-b810-3699c1f9e1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.535351Z",
     "iopub.status.busy": "2023-11-17T23:50:08.535080Z",
     "iopub.status.idle": "2023-11-17T23:50:08.541585Z",
     "shell.execute_reply": "2023-11-17T23:50:08.541199Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.535337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375\n",
      "3689\n",
      "3804\n",
      "2104\n",
      "7936\n",
      "3802\n",
      "10669\n",
      "2171\n",
      "3568\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(dataset[i][\"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ad0bf0-41ec-4a90-98d7-db8ab7150b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.542513Z",
     "iopub.status.busy": "2023-11-17T23:50:08.542112Z",
     "iopub.status.idle": "2023-11-17T23:50:08.557331Z",
     "shell.execute_reply": "2023-11-17T23:50:08.556940Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.542501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c375df-43ab-44f7-a3e0-4ec139734e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.558032Z",
     "iopub.status.busy": "2023-11-17T23:50:08.557899Z",
     "iopub.status.idle": "2023-11-17T23:50:08.573642Z",
     "shell.execute_reply": "2023-11-17T23:50:08.573227Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.558022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(text = dataset[0:10][\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73874f-5726-4e82-941f-ccf68c109291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.574256Z",
     "iopub.status.busy": "2023-11-17T23:50:08.574143Z",
     "iopub.status.idle": "2023-11-17T23:50:08.580661Z",
     "shell.execute_reply": "2023-11-17T23:50:08.580268Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.574248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048 1630\n",
      "1 2048 1160\n",
      "2 2048 1162\n",
      "3 2048 658\n",
      "4 2048 2048\n",
      "4 2048 1741\n",
      "5 2048 1175\n",
      "6 2048 2048\n",
      "6 2048 2048\n",
      "6 2048 1234\n",
      "7 2048 648\n",
      "8 2048 1072\n",
      "9 2048 585\n"
     ]
    }
   ],
   "source": [
    "for sample_mapping,input_ids,attention_mask in zip(encodings[\"overflow_to_sample_mapping\"],encodings[\"input_ids\"],encodings[\"attention_mask\"]):\n",
    "    print(sample_mapping,len(input_ids),sum(attention_mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b66afe1-835e-4f37-b749-9066805d35ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.581766Z",
     "iopub.status.busy": "2023-11-17T23:50:08.581571Z",
     "iopub.status.idle": "2023-11-17T23:50:08.588527Z",
     "shell.execute_reply": "2023-11-17T23:50:08.588112Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.581757Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dataset = dataset.sort(\"Words\").filter(lambda example: example[\"Words\"]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f520b9-2e06-4158-ab39-476563ed75fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.589061Z",
     "iopub.status.busy": "2023-11-17T23:50:08.588968Z",
     "iopub.status.idle": "2023-11-17T23:50:08.591277Z",
     "shell.execute_reply": "2023-11-17T23:50:08.590863Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.589053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(batch_size=32):\n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294cc1d6-bb4a-4253-a249-3af30b0ee43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.591941Z",
     "iopub.status.busy": "2023-11-17T23:50:08.591730Z",
     "iopub.status.idle": "2023-11-17T23:50:08.594172Z",
     "shell.execute_reply": "2023-11-17T23:50:08.593798Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.591928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(dataset_batch):\n",
    "    return tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=int(tokenizer.model_max_length/2),\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16,\n",
    "                      return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6f843d-e0ab-41db-b6ef-15334d943e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T23:50:08.594732Z",
     "iopub.status.busy": "2023-11-17T23:50:08.594630Z",
     "iopub.status.idle": "2023-11-17T23:50:08.614329Z",
     "shell.execute_reply": "2023-11-17T23:50:08.613824Z",
     "shell.execute_reply.started": "2023-11-17T23:50:08.594724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  4,  6,  5,  7, 23,  7,  7,  7,  7,  7, 10,  7,  7, 13,  6,  4,  6,\n",
      "         6, 15,  4,  4,  4, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 32])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12,  6, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "torch.Size([32, 16])\n",
      "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         6, 12, 12, 12,  7, 12, 12, 12, 12, 12, 12, 12, 12, 47])\n",
      "torch.Size([32, 48])\n",
      "tensor([45, 45, 44, 48, 55, 39, 51, 46, 43, 42, 40, 47, 41, 44, 47, 42, 40, 62,\n",
      "        43, 55, 34, 44, 42, 53, 53, 30, 56, 52, 51, 53, 37, 41])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for idx,dataset_batch in enumerate(get_dataset_batches()):\n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "    print(encodings[\"attention_mask\"].sum(axis=1))\n",
    "    print(encodings[\"input_ids\"].size())\n",
    "    if idx == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe85da1-182d-4d7b-8cac-452fb26bc76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nlls = []\n",
    "for idx,dataset_batch in enumerate(get_dataset_batches(batch_size)):\n",
    "    \n",
    "    encodings = encode_dataset_batch(dataset_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    \n",
    "    if idx%10==0: print(f\"{(idx+1)*batch_size} / {len(dataset)}: {neg_log_likelihood}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b74e49-e5f3-48c4-86ba-7f84fbdb6cfb",
   "metadata": {},
   "source": [
    "15 min\n",
    "\n",
    "32 / 85229: 7.71484375\n",
    "352 / 85229: 7.09765625\n",
    "672 / 85229: 8.1015625\n",
    "992 / 85229: 5.12109375\n",
    "1312 / 85229: 5.78125\n",
    "1632 / 85229: 5.51953125\n",
    "1952 / 85229: 6.12109375\n",
    "2272 / 85229: 3.36328125\n",
    "2592 / 85229: 5.26171875\n",
    "2912 / 85229: 5.0703125\n",
    "3232 / 85229: 4.87109375\n",
    "3552 / 85229: 4.74609375\n",
    "3872 / 85229: 4.046875\n",
    "4192 / 85229: 4.3046875\n",
    "4512 / 85229: 4.11328125\n",
    "4832 / 85229: 4.56640625\n",
    "5152 / 85229: 4.51171875\n",
    "5472 / 85229: 4.359375\n",
    "5792 / 85229: 4.32421875\n",
    "6112 / 85229: 4.68359375\n",
    "6432 / 85229: 4.6015625\n",
    "6752 / 85229: 3.658203125\n",
    "7072 / 85229: 4.7734375\n",
    "7392 / 85229: 6.6328125\n",
    "7712 / 85229: 3.869140625\n",
    "8032 / 85229: 4.9375\n",
    "8352 / 85229: 4.359375\n",
    "8672 / 85229: 5.0078125\n",
    "8992 / 85229: 4.09375\n",
    "9312 / 85229: 3.513671875\n",
    "9632 / 85229: 3.578125\n",
    "9952 / 85229: 3.609375\n",
    "10272 / 85229: 3.505859375\n",
    "10592 / 85229: 2.99609375\n",
    "10912 / 85229: 3.5859375\n",
    "11232 / 85229: 3.18359375\n",
    "11552 / 85229: 3.48046875\n",
    "11872 / 85229: 3.11328125\n",
    "12192 / 85229: 3.40234375\n",
    "12512 / 85229: 3.189453125\n",
    "12832 / 85229: 4.3515625\n",
    "13152 / 85229: 3.306640625\n",
    "13472 / 85229: 3.40234375\n",
    "13792 / 85229: 3.162109375\n",
    "14112 / 85229: 3.451171875\n",
    "14432 / 85229: 2.845703125\n",
    "14752 / 85229: 4.3515625\n",
    "15072 / 85229: 4.2890625\n",
    "15392 / 85229: 2.91015625\n",
    "15712 / 85229: 3.1328125\n",
    "16032 / 85229: 4.02734375\n",
    "16352 / 85229: 3.126953125\n",
    "16672 / 85229: 3.478515625\n",
    "16992 / 85229: 3.193359375\n",
    "17312 / 85229: 3.3203125\n",
    "17632 / 85229: 3.0078125\n",
    "\n",
    "OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6177ee4-eb93-44c3-a6ad-52784cc55742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T06:49:07.814995Z",
     "iopub.status.busy": "2023-11-18T06:49:07.814819Z",
     "iopub.status.idle": "2023-11-18T06:49:07.841774Z",
     "shell.execute_reply": "2023-11-18T06:49:07.841206Z",
     "shell.execute_reply.started": "2023-11-18T06:49:07.814984Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.5841)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.stack(nlls).mean().float())\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5611711-1e58-46d7-84bf-8c4263ed78d8",
   "metadata": {},
   "source": [
    "# Memory study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a5319-aca9-41ac-816e-1b65065ac3e5",
   "metadata": {},
   "source": [
    "## CUDA helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585551-ae01-4dd9-a0d8-7bb4946bb196",
   "metadata": {},
   "source": [
    "[CUDA semantics / Memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management)\n",
    "\n",
    "[Understanding CUDA Memory Usage](https://pytorch.org/docs/stable/torch_cuda_memory.html#torch-cuda-memory)\n",
    "\n",
    "[CUDA memory management API](https://pytorch.org/docs/stable/cuda.html#cuda-memory-management-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d2a5f9-40a1-4fce-a106-c3f16cf58484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T21:55:37.303562Z",
     "iopub.status.busy": "2023-11-24T21:55:37.302740Z",
     "iopub.status.idle": "2023-11-24T21:55:38.016517Z",
     "shell.execute_reply": "2023-11-24T21:55:38.016087Z",
     "shell.execute_reply.started": "2023-11-24T21:55:37.303527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved :      0.0 MB -   0 %\n",
      "Free     : 23,008.0 MB -  93 %\n",
      "------------------------------\n",
      "Used     :      0.0 MB -   0 %\n",
      "Max used :      0.0 MB -   0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "    \n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeea17f5-409d-4230-bab9-4010f3b6cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:55:15.162845Z",
     "iopub.status.busy": "2023-11-18T12:55:15.162447Z",
     "iopub.status.idle": "2023-11-18T12:55:15.167405Z",
     "shell.execute_reply": "2023-11-18T12:55:15.166833Z",
     "shell.execute_reply.started": "2023-11-18T12:55:15.162820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41785e96-8956-4aee-97d5-d0479c89eb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T12:56:49.681121Z",
     "iopub.status.busy": "2023-11-18T12:56:49.680290Z",
     "iopub.status.idle": "2023-11-18T12:56:55.117481Z",
     "shell.execute_reply": "2023-11-18T12:56:55.117059Z",
     "shell.execute_reply.started": "2023-11-18T12:56:49.681088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a128d7-8070-4de1-bf66-b6683881921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:00.726679Z",
     "iopub.status.busy": "2023-11-18T13:05:00.725839Z",
     "iopub.status.idle": "2023-11-18T13:05:00.739971Z",
     "shell.execute_reply": "2023-11-18T13:05:00.739570Z",
     "shell.execute_reply.started": "2023-11-18T13:05:00.726644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231118_130500.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e22441-8d1d-43c8-90eb-abb78cd3fa2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T13:05:06.460703Z",
     "iopub.status.busy": "2023-11-18T13:05:06.459810Z",
     "iopub.status.idle": "2023-11-18T13:05:06.468484Z",
     "shell.execute_reply": "2023-11-18T13:05:06.467915Z",
     "shell.execute_reply.started": "2023-11-18T13:05:06.460670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad0d6c-289c-4e79-9c6b-c95c0d033581",
   "metadata": {},
   "source": [
    "## Pytorch models exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85be5910-7850-4787-b36c-203bdf93ac92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:40:02.824012Z",
     "iopub.status.busy": "2023-11-19T16:40:02.823531Z",
     "iopub.status.idle": "2023-11-19T16:40:02.826926Z",
     "shell.execute_reply": "2023-11-19T16:40:02.826475Z",
     "shell.execute_reply.started": "2023-11-19T16:40:02.823996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def find_attribute_origin(obj, attr_name):\n",
    "    for cls in obj.__class__.__mro__:\n",
    "        if attr_name in dir(cls):\n",
    "            return cls.__name__\n",
    "    return obj.__class__.__name__\n",
    "\n",
    "def display_members(obj):\n",
    "    obj_attributes = {}\n",
    "    for member_name in dir(obj):\n",
    "        if member_name[0:1]!=\"_\":\n",
    "            obj_attributes[getattr(obj,member_name).__qualname__ if hasattr(getattr(obj,member_name),\"__qualname__\") else f\"{find_attribute_origin(obj,member_name)}.{member_name}\"] = str(type(getattr(obj,member_name)))\n",
    "    obj_attributes = {k: obj_attributes[k] for k in sorted(obj_attributes)}\n",
    "    for member_name in obj_attributes.keys():\n",
    "        print(member_name, obj_attributes[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "663d0ad3-8085-4699-a6b4-c176eb76aca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T16:24:40.156700Z",
     "iopub.status.busy": "2023-11-18T16:24:40.156401Z",
     "iopub.status.idle": "2023-11-18T16:24:40.186241Z",
     "shell.execute_reply": "2023-11-18T16:24:40.185806Z",
     "shell.execute_reply.started": "2023-11-18T16:24:40.156689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXConfig <class 'type'>\n",
      "GPTNeoXForCausalLM.T_destination <class 'typing.TypeVar'>\n",
      "GPTNeoXForCausalLM.base_model <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.base_model_prefix <class 'str'>\n",
      "GPTNeoXForCausalLM.call_super_init <class 'bool'>\n",
      "GPTNeoXForCausalLM.config <class 'transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig'>\n",
      "GPTNeoXForCausalLM.device <class 'torch.device'>\n",
      "GPTNeoXForCausalLM.dtype <class 'torch.dtype'>\n",
      "GPTNeoXForCausalLM.dummy_inputs <class 'dict'>\n",
      "GPTNeoXForCausalLM.dump_patches <class 'bool'>\n",
      "GPTNeoXForCausalLM.embed_out <class 'torch.nn.modules.linear.Linear'>\n",
      "GPTNeoXForCausalLM.forward <class 'functools.partial'>\n",
      "GPTNeoXForCausalLM.framework <class 'str'>\n",
      "GPTNeoXForCausalLM.generation_config <class 'transformers.generation.configuration_utils.GenerationConfig'>\n",
      "GPTNeoXForCausalLM.get_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.gpt_neox <class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel'>\n",
      "GPTNeoXForCausalLM.hf_device_map <class 'dict'>\n",
      "GPTNeoXForCausalLM.is_8bit_serializable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_4bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_loaded_in_8bit <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_parallelizable <class 'bool'>\n",
      "GPTNeoXForCausalLM.is_quantized <class 'bool'>\n",
      "GPTNeoXForCausalLM.main_input_name <class 'str'>\n",
      "GPTNeoXForCausalLM.name_or_path <class 'str'>\n",
      "GPTNeoXForCausalLM.prepare_inputs_for_generation <class 'method'>\n",
      "GPTNeoXForCausalLM.quantization_method <enum 'QuantizationMethod'>\n",
      "GPTNeoXForCausalLM.set_output_embeddings <class 'method'>\n",
      "GPTNeoXForCausalLM.supports_gradient_checkpointing <class 'bool'>\n",
      "GPTNeoXForCausalLM.training <class 'bool'>\n",
      "GPTNeoXForCausalLM.warnings_issued <class 'dict'>\n",
      "GenerationMixin.assisted_decoding <class 'method'>\n",
      "GenerationMixin.beam_sample <class 'method'>\n",
      "GenerationMixin.beam_search <class 'method'>\n",
      "GenerationMixin.compute_transition_scores <class 'method'>\n",
      "GenerationMixin.constrained_beam_search <class 'method'>\n",
      "GenerationMixin.contrastive_search <class 'method'>\n",
      "GenerationMixin.generate <class 'method'>\n",
      "GenerationMixin.greedy_search <class 'method'>\n",
      "GenerationMixin.group_beam_search <class 'method'>\n",
      "GenerationMixin.sample <class 'method'>\n",
      "Module.add_module <class 'method'>\n",
      "Module.apply <class 'method'>\n",
      "Module.bfloat16 <class 'method'>\n",
      "Module.buffers <class 'method'>\n",
      "Module.children <class 'method'>\n",
      "Module.cpu <class 'method'>\n",
      "Module.cuda <class 'function'>\n",
      "Module.double <class 'method'>\n",
      "Module.eval <class 'method'>\n",
      "Module.extra_repr <class 'method'>\n",
      "Module.get_buffer <class 'method'>\n",
      "Module.get_extra_state <class 'method'>\n",
      "Module.get_parameter <class 'method'>\n",
      "Module.get_submodule <class 'method'>\n",
      "Module.ipu <class 'method'>\n",
      "Module.load_state_dict <class 'method'>\n",
      "Module.modules <class 'method'>\n",
      "Module.named_buffers <class 'method'>\n",
      "Module.named_children <class 'method'>\n",
      "Module.named_modules <class 'method'>\n",
      "Module.named_parameters <class 'method'>\n",
      "Module.parameters <class 'method'>\n",
      "Module.register_backward_hook <class 'method'>\n",
      "Module.register_buffer <class 'method'>\n",
      "Module.register_forward_hook <class 'method'>\n",
      "Module.register_forward_pre_hook <class 'method'>\n",
      "Module.register_full_backward_hook <class 'method'>\n",
      "Module.register_full_backward_pre_hook <class 'method'>\n",
      "Module.register_load_state_dict_post_hook <class 'method'>\n",
      "Module.register_module <class 'method'>\n",
      "Module.register_parameter <class 'method'>\n",
      "Module.register_state_dict_pre_hook <class 'method'>\n",
      "Module.requires_grad_ <class 'method'>\n",
      "Module.set_extra_state <class 'method'>\n",
      "Module.share_memory <class 'method'>\n",
      "Module.state_dict <class 'method'>\n",
      "Module.to <class 'function'>\n",
      "Module.to_empty <class 'method'>\n",
      "Module.train <class 'method'>\n",
      "Module.type <class 'method'>\n",
      "Module.xpu <class 'method'>\n",
      "Module.zero_grad <class 'method'>\n",
      "ModuleUtilsMixin.add_memory_hooks <class 'method'>\n",
      "ModuleUtilsMixin.create_extended_attention_mask_for_decoder <class 'function'>\n",
      "ModuleUtilsMixin.estimate_tokens <class 'method'>\n",
      "ModuleUtilsMixin.floating_point_ops <class 'method'>\n",
      "ModuleUtilsMixin.get_extended_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.get_head_mask <class 'method'>\n",
      "ModuleUtilsMixin.invert_attention_mask <class 'method'>\n",
      "ModuleUtilsMixin.num_parameters <class 'method'>\n",
      "ModuleUtilsMixin.reset_memory_hooks_state <class 'method'>\n",
      "PeftAdapterMixin.active_adapter <class 'method'>\n",
      "PeftAdapterMixin.active_adapters <class 'method'>\n",
      "PeftAdapterMixin.add_adapter <class 'method'>\n",
      "PeftAdapterMixin.disable_adapters <class 'method'>\n",
      "PeftAdapterMixin.enable_adapters <class 'method'>\n",
      "PeftAdapterMixin.get_adapter_state_dict <class 'method'>\n",
      "PeftAdapterMixin.load_adapter <class 'method'>\n",
      "PeftAdapterMixin.set_adapter <class 'method'>\n",
      "PreTrainedModel.can_generate <class 'method'>\n",
      "PreTrainedModel.disable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.enable_input_require_grads <class 'method'>\n",
      "PreTrainedModel.float <class 'method'>\n",
      "PreTrainedModel.from_pretrained <class 'method'>\n",
      "PreTrainedModel.get_input_embeddings <class 'method'>\n",
      "PreTrainedModel.get_memory_footprint <class 'method'>\n",
      "PreTrainedModel.get_position_embeddings <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_disable <class 'method'>\n",
      "PreTrainedModel.gradient_checkpointing_enable <class 'method'>\n",
      "PreTrainedModel.half <class 'method'>\n",
      "PreTrainedModel.init_weights <class 'method'>\n",
      "PreTrainedModel.post_init <class 'method'>\n",
      "PreTrainedModel.prune_heads <class 'method'>\n",
      "PreTrainedModel.register_for_auto_class <class 'method'>\n",
      "PreTrainedModel.resize_position_embeddings <class 'method'>\n",
      "PreTrainedModel.resize_token_embeddings <class 'method'>\n",
      "PreTrainedModel.retrieve_modules_from_names <class 'method'>\n",
      "PreTrainedModel.reverse_bettertransformer <class 'method'>\n",
      "PreTrainedModel.save_pretrained <class 'method'>\n",
      "PreTrainedModel.set_input_embeddings <class 'method'>\n",
      "PreTrainedModel.tie_weights <class 'method'>\n",
      "PreTrainedModel.to_bettertransformer <class 'method'>\n",
      "PreTrainedModel.warn_if_padding_and_no_attention_mask <class 'method'>\n",
      "PushToHubMixin.push_to_hub <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "display_members(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd82911-00ea-48a4-80d2-709a0797818a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T16:41:51.525145Z",
     "iopub.status.busy": "2023-11-19T16:41:51.524456Z",
     "iopub.status.idle": "2023-11-19T16:41:51.534152Z",
     "shell.execute_reply": "2023-11-19T16:41:51.533665Z",
     "shell.execute_reply.started": "2023-11-19T16:41:51.525118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "56d76159-52cd-4a11-acae-b3461ce566b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:31:35.570614Z",
     "iopub.status.busy": "2023-11-19T07:31:35.569668Z",
     "iopub.status.idle": "2023-11-19T07:31:35.592081Z",
     "shell.execute_reply": "2023-11-19T07:31:35.591622Z",
     "shell.execute_reply.started": "2023-11-19T07:31:35.570571Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "GPTNeoXForCausalLM\n",
      "> submodules\n",
      "- gpt_neox: GPTNeoXModel\n",
      "- embed_out: Linear\n",
      "  ---------------------\n",
      "  gpt_neox#GPTNeoXModel\n",
      "  > submodules\n",
      "  - embed_in: Embedding\n",
      "  - emb_dropout: Dropout\n",
      "  - layers: ModuleList\n",
      "  - final_layer_norm: LayerNorm\n",
      "    ---------------------\n",
      "    embed_in#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [50432, 2560] (246.2 MB)\n",
      "    ---------------------\n",
      "    emb_dropout#Dropout\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X GPTNeoXLayer\n",
      "      ---------------------\n",
      "      0..31#GPTNeoXLayer\n",
      "      > submodules\n",
      "      - input_layernorm: LayerNorm\n",
      "      - post_attention_layernorm: LayerNorm\n",
      "      - post_attention_dropout: Dropout\n",
      "      - post_mlp_dropout: Dropout\n",
      "      - attention: GPTNeoXAttention\n",
      "      - mlp: GPTNeoXMLP\n",
      "        ---------------------\n",
      "        input_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LayerNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2560] (0.0 MB)\n",
      "        - bias: float16 [2560] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        post_mlp_dropout#Dropout\n",
      "        ---------------------\n",
      "        attention#GPTNeoXAttention\n",
      "        > buffers\n",
      "        - bias: bool [1, 1, 2048, 2048] (4.0 MB)\n",
      "        - masked_bias: float16 [] (0.0 MB)\n",
      "        > submodules\n",
      "        - rotary_emb: GPTNeoXRotaryEmbedding\n",
      "        - query_key_value: Linear8bitLt\n",
      "        - dense: Linear8bitLt\n",
      "        - attention_dropout: Dropout\n",
      "          ---------------------\n",
      "          rotary_emb#GPTNeoXRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [40] (0.0 MB)\n",
      "          - cos_cached: float16 [2048, 80] (0.3 MB)\n",
      "          - sin_cached: float16 [2048, 80] (0.3 MB)\n",
      "          ---------------------\n",
      "          query_key_value#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [7680, 2560] (18.8 MB)\n",
      "          - bias: float16 [7680] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 2560] (6.2 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          attention_dropout#Dropout\n",
      "        ---------------------\n",
      "        mlp#GPTNeoXMLP\n",
      "        > submodules\n",
      "        - dense_h_to_4h: Linear8bitLt\n",
      "        - dense_4h_to_h: Linear8bitLt\n",
      "        - act: GELUActivation\n",
      "          ---------------------\n",
      "          dense_h_to_4h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [10240, 2560] (25.0 MB)\n",
      "          - bias: float16 [10240] (0.0 MB)\n",
      "          ---------------------\n",
      "          dense_4h_to_h#Linear8bitLt\n",
      "          > parameters\n",
      "          - weight: int8 [2560, 10240] (25.0 MB)\n",
      "          - bias: float16 [2560] (0.0 MB)\n",
      "          ---------------------\n",
      "          act#GELUActivation\n",
      "    ---------------------\n",
      "    final_layer_norm#LayerNorm\n",
      "    > parameters\n",
      "    - weight: float16 [2560] (0.0 MB)\n",
      "    - bias: float16 [2560] (0.0 MB)\n",
      "  ---------------------\n",
      "  embed_out#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [50432, 2560] (246.2 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "GPTNeoXForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "            only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "            `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "        >>> import torch\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "        >>> config.is_decoder = True\n",
      "        >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "        >>> outputs = model(**inputs)\n",
      "\n",
      "        >>> prediction_logits = outputs.logits\n",
      "        ```\"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.gpt_neox(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            past_key_values=past_key_values,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        lm_logits = self.embed_out(hidden_states)\n",
      "\n",
      "        lm_loss = None\n",
      "        if labels is not None:\n",
      "            # move labels to correct device to enable model parallelism\n",
      "            labels = labels.to(lm_logits.device)\n",
      "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
      "            shift_logits = lm_logits[:, :-1, :].contiguous()\n",
      "            labels = labels[:, 1:].contiguous()\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (lm_logits,) + outputs[1:]\n",
      "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=lm_loss,\n",
      "            logits=lm_logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "GPTNeoXModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPast,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \"\"\"\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
      "            input_shape = input_ids.size()\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        batch_size, seq_length = input_shape\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_length = 0\n",
      "            past_key_values = tuple([None] * self.config.num_hidden_layers)\n",
      "        else:\n",
      "            past_length = past_key_values[0][0].size(-2)\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n",
      "            position_ids = position_ids.unsqueeze(0)\n",
      "\n",
      "        # Attention mask.\n",
      "        if attention_mask is not None:\n",
      "            assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
      "            attention_mask = attention_mask.view(batch_size, -1)\n",
      "            # We create a 3D attention mask from a 2D tensor mask.\n",
      "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
      "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
      "            # this attention mask is more simple than the triangular masking of causal attention\n",
      "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
      "            attention_mask = attention_mask[:, None, None, :]\n",
      "\n",
      "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "            # masked positions, this operation will create a tensor which is 0.0 for\n",
      "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
      "            # Since we are adding it to the raw scores before the softmax, this is\n",
      "            # effectively the same as removing these entirely.\n",
      "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
      "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
      "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_in(input_ids)\n",
      "\n",
      "        hidden_states = self.emb_dropout(inputs_embeds)\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        presents = () if use_cache else None\n",
      "        all_attentions = () if output_attentions else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        for i, (layer, layer_past) in enumerate(zip(self.layers, past_key_values)):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                outputs = self._gradient_checkpointing_func(\n",
      "                    layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    head_mask[i],\n",
      "                    use_cache,\n",
      "                    None,\n",
      "                    output_attentions,\n",
      "                )\n",
      "            else:\n",
      "                outputs = layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    head_mask=head_mask[i],\n",
      "                    layer_past=layer_past,\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                )\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "            if output_attentions:\n",
      "                all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n",
      "\n",
      "        hidden_states = self.final_layer_norm(hidden_states)\n",
      "        # Add last hidden state\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
      "\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "Dropout.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.dropout(input, self.p, self.training, self.inplace)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[torch.FloatTensor],\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        attention_layer_outputs = self.attention(\n",
      "            self.input_layernorm(hidden_states),\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            layer_past=layer_past,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "        )\n",
      "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
      "        attn_output = self.post_attention_dropout(attn_output)\n",
      "        outputs = attention_layer_outputs[1:]\n",
      "\n",
      "        if self.use_parallel_residual:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output + hidden_states\n",
      "        else:\n",
      "            # pseudocode:\n",
      "            # x = x + attn(ln1(x))\n",
      "            # x = x + mlp(ln2(x))\n",
      "            attn_output = attn_output + hidden_states\n",
      "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
      "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
      "            hidden_states = mlp_output + attn_output\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
      "        else:\n",
      "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LayerNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.layer_norm(\n",
      "            input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "\n",
      "---------------------\n",
      "GPTNeoXAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.FloatTensor,\n",
      "        attention_mask: torch.FloatTensor,\n",
      "        position_ids: torch.LongTensor,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ):\n",
      "        has_layer_past = layer_past is not None\n",
      "\n",
      "        # Compute QKV\n",
      "        # Attention heads [batch, seq_len, hidden_size]\n",
      "        #   --> [batch, seq_len, (np * 3 * head_size)]\n",
      "        qkv = self.query_key_value(hidden_states)\n",
      "\n",
      "        # [batch, seq_len, (num_heads * 3 * head_size)]\n",
      "        #   --> [batch, seq_len, num_heads, 3 * head_size]\n",
      "        new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
      "        qkv = qkv.view(*new_qkv_shape)\n",
      "\n",
      "        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
      "        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
      "        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
      "        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
      "\n",
      "        # Compute rotary embeddings on rotary_ndims\n",
      "        query_rot = query[..., : self.rotary_ndims]\n",
      "        query_pass = query[..., self.rotary_ndims :]\n",
      "        key_rot = key[..., : self.rotary_ndims]\n",
      "        key_pass = key[..., self.rotary_ndims :]\n",
      "\n",
      "        # Compute token offset for rotary embeddings (when decoding)\n",
      "        seq_len = key.shape[-2]\n",
      "        if has_layer_past:\n",
      "            seq_len += layer_past[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
      "        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
      "        query = torch.cat((query, query_pass), dim=-1)\n",
      "        key = torch.cat((key, key_pass), dim=-1)\n",
      "\n",
      "        # Cache QKV values\n",
      "        if has_layer_past:\n",
      "            past_key = layer_past[0]\n",
      "            past_value = layer_past[1]\n",
      "            key = torch.cat((past_key, key), dim=-2)\n",
      "            value = torch.cat((past_value, value), dim=-2)\n",
      "        present = (key, value) if use_cache else None\n",
      "\n",
      "        # Compute attention\n",
      "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
      "\n",
      "        # Reshape outputs\n",
      "        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n",
      "        attn_output = self.dense(attn_output)\n",
      "\n",
      "        outputs = (attn_output, present)\n",
      "        if output_attentions:\n",
      "            outputs += (attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "GPTNeoXRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Linear8bitLt.forward()\n",
      "---------------------\n",
      "    def forward(self, x: torch.Tensor):\n",
      "        self.state.is_training = self.training\n",
      "        if self.weight.CB is not None:\n",
      "            self.init_8bit_state()\n",
      "\n",
      "        # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
      "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
      "            self.bias.data = self.bias.data.to(x.dtype)\n",
      "\n",
      "        out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
      "\n",
      "        if not self.state.has_fp16_weights:\n",
      "            if self.state.CB is not None and self.state.CxB is not None:\n",
      "                # we converted 8-bit row major to turing/ampere format in the first inference pass\n",
      "                # we no longer need the row-major weight\n",
      "                del self.state.CB\n",
      "                self.weight.data = self.state.CxB\n",
      "        return out\n",
      "\n",
      "---------------------\n",
      "GPTNeoXMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "---------------------\n",
      "GELUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self.act(input)\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72c72-5447-48ac-a895-4fda1ae5d6fe",
   "metadata": {},
   "source": [
    "# Huggingface language models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526914e-3ed9-454a-8005-aa3bf5b85594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T14:24:29.139926Z",
     "iopub.status.busy": "2023-11-19T14:24:29.139496Z",
     "iopub.status.idle": "2023-11-19T14:24:30.216800Z",
     "shell.execute_reply": "2023-11-19T14:24:30.214848Z",
     "shell.execute_reply.started": "2023-11-19T14:24:29.139898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a2337c-99b4-473c-90f2-75c82ebe7c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T00:17:12.657016Z",
     "iopub.status.busy": "2023-11-25T00:17:12.656263Z",
     "iopub.status.idle": "2023-11-25T00:17:19.963685Z",
     "shell.execute_reply": "2023-11-25T00:17:19.961813Z",
     "shell.execute_reply.started": "2023-11-25T00:17:12.656983Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.3.4.tar.gz (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from flash-attn) (21.3)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from flash-attn) (2.0.1)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[83 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.0.1\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.4/flash_attn-2.3.4+cu122torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/torch/utils/cpp_extension.py:484: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/block.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/vit.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/llama.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bert.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/opt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/layernorm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'flash_attn_2_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn/src\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/flash_attn -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/flash_attn/src -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/cutlass/include -I/usr/lib/python3/dist-packages/torch/include -I/usr/lib/python3/dist-packages/torch/include/torch/csrc/api/include -I/usr/lib/python3/dist-packages/torch/include/TH -I/usr/lib/python3/dist-packages/torch/include/THC -I/workspace/wordslab-llms/.venv/include -I/usr/include/python3.10 -c csrc/flash_attn/flash_api.cpp -o build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1\n",
      "  \u001b[31m   \u001b[0m In file included from /usr/lib/python3/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/flash_attn/flash_api.cpp:6:\n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/torch/include/torch/csrc/Exceptions.h:14:10: fatal error: pybind11/pybind11.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m    14 | #include <pybind11/pybind11.h>\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
      "Failed to build flash-attn\n",
      "Installing collected packages: ninja, einops, flash-attn\n",
      "  Running setup.py install for flash-attn ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for flash-attn\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[83 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.0.1\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running install\n",
      "  \u001b[31m   \u001b[0m /workspace/wordslab-llms/.venv/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/block.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/vit.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/llama.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bert.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/opt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/layernorm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/torch/utils/cpp_extension.py:484: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m building 'flash_attn_2_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn/src\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/flash_attn -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/flash_attn/src -I/tmp/pip-install-cxbndjho/flash-attn_6e0332a61e694056803d3c27a19a5d52/csrc/cutlass/include -I/usr/lib/python3/dist-packages/torch/include -I/usr/lib/python3/dist-packages/torch/include/torch/csrc/api/include -I/usr/lib/python3/dist-packages/torch/include/TH -I/usr/lib/python3/dist-packages/torch/include/THC -I/workspace/wordslab-llms/.venv/include -I/usr/include/python3.10 -c csrc/flash_attn/flash_api.cpp -o build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=1\n",
      "  \u001b[31m   \u001b[0m In file included from /usr/lib/python3/dist-packages/torch/include/torch/csrc/api/include/torch/python.h:11,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/flash_attn/flash_api.cpp:6:\n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/torch/include/torch/csrc/Exceptions.h:14:10: fatal error: pybind11/pybind11.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m    14 | #include <pybind11/pybind11.h>\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
      "\u001b[31m╰─>\u001b[0m flash-attn\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bbef6d-f92b-44e3-9a41-b8db521c49a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:15:59.131118Z",
     "iopub.status.busy": "2023-11-25T11:15:59.130020Z",
     "iopub.status.idle": "2023-11-25T11:16:00.206536Z",
     "shell.execute_reply": "2023-11-25T11:16:00.206079Z",
     "shell.execute_reply.started": "2023-11-25T11:15:59.131081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.modules.linear.Linear.forward(self, input: torch.Tensor) -> torch.Tensor>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.modules.Linear.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8967c84-c3ff-4d1d-ac12-e053462ab2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:39:48.287488Z",
     "iopub.status.busy": "2023-11-25T11:39:48.286757Z",
     "iopub.status.idle": "2023-11-25T11:39:48.292507Z",
     "shell.execute_reply": "2023-11-25T11:39:48.291894Z",
     "shell.execute_reply.started": "2023-11-25T11:39:48.287456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import textwrap\n",
    "\n",
    "def find_called_functions(func):\n",
    "    source = inspect.getsource(func)\n",
    "    source = textwrap.dedent(source)\n",
    "    tree = ast.parse(source)\n",
    "    \n",
    "    called_functions = []\n",
    "\n",
    "    class FunctionCallVisitor(ast.NodeVisitor):\n",
    "        def visit_Call(self, node):\n",
    "            called_functions.append(node)\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                called_functions.append(node.func.id)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    FunctionCallVisitor().visit(tree)\n",
    "    return called_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b21b728-de1f-47d2-884c-9dcfe780361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:22:59.203254Z",
     "iopub.status.busy": "2023-11-25T13:22:59.201912Z",
     "iopub.status.idle": "2023-11-25T13:22:59.260171Z",
     "shell.execute_reply": "2023-11-25T13:22:59.259714Z",
     "shell.execute_reply.started": "2023-11-25T13:22:59.203213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Script: '/usr/lib/python3/dist-packages/torch/nn/modules/linear.py' <SameEnvironment: 3.10.12 in /workspace/wordslab-llms/.venv>>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import jedi\n",
    "\n",
    "classobj = torch.nn.modules.Linear\n",
    "script = jedi.Script(path=inspect.getfile(classobj))\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a87b7371-faa8-45cd-9212-cfe1db3e713b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:42:30.005791Z",
     "iopub.status.busy": "2023-11-25T13:42:30.005055Z",
     "iopub.status.idle": "2023-11-25T13:42:30.017508Z",
     "shell.execute_reply": "2023-11-25T13:42:30.017186Z",
     "shell.execute_reply.started": "2023-11-25T13:42:30.005760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Name full_name='torch.nn.modules.linear.Linear.forward.linear', description='linear'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall = None\n",
    "\n",
    "for name in script.get_names(all_scopes=True, definitions=True, references=True):\n",
    "    if name.full_name == \"torch.nn.modules.linear.Linear.forward.linear\":\n",
    "        myfunctioncall = name\n",
    "        break\n",
    "\n",
    "myfunctioncall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0501354c-ee1d-44cc-a191-47a03f6b7af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:48:57.920380Z",
     "iopub.status.busy": "2023-11-25T13:48:57.919635Z",
     "iopub.status.idle": "2023-11-25T13:48:57.927712Z",
     "shell.execute_reply": "2023-11-25T13:48:57.926688Z",
     "shell.execute_reply.started": "2023-11-25T13:48:57.920331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name full_name='torch.nn.functional.linear', description='linear = _add_docstr( torch._C._nn.linear, r\"\"\" linear(input, weight, bias=None) -> Tensor Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. This operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>` {sparse_beta_warning} This operator supports :ref:`TensorFloat32<tf32_on_ampere>`. Shape: - Input: :math:`(*, in\\\\_features)` where `*` means any number of additional dimensions, including none - Weight: :math:`(out\\\\_features, in\\\\_features)` or :math:`(in\\\\_features)` - Bias: :math:`(out\\\\_features)` or :math:`()` - Output: :math:`(*, out\\\\_features)` or :math:`(*)`, based on the shape of the weight \"\"\".format(**sparse_support_notes))'>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunctioncall.goto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "432af689-ab81-406a-b4ec-d6bf5c860b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:43:31.131568Z",
     "iopub.status.busy": "2023-11-25T13:43:31.130642Z",
     "iopub.status.idle": "2023-11-25T13:43:31.140520Z",
     "shell.execute_reply": "2023-11-25T13:43:31.139446Z",
     "shell.execute_reply.started": "2023-11-25T13:43:31.131514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attr for attr in dir(myfunctioncall) if not callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunctioncall) if callable(getattr(myfunctioncall, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "634c5e15-19dc-4cd4-9d98-273bc123aef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:54:02.333093Z",
     "iopub.status.busy": "2023-11-25T13:54:02.332278Z",
     "iopub.status.idle": "2023-11-25T13:54:02.343608Z",
     "shell.execute_reply": "2023-11-25T13:54:02.343184Z",
     "shell.execute_reply.started": "2023-11-25T13:54:02.333056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function torch._C._nn.linear>, True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "called_function = eval(script.search(\"F.linear\")[0].full_name)\n",
    "called_function, isinstance(called_function, types.BuiltinFunctionType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01f57ae2-097b-4ccf-9c53-122b97363826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:09.977323Z",
     "iopub.status.busy": "2023-11-25T13:17:09.976180Z",
     "iopub.status.idle": "2023-11-25T13:17:09.986413Z",
     "shell.execute_reply": "2023-11-25T13:17:09.985735Z",
     "shell.execute_reply.started": "2023-11-25T13:17:09.977274Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_inference_state',\n",
       "  '_mapping',\n",
       "  '_name',\n",
       "  '_tuple_mapping',\n",
       "  'column',\n",
       "  'description',\n",
       "  'full_name',\n",
       "  'is_keyword',\n",
       "  'line',\n",
       "  'module_name',\n",
       "  'module_path',\n",
       "  'name',\n",
       "  'type'],\n",
       " ['defined_names',\n",
       "  'docstring',\n",
       "  'execute',\n",
       "  'get_definition_end_position',\n",
       "  'get_definition_start_position',\n",
       "  'get_line_code',\n",
       "  'get_signatures',\n",
       "  'get_type_hint',\n",
       "  'goto',\n",
       "  'in_builtin_module',\n",
       "  'infer',\n",
       "  'is_definition',\n",
       "  'is_side_effect',\n",
       "  'is_stub',\n",
       "  'parent'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://jedi.readthedocs.io/en/latest/docs/api-classes.html#name\n",
    "\n",
    "myfunction = None\n",
    "\n",
    "for function in script.get_names(all_scopes=True):\n",
    "    if function.name == \"forward\" and function.type == 'function':\n",
    "        myfunction = function\n",
    "        break\n",
    "        \n",
    "[attr for attr in dir(myfunction) if not callable(getattr(myfunction, attr)) and not attr.startswith(\"__\")], [attr for attr in dir(myfunction) if callable(getattr(myfunction, attr)) and not attr.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "916f8095-b82e-4f35-9a70-adf9afb71aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:21:10.665309Z",
     "iopub.status.busy": "2023-11-25T13:21:10.664447Z",
     "iopub.status.idle": "2023-11-25T13:21:10.673674Z",
     "shell.execute_reply": "2023-11-25T13:21:10.672333Z",
     "shell.execute_reply.started": "2023-11-25T13:21:10.665259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Name name='self', description='param self'>,\n",
       " <Name name='input', description='param input: Tensor'>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e226688-03ec-4a9b-8e5b-7d5383215253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T13:17:29.900165Z",
     "iopub.status.busy": "2023-11-25T13:17:29.899925Z",
     "iopub.status.idle": "2023-11-25T13:17:29.903541Z",
     "shell.execute_reply": "2023-11-25T13:17:29.903082Z",
     "shell.execute_reply.started": "2023-11-25T13:17:29.900152Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Call at 0x7f23381ad4e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_call = called_functions[0]\n",
    "ast_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "374e43f5-7461-49e9-9f9e-ce48991df618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:41:06.795032Z",
     "iopub.status.busy": "2023-11-25T12:41:06.794448Z",
     "iopub.status.idle": "2023-11-25T12:41:06.800139Z",
     "shell.execute_reply": "2023-11-25T12:41:06.799738Z",
     "shell.execute_reply.started": "2023-11-25T12:41:06.795005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F.linear'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = inspect.getsource(torch.nn.modules.Linear.forward)\n",
    "source = textwrap.dedent(source)\n",
    "function_called = \"\"\n",
    "for idx,line in enumerate(source.splitlines()):\n",
    "    if idx+1 == ast_func.lineno:\n",
    "        if ast_func.end_lineno > ast_func.lineno:\n",
    "            function_called += line[ast_func.col_offset:].strip()\n",
    "        else:\n",
    "            function_called += line[ast_func.col_offset:ast_func.end_col_offset] \n",
    "    elif idx+1 > ast_func.lineno and idx+1 < ast_func.end_lineno:\n",
    "        function_called += \" \" + line.strip()\n",
    "    elif ast_func.end_lineno > ast_func.lineno and idx+1 == ast_func.end_lineno:\n",
    "        function_calledµ += \" \" + line[:ast_func.end_col_offset].strip()\n",
    "    elif idx+1 > ast_func.end_lineno:\n",
    "        break\n",
    "        \n",
    "function_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9287d570-50bf-4676-bd3b-18992ae4b7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T11:45:56.280459Z",
     "iopub.status.busy": "2023-11-25T11:45:56.280108Z",
     "iopub.status.idle": "2023-11-25T11:45:56.285847Z",
     "shell.execute_reply": "2023-11-25T11:45:56.285195Z",
     "shell.execute_reply.started": "2023-11-25T11:45:56.280437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ast.Attribute at 0x7f23381ad5d0>,\n",
       " [<ast.Name at 0x7f23381aec20>,\n",
       "  <ast.Attribute at 0x7f23381ae4d0>,\n",
       "  <ast.Attribute at 0x7f23381ae290>],\n",
       " [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func = ast_call.func\n",
    "ast_args = ast_call.args\n",
    "ast_keywords = ast_call.keywords\n",
    "(ast_func,ast_args,ast_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3637b890-a087-4b96-8042-9f9e2d47f96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:14:06.116817Z",
     "iopub.status.busy": "2023-11-25T12:14:06.116420Z",
     "iopub.status.idle": "2023-11-25T12:14:06.123582Z",
     "shell.execute_reply": "2023-11-25T12:14:06.123077Z",
     "shell.execute_reply.started": "2023-11-25T12:14:06.116790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F', 'linear')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_func.value.id, ast_func.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3c05174-7530-4616-bad1-5cb1d757a0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:19:56.383491Z",
     "iopub.status.busy": "2023-11-25T12:19:56.382989Z",
     "iopub.status.idle": "2023-11-25T12:19:56.390702Z",
     "shell.execute_reply": "2023-11-25T12:19:56.390280Z",
     "shell.execute_reply.started": "2023-11-25T12:19:56.383469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input', 'self', 'weight', 'self', 'bias')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_args[0].id,  ast_args[1].value.id, ast_args[1].attr, ast_args[2].value.id, ast_args[2].attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e86d8aa-616c-4b74-9959-444e3a46933d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:20:09.664297Z",
     "iopub.status.busy": "2023-11-25T12:20:09.663902Z",
     "iopub.status.idle": "2023-11-25T12:20:09.666740Z",
     "shell.execute_reply": "2023-11-25T12:20:09.666369Z",
     "shell.execute_reply.started": "2023-11-25T12:20:09.664282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9847189a-0abb-41d7-9c50-648b313104b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T12:31:04.364054Z",
     "iopub.status.busy": "2023-11-25T12:31:04.363488Z",
     "iopub.status.idle": "2023-11-25T12:31:04.368369Z",
     "shell.execute_reply": "2023-11-25T12:31:04.367844Z",
     "shell.execute_reply.started": "2023-11-25T12:31:04.364015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_attributes',\n",
       " '_fields',\n",
       " 'attr',\n",
       " 'col_offset',\n",
       " 'ctx',\n",
       " 'end_col_offset',\n",
       " 'end_lineno',\n",
       " 'lineno',\n",
       " 'value']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = ast_func\n",
    "[attr for attr in dir(obj) if not callable(getattr(obj, attr)) and not attr.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38f749-d8fe-46ad-8efe-1d08d696ebe0",
   "metadata": {},
   "source": [
    "https://github.com/stas00/ipyexperiments/blob/master/README.md\n",
    "\n",
    "> pip install ipyexperiments\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "exp1 = IPyExperimentsPytorch()\n",
    "...\n",
    "exp1.keep_var_names('var1', 'var2')\n",
    "\n",
    "# optional\n",
    "data = exp1.finish()\n",
    "cpu_data_final = data.cpu\n",
    "gpu_data_final = data.gpu\n",
    "\n",
    "del exp1\n",
    "```\n",
    "\n",
    "Detailed syntax:\n",
    "\n",
    "```python\n",
    "exp = IPyExperimentsPytorch(cl_enable=True, cl_compact=False, cl_gc_collect=True, cl_set_seed=0)\n",
    "```\n",
    "\n",
    "- cl_enable - enable the subsystem\n",
    "- cl_compact - use compact one line printouts\n",
    "- cl_gc_collect - get correct memory usage reports. Don't use when tracking memory leaks (objects with circular reference).\n",
    "- cl_set_seed - set RNG seed before each cell is run to the provided seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56717d6-49ea-4e04-8f68-59dc6b92fac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757796c0-57bd-419f-828e-2d845e430877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:01:58.620179Z",
     "iopub.status.busy": "2023-11-25T21:01:58.620090Z",
     "iopub.status.idle": "2023-11-25T21:01:59.267756Z",
     "shell.execute_reply": "2023-11-25T21:01:59.267172Z",
     "shell.execute_reply.started": "2023-11-25T21:01:58.620170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "# Need to test the line below with Pytorch 2.1\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.8,expandable_segments:True\"\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            total_size += os.path.getsize(full_entry_path)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_used_cpu_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_memory = process.memory_info().rss\n",
    "    return process_memory\n",
    "\n",
    "def get_used_and_max_gpu_memory():\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    return used_memory,max_used_memory\n",
    "\n",
    "def reset_max_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c95b2c-f905-477a-aa52-f685e848ff9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:01:59.269018Z",
     "iopub.status.busy": "2023-11-25T21:01:59.268689Z",
     "iopub.status.idle": "2023-11-25T21:01:59.314934Z",
     "shell.execute_reply": "2023-11-25T21:01:59.314514Z",
     "shell.execute_reply.started": "2023-11-25T21:01:59.269005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000\n",
    "\n",
    "def get_tensor_params_size_and_dim(param):\n",
    "    if param is None:\n",
    "        return 0,\"\"\n",
    "    elif isinstance(param, torch.Tensor):\n",
    "        psize = param.numel() * param.element_size()\n",
    "        pdim = f\"{str(param.dtype)[6:]}{str(param.size())[11:-1]}\"\n",
    "        return psize,pdim\n",
    "    elif isinstance(param, dict):\n",
    "        size = 0\n",
    "        dim = \"\"\n",
    "        for value in param.values():            \n",
    "            psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "            size += psize\n",
    "            dim += pdim\n",
    "        return size, dim\n",
    "    else:\n",
    "        try:\n",
    "            iter(param)\n",
    "            size = 0\n",
    "            dim = \"\"\n",
    "            for value in param:            \n",
    "                psize, pdim = get_tensor_params_size_and_dim(value)\n",
    "                size += psize\n",
    "                dim += pdim\n",
    "            return size, dim\n",
    "        except TypeError:\n",
    "            return 0,\"\"\n",
    "\n",
    "class ModulePerf:\n",
    "    \n",
    "    def __init__(self, module_name, module, is_leaf_module):\n",
    "        self.module_name = module_name\n",
    "        self.module = module\n",
    "        self.is_leaf_module = is_leaf_module\n",
    "        \n",
    "        self.before_forward_time_ns = 0\n",
    "        self.before_forward_used_memory = 0\n",
    "        self.forward_inputs_memory_size = 0 \n",
    "        self.forward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_forward_time_ns = 0\n",
    "        self.after_forward_used_memory = 0\n",
    "        self.forward_max_used_memory = 0        \n",
    "        self.forward_outputs_memory_size = 0\n",
    "        self.forward_outputs_memory_dim = \"\"\n",
    "        \n",
    "        self.before_backward_time_ns = 0\n",
    "        self.before_backward_used_memory = 0\n",
    "        self.backward_inputs_memory_size = 0\n",
    "        self.backward_inputs_memory_dim = \"\" \n",
    "        \n",
    "        self.after_backward_time_ns = 0\n",
    "        self.after_backward_used_memory = 0\n",
    "        self.backward_max_used_memory = 0\n",
    "        self.backward_outputs_memory_size = 0\n",
    "        self.backward_outputs_memory_dim = \"\"\n",
    "        \n",
    "    def before_forward(self, module, args, kwargs):\n",
    "        self.before_forward_time_ns = perf_counter_ns()\n",
    "        self.before_forward_used_memory,_ = get_used_and_max_gpu_memory()   \n",
    "        args_size,args_dim = get_tensor_params_size_and_dim(args)\n",
    "        kwargs_size,kwargs_dim = get_tensor_params_size_and_dim(kwargs) \n",
    "        self.forward_inputs_memory_size = args_size + kwargs_size\n",
    "        self.forward_inputs_memory_dim = args_dim + kwargs_dim\n",
    "        if self.is_leaf_module: reset_max_gpu_memory()\n",
    "        \n",
    "    def after_forward(self, module, args, kwargs, output):\n",
    "        self.after_forward_time_ns = perf_counter_ns()\n",
    "        self.after_forward_used_memory, self.forward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.forward_outputs_memory_size, self.forward_outputs_memory_dim = get_tensor_params_size_and_dim(output) \n",
    "        \n",
    "    def before_backward(self, module, grad_output):\n",
    "        self.before_backward_time_ns = perf_counter_ns()\n",
    "        self.before_backward_used_memory,_ = get_used_and_max_gpu_memory()\n",
    "        self.backward_inputs_memory_size, self.backward_inputs_memory_dim = get_tensor_params_size_and_dim(grad_output) \n",
    "        \n",
    "    def after_backward(self, module, grad_input, grad_output):\n",
    "        self.after_backward_time_ns = perf_counter_ns()\n",
    "        self.after_backward_used_memory, self.backward_max_used_memory = get_used_and_max_gpu_memory()\n",
    "        self.backward_outputs_memory_size, self.backward_outputs_memory_dim = get_tensor_params_size_and_dim(grad_input) \n",
    "    \n",
    "    def get_stats_line(self, initial_used_memory):\n",
    "        return f\"{self.module_name};{self.is_leaf_module};;{(self.after_forward_time_ns-self.before_forward_time_ns)/time_unit_µs:.1f};;{self.forward_inputs_memory_dim};{self.forward_inputs_memory_size/memory_unit_mb:.1f};{(self.before_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.forward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.after_forward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.forward_outputs_memory_dim};{self.forward_outputs_memory_size/memory_unit_mb:.1f};;{(self.after_backward_time_ns-self.before_backward_time_ns)/time_unit_µs:.1f};;{self.backward_inputs_memory_dim};{(self.backward_inputs_memory_size-initial_used_memory)/memory_unit_mb:.1f};{(self.before_backward_used_memory-initial_used_memory)/memory_unit_mb:.1f};{(self.backward_max_used_memory-initial_used_memory)/memory_unit_mb:.1f};{self.after_backward_used_memory/memory_unit_mb:.1f};{self.backward_outputs_memory_dim};{self.backward_outputs_memory_size/memory_unit_mb:.1f}\"\n",
    "    \n",
    "class ModelForCausalLMBenchmark:   \n",
    "    \n",
    "    @staticmethod\n",
    "    def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        path,size = get_model_path_and_size_on_disk(model)\n",
    "        print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"--> stored in directory: {path}\")\n",
    "        print()\n",
    "    \n",
    "    def __init__(self, pretrained_model_id):\n",
    "        self.pretrained_model_id = pretrained_model_id\n",
    "        self.tokenizer = None \n",
    "        self.model = None\n",
    "        \n",
    "        self.model_path = None\n",
    "        self.model_size_on_disk = 0\n",
    "        self.tokenizer_load_time_ns = 0\n",
    "        self.tokenizer_cpu_memory = 0\n",
    "        self.model_load_time_ns = 0\n",
    "        self.model_cpu_memory = 0\n",
    "        self.model_gpu_memory = 0\n",
    "        self.model_load_max_gpu_memory = 0\n",
    "        \n",
    "    def trace_load_from_cache(self, **kwargs):\n",
    "        cpu_memory_before = get_used_cpu_memory()\n",
    "        gpu_memory_before = get_used_and_max_gpu_memory()[0]\n",
    "        reset_max_gpu_memory()        \n",
    "        time_before = perf_counter_ns()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_tokenizer = get_used_cpu_memory()\n",
    "        time_tokenizer = perf_counter_ns()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_model = get_used_cpu_memory()\n",
    "        gpu_memory_model,max_gpu_memory_model = get_used_and_max_gpu_memory()     \n",
    "        time_model = perf_counter_ns()\n",
    "        \n",
    "        self.model_path,self.model_size_on_disk = get_model_path_and_size_on_disk(self.model)\n",
    "        self.tokenizer_load_time_ns = time_tokenizer-time_before\n",
    "        self.tokenizer_cpu_memory = cpu_memory_tokenizer-cpu_memory_before\n",
    "        self.model_load_time_ns = time_model-time_tokenizer\n",
    "        self.model_cpu_memory = cpu_memory_model-cpu_memory_tokenizer\n",
    "        self.model_gpu_memory = gpu_memory_model-gpu_memory_before\n",
    "        self.model_load_max_gpu_memory = max_gpu_memory_model\n",
    "        \n",
    "        self.display_load_results()            \n",
    "    \n",
    "    def display_load_results(self):\n",
    "        print(f\"Model files: {(self.model_size_on_disk/1024/1024/1024):.2f} GB on disk\")\n",
    "        print(\"\"f\"(cache path: {self.model_path})\")\n",
    "        print()\n",
    "        print(f\"Tokenizer load time : {(self.tokenizer_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Tokenizer CPU memory: {(self.tokenizer_cpu_memory/memory_unit_mb):.2f} MB\")\n",
    "        print()\n",
    "        print(f\"Model load time : {(self.model_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Model CPU memory: {(self.model_cpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Model GPU memory: {(self.model_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Max   GPU memory: {(self.model_load_max_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print()\n",
    "        \n",
    "    def trace_prefill(self, batch_size, seq_length):\n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "        \n",
    "        # measure perfs\n",
    "        moduleperfs = []\n",
    "        hookhandles = []\n",
    "        try:\n",
    "            for module_name,module in self.model.named_modules():\n",
    "                if module_name==\"\": module_name=\"<model>\"\n",
    "                mperf = ModulePerf(module_name, module, len(list(module.children())) == 0)\n",
    "                moduleperfs.append(mperf)                \n",
    "                hookhandles.append(module.register_forward_pre_hook(mperf.before_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_forward_hook(mperf.after_forward, with_kwargs=True))\n",
    "                hookhandles.append(module.register_full_backward_pre_hook(mperf.before_backward))\n",
    "                hookhandles.append(module.register_full_backward_hook(mperf.after_backward))\n",
    "            \n",
    "            # perf test\n",
    "            input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "            attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "        finally:\n",
    "            for handle in hookhandles:\n",
    "                handle.remove()    \n",
    "                \n",
    "        # sort modules\n",
    "        sorted_moduleperfs = sorted(moduleperfs, key=lambda mp: mp.after_forward_time_ns)\n",
    "        first_mperf = None\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.before_forward_used_memory>0:\n",
    "                first_mperf = mperf\n",
    "                break\n",
    "        initial_used_memory = first_mperf.before_forward_used_memory - first_mperf.forward_inputs_memory_size\n",
    "        \n",
    "        # display results\n",
    "        print(f\"Prefill test for batch size {batch_size} and sequence length {seq_length}:\")\n",
    "        for mperf in sorted_moduleperfs:\n",
    "            if mperf.after_forward_time_ns>0:\n",
    "                print(mperf.get_stats_line(initial_used_memory))\n",
    "    \n",
    "    def check_prefill(self, max_batch_size):\n",
    "        seq_length = self.tokenizer.model_max_length\n",
    "        batch_size = 1\n",
    "        \n",
    "        # warmup\n",
    "        input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "        attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "        # perf test\n",
    "        base_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "        seq_length = 128\n",
    "        while seq_length <= self.tokenizer.model_max_length:\n",
    "            for batch_size in range(1,max_batch_size):\n",
    "                #print(f\"--- {batch_size} x {seq_length} ---\")\n",
    "                reset_max_gpu_memory()\n",
    "                initial_gpu_memory,_ = get_used_and_max_gpu_memory()\n",
    "                input_ids = torch.randint(low=0, high=self.tokenizer.vocab_size, size=(batch_size,seq_length), dtype=torch.int64).to(self.model.device)\n",
    "                attention_mask = torch.ones(batch_size,seq_length).to(self.model.device)\n",
    "                before_forward_time_ns = perf_counter_ns()\n",
    "                with torch.no_grad():\n",
    "                    self.model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "                after_forward_time_ns = perf_counter_ns()\n",
    "                before_release = perf_counter_ns()\n",
    "                # https://pytorch.org/docs/stable/notes/cuda.html#environment-variables\n",
    "                # see \"expandable_segments\": Pytorch allocator doesn't work when we gradually increase batch size !\n",
    "                # when inferencing with a constant batch size, this should not be needed\n",
    "                release_cached_memory()\n",
    "                after_release = perf_counter_ns()            \n",
    "                gpu_memory, max_gpu_memory = get_used_and_max_gpu_memory()\n",
    "                #print(f\"Forward pass  : {(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.1f} ms\")\n",
    "                #print(f\"Initial memory  : {((initial_gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Maximum memory: {((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"Final memory  : {((gpu_memory-base_gpu_memory)/memory_unit_gb):.2f} GB\")\n",
    "                #print(f\"+ GPU cache release  : {(after_release-before_release)/time_unit_ms:.1f} ms\")\n",
    "                print(f\"{batch_size},{seq_length},{(after_forward_time_ns-before_forward_time_ns)/time_unit_ms:.2f},{((max_gpu_memory-initial_gpu_memory)/memory_unit_gb):.2f}\")\n",
    "            seq_length *= 2\n",
    "    \n",
    "    def trace_generate(self):\n",
    "        return\n",
    "    \n",
    "    def trace_train(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e2e674-839c-4675-8589-13f17e33b6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:00.239581Z",
     "iopub.status.busy": "2023-11-25T21:02:00.238335Z",
     "iopub.status.idle": "2023-11-25T21:02:00.244502Z",
     "shell.execute_reply": "2023-11-25T21:02:00.244083Z",
     "shell.execute_reply.started": "2023-11-25T21:02:00.239549Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipyexperiments import IPyExperimentsPytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d5635-beae-4071-9a4b-c9707160f30c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Redpajama-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faf697d-93d3-4712-88ff-b524e9e0ac42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:16:09.894748Z",
     "iopub.status.busy": "2023-11-25T18:16:09.894043Z",
     "iopub.status.idle": "2023-11-25T18:16:30.782608Z",
     "shell.execute_reply": "2023-11-25T18:16:30.781887Z",
     "shell.execute_reply.started": "2023-11-25T18:16:09.894725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     187  14,273  15,837 MB   1.18% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n",
      "Loading model togethercomputer/RedPajama-INCITE-Base-3B-v1 in local cache ...\n",
      "--> model files size   : 5.30 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:19 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      993     -111 MB (-11.25%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,293  13,175  15,837 MB   8.17% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f082f-d604-4a13-a8e8-198824f5723f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78895cd2-c609-4c2b-b772-26084c30bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd6f6f-2c07-4126-a332-919716b51166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7d86f-040d-4af2-8c6c-6decdb8a894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_benchmark.trace_prefill(20, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8559f3-f244-41c6-9f88-bca7ff7ca1f6",
   "metadata": {},
   "source": [
    "For MPT-3B, the line of code which triggers the maximum memory is this one:\n",
    "\n",
    "transformers/models/gpt_neox/modeling_gpt_neox.py\n",
    "```\n",
    "def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "...\n",
    "    attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    " \n",
    "attn_weights is a very large matrix of size: float16[20, 32, 2048, 2048] => 5120 MB of memory.\n",
    "\n",
    "On this line we need to allocate it twice.\n",
    "An inplace softmax would divide the memory requirements by a factor of 2 !\n",
    "\n",
    "https://lernapparat.de/pytorch-inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48c2cb-ff83-43dc-8fdd-b69b7dd24b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5011ed-6662-4dd1-825a-20a0f4584e31",
   "metadata": {},
   "source": [
    "## 16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cc6fab-c238-4ecd-8cd2-fdfe20baf888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:20:43.832126Z",
     "iopub.status.busy": "2023-11-25T18:20:43.831302Z",
     "iopub.status.idle": "2023-11-25T18:20:43.885587Z",
     "shell.execute_reply": "2023-11-25T18:20:43.885119Z",
     "shell.execute_reply.started": "2023-11-25T18:20:43.832094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,303  13,105  15,837 MB   8.23% \n",
      "GPU:   1,227  23,336  24,564 MB   5.00% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85524c6f-dced-412b-8852-1cd78ca99260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:21:20.689065Z",
     "iopub.status.busy": "2023-11-25T18:21:20.687548Z",
     "iopub.status.idle": "2023-11-25T18:21:28.427278Z",
     "shell.execute_reply": "2023-11-25T18:21:28.426716Z",
     "shell.execute_reply.started": "2023-11-25T18:21:20.689015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 403.71 ms\n",
      "Tokenizer CPU memory: 24.60 MB\n",
      "\n",
      "Model load time : 7301.54 ms\n",
      "Model CPU memory: 0.48 GB\n",
      "Model GPU memory: 5.33 GB\n",
      "Max   GPU memory: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeae00e8-b62a-4307-9fe8-c98b5af4551d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:22:15.150822Z",
     "iopub.status.busy": "2023-11-25T18:22:15.150032Z",
     "iopub.status.idle": "2023-11-25T18:24:42.179098Z",
     "shell.execute_reply": "2023-11-25T18:24:42.178685Z",
     "shell.execute_reply.started": "2023-11-25T18:22:15.150789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,26.00,0.01\n",
      "2,128,21.64,0.03\n",
      "3,128,23.94,0.04\n",
      "4,128,26.08,0.05\n",
      "5,128,33.23,0.06\n",
      "6,128,37.86,0.08\n",
      "7,128,44.32,0.09\n",
      "8,128,44.16,0.10\n",
      "9,128,51.76,0.11\n",
      "10,128,57.60,0.13\n",
      "11,128,61.57,0.14\n",
      "12,128,78.21,0.15\n",
      "13,128,70.41,0.16\n",
      "14,128,75.92,0.18\n",
      "15,128,79.01,0.19\n",
      "16,128,83.47,0.20\n",
      "17,128,89.58,0.22\n",
      "18,128,95.11,0.23\n",
      "19,128,99.08,0.24\n",
      "20,128,104.85,0.25\n",
      "21,128,111.69,0.27\n",
      "22,128,130.38,0.28\n",
      "23,128,120.73,0.29\n",
      "24,128,124.92,0.30\n",
      "25,128,132.10,0.32\n",
      "26,128,140.93,0.33\n",
      "27,128,141.85,0.34\n",
      "28,128,148.08,0.35\n",
      "29,128,154.12,0.37\n",
      "1,256,31.73,0.03\n",
      "2,256,28.20,0.05\n",
      "3,256,39.70,0.08\n",
      "4,256,45.57,0.10\n",
      "5,256,57.65,0.13\n",
      "6,256,66.39,0.15\n",
      "7,256,90.81,0.18\n",
      "8,256,88.43,0.20\n",
      "9,256,102.81,0.23\n",
      "10,256,115.68,0.25\n",
      "11,256,127.42,0.28\n",
      "12,256,138.54,0.30\n",
      "13,256,152.49,0.33\n",
      "14,256,163.05,0.35\n",
      "15,256,174.82,0.38\n",
      "16,256,185.17,0.40\n",
      "17,256,200.74,0.43\n",
      "18,256,211.15,0.46\n",
      "19,256,224.28,0.48\n",
      "20,256,235.55,0.51\n",
      "21,256,249.25,0.53\n",
      "22,256,262.17,0.56\n",
      "23,256,279.91,0.58\n",
      "24,256,284.02,0.61\n",
      "25,256,298.43,0.63\n",
      "26,256,348.39,0.66\n",
      "27,256,331.03,0.68\n",
      "28,256,336.68,0.71\n",
      "29,256,349.37,0.73\n",
      "1,512,30.82,0.05\n",
      "2,512,49.98,0.11\n",
      "3,512,79.85,0.17\n",
      "4,512,108.43,0.22\n",
      "5,512,136.74,0.28\n",
      "6,512,162.38,0.34\n",
      "7,512,190.85,0.39\n",
      "8,512,215.60,0.45\n",
      "9,512,245.34,0.50\n",
      "10,512,271.71,0.56\n",
      "11,512,301.73,0.62\n",
      "12,512,328.14,0.67\n",
      "13,512,356.85,0.72\n",
      "14,512,390.02,0.79\n",
      "15,512,415.09,0.84\n",
      "16,512,440.04,0.89\n",
      "17,512,474.15,0.95\n",
      "18,512,502.06,1.01\n",
      "19,512,524.08,1.06\n",
      "20,512,552.86,1.11\n",
      "21,512,584.60,1.17\n",
      "22,512,607.31,1.23\n",
      "23,512,634.40,1.28\n",
      "24,512,661.45,1.34\n",
      "25,512,694.51,1.39\n",
      "26,512,719.78,1.46\n",
      "27,512,752.86,1.51\n",
      "28,512,769.67,1.56\n",
      "29,512,803.23,1.61\n",
      "1,1024,81.24,0.17\n",
      "2,1024,140.51,0.35\n",
      "3,1024,207.61,0.53\n",
      "4,1024,275.44,0.70\n",
      "5,1024,348.17,0.88\n",
      "6,1024,443.23,1.04\n",
      "7,1024,494.68,1.22\n",
      "8,1024,559.51,1.39\n",
      "9,1024,637.32,1.57\n",
      "10,1024,715.64,1.74\n",
      "11,1024,772.57,1.92\n",
      "12,1024,861.62,2.09\n",
      "13,1024,914.35,2.27\n",
      "14,1024,980.50,2.43\n",
      "15,1024,1050.63,2.62\n",
      "16,1024,1119.38,2.78\n",
      "17,1024,1191.98,2.96\n",
      "18,1024,1264.71,3.13\n",
      "19,1024,1329.68,3.31\n",
      "20,1024,1408.70,3.48\n",
      "21,1024,1510.00,3.66\n",
      "22,1024,1568.26,3.82\n",
      "23,1024,1635.51,4.01\n",
      "24,1024,1681.92,4.17\n",
      "25,1024,1753.95,4.35\n",
      "26,1024,1823.89,4.52\n",
      "27,1024,1894.38,4.70\n",
      "28,1024,1961.38,4.87\n",
      "29,1024,2033.24,5.05\n",
      "1,2048,198.67,0.59\n",
      "2,2048,404.08,1.20\n",
      "3,2048,596.81,1.79\n",
      "4,2048,794.71,2.39\n",
      "5,2048,997.44,2.99\n",
      "6,2048,1203.87,3.59\n",
      "7,2048,1397.73,4.18\n",
      "8,2048,1593.36,4.78\n",
      "9,2048,1793.75,5.38\n",
      "10,2048,1990.36,5.98\n",
      "11,2048,2190.12,6.57\n",
      "12,2048,2387.64,7.17\n",
      "13,2048,2587.98,7.77\n",
      "14,2048,2796.69,8.37\n",
      "15,2048,2987.08,8.96\n",
      "16,2048,3208.74,9.56\n",
      "17,2048,3395.49,10.16\n",
      "18,2048,3589.17,10.76\n",
      "19,2048,3809.31,11.36\n",
      "20,2048,4016.76,11.95\n",
      "21,2048,4213.65,12.55\n",
      "22,2048,4410.30,13.15\n",
      "23,2048,4610.12,13.75\n",
      "24,2048,4811.09,14.34\n",
      "25,2048,5012.10,14.94\n",
      "26,2048,5216.95,15.54\n",
      "27,2048,5413.27,16.14\n",
      "28,2048,5626.83,16.73\n",
      "29,2048,5895.19,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354f0cbc-031d-46ef-bfc3-fe77c1f48008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:24:58.160134Z",
     "iopub.status.busy": "2023-11-25T18:24:58.158780Z",
     "iopub.status.idle": "2023-11-25T18:24:58.256042Z",
     "shell.execute_reply": "2023-11-25T18:24:58.255606Z",
     "shell.execute_reply.started": "2023-11-25T18:24:58.160096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:04:14 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 2046 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      576        0 MB (  0.00%)\n",
      "GPU:    5,296    5,524 MB (104.29%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,604  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e94de-0941-4598-9d82-16fac43b8f28",
   "metadata": {},
   "source": [
    "## 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1462911-a827-4c0a-a996-bed393c311bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:25:57.106319Z",
     "iopub.status.busy": "2023-11-25T18:25:57.105737Z",
     "iopub.status.idle": "2023-11-25T18:25:57.160960Z",
     "shell.execute_reply": "2023-11-25T18:25:57.160534Z",
     "shell.execute_reply.started": "2023-11-25T18:25:57.106301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   1,880  12,601  15,837 MB  11.87% \n",
      "GPU:   1,000  23,563  24,564 MB   4.07% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbcbf67-c0e1-4d19-9e75-6743faeb0dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:25.556750Z",
     "iopub.status.busy": "2023-11-25T18:26:25.555375Z",
     "iopub.status.idle": "2023-11-25T18:26:29.675190Z",
     "shell.execute_reply": "2023-11-25T18:26:29.674654Z",
     "shell.execute_reply.started": "2023-11-25T18:26:25.556698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 234.35 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 3867.51 ms\n",
      "Model CPU memory: 0.43 GB\n",
      "Model GPU memory: 3.06 GB\n",
      "Max   GPU memory: 3.07 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b05918-fd0f-4a58-b4d3-77dc494eff25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:26:59.213176Z",
     "iopub.status.busy": "2023-11-25T18:26:59.212633Z",
     "iopub.status.idle": "2023-11-25T18:36:48.697127Z",
     "shell.execute_reply": "2023-11-25T18:36:48.696617Z",
     "shell.execute_reply.started": "2023-11-25T18:26:59.213156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:26:59.999616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-25 18:27:00.165015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,139.23,0.01\n",
      "2,128,126.06,0.03\n",
      "3,128,150.38,0.04\n",
      "4,128,128.92,0.06\n",
      "5,128,148.06,0.07\n",
      "6,128,99.86,0.09\n",
      "7,128,110.90,0.10\n",
      "8,128,112.90,0.11\n",
      "9,128,146.82,0.13\n",
      "10,128,98.36,0.14\n",
      "11,128,118.09,0.15\n",
      "12,128,162.51,0.17\n",
      "13,128,125.73,0.18\n",
      "14,128,131.17,0.20\n",
      "15,128,181.03,0.21\n",
      "16,128,150.02,0.22\n",
      "17,128,168.77,0.23\n",
      "18,128,170.73,0.25\n",
      "19,128,159.60,0.26\n",
      "20,128,188.03,0.28\n",
      "21,128,192.24,0.29\n",
      "22,128,243.53,0.30\n",
      "23,128,175.19,0.32\n",
      "24,128,192.78,0.33\n",
      "25,128,215.48,0.35\n",
      "26,128,227.16,0.36\n",
      "27,128,235.21,0.37\n",
      "28,128,236.02,0.39\n",
      "29,128,240.61,0.40\n",
      "1,256,119.39,0.03\n",
      "2,256,144.90,0.06\n",
      "3,256,136.77,0.08\n",
      "4,256,112.83,0.11\n",
      "5,256,153.43,0.14\n",
      "6,256,144.35,0.16\n",
      "7,256,158.68,0.20\n",
      "8,256,160.82,0.22\n",
      "9,256,190.68,0.25\n",
      "10,256,199.55,0.28\n",
      "11,256,210.79,0.30\n",
      "12,256,223.55,0.33\n",
      "13,256,232.53,0.36\n",
      "14,256,258.61,0.39\n",
      "15,256,258.73,0.41\n",
      "16,256,283.96,0.44\n",
      "17,256,284.48,0.47\n",
      "18,256,277.84,0.50\n",
      "19,256,302.58,0.53\n",
      "20,256,322.53,0.55\n",
      "21,256,342.52,0.58\n",
      "22,256,335.89,0.61\n",
      "23,256,360.73,0.63\n",
      "24,256,382.35,0.66\n",
      "25,256,388.89,0.69\n",
      "26,256,396.49,0.72\n",
      "27,256,419.27,0.74\n",
      "28,256,433.78,0.77\n",
      "29,256,454.33,0.80\n",
      "1,512,126.69,0.05\n",
      "2,512,110.98,0.11\n",
      "3,512,134.16,0.17\n",
      "4,512,173.40,0.22\n",
      "5,512,194.66,0.28\n",
      "6,512,252.41,0.34\n",
      "7,512,282.69,0.39\n",
      "8,512,298.69,0.45\n",
      "9,512,330.79,0.50\n",
      "10,512,368.96,0.56\n",
      "11,512,387.23,0.61\n",
      "12,512,425.58,0.67\n",
      "13,512,458.24,0.72\n",
      "14,512,486.69,0.78\n",
      "15,512,521.96,0.84\n",
      "16,512,555.36,0.89\n",
      "17,512,595.87,0.95\n",
      "18,512,627.89,1.01\n",
      "19,512,653.96,1.06\n",
      "20,512,688.20,1.11\n",
      "21,512,739.88,1.17\n",
      "22,512,751.76,1.23\n",
      "23,512,783.23,1.28\n",
      "24,512,820.04,1.34\n",
      "25,512,848.24,1.39\n",
      "26,512,886.66,1.45\n",
      "27,512,922.13,1.50\n",
      "28,512,953.99,1.56\n",
      "29,512,989.09,1.62\n",
      "1,1024,143.60,0.17\n",
      "2,1024,211.74,0.35\n",
      "3,1024,296.30,0.53\n",
      "4,1024,363.21,0.70\n",
      "5,1024,427.58,0.87\n",
      "6,1024,512.56,1.04\n",
      "7,1024,594.01,1.22\n",
      "8,1024,672.58,1.39\n",
      "9,1024,754.09,1.57\n",
      "10,1024,831.65,1.74\n",
      "11,1024,911.53,1.92\n",
      "12,1024,994.36,2.09\n",
      "13,1024,1074.88,2.26\n",
      "14,1024,1157.32,2.43\n",
      "15,1024,1244.02,2.61\n",
      "16,1024,1325.28,2.78\n",
      "17,1024,1410.68,2.96\n",
      "18,1024,1508.79,3.13\n",
      "19,1024,1621.96,3.31\n",
      "20,1024,1738.23,3.48\n",
      "21,1024,1822.41,3.65\n",
      "22,1024,1929.39,3.82\n",
      "23,1024,2010.18,4.00\n",
      "24,1024,2113.01,4.17\n",
      "25,1024,2215.16,4.35\n",
      "26,1024,2323.07,4.52\n",
      "27,1024,2448.84,4.70\n",
      "28,1024,2589.33,4.87\n",
      "29,1024,2713.17,5.04\n",
      "1,2048,260.51,0.59\n",
      "2,2048,491.49,1.20\n",
      "3,2048,704.28,1.80\n",
      "4,2048,924.34,2.39\n",
      "5,2048,1152.22,2.99\n",
      "6,2048,1372.37,3.59\n",
      "7,2048,1613.14,4.19\n",
      "8,2048,1833.02,4.78\n",
      "9,2048,2079.61,5.38\n",
      "10,2048,2356.59,5.98\n",
      "11,2048,2605.31,6.58\n",
      "12,2048,2901.83,7.17\n",
      "13,2048,3178.02,7.77\n",
      "14,2048,3458.56,8.37\n",
      "15,2048,3787.02,8.97\n",
      "16,2048,4034.38,9.56\n",
      "17,2048,4298.46,10.16\n",
      "18,2048,4534.19,10.76\n",
      "19,2048,4773.00,11.36\n",
      "20,2048,5028.51,11.95\n",
      "21,2048,5325.63,12.55\n",
      "22,2048,5606.51,13.15\n",
      "23,2048,8651.16,13.75\n",
      "24,2048,12485.91,14.34\n",
      "25,2048,16329.58,14.94\n",
      "26,2048,20278.89,15.54\n",
      "27,2048,104959.78,16.14\n",
      "28,2048,128232.94,16.74\n",
      "29,2048,155784.38,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e078485-b8fc-4898-b121-ca3035410013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.698207Z",
     "iopub.status.busy": "2023-11-25T18:36:48.697921Z",
     "iopub.status.idle": "2023-11-25T18:36:48.867224Z",
     "shell.execute_reply": "2023-11-25T18:36:48.866826Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.698196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:51 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 11457 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      802        0 MB (  0.00%)\n",
      "GPU:    3,400    3,324 MB ( 97.76%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1af3b5-0c94-4981-aae7-ff5db560d54a",
   "metadata": {},
   "source": [
    "## 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5bfc6bf-1eae-49ae-a0d6-c6005f939637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.867829Z",
     "iopub.status.busy": "2023-11-25T18:36:48.867719Z",
     "iopub.status.idle": "2023-11-25T18:36:48.969935Z",
     "shell.execute_reply": "2023-11-25T18:36:48.969501Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.867820Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,683  12,049  15,837 MB  16.94% \n",
      "GPU:   1,076  23,487  24,564 MB   4.38% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df1d8cdc-6bb6-4de7-84b2-db67b517f25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:48.970943Z",
     "iopub.status.busy": "2023-11-25T18:36:48.970653Z",
     "iopub.status.idle": "2023-11-25T18:36:53.462226Z",
     "shell.execute_reply": "2023-11-25T18:36:53.461768Z",
     "shell.execute_reply.started": "2023-11-25T18:36:48.970926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.30 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-Base-3B-v1/snapshots/094fbdd0c911feb485ce55de1952ab2e75277e1e)\n",
      "\n",
      "Tokenizer load time : 244.97 ms\n",
      "Tokenizer CPU memory: 0.00 MB\n",
      "\n",
      "Model load time : 4242.37 ms\n",
      "Model CPU memory: 0.40 GB\n",
      "Model GPU memory: 1.95 GB\n",
      "Max   GPU memory: 1.96 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86db86aa-38c6-46bf-8cad-6f2e80d6f9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:36:53.462979Z",
     "iopub.status.busy": "2023-11-25T18:36:53.462843Z",
     "iopub.status.idle": "2023-11-25T18:47:10.690379Z",
     "shell.execute_reply": "2023-11-25T18:47:10.689843Z",
     "shell.execute_reply.started": "2023-11-25T18:36:53.462970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,128,60.92,0.16\n",
      "2,128,71.53,0.17\n",
      "3,128,84.30,0.18\n",
      "4,128,97.68,0.19\n",
      "5,128,118.82,0.20\n",
      "6,128,129.78,0.21\n",
      "7,128,145.19,0.22\n",
      "8,128,155.80,0.23\n",
      "9,128,175.67,0.24\n",
      "10,128,191.66,0.25\n",
      "11,128,203.37,0.26\n",
      "12,128,221.43,0.27\n",
      "13,128,241.67,0.28\n",
      "14,128,254.04,0.29\n",
      "15,128,271.51,0.30\n",
      "16,128,282.05,0.31\n",
      "17,128,298.93,0.32\n",
      "18,128,320.51,0.33\n",
      "19,128,339.63,0.35\n",
      "20,128,353.81,0.36\n",
      "21,128,369.18,0.37\n",
      "22,128,383.35,0.38\n",
      "23,128,405.28,0.39\n",
      "24,128,412.20,0.40\n",
      "25,128,434.94,0.41\n",
      "26,128,453.60,0.42\n",
      "27,128,469.91,0.43\n",
      "28,128,527.36,0.44\n",
      "29,128,499.12,0.45\n",
      "1,256,70.25,0.17\n",
      "2,256,97.60,0.19\n",
      "3,256,133.43,0.21\n",
      "4,256,158.39,0.23\n",
      "5,256,192.77,0.25\n",
      "6,256,223.31,0.27\n",
      "7,256,255.22,0.29\n",
      "8,256,289.06,0.31\n",
      "9,256,324.01,0.33\n",
      "10,256,359.26,0.36\n",
      "11,256,397.50,0.38\n",
      "12,256,422.38,0.40\n",
      "13,256,465.23,0.42\n",
      "14,256,494.10,0.44\n",
      "15,256,524.89,0.46\n",
      "16,256,554.36,0.48\n",
      "17,256,594.87,0.50\n",
      "18,256,625.88,0.52\n",
      "19,256,657.29,0.54\n",
      "20,256,693.34,0.57\n",
      "21,256,734.68,0.59\n",
      "22,256,765.58,0.61\n",
      "23,256,798.60,0.63\n",
      "24,256,865.42,0.66\n",
      "25,256,869.00,0.68\n",
      "26,256,945.38,0.70\n",
      "27,256,938.99,0.73\n",
      "28,256,968.48,0.75\n",
      "29,256,1013.42,0.77\n",
      "1,512,101.13,0.19\n",
      "2,512,162.31,0.23\n",
      "3,512,231.70,0.27\n",
      "4,512,300.92,0.31\n",
      "5,512,378.70,0.36\n",
      "6,512,444.53,0.40\n",
      "7,512,514.73,0.44\n",
      "8,512,577.65,0.48\n",
      "9,512,652.16,0.52\n",
      "10,512,722.41,0.56\n",
      "11,512,801.97,0.61\n",
      "12,512,867.43,0.67\n",
      "13,512,951.10,0.73\n",
      "14,512,1016.43,0.78\n",
      "15,512,1118.21,0.84\n",
      "16,512,1161.92,0.89\n",
      "17,512,1262.12,0.95\n",
      "18,512,1301.05,1.00\n",
      "19,512,1374.66,1.06\n",
      "20,512,1446.17,1.11\n",
      "21,512,1516.33,1.17\n",
      "22,512,1621.12,1.23\n",
      "23,512,1668.60,1.28\n",
      "24,512,1741.53,1.34\n",
      "25,512,1824.29,1.39\n",
      "26,512,1876.20,1.45\n",
      "27,512,1957.67,1.51\n",
      "28,512,2018.63,1.56\n",
      "29,512,2111.34,1.62\n",
      "1,1024,177.56,0.23\n",
      "2,1024,325.82,0.35\n",
      "3,1024,481.74,0.52\n",
      "4,1024,626.40,0.70\n",
      "5,1024,783.28,0.87\n",
      "6,1024,947.58,1.04\n",
      "7,1024,1113.18,1.22\n",
      "8,1024,1252.86,1.39\n",
      "9,1024,1423.26,1.57\n",
      "10,1024,1582.51,1.74\n",
      "11,1024,1743.23,1.92\n",
      "12,1024,1876.38,2.09\n",
      "13,1024,2053.46,2.26\n",
      "14,1024,2214.09,2.43\n",
      "15,1024,2355.47,2.61\n",
      "16,1024,2551.10,2.78\n",
      "17,1024,2707.79,2.96\n",
      "18,1024,2840.95,3.13\n",
      "19,1024,3022.16,3.31\n",
      "20,1024,3155.51,3.48\n",
      "21,1024,3326.08,3.65\n",
      "22,1024,3477.24,3.82\n",
      "23,1024,3649.82,4.00\n",
      "24,1024,3793.07,4.17\n",
      "25,1024,3983.23,4.35\n",
      "26,1024,4103.07,4.52\n",
      "27,1024,4321.68,4.70\n",
      "28,1024,4432.53,4.87\n",
      "29,1024,4618.06,5.04\n",
      "1,2048,380.29,0.59\n",
      "2,2048,735.99,1.20\n",
      "3,2048,1109.80,1.79\n",
      "4,2048,1481.10,2.39\n",
      "5,2048,1857.48,2.99\n",
      "6,2048,2220.60,3.59\n",
      "7,2048,2597.17,4.18\n",
      "8,2048,3008.82,4.78\n",
      "9,2048,3350.06,5.38\n",
      "10,2048,3721.51,5.98\n",
      "11,2048,4100.85,6.57\n",
      "12,2048,4472.83,7.17\n",
      "13,2048,4874.75,7.77\n",
      "14,2048,5250.82,8.37\n",
      "15,2048,5607.23,8.96\n",
      "16,2048,6032.71,9.56\n",
      "17,2048,6416.30,10.16\n",
      "18,2048,6791.63,10.76\n",
      "19,2048,7186.32,11.36\n",
      "20,2048,7609.28,11.95\n",
      "21,2048,8022.64,12.55\n",
      "22,2048,8408.02,13.15\n",
      "23,2048,8836.85,13.75\n",
      "24,2048,13241.47,14.34\n",
      "25,2048,30777.34,14.94\n",
      "26,2048,44202.53,15.54\n",
      "27,2048,61871.65,16.14\n",
      "28,2048,78737.15,16.73\n",
      "29,2048,138651.96,17.33\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.check_prefill(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bf77fa-129e-45a1-b5f9-97bd8bd89eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T18:47:10.691037Z",
     "iopub.status.busy": "2023-11-25T18:47:10.690918Z",
     "iopub.status.idle": "2023-11-25T18:47:10.827226Z",
     "shell.execute_reply": "2023-11-25T18:47:10.826730Z",
     "shell.execute_reply.started": "2023-11-25T18:47:10.691028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:10:21 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 13086 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      406      416 MB (102.46%)\n",
      "GPU:    2,130    2,002 MB ( 93.99%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,673  12,018  15,837 MB  16.88% \n",
      "GPU:   1,204  23,359  24,564 MB   4.91% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d779f88-0477-4b7d-9111-3a3bcecc092e",
   "metadata": {},
   "source": [
    "# StableLM-3B\n",
    "\n",
    "stabilityai/stablelm-3b-4e1t\n",
    "\n",
    "https://huggingface.co/stabilityai/stablelm-3b-4e1t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae435303-cdac-49c6-89fc-f9a970127f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:04.210862Z",
     "iopub.status.busy": "2023-11-25T21:02:04.210216Z",
     "iopub.status.idle": "2023-11-25T21:02:04.215891Z",
     "shell.execute_reply": "2023-11-25T21:02:04.215091Z",
     "shell.execute_reply.started": "2023-11-25T21:02:04.210833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b18a20-0ca7-47e7-9279-e6ee4eb7e323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T20:49:27.416244Z",
     "iopub.status.busy": "2023-11-25T20:49:27.414961Z",
     "iopub.status.idle": "2023-11-25T20:49:41.696725Z",
     "shell.execute_reply": "2023-11-25T20:49:41.695754Z",
     "shell.execute_reply.started": "2023-11-25T20:49:27.415926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     230  14,298  15,837 MB   1.45% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n",
      "Loading model stabilityai/stablelm-3b-4e1t in local cache ...\n",
      "--> model files size   : 5.21 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813\n",
      "\n",
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:12 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 93 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:     -180     -110 MB ( 61.27%)\n",
      "GPU:        0        0 MB (100.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     160  14,382  15,837 MB   1.01% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id, trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d606c66-5b9e-4329-8397-1ffebb7d77e9",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9084f350-715a-40b1-92a4-acdd6f002e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:06.734449Z",
     "iopub.status.busy": "2023-11-25T21:02:06.733774Z",
     "iopub.status.idle": "2023-11-25T21:02:06.968274Z",
     "shell.execute_reply": "2023-11-25T21:02:06.967859Z",
     "shell.execute_reply.started": "2023-11-25T21:02:06.734422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     568  14,094  15,837 MB   3.59% \n",
      "GPU:   1,122  23,441  24,564 MB   4.57% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cba0519-c2fb-4945-9c0c-dbca1fc4a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:09.123786Z",
     "iopub.status.busy": "2023-11-25T21:02:09.122675Z",
     "iopub.status.idle": "2023-11-25T21:02:11.171270Z",
     "shell.execute_reply": "2023-11-25T21:02:11.170770Z",
     "shell.execute_reply.started": "2023-11-25T21:02:09.123747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 5.21 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--stabilityai--stablelm-3b-4e1t/snapshots/c6554ba60f40a8252d2a43e38e55ee2e3a645813)\n",
      "\n",
      "Tokenizer load time : 412.45 ms\n",
      "Tokenizer CPU memory: 23.14 MB\n",
      "\n",
      "Model load time : 1629.85 ms\n",
      "Model CPU memory: 0.03 GB\n",
      "Model GPU memory: 5.24 GB\n",
      "Max   GPU memory: 5.24 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stablelm-3b-4e1t\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e583cd32-3b5e-47bc-a7be-0678447f84a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T20:58:23.758352Z",
     "iopub.status.busy": "2023-11-25T20:58:23.757589Z",
     "iopub.status.idle": "2023-11-25T20:58:23.765611Z",
     "shell.execute_reply": "2023-11-25T20:58:23.765240Z",
     "shell.execute_reply.started": "2023-11-25T20:58:23.758318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableLMEpochForCausalLM(\n",
       "  (model): StableLMEpochModel(\n",
       "    (embed_tokens): Embedding(50304, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_benchmark.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06aa726-38a3-4661-a6d3-d0d9e37f689d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:19.785035Z",
     "iopub.status.busy": "2023-11-25T21:02:19.784603Z",
     "iopub.status.idle": "2023-11-25T21:02:29.062651Z",
     "shell.execute_reply": "2023-11-25T21:02:29.062365Z",
     "shell.execute_reply.started": "2023-11-25T21:02:19.785005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 10 and sequence length 2048:\n",
      "model.embed_tokens;True;;270.7;;int64[10, 2048];0.2;0.2;100.2;100.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;184.0;;bfloat16[10, 2048, 2560];100.0;180.2;280.3;280.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;161.9;;bfloat16[10, 2048, 2560];100.0;280.2;380.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;147.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;149.0;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;110.5;;bfloat16[10, 32, 2048, 80];100.0;580.2;580.2;580.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;425.4;;bfloat16[10, 2048, 2560];100.0;3440.2;3540.2;3540.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;127126.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;3540.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;149.2;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;136.0;;bfloat16[10, 2048, 2560];100.0;380.2;650.2;650.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;240.4;;bfloat16[10, 2048, 6912];270.0;650.2;920.2;920.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;107.6;;bfloat16[10, 2048, 2560];100.0;650.2;920.2;920.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;120.0;;bfloat16[10, 2048, 6912];270.0;650.2;750.2;750.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1136.6;;bfloat16[10, 2048, 2560];100.0;380.2;750.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.0;False;;129240.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;180.2;750.2;280.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;169.7;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;110.7;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;96.8;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;95.3;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;156.1;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;126.4;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;1639.2;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;175.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;115.3;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;97.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;102.3;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;113.7;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1080.8;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.1;False;;3698.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;110.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;102.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;97.4;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;321.6;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;94.5;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;130.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;1625.7;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;717.4;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;113.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;94.1;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;100.8;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;109.7;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2.mlp;False;;981.5;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.2;False;;3871.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;102.0;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;105.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;96.4;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;96.5;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;93.0;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;430.3;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;2110.7;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;125.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;143.3;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;101.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;108.7;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;287.7;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1123.0;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.3;False;;4219.7;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;108.6;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;104.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;125.5;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;98.1;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;92.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;127.3;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;1588.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;104.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;253.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;92.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;98.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;107.0;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4.mlp;False;;969.8;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.4;False;;3224.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;280.6;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;206.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;103.4;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;98.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;97.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;256.0;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;1626.9;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;103.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;99.6;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;89.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;143.8;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;148.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1089.2;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.5;False;;3628.7;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;107.5;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;108.9;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;249.7;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;101.9;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;92.7;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;148.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1655.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;114.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;109.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;151.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;121.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;114.7;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6.mlp;False;;1173.6;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.6;False;;3508.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;106.3;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;103.7;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;98.2;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;98.3;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;297.3;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;173.2;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1926.9;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;257.9;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;137.3;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;105.0;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;114.3;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;121.0;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1106.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.7;False;;4077.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;104.1;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;260.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;113.3;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;97.8;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;92.2;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;130.2;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;1566.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;263.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;107.8;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;92.6;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;110.3;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;118.6;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8.mlp;False;;870.8;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.8;False;;3379.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;101.9;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;104.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;114.1;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;98.5;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;95.9;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;147.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;1559.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;105.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;102.9;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;247.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;103.7;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;108.9;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9.mlp;False;;978.4;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.9;False;;3173.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;101.0;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;108.4;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;97.6;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;95.6;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;91.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;117.9;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;1503.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;106.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;123.2;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;92.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;98.4;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;111.1;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10.mlp;False;;972.6;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.10;False;;3260.4;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;103.7;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;101.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;98.0;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;304.4;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;97.3;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;155.3;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;1957.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;113.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;151.5;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;99.1;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;107.7;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;117.8;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1053.7;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.11;False;;3682.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;108.3;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;111.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;149.1;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;110.8;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;96.7;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;129.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1707.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;109.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;104.9;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;92.7;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;103.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;243.2;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12.mlp;False;;976.4;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.12;False;;3473.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;99.2;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;97.7;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;97.6;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;97.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;92.1;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;126.7;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1473.9;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;106.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;239.6;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;91.0;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;100.8;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;107.1;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13.mlp;False;;947.9;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.13;False;;3053.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;234.5;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;117.9;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;100.7;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;96.3;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;93.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;287.7;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1828.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;139.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;156.1;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;108.3;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;110.3;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;143.9;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1121.7;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.14;False;;3903.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;120.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;124.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;258.6;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;105.3;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;93.6;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;135.1;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;1717.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;110.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;131.0;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;95.1;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;104.8;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;201.3;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15.mlp;False;;1104.1;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.15;False;;3532.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;108.7;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;119.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;104.4;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;98.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;229.2;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;115.3;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;1566.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;150.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;104.5;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;103.2;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;105.8;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;108.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16.mlp;False;;991.7;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.16;False;;3393.4;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;98.2;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;277.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;98.1;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;95.8;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;186.7;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;133.2;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;1666.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;258.9;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;106.0;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;92.2;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;100.4;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;118.8;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17.mlp;False;;859.8;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.17;False;;3474.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;103.3;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;120.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;98.0;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;97.7;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;103.6;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;131.4;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;1559.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;106.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;105.3;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;934.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;120.1;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;110.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1688.0;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.18;False;;3883.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;103.0;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;111.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;97.2;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;94.8;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;91.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;118.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;1522.9;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;108.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;104.2;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;93.6;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;101.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;111.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19.mlp;False;;987.1;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.19;False;;3297.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;115.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;105.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;99.3;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;240.8;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;100.5;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;118.0;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;1539.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;103.2;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;116.4;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;95.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;102.1;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;109.2;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20.mlp;False;;991.7;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.20;False;;3207.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;108.9;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;108.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;116.1;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;99.1;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;93.9;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;138.6;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;1579.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;109.0;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;105.4;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;93.7;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;121.1;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;281.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21.mlp;False;;1025.1;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.21;False;;3401.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;103.4;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;107.4;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;111.7;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;104.3;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;92.2;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;127.5;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;1600.4;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;105.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;360.8;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;128.9;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;122.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;125.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22.mlp;False;;1218.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.22;False;;3505.4;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;260.6;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;133.2;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;163.6;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;170.5;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;176.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;437.9;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;811982.0;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;118.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;106.1;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;95.6;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;102.0;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;112.8;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23.mlp;False;;967.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.23;False;;813774.9;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;103.4;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;104.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;235.9;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;95.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;91.5;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;366.4;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;650171.4;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;158.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;118.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;96.6;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;99.9;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;112.0;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24.mlp;False;;1008.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.24;False;;651950.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;100.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;106.8;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;96.2;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;93.9;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;235.4;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;224.8;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;649463.2;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;141.7;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;116.8;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;110.9;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;103.4;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;112.9;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25.mlp;False;;1054.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.25;False;;651432.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;102.9;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;260.3;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;96.0;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;92.7;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;93.0;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;234.0;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;649959.7;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;351.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;116.6;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;96.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;98.4;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;113.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26.mlp;False;;844.1;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.26;False;;651880.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;108.0;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;144.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;98.4;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;97.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;93.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;316.1;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;650469.1;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;174.2;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;119.4;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;269.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;111.0;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;125.2;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27.mlp;False;;1101.5;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.27;False;;652351.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;109.6;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;121.4;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;96.2;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;94.7;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;92.2;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;354.3;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;649687.5;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;159.6;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;115.1;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;110.2;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;111.6;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;118.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28.mlp;False;;1070.5;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.28;False;;651826.8;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;108.0;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;103.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;96.5;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;244.2;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;94.5;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;229.7;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;649548.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;157.5;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;119.2;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;99.4;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;99.5;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;111.8;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29.mlp;False;;1035.4;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.29;False;;651334.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;110.3;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;108.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;97.1;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;95.1;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;97.8;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;238.2;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;650037.6;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;131.9;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;112.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;103.8;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;102.7;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;257.6;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30.mlp;False;;1005.3;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.30;False;;651879.2;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;110.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;104.7;;bfloat16[10, 2048, 2560];100.0;380.2;480.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;106.9;;bfloat16[10, 2048, 2560];100.0;480.2;580.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;99.9;;bfloat16[10, 2048, 2560];100.0;580.2;680.2;680.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;108.1;;bfloat16[10, 32, 2048, 80];100.0;680.2;680.2;680.2;bfloat16[1, 1, 2048, 20]bfloat16[1, 1, 2048, 20];0.2;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;340.2;;bfloat16[10, 2048, 2560];100.0;3540.2;3640.2;3640.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;649523.2;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;380.2;3640.2;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;160.1;;bfloat16[10, 2048, 2560];100.0;380.2;480.3;480.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;281.7;;bfloat16[10, 2048, 2560];100.0;480.2;750.2;750.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;103.5;;bfloat16[10, 2048, 6912];270.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;105.6;;bfloat16[10, 2048, 2560];100.0;750.2;1020.2;1020.2;bfloat16[10, 2048, 6912];270.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;117.4;;bfloat16[10, 2048, 6912];270.0;750.2;850.2;850.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31.mlp;False;;1202.1;;bfloat16[10, 2048, 2560];100.0;480.2;850.2;580.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.layers.31;False;;651596.3;;bfloat16[10, 2048, 2560]bfloat16[10, 1, 2048, 2048]int64[1, 2048];180.0;280.2;850.2;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model.norm;True;;146.8;;bfloat16[10, 2048, 2560];100.0;280.2;380.3;380.2;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "model;False;;6239137.6;;int64[10, 2048]float32[10, 2048];0.2;0.1;380.3;100.1;bfloat16[10, 2048, 2560];100.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "lm_head;True;;295.7;;bfloat16[10, 2048, 2560];100.0;100.1;2065.1;2065.1;bfloat16[10, 2048, 50304];1965.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n",
      "<model>;False;;6240409.3;;int64[10, 2048]float32[10, 2048];0.2;0.1;5995.1;3930.1;float32[10, 2048, 50304];3930.0;;0.0;;;-9304.9;-9304.9;-9304.9;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/usr/lib/python3/dist-packages/torch/nn/modules/module.py:1554: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "model_benchmark.trace_prefill(10, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6972b666-9b87-4d7f-a49f-32de8be0e306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T21:02:46.838069Z",
     "iopub.status.busy": "2023-11-25T21:02:46.837287Z",
     "iopub.status.idle": "2023-11-25T21:02:47.258745Z",
     "shell.execute_reply": "2023-11-25T21:02:47.258271Z",
     "shell.execute_reply.started": "2023-11-25T21:02:46.838036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:00:40 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_benchmark, model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 45 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:      376        0 MB (  0.00%)\n",
      "GPU:    5,654    5,560 MB ( 98.34%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     944  13,751  15,837 MB   5.97% \n",
      "GPU:   1,216  23,347  24,564 MB   4.95% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa330f-5df8-4ab8-8c68-cbb5faf8b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bcae39-2ec1-4674-9872-bd07b944499d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:28.932661Z",
     "iopub.status.busy": "2023-11-22T23:14:28.931840Z",
     "iopub.status.idle": "2023-11-22T23:14:28.935824Z",
     "shell.execute_reply": "2023-11-22T23:14:28.935080Z",
     "shell.execute_reply.started": "2023-11-22T23:14:28.932634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs['logits']\n",
    "past_key_values = outputs['past_key_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e6b133-7b78-48ac-ba06-938daf8a44f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:14:59.065051Z",
     "iopub.status.busy": "2023-11-22T23:14:59.064635Z",
     "iopub.status.idle": "2023-11-22T23:14:59.067809Z",
     "shell.execute_reply": "2023-11-22T23:14:59.067428Z",
     "shell.execute_reply.started": "2023-11-22T23:14:59.065034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 50432]), torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size(),logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915d0c6-9bae-47eb-9712-44a9de6892e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T23:12:26.134495Z",
     "iopub.status.busy": "2023-11-22T23:12:26.133905Z",
     "iopub.status.idle": "2023-11-22T23:12:26.141171Z",
     "shell.execute_reply": "2023-11-22T23:12:26.140688Z",
     "shell.execute_reply.started": "2023-11-22T23:12:26.134464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16,\n",
       " torch.Size([2, 32, 1000, 80]),\n",
       " torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_key_values), past_key_values[0][0].size(), past_key_values[0][0].dtype, past_key_values[0][1].size(), past_key_values[0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d13c74a-e0c1-449e-b2fa-59943661c78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:56.688325Z",
     "iopub.status.busy": "2023-11-20T21:23:56.687590Z",
     "iopub.status.idle": "2023-11-20T21:23:57.046020Z",
     "shell.execute_reply": "2023-11-20T21:23:57.045602Z",
     "shell.execute_reply.started": "2023-11-20T21:23:56.688294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291d9889-5bb6-4635-8ec1-17dfd0439542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:23:57.100183Z",
     "iopub.status.busy": "2023-11-20T21:23:57.099421Z",
     "iopub.status.idle": "2023-11-20T21:23:57.109529Z",
     "shell.execute_reply": "2023-11-20T21:23:57.109088Z",
     "shell.execute_reply.started": "2023-11-20T21:23:57.100149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer([\"un test\",\"un deuxième test\"], padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf3dd39-1341-44a7-9119-85b4d1298d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:24:01.082171Z",
     "iopub.status.busy": "2023-11-20T21:24:01.081390Z",
     "iopub.status.idle": "2023-11-20T21:24:01.087945Z",
     "shell.execute_reply": "2023-11-20T21:24:01.087300Z",
     "shell.execute_reply.started": "2023-11-20T21:24:01.082139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  328,  1071,     0,     0,     0],\n",
       "        [  328, 23156,    74, 22722,  1071]]), 'attention_mask': tensor([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fe31a0-9b15-421f-b069-e67546d7ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:32:12.116422Z",
     "iopub.status.busy": "2023-11-20T21:32:12.116190Z",
     "iopub.status.idle": "2023-11-20T21:32:12.120878Z",
     "shell.execute_reply": "2023-11-20T21:32:12.120463Z",
     "shell.execute_reply.started": "2023-11-20T21:32:12.116408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"input_ids\"].size(), encodings[\"input_ids\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef91e00-7d3c-452f-9d93-ec2d707215d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:34:04.936740Z",
     "iopub.status.busy": "2023-11-20T21:34:04.936503Z",
     "iopub.status.idle": "2023-11-20T21:34:04.939242Z",
     "shell.execute_reply": "2023-11-20T21:34:04.938877Z",
     "shell.execute_reply.started": "2023-11-20T21:34:04.936731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[\"attention_mask\"].size(), encodings[\"attention_mask\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca42a0a-eabb-46a6-aa77-4e925029d168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:22.386686Z",
     "iopub.status.busy": "2023-11-20T21:41:22.385561Z",
     "iopub.status.idle": "2023-11-20T21:41:22.393072Z",
     "shell.execute_reply": "2023-11-20T21:41:22.392631Z",
     "shell.execute_reply.started": "2023-11-20T21:41:22.386648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 50432]), torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"logits\"].size(), output[\"logits\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e887edcc-6554-466d-970e-9a800e7193fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:41:02.274392Z",
     "iopub.status.busy": "2023-11-20T21:41:02.273974Z",
     "iopub.status.idle": "2023-11-20T21:41:02.281515Z",
     "shell.execute_reply": "2023-11-20T21:41:02.280358Z",
     "shell.execute_reply.started": "2023-11-20T21:41:02.274364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172ffae5-19b4-42e4-a072-57882f130641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T21:46:28.031823Z",
     "iopub.status.busy": "2023-11-20T21:46:28.031023Z",
     "iopub.status.idle": "2023-11-20T21:46:28.037420Z",
     "shell.execute_reply": "2023-11-20T21:46:28.036755Z",
     "shell.execute_reply.started": "2023-11-20T21:46:28.031790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, torch.Size([2, 32, 5, 80]), torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"past_key_values\"]),output[\"past_key_values\"][0][1].size(),output[\"past_key_values\"][0][1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67521338-e1f2-4ed2-bb0e-a7d4f77aef74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
