{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d1ebcd-3a20-4e59-9779-cf97a3cbb826",
   "metadata": {},
   "source": [
    "# vLLM batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de3b0d5-2eb0-4c86-8c46-8cdda39d8fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:50:00.377376Z",
     "iopub.status.busy": "2025-11-11T08:50:00.377227Z",
     "iopub.status.idle": "2025-11-11T08:50:00.380157Z",
     "shell.execute_reply": "2025-11-11T08:50:00.379635Z",
     "shell.execute_reply.started": "2025-11-11T08:50:00.377366Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29e1ae1-625f-4bb7-aee7-1c0b53d895ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:50:08.131225Z",
     "iopub.status.busy": "2025-11-11T08:50:08.131065Z",
     "iopub.status.idle": "2025-11-11T08:50:08.150038Z",
     "shell.execute_reply": "2025-11-11T08:50:08.149671Z",
     "shell.execute_reply.started": "2025-11-11T08:50:08.131215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcf936-d728-4b8d-87bc-2f8e86384157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:51:48.834121Z",
     "iopub.status.busy": "2025-11-11T08:51:48.833842Z",
     "iopub.status.idle": "2025-11-11T08:51:48.837642Z",
     "shell.execute_reply": "2025-11-11T08:51:48.836643Z",
     "shell.execute_reply.started": "2025-11-11T08:51:48.834102Z"
    }
   },
   "source": [
    "## vLLM 0.11.0 documentation\n",
    "\n",
    "https://docs.vllm.ai/en/stable\n",
    "\n",
    "### Python API\n",
    "\n",
    "Quick start\n",
    "\n",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/#offline-batched-inference\n",
    "\n",
    "Examples\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/\n",
    "\n",
    "User guide\n",
    "\n",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/generative_models/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/pooling_models/\n",
    "\n",
    "API reference\n",
    "\n",
    "https://docs.vllm.ai/en/latest/api/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/api/vllm/#vllm.LLM\n",
    "\n",
    "Config arguments you can pass\n",
    "\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ModelConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.CacheConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.LoadConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ParallelConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.SchedulerConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.DeviceConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.SpeculativeConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.LoRAConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.MultiModalConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.PoolerConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.StructuredOutputsConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ObservabilityConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.KVTransferConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.CompilationConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.VllmConfig\n",
    "\n",
    "Supported models\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/supported_models/\n",
    "\n",
    "https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/models\n",
    "\n",
    "### OpenAI-Compatible RESTful API server\n",
    "\n",
    "Quick start\n",
    "\n",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/#openai-compatible-server\n",
    "\n",
    "Examples\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/\n",
    "\n",
    "User guide\n",
    "\n",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/\n",
    "\n",
    "Configuration\n",
    "\n",
    "https://docs.vllm.ai/en/latest/configuration/\n",
    "\n",
    "Syntax reference\n",
    "\n",
    "https://docs.vllm.ai/en/latest/cli/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/cli/serve/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107979b6-a56e-4dcf-abdb-c8f8fdf7889d",
   "metadata": {},
   "source": [
    "## Streaming a response in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30540e-e0b2-4748-be85-e7559cc447b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
