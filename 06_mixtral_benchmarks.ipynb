{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b02285-f7ac-4153-a5e5-486baa6fdd78",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B-v0.1 benchmarks - single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907384e6-5899-4278-89a2-ce962aaf5cb3",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c505fe8-eb30-4f48-91c2-30b3107defc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d391d-17ef-4c10-87c6-d5c281fc2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7da1-d5a1-45c5-aebc-e95a45a33663",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcaa48-499b-40e7-8ec2-7389958e8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e292b-ac5f-4977-a29c-bc13b7bd5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee75a6-20da-4d3c-a4a3-ddf79dd48fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbb181-2561-496f-958f-24e045c3023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e916a2-4c9e-43b9-a447-62be56160086",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6484fb21-f68d-4e74-ab15-b2d386109544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.37.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744019ab-360d-48c4-a485-9dfff831289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3f933f-7c94-4734-a1de-4bcc14542ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.27.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"accelerate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c5dc89-b322-4ab0-a4a9-0744f8aa50c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.42.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce114ae-5431-453f-8b62-580317deca1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"flash-attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67edf1a-f345-404e-b197-c07d76bfb27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"auto-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19e2c0c-94f6-4c78-9105-12605d49f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"optimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0512e00-0643-4b21-b265-5f35014ab169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"autoawq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bbed-3a80-4b29-8aa4-03c635906b2f",
   "metadata": {},
   "source": [
    "## mistralai/Mixtral-8x7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8a2806-f49b-4cbc-bb13-346b50053131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b2615ad6714c188a399b34dc7a0302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ad0a1-9fea-4782-b427-a41358d987c3",
   "metadata": {},
   "source": [
    "## lightonai/alfred-40b-1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f79310-cb23-4b52-a366-adc315a6e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37de9dd26297474a9da1d45b767c5862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4909f4c21b6b4191884c21a30c7b7c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba89a73b6394279b48db66477d715e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/lightonai/alfred-40b-1023:\n",
      "- configuration_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/lightonai/alfred-40b-1023:\n",
      "- modeling_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a048b8c69944e2a78b0cab203e03e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadde1a4785f438e9b6dda2e4da9dc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"lightonai/alfred-40b-1023\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, use_safetensors=False, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f04498-532a-4ff0-a72c-31932620ab37",
   "metadata": {},
   "source": [
    "## tiiuae/falcon-40b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bae2b-d35a-4ce4-ad11-fdae109219bb",
   "metadata": {},
   "source": [
    "\"falcon_40b\" : \"tiiuae/falcon-40b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae8345-2590-4056-b639-0d4a454996f4",
   "metadata": {},
   "source": [
    "## TheBloke/Mixtral-8x7B-v0.1-GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed85a9-7a3c-43f0-98ff-4ecf3b164832",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ac0e6-3801-44e8-a5a9-84a1415f6013",
   "metadata": {},
   "source": [
    "## casperhansen/mixtral-instruct-awq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433bb24-6634-4b9d-8d54-8f52db7fb3bb",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"casperhansen/mixtral-instruct-awq\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc05f5d-eee6-4e86-af01-0effec91b843",
   "metadata": {},
   "source": [
    "## Cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808aa5f5-9252-441c-bb30-f7ef39fd2ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n",
      "Total    : 48,676.8 MB\n",
      "------------------------------\n",
      "Overhead :    312.0 MB -   0 %\n",
      "Reserved : 23,252.0 MB -  47 %\n",
      "Free     : 25,112.8 MB -  51 %\n",
      "------------------------------\n",
      "Used     : 22,889.9 MB -  47 %\n",
      "Max used : 23,905.9 MB -  49 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "\n",
    "# https://zdevito.github.io/2022/08/16/memory-snapshots.html\n",
    "# https://zdevito.github.io/2022/12/09/memory-traces.html\n",
    "\n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6cf4763-e756-4802-b057-c9d2998fd7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lightonai/alfred-40b-1023 size on disk:\n",
      "- model files size   : 155.86 GB\n",
      "- stored in directory: /home/.cache/huggingface/hub/models--lightonai--alfred-40b-1023/snapshots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "print(f\"Model {model_name} size on disk:\")\n",
    "path,size = get_model_path_and_size_on_disk(model_name)\n",
    "print(f\"- model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "print(f\"- stored in directory: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e851-39ff-4906-8d22-4545839673c3",
   "metadata": {},
   "source": [
    "## Perplexity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95e5814-6469-45f0-993d-e1f968ead777",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42192ae5-9501-4dd5-bcad-84cb0d7ecb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fd08c24f624199bbbe8324a1a278ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03633f897c3e4641995411f9d15b708f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e5a0bd3a2e4af78c42b95e82110509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff814b75b7ca44dba3ec416f2f1cdded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36778eec9e344418fcd19f28f45f767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633be45ea0564990935143b1287b1baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "#dataset_fr = load_dataset(dataset_name_fr, token=myhftoken)\n",
    "\n",
    "#dataset_name_en = \"frenchtext/bank-en-2401\"\n",
    "#dataset_en = load_dataset(dataset_name_en, token=myhftoken)\n",
    "\n",
    "dataset_name_de = \"frenchtext/bank-de-2401\"\n",
    "dataset_de = load_dataset(dataset_name_de, token=myhftoken)\n",
    "\n",
    "#dataset_name_es = \"frenchtext/bank-es-2401\"\n",
    "#dataset_es = load_dataset(dataset_name_es, token=myhftoken)\n",
    "\n",
    "dataset_name = dataset_name_de\n",
    "split = \"valid\"\n",
    "dataset = dataset_de[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa44df5-7753-435f-8dbd-58e809e68cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>15)\n",
    "    sorted_dataset = filtered_dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]\n",
    "\n",
    "def get_encoding_offsets(encoding):\n",
    "    start_index = encoding.offsets[0][0]\n",
    "    end_index = encoding.offsets[-1][1]\n",
    "    if end_index==0: end_index = -1\n",
    "    return (start_index, end_index)\n",
    "\n",
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "    encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5786d0e2-8ae8-498b-aa65-cc6ac45b14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        if hasattr(tokenizer,\"vocab\"):\n",
    "            self.vocab_size = len(tokenizer.vocab)\n",
    "        else:\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Number of tokens predicted, ignoring padding tokens\n",
    "        predicted_tokens_count = labels_to_ignore.sum(dim=1)\n",
    "        \n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = (1/predicted_tokens_count)*self.perplexity_loss(logits, labels_for_crossentropy).sum(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/predicted_tokens_count)*torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return predicted_tokens_count, batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities\n",
    "\n",
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f31dcf-0270-47eb-8ba1-3eee78471991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/bank-de-2401:valid for lightonai/alfred-40b-1023\n",
      "- model vocabulary: 65024\n",
      "- model sequence length: 8192\n",
      "- model torch dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#tokenizer.model_max_length = int(min(tokenizer.model_max_length, model.config.max_position_embeddings))\n",
    "# Align to memory limit of RTX 4090\n",
    "if tokenizer.model_max_length>8192:\n",
    "    tokenizer.model_max_length = 8192\n",
    "    \n",
    "print(f\"Computing perplexity on dataset {dataset_name}:{split} for {model_name}\")\n",
    "print(f\"- model vocabulary: {len(tokenizer.vocab)}\")\n",
    "print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")\n",
    "print(f\"- model torch dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "674d25a8-751c-4ccd-a919-db43a63efad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 6,062,824 tokens\n",
      "Done: 9,719,122 tokens\n",
      "CPU times: user 1min 7s, sys: 1.39 s, total: 1min 9s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53f6fe1c-979a-4aa3-a2cc-5309f2d58da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dataset examples: 2977\n",
      "- batch_size=6, stride=256\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "#batch_size = 4\n",
    "stride = 256\n",
    "\n",
    "print(f\"- dataset examples: {len(dataset)}\")\n",
    "print(f\"- batch_size={batch_size}, stride={stride}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45906a4a-b798-4a57-b2df-5b55981a1926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 encodings processed\n",
      "-> perplexity = 3.166\n",
      "-> unigram-normalized perplexity = 3.865 (x1000)\n",
      "66 encodings processed\n",
      "-> perplexity = 3.373\n",
      "-> unigram-normalized perplexity = 3.787 (x1000)\n",
      "126 encodings processed\n",
      "-> perplexity = 3.459\n",
      "-> unigram-normalized perplexity = 3.857 (x1000)\n",
      "186 encodings processed\n",
      "-> perplexity = 3.573\n",
      "-> unigram-normalized perplexity = 3.757 (x1000)\n",
      "246 encodings processed\n",
      "-> perplexity = 3.570\n",
      "-> unigram-normalized perplexity = 3.671 (x1000)\n",
      "306 encodings processed\n",
      "-> perplexity = 3.548\n",
      "-> unigram-normalized perplexity = 3.654 (x1000)\n",
      "366 encodings processed\n",
      "-> perplexity = 3.547\n",
      "-> unigram-normalized perplexity = 3.706 (x1000)\n",
      "426 encodings processed\n",
      "-> perplexity = 3.552\n",
      "-> unigram-normalized perplexity = 3.754 (x1000)\n",
      "486 encodings processed\n",
      "-> perplexity = 3.590\n",
      "-> unigram-normalized perplexity = 3.733 (x1000)\n",
      "546 encodings processed\n",
      "-> perplexity = 3.632\n",
      "-> unigram-normalized perplexity = 3.774 (x1000)\n",
      "606 encodings processed\n",
      "-> perplexity = 3.667\n",
      "-> unigram-normalized perplexity = 3.733 (x1000)\n",
      "666 encodings processed\n",
      "-> perplexity = 3.718\n",
      "-> unigram-normalized perplexity = 3.710 (x1000)\n",
      "726 encodings processed\n",
      "-> perplexity = 3.742\n",
      "-> unigram-normalized perplexity = 3.704 (x1000)\n",
      "786 encodings processed\n",
      "-> perplexity = 3.756\n",
      "-> unigram-normalized perplexity = 3.684 (x1000)\n",
      "846 encodings processed\n",
      "-> perplexity = 3.775\n",
      "-> unigram-normalized perplexity = 3.662 (x1000)\n",
      "906 encodings processed\n",
      "-> perplexity = 3.789\n",
      "-> unigram-normalized perplexity = 3.646 (x1000)\n",
      "966 encodings processed\n",
      "-> perplexity = 3.788\n",
      "-> unigram-normalized perplexity = 3.615 (x1000)\n",
      "1026 encodings processed\n",
      "-> perplexity = 3.783\n",
      "-> unigram-normalized perplexity = 3.590 (x1000)\n",
      "1086 encodings processed\n",
      "-> perplexity = 3.777\n",
      "-> unigram-normalized perplexity = 3.584 (x1000)\n",
      "1146 encodings processed\n",
      "-> perplexity = 3.773\n",
      "-> unigram-normalized perplexity = 3.571 (x1000)\n",
      "1206 encodings processed\n",
      "-> perplexity = 3.772\n",
      "-> unigram-normalized perplexity = 3.559 (x1000)\n",
      "1266 encodings processed\n",
      "-> perplexity = 3.773\n",
      "-> unigram-normalized perplexity = 3.548 (x1000)\n",
      "1326 encodings processed\n",
      "-> perplexity = 3.777\n",
      "-> unigram-normalized perplexity = 3.544 (x1000)\n",
      "1386 encodings processed\n",
      "-> perplexity = 3.780\n",
      "-> unigram-normalized perplexity = 3.540 (x1000)\n",
      "1446 encodings processed\n",
      "-> perplexity = 3.788\n",
      "-> unigram-normalized perplexity = 3.538 (x1000)\n",
      "1506 encodings processed\n",
      "-> perplexity = 3.792\n",
      "-> unigram-normalized perplexity = 3.536 (x1000)\n",
      "1566 encodings processed\n",
      "-> perplexity = 3.796\n",
      "-> unigram-normalized perplexity = 3.535 (x1000)\n",
      "1626 encodings processed\n",
      "-> perplexity = 3.798\n",
      "-> unigram-normalized perplexity = 3.531 (x1000)\n",
      "1686 encodings processed\n",
      "-> perplexity = 3.806\n",
      "-> unigram-normalized perplexity = 3.530 (x1000)\n",
      "1746 encodings processed\n",
      "-> perplexity = 3.811\n",
      "-> unigram-normalized perplexity = 3.528 (x1000)\n",
      "1806 encodings processed\n",
      "-> perplexity = 3.818\n",
      "-> unigram-normalized perplexity = 3.526 (x1000)\n",
      "1866 encodings processed\n",
      "-> perplexity = 3.823\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "1926 encodings processed\n",
      "-> perplexity = 3.825\n",
      "-> unigram-normalized perplexity = 3.524 (x1000)\n",
      "1986 encodings processed\n",
      "-> perplexity = 3.828\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2046 encodings processed\n",
      "-> perplexity = 3.831\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2106 encodings processed\n",
      "-> perplexity = 3.834\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2166 encodings processed\n",
      "-> perplexity = 3.836\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2226 encodings processed\n",
      "-> perplexity = 3.839\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2286 encodings processed\n",
      "-> perplexity = 3.841\n",
      "-> unigram-normalized perplexity = 3.523 (x1000)\n",
      "2346 encodings processed\n",
      "-> perplexity = 3.844\n",
      "-> unigram-normalized perplexity = 3.525 (x1000)\n",
      "2406 encodings processed\n",
      "-> perplexity = 3.848\n",
      "-> unigram-normalized perplexity = 3.527 (x1000)\n",
      "2466 encodings processed\n",
      "-> perplexity = 3.851\n",
      "-> unigram-normalized perplexity = 3.527 (x1000)\n",
      "2526 encodings processed\n",
      "-> perplexity = 3.853\n",
      "-> unigram-normalized perplexity = 3.528 (x1000)\n",
      "2586 encodings processed\n",
      "-> perplexity = 3.856\n",
      "-> unigram-normalized perplexity = 3.530 (x1000)\n",
      "2646 encodings processed\n",
      "-> perplexity = 3.859\n",
      "-> unigram-normalized perplexity = 3.532 (x1000)\n",
      "2706 encodings processed\n",
      "-> perplexity = 3.862\n",
      "-> unigram-normalized perplexity = 3.533 (x1000)\n",
      "2766 encodings processed\n",
      "-> perplexity = 3.865\n",
      "-> unigram-normalized perplexity = 3.537 (x1000)\n",
      "2826 encodings processed\n",
      "-> perplexity = 3.868\n",
      "-> unigram-normalized perplexity = 3.539 (x1000)\n",
      "2886 encodings processed\n",
      "-> perplexity = 3.870\n",
      "-> unigram-normalized perplexity = 3.541 (x1000)\n",
      "2946 encodings processed\n",
      "-> perplexity = 3.872\n",
      "-> unigram-normalized perplexity = 3.544 (x1000)\n",
      "3006 encodings processed\n",
      "-> perplexity = 3.875\n",
      "-> unigram-normalized perplexity = 3.546 (x1000)\n",
      "3066 encodings processed\n",
      "-> perplexity = 3.876\n",
      "-> unigram-normalized perplexity = 3.548 (x1000)\n",
      "3126 encodings processed\n",
      "-> perplexity = 3.879\n",
      "-> unigram-normalized perplexity = 3.551 (x1000)\n",
      "3186 encodings processed\n",
      "-> perplexity = 3.881\n",
      "-> unigram-normalized perplexity = 3.553 (x1000)\n",
      "3246 encodings processed\n",
      "-> perplexity = 3.883\n",
      "-> unigram-normalized perplexity = 3.556 (x1000)\n",
      "3306 encodings processed\n",
      "-> perplexity = 3.885\n",
      "-> unigram-normalized perplexity = 3.558 (x1000)\n",
      "3366 encodings processed\n",
      "-> perplexity = 3.887\n",
      "-> unigram-normalized perplexity = 3.560 (x1000)\n",
      "3426 encodings processed\n",
      "-> perplexity = 3.889\n",
      "-> unigram-normalized perplexity = 3.562 (x1000)\n",
      "3486 encodings processed\n",
      "-> perplexity = 3.890\n",
      "-> unigram-normalized perplexity = 3.563 (x1000)\n",
      "FINAL RESULT: 3498 encodings processed\n",
      "-> perplexity = 3.890\n",
      "-> unigram-normalized perplexity = 3.564 (x1000)\n",
      "CPU times: user 2h 58min 8s, sys: 9.4 s, total: 2h 58min 17s\n",
      "Wall time: 2h 57min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(pred_tokens_count, ppl_losses, unigram_losses):        \n",
    "    pt_pred_tokens_count = torch.Tensor(pred_tokens_count)\n",
    "    total_pred_tokens_count = pt_pred_tokens_count.sum().item()\n",
    "    \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp((pt_ppl_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "    pplu = math.exp((pt_pplu_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "pred_tokens_count = [] \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n",
    "        ppl_losses.extend(batch_ppl_losses.tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.tolist(), batch_pplu.tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "        display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec927146-532e-4ad5-9634-d7256288f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n",
      "Total    : 48,676.8 MB\n",
      "------------------------------\n",
      "Overhead :    328.0 MB -   0 %\n",
      "Reserved : 47,846.0 MB -  98 %\n",
      "Free     :    502.8 MB -   1 %\n",
      "------------------------------\n",
      "Used     : 22,959.0 MB -  47 %\n",
      "Max used : 41,307.9 MB -  84 %\n"
     ]
    }
   ],
   "source": [
    "display_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31267e9-e308-44ba-91e3-3964a9296b80",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 9,243,621 tokens in 14 sec\n",
    "- perplexity = 4.690\n",
    "- unigram-normalized perplexity = 6.273 (x1000)\n",
    "\n",
    "2 h 22 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142a66-45c7-4ead-b730-d976fa2823d1",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for lightonai/alfred-40b-1023\r\n",
    "- model vocabulary: 65024\r\n",
    "- model sequence length: 8192\r\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\r\n",
    "- batch_size=4, stride=256- 13,622,486 tokens in 14 sec\n",
    "\n",
    "- perplexity = 3.- 5\r\n",
    "> unigram-normalized perplexity = 4.098 (x100 0)\n",
    "\n",
    "3h 17min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e57a0-38d4-4301-9596-6b54845d6228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
