{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12919a4-709e-42d2-91c8-045d32f35a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ab9513-4060-400c-aa47-5e1be91e9d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:50:05.520084Z",
     "iopub.status.busy": "2024-04-06T21:50:05.519190Z",
     "iopub.status.idle": "2024-04-06T21:50:05.529251Z",
     "shell.execute_reply": "2024-04-06T21:50:05.528780Z",
     "shell.execute_reply.started": "2024-04-06T21:50:05.520045Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29911428-3337-4602-b686-1c0a5f0d6936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:50:05.944907Z",
     "iopub.status.busy": "2024-04-06T21:50:05.943931Z",
     "iopub.status.idle": "2024-04-06T21:50:05.961092Z",
     "shell.execute_reply": "2024-04-06T21:50:05.960432Z",
     "shell.execute_reply.started": "2024-04-06T21:50:05.944866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0.post1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b79db4-b144-4998-a885-724b720f7241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:50:06.370237Z",
     "iopub.status.busy": "2024-04-06T21:50:06.369279Z",
     "iopub.status.idle": "2024-04-06T21:50:06.388022Z",
     "shell.execute_reply": "2024-04-06T21:50:06.387267Z",
     "shell.execute_reply.started": "2024-04-06T21:50:06.370181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.23.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"xformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c6b1cf-0d38-4157-ac80-71e3a872631f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:50:07.860458Z",
     "iopub.status.busy": "2024-04-06T21:50:07.859946Z",
     "iopub.status.idle": "2024-04-06T21:50:07.877766Z",
     "shell.execute_reply": "2024-04-06T21:50:07.877127Z",
     "shell.execute_reply.started": "2024-04-06T21:50:07.860423Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.6'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"flash_attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d951bea2-f9f6-4c4e-9557-03d823bef37a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T08:51:55.862027Z",
     "iopub.status.busy": "2024-04-07T08:51:55.861876Z",
     "iopub.status.idle": "2024-04-07T08:51:55.868276Z",
     "shell.execute_reply": "2024-04-07T08:51:55.867435Z",
     "shell.execute_reply.started": "2024-04-07T08:51:55.862016Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"solidrust/Nous-Hermes-2-Mistral-7B-DPO-AWQ\"\n",
    "max_model_len=32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e40e95-4d23-4664-96db-05eb08864c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T08:51:55.873684Z",
     "iopub.status.busy": "2024-04-07T08:51:55.873484Z",
     "iopub.status.idle": "2024-04-07T08:52:08.140210Z",
     "shell.execute_reply": "2024-04-07T08:52:08.139679Z",
     "shell.execute_reply.started": "2024-04-07T08:51:55.873675Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-07 08:51:57 config.py:211] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-07 08:51:57 utils.py:256] Not found nvcc in /usr/local/cuda. Skip cuda version check!\n",
      "INFO 04-07 08:51:57 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n",
      "INFO 04-07 08:51:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='solidrust/Nous-Hermes-2-Mistral-7B-DPO-AWQ', tokenizer='solidrust/Nous-Hermes-2-Mistral-7B-DPO-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=False, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-07 08:51:58 utils.py:357] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 04-07 08:51:58 selector.py:16] Using FlashAttention backend.\n",
      "INFO 04-07 08:51:59 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-07 08:52:00 model_runner.py:104] Loading model weights took 3.8823 GB\n",
      "INFO 04-07 08:52:04 gpu_executor.py:94] # GPU blocks: 15949, # CPU blocks: 4096\n",
      "INFO 04-07 08:52:04 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-07 08:52:04 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-07 08:52:08 model_runner.py:867] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Pass the default decoding hyperparameters of Qwen1.5-7B-Chat\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "\n",
    "llm = LLM(model, quantization=\"awq\", kv_cache_dtype=\"fp8_e5m2\", gpu_memory_utilization=0.99)\n",
    "#llm = LLM(model, quantization=\"gptq\", kv_cache_dtype=\"fp8_e5m2\", max_model_len=max_model_len)\n",
    "#llm = LLM(model, quantization=\"gptq\", max_model_len=max_model_len)\n",
    "#llm = LLM(model, quantization=\"awq\", max_model_len=max_model_len)\n",
    "#llm = LLM(model, quantization=\"awq\", kv_cache_dtype=\"fp8_e5m2\", max_model_len=max_model_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc490a3-a9ab-4bfe-ac75-7211edbc0dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T08:52:08.141201Z",
     "iopub.status.busy": "2024-04-07T08:52:08.140761Z",
     "iopub.status.idle": "2024-04-07T08:52:08.143443Z",
     "shell.execute_reply": "2024-04-07T08:52:08.142986Z",
     "shell.execute_reply.started": "2024-04-07T08:52:08.141186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8e7d80-1e92-4a8b-8e1e-4fc2a4f463e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T08:52:08.144191Z",
     "iopub.status.busy": "2024-04-07T08:52:08.144080Z",
     "iopub.status.idle": "2024-04-07T08:52:11.875296Z",
     "shell.execute_reply": "2024-04-07T08:52:11.874801Z",
     "shell.execute_reply.started": "2024-04-07T08:52:08.144183Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|im_start|>system\\nTu es un assistant utile et professionnel qui répond toujours en français.<|im_end|>\\n<|im_start|>user\\nQuels sont les avantages du Crédit Mutuel ?<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"Le Crédit Mutuel est un groupe bancaire coopératif et mutuel qui offre plusieurs avantages à ses adhérents :\\n\\n1. Solidarité financière : Le Crédit Mutuel est basé sur la solidarité financière, c'est-à-dire que les bénéfices réalisés par les établissements du groupe sont redistribués entre les adhérents sous forme de remises d'intérêt ou de dividendes.\\n\\n2. Tarifs compétitifs : Le Crédit Mutuel propose généralement des tarifs compétitifs sur ses produits bancaires, notamment pour le prêt, le crédit et les cartes bancaires.\\n\\n3. Service client : Les établissements du Crédit Mutuel sont réputés pour leur service client de qualité, avec un accompagnement personnelisé et une attention portée aux besoins individuels des adhérents.\\n\\n4. Stabilité financière : En tant que groupe bancaire coopératif, le Crédit Mutuel dispose d'une solide base de membres et de capital, ce qui contribue à sa stabilité financière.\\n\\n5. Responsabilité sociale et environnementale : Le Crédit Mutuel s'engage dans des actions de responsabilité sociale et environnementale, notamment en soutenant des projets locaux et en mettant en œuvre des pratiques durables.\\n\\n6. Accessibilité : Les établissements du Crédit Mutuel sont généralement présents sur l'ensemble du territoire national, ce qui facilite l'accès aux services bancaires pour les adhérents.\\n\\nCependant, il convient de noter que ces avantages peuvent varier en fonction des établissements du Crédit Mutuel et de leurs offres spécifiques. Il est recommandé de vous informer auprès de votre établissement local pour obtenir des informations précises et actualisées.\", Performance: 128 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quels sont les avantages du Crédit Mutuel ?\"\n",
    "\n",
    "# System prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Generate outputs\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "start_time = time.time()  # Record the start time\n",
    "outputs = llm.generate([text], sampling_params)\n",
    "end_time = time.time()  # Record the end time\n",
    "    \n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}, Performance: {int(len(output.outputs[0].token_ids)/(end_time-start_time))} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93564d8e-ea69-43ec-82a4-f955d0c4f6e1",
   "metadata": {},
   "source": [
    "- model = \"Qwen/Qwen1.5-32B-Chat-GPTQ-Int4\"\n",
    "- max_model_len=10576\n",
    "- quantization=\"gptq\"\n",
    "- kv_cache_dtype=\"fp8_e5m2\"\n",
    "- gpu_memory_utilization=0.99\n",
    "\n",
    "42 tokens / sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca346a44-6147-4403-99cf-248b80c2925f",
   "metadata": {},
   "source": [
    "- model = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\"\n",
    "- quantization=\"gptq\"\n",
    "- kv_cache_dtype=\"fp8_e5m2\"\n",
    "- max_model_len=22624\n",
    "\n",
    "80 tokens/sec\n",
    "\n",
    "- model = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\"\n",
    "- quantization=\"gptq\"\n",
    "- max_model_len=11744\n",
    "\n",
    "80 tokens/sec\n",
    "\n",
    "- model = \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\"\n",
    "- quantization=\"gptq\"\n",
    "- kv_cache_dtype=\"fp8_e5m2\"\n",
    "- max_model_len=4016\n",
    "- gpu_memory_utilization=1\n",
    "\n",
    "50 tokens/sec\n",
    "\n",
    "- model = \"Qwen/Qwen1.5-14B-Chat-AWQ\"\n",
    "- quantization=\"awq\"\n",
    "- max_model_len=10544\n",
    "\n",
    "74 tokens/sec\n",
    "\n",
    "- model = \"Qwen/Qwen1.5-14B-Chat-AWQ\"\n",
    "- quantization=\"awq\"\n",
    "- kv_cache_dtype=\"fp8_e5m2\"\n",
    "- max_model_len=21104\n",
    "\n",
    "74 tokens/sec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a4960-ce95-4256-9109-47204bf23a51",
   "metadata": {},
   "source": [
    "Tongyi Qianwen LICENSE AGREEMENT\n",
    "\n",
    "- \"We\"(or \"Us\") shall mean Alibaba Cloud.\n",
    "- You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\n",
    "- Restrictions: If you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us. You cannot exercise your rights under this Agreement without our express authorization.\n",
    "- Rules of use: **You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).**\n",
    "- Governing Law and Jurisdiction: This Agreement and any dispute arising out of or relating to it will be governed by the laws of China"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e25f8e-070d-49d0-b7b4-f5e078dc5c7b",
   "metadata": {},
   "source": [
    "- model = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "- max_model_len=12544\n",
    "- gpu_memory_utilization=0.95\n",
    "\n",
    "55 tokens/sec\n",
    "\n",
    "- solidrust/Nous-Hermes-2-Mistral-7B-DPO-AWQ\n",
    "- quantization=\"awq\"\n",
    "- kv_cache_dtype=\"fp8_e5m2\"\n",
    "- gpu_memory_utilization=0.99\n",
    "- (max_seq_len=32768)\n",
    "\n",
    "125 tokens/sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2505d-a827-4dc2-a944-d8931758d335",
   "metadata": {},
   "source": [
    "Hermes 2 Pro - Mistral 7B LICENSE : Apache 2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
