{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e795d02-d492-41f5-bb89-4e7448c4da71",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058ff6-f379-41b0-8646-25b9cf380087",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925de6e-6681-48b3-8763-134fa1038b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2179c2-e0a1-4dc9-aaee-772fdc22333d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T15:17:50.704982Z",
     "iopub.status.busy": "2023-12-23T15:17:50.704466Z",
     "iopub.status.idle": "2023-12-23T15:17:50.712616Z",
     "shell.execute_reply": "2023-12-23T15:17:50.711623Z",
     "shell.execute_reply.started": "2023-12-23T15:17:50.704952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255816-433e-4ba7-9793-9c61ba2eea8d",
   "metadata": {},
   "source": [
    "For Qwen/Qwen-7B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c814e-348f-4055-bec7-0ea461515b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203ea47-e9f0-40f6-8810-bb8a777c9826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers_stream_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42929969-266f-427c-bd10-376aeb4d4920",
   "metadata": {},
   "source": [
    "For 01-ai/Yi-6B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017470c-95a2-41fe-9e9d-f0aaea0a02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60e1f8-7c73-45b9-9b53-e09fd473ff27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T20:31:56.753294Z",
     "iopub.status.busy": "2023-12-26T20:31:56.753001Z",
     "iopub.status.idle": "2023-12-26T20:31:56.784205Z",
     "shell.execute_reply": "2023-12-26T20:31:56.783477Z",
     "shell.execute_reply.started": "2023-12-26T20:31:56.753273Z"
    },
    "tags": []
   },
   "source": [
    "## Models dictionary\n",
    "\n",
    "Models are selected based on the following criteria:\n",
    "- Base models only: to test their initial strengh on multilingual text before fine-tuning\n",
    "- State of the art or very significant at the time of their release\n",
    "- For local use: should run reasonably well on a 24 GB consumer GPU machine\n",
    "\n",
    "For the 30B-45B parameters range of models:\n",
    "- Only consider 4 bits quantized versions: 16 bits versions are too big to store on disk on a consumer machine\n",
    "- Use fine-tuned models: 4 bits quantized versions of base models unfortunately don't exist on HuggingFace\n",
    "\n",
    "### 1B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/12/29 | tinyllama_1b | ?? GB | No | No | https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T | 1.1 B | 2048 | 2.5 T | Apache 2.0 |\n",
    "\n",
    "### 3B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/05/04 | redpajama_3b | 5.30 GB | No | No | https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1 | 2.8 B | 2048 | 800 B | Apache 2.0 |\n",
    "| 2023/07/14 | btlm_3b | 4.93 GB | No | YES | https://huggingface.co/cerebras/btlm-3b-8k-base | 3 B | 8192 | 627 B | Apache 2.0 |\n",
    "| 2023/07/16 | openllama2_3b | 6.38 GB | No | No | https://huggingface.co/openlm-research/open_llama_3b_v2 | 3 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/09/29 | stablelm_3b | 5.21 GB | YES | YES | https://huggingface.co/stabilityai/stablelm-3b-4e1t | 3 B | 4096 | 1 T | CC BY-SA-4.0 |\n",
    "| 2023/12/13 | phi2_3b | 5.18 GB | No | No | https://huggingface.co/microsoft/phi-2 | 2.7 B | 2048 | 1.4 T | MIT |\n",
    "\n",
    "### 7B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2022/10/09 | bloomz_7b | 13.18 GB | No | No | https://huggingface.co/bigscience/bloomz-7b1-mt | 7.1 B | 2048 | 350 B | BigScience RAIL License  |\n",
    "| 2023/04/24 | falcon_7b | 13.45 GB  | No | No | https://huggingface.co/tiiuae/falcon-7b | 7 B | 2048 | 1.5 T | Apache 2.0 |\n",
    "| 2024/05/04 | redpajama_7b | 12.90 GB | No | No | https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base | 6.9 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/05/05 | mpt_7b | 12.39 GB | No | YES | https://huggingface.co/mosaicml/mpt-7b | 6.7 B | 2048 | 1 T | Apache-2.0 |\n",
    "| 2023/06/30 | mpt_7b_8k| 12.39 GB | No | YES | https://huggingface.co/mosaicml/mpt-7b-8k | 6.7 B | 8192 | 1.5 T | Apache-2.0 |\n",
    "| 2023/07/06 | openllama2_7b | 12.55 GB | No | No | https://huggingface.co/openlm-research/open_llama_7b_v2 | 7 B | 2048 | 1 T | Apache 2.0 | \n",
    "| 2023/07/18 | llama2_7b | 12.55 GB | YES | No | https://huggingface.co/meta-llama/Llama-2-7b-hf | 7 B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/07/26 | llama2_7b_32k | 12.55 GB | No | YES | https://huggingface.co/togethercomputer/LLaMA-2-7B-32K | 7 B | 32768 | fine-tuned | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/20 | mistral_7b | 13.49 GB | No | No | https://huggingface.co/mistralai/Mistral-7B-v0.1 | 7.3 B | 8192 | ?? | Apache 2.0 |\n",
    "| 2023/09/24 | qwen_7b | 14.38 GB | No | YES | https://huggingface.co/Qwen/Qwen-7B | 7 B | 8192 | 2.4 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_6b | 11.29 GB | No | No | https://huggingface.co/01-ai/Yi-6B | 6 B | 4096 | 3 T | Yi Series Models Community License Agreement |\n",
    "| 2023/12/10 | decilm_7b | 13.12 GB | No | YES | https://huggingface.co/Deci/DeciLM-7B | 7 B | 8192 | ?? | Apache 2.0 |\n",
    "\n",
    "### 13B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/06/15 | openllama1_13b | 24.24 GB | No | No | https://huggingface.co/openlm-research/open_llama_13b | 13 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/07/18 | llama2_13b | 24.25 GB | YES | No | https://huggingface.co/meta-llama/Llama-2-13b-hf | 13B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/24 | qwen_14b | 26.39 GB | No | YES | https://huggingface.co/Qwen/Qwen-14B | 14 B | 2048 | 3 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/12/12 | solar_10b | 19.99 GB | No | No | https://huggingface.co/upstage/SOLAR-10.7B-v1.0 | 10.7 B | 4096 | 3 T | Apache 2.0 |\n",
    "\n",
    "### 30B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/06/20 | mpt_30b | ?? GB | No | YES | https://huggingface.co/mosaicml/mpt-30b | 30 B | 8192 | 1 T | Apache-2.0 |\n",
    "| 2023/08/24 | codellama_34b | ?? GB | No | No | https://huggingface.co/codellama/CodeLlama-34b-hf | 34 B | 16384 | 2.5 T  | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_34b | ?? GB | No | No | https://huggingface.co/01-ai/Yi-34B | 34 B | 4096 | 3 T | Yi Series Models Community License Agreement |\n",
    "\n",
    "### 40B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/05/24 | falcon_40b | ?? GB | No | YES | https://huggingface.co/tiiuae/falcon-40b | 40 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/11/17 | alfred_40b | ?? GB | No | No | https://huggingface.co/lightonai/alfred-40b-1023 | 40 B | 8192 | 1.1 T | Apache 2.0 |\n",
    "| 2023/12/11 | mixtral_8x7B | ?? GB | No | No | https://huggingface.co/mistralai/Mixtral-8x7B-v0.1 | 46.7 B -> 12.9 B | 32768 | ??  | Apache 2.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342144-2d6f-4df1-ae83-13182f2625d6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2ef01b-6443-470a-a8aa-50b9735ed904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:26.786922Z",
     "iopub.status.busy": "2024-01-18T03:57:26.786813Z",
     "iopub.status.idle": "2024-01-18T03:57:26.791901Z",
     "shell.execute_reply": "2024-01-18T03:57:26.791341Z",
     "shell.execute_reply.started": "2024-01-18T03:57:26.786910Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " models = { \n",
    "    \"tinyllama_1b\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\", # 4.10 GB\n",
    "     \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\", # 55.80 GB \n",
    "    \"codellama_34b\" : \"codellama/CodeLlama-34b-hf\", # 62.86 GB \n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\", # 64.06 GB    \n",
    "     \n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\", # ?? GB \n",
    "    \"alfred_40b\": \"lightonai/alfred-40b-1023\", # ?? GB\n",
    "    \"mixtral_8x7B\" : \"mistralai/Mixtral-8x7B-v0.1\" # ?? GB \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013cd80-b2c8-4821-a763-adc990e3f8e6",
   "metadata": {},
   "source": [
    "## Models download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deaaa43-17b4-40ff-aa26-76efe13a67e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to be able to access gated HuggingFace repositories:\n",
    "\n",
    "1. Login to your HuggingFace account, go to https://huggingface.co/settings/tokens, create a READ access token and copy it\n",
    "2. Paste your HuggingFace access token in the local file /workspace/hftoken\n",
    "3. Load it in Python with the the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18c8a66-c1ff-4eac-9964-4d0a3855453b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:26.792658Z",
     "iopub.status.busy": "2024-01-18T03:57:26.792550Z",
     "iopub.status.idle": "2024-01-18T03:57:26.811272Z",
     "shell.execute_reply": "2024-01-18T03:57:26.810851Z",
     "shell.execute_reply.started": "2024-01-18T03:57:26.792650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c09e1b-567c-4dd4-87f8-89da992aa369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:26.888771Z",
     "iopub.status.busy": "2024-01-18T03:57:26.887976Z",
     "iopub.status.idle": "2024-01-18T03:57:26.893890Z",
     "shell.execute_reply": "2024-01-18T03:57:26.893161Z",
     "shell.execute_reply.started": "2024-01-18T03:57:26.888739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizers_load_kwargs = { \n",
    "    \"tinyllama_1b\": {},\n",
    "     \n",
    "    \"redpajama_3b\" : {},\n",
    "    \"btlm_3b\" : {},\n",
    "    \"openllama2_3b\" : {},\n",
    "    \"stablelm_3b\" : { \"token\": myhftoken },\n",
    "    \"phi2_3b\" : {},\n",
    "\n",
    "    \"bloomz_7b\" : {},\n",
    "    \"falcon_7b\" :  {},\n",
    "    \"redpajama_7b\" : {},\n",
    "    \"mpt_7b\" :  {},\n",
    "    \"mpt_7b_8k\" :  {},\n",
    "    \"openllama2_7b\" : {},\n",
    "    \"llama2_7b\" : { \"token\": myhftoken },\n",
    "    \"llama2_7b_32k\" : {},\n",
    "    \"mistral_7b\" : {},\n",
    "    \"qwen_7b\" : { \"trust_remote_code\": True },\n",
    "    \"yi_6b\" : {},\n",
    "    \"decilm_7b\" : {},\n",
    "    \n",
    "    \"openllama1_13b\" : {},\n",
    "    \"llama2_13b\" : { \"token\": myhftoken },\n",
    "    \"qwen_14b\" : { \"trust_remote_code\": True },\n",
    "    \"solar_10b\" : {},\n",
    "    \n",
    "    \"mpt_30b\" : {},\n",
    "    \"codellama_34b\" : {},\n",
    "    \"yi_34b\" : {},\n",
    "     \n",
    "    \"falcon_40b\" : {},\n",
    "    \"alfred_40b\": {},\n",
    "    \"mixtral_8x7B\" : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a64aa0-acc0-481a-921a-c64a11574046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:27.106436Z",
     "iopub.status.busy": "2024-01-18T03:57:27.106187Z",
     "iopub.status.idle": "2024-01-18T03:57:27.110586Z",
     "shell.execute_reply": "2024-01-18T03:57:27.110005Z",
     "shell.execute_reply.started": "2024-01-18T03:57:27.106416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_load_kwargs = { \n",
    "    \"tinyllama_1b\": {},\n",
    "     \n",
    "    \"redpajama_3b\" : {},\n",
    "    \"btlm_3b\" : { \"trust_remote_code\": True },\n",
    "    \"openllama2_3b\" : {},\n",
    "    \"stablelm_3b\" : { \"trust_remote_code\": True, \"token\": myhftoken },\n",
    "    \"phi2_3b\" : {},\n",
    "\n",
    "    \"bloomz_7b\" : {},\n",
    "    \"falcon_7b\" :  {},\n",
    "    \"redpajama_7b\" : {},\n",
    "    \"mpt_7b\" :  { \"use_safetensors\": False }, # 01/17/2024: for some reason, impossible to load the safetensors PR => fall back to *.bin\n",
    "    \"mpt_7b_8k\" :  {},\n",
    "    \"openllama2_7b\" : {},\n",
    "    \"llama2_7b\" : { \"token\": myhftoken },\n",
    "    \"llama2_7b_32k\" : {},\n",
    "    \"mistral_7b\" : {},\n",
    "    \"qwen_7b\" : { \"trust_remote_code\": True },\n",
    "    \"yi_6b\" : {},\n",
    "    \"decilm_7b\" : { \"trust_remote_code\": True },\n",
    "    \n",
    "    \"openllama1_13b\" : { \"use_safetensors\": False }, # 01/17/2024: for some reason, impossible to load the safetensors PR => fall back to *.bin\n",
    "    \"llama2_13b\" : { \"token\": myhftoken },\n",
    "    \"qwen_14b\" : { \"trust_remote_code\": True },\n",
    "    \"solar_10b\" : {},\n",
    "\n",
    "    \"mpt_30b\" : { \"use_safetensors\": False }, # 01/17/2024: script conversion error when using safetensors\n",
    "    \"codellama_34b\" : {},\n",
    "    \"yi_34b\" : {},\n",
    "     \n",
    "    \"falcon_40b\" : { \"use_safetensors\": False }, # 01/17/2024: for some reason, impossible to load the safetensors PR => fall back to *.bin\n",
    "    \"alfred_40b\": { \"trust_remote_code\": True },\n",
    "    \"mixtral_8x7B\" : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa843-2258-443b-9b83-45b990f25a87",
   "metadata": {},
   "source": [
    "Download all models in HF local cache and measure the model files size on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6e717e-5686-4fab-b787-bd0a0ce16dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:28.616160Z",
     "iopub.status.busy": "2024-01-18T03:57:28.614660Z",
     "iopub.status.idle": "2024-01-18T03:57:29.621668Z",
     "shell.execute_reply": "2024-01-18T03:57:29.621032Z",
     "shell.execute_reply.started": "2024-01-18T03:57:28.616105Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def download_in_local_cache(model_key):\n",
    "    pretrained_model_id = models[model_key]\n",
    "    print(f\"Loading model {model_key} -> {pretrained_model_id} in local cache ...\")\n",
    "    AutoTokenizer.from_pretrained(pretrained_model_id, **tokenizers_load_kwargs[model_key])\n",
    "    #try:            \n",
    "    model_load_kwargs = models_load_kwargs[model_key]\n",
    "    # Always use safetensors by default !\n",
    "    if not \"use_safetensors\" in model_load_kwargs:\n",
    "        model_load_kwargs[\"use_safetensors\"] = True\n",
    "    AutoModelForCausalLM.from_pretrained(pretrained_model_id, **models_load_kwargs[model_key], device_map=\"meta\")\n",
    "    #except Exception as e:\n",
    "    #    print(\"Ignored exceptions while loading the model in memory: not the goal here\")\n",
    "    #    print(e)\n",
    "    path,size = get_model_path_and_size_on_disk(pretrained_model_id)\n",
    "    print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"--> stored in directory: {path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fb4f5-8adf-46c0-b402-2a79a1e3e172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T03:57:41.167645Z",
     "iopub.status.busy": "2024-01-18T03:57:41.167324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model mpt_30b -> mosaicml/mpt-30b in local cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2693626cfc7e48bf942f162363de9aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81390c38ec4119a18ae7a3786dc4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 55.80 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--mosaicml--mpt-30b/snapshots\n",
      "\n",
      "Loading model codellama_34b -> codellama/CodeLlama-34b-hf in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85ee6236650479d8f1489340fca817f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86afe6275b6a441fb4205e5ec3544dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 62.86 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--codellama--CodeLlama-34b-hf/snapshots\n",
      "\n",
      "Loading model yi_34b -> 01-ai/Yi-34B in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe7131ba06d43fd80d4f0f0fb88c1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4fffc81dbe417badaf7956d46b9616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> model files size   : 192.18 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--01-ai--Yi-34B/snapshots\n",
      "\n",
      "Loading model falcon_40b -> tiiuae/falcon-40b in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577043109486494789db897aa91d8280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a83bf55f2244bbb856c8fb4f9dee9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f59d5aaa5f48ab88e8f1b2c601337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00009.bin:   0%|          | 0.00/9.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_with_model_key = \"mpt_30b\"\n",
    "\n",
    "model_seen = start_with_model_key is None or False \n",
    "for model_key in models.keys():    \n",
    "    \n",
    "    if not model_seen:\n",
    "        if model_key == start_with_model_key:\n",
    "            model_seen = True\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    download_in_local_cache(model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa186-370c-4984-b8f3-5d663e3248a7",
   "metadata": {},
   "source": [
    "**Disk space:** here is a bash command to monitor the disk space used by the datasets and the models.\n",
    "\n",
    "```bash\n",
    "du -h /models/huggingface --max-depth=2\n",
    "```\n",
    "\n",
    "**Cleanup:** here are bash commands to delete Pytorch .bin files in the /models/huggingface/transformers/models--*** cache directories.\n",
    "\n",
    "```bash\n",
    "cd /models/huggingface/transformers/models/huggingface/transformers\n",
    "\n",
    "# linknames\n",
    "find . -name \"*.bin\"\n",
    "\n",
    "# pysical files\n",
    "find . -name \"*.bin\" -exec readlink -f {} \\; | grep blobsfind . -name \"*.bin\" -exec readlink -f {} \\; | grep blobs\n",
    "\n",
    "# delete unused *.bin files\n",
    "rm [linknames] [physicalfiles]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a37a24-9b6e-4652-8dd4-c8b11499860f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
