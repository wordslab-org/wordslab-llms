{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e795d02-d492-41f5-bb89-4e7448c4da71",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058ff6-f379-41b0-8646-25b9cf380087",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925de6e-6681-48b3-8763-134fa1038b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2179c2-e0a1-4dc9-aaee-772fdc22333d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T15:17:50.704982Z",
     "iopub.status.busy": "2023-12-23T15:17:50.704466Z",
     "iopub.status.idle": "2023-12-23T15:17:50.712616Z",
     "shell.execute_reply": "2023-12-23T15:17:50.711623Z",
     "shell.execute_reply.started": "2023-12-23T15:17:50.704952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255816-433e-4ba7-9793-9c61ba2eea8d",
   "metadata": {},
   "source": [
    "For Qwen/Qwen-7B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c814e-348f-4055-bec7-0ea461515b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203ea47-e9f0-40f6-8810-bb8a777c9826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers_stream_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42929969-266f-427c-bd10-376aeb4d4920",
   "metadata": {},
   "source": [
    "For 01-ai/Yi-6B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017470c-95a2-41fe-9e9d-f0aaea0a02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60e1f8-7c73-45b9-9b53-e09fd473ff27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-26T20:31:56.753294Z",
     "iopub.status.busy": "2023-12-26T20:31:56.753001Z",
     "iopub.status.idle": "2023-12-26T20:31:56.784205Z",
     "shell.execute_reply": "2023-12-26T20:31:56.783477Z",
     "shell.execute_reply.started": "2023-12-26T20:31:56.753273Z"
    },
    "tags": []
   },
   "source": [
    "## Models dictionary\n",
    "\n",
    "Models are selected based on the following criteria:\n",
    "- Base models only: to test their initial strengh on multilingual text before fine-tuning\n",
    "- State of the art or very significant at the time of their release\n",
    "- For local use: should run reasonably well on a 24 GB consumer GPU machine\n",
    "\n",
    "For the 30B-45B parameters range of models:\n",
    "- Only consider 4 bits quantized versions: 16 bits versions are too big to store on disk on a consumer machine\n",
    "- Use fine-tuned models: 4 bits quantized versions of base models unfortunately don't exist on HuggingFace\n",
    "\n",
    "### 1B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/12/29 | tinyllama_1b | ?? GB | No | No | https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T | 1.1 B | 2048 | 2.5 T | Apache 2.0 |\n",
    "\n",
    "### 3B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/05/04 | redpajama_3b | 5.30 GB | No | No | https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1 | 2.8 B | 2048 | 800 B | Apache 2.0 |\n",
    "| 2023/07/14 | btlm_3b | 4.93 GB | No | YES | https://huggingface.co/cerebras/btlm-3b-8k-base | 3 B | 8192 | 627 B | Apache 2.0 |\n",
    "| 2023/07/16 | openllama2_3b | 6.38 GB | No | No | https://huggingface.co/openlm-research/open_llama_3b_v2 | 3 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/09/29 | stablelm_3b | 5.21 GB | YES | YES | https://huggingface.co/stabilityai/stablelm-3b-4e1t | 3 B | 4096 | 1 T | CC BY-SA-4.0 |\n",
    "| 2023/12/13 | phi2_3b | 5.18 GB | No | YES | https://huggingface.co/microsoft/phi-2 | 2.7 B | 2048 | 1.4 T | MIT |\n",
    "\n",
    "### 7B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2022/10/09 | bloomz_7b | 13.18 GB | No | No | https://huggingface.co/bigscience/bloomz-7b1-mt | 7.1 B | 2048 | 350 B | BigScience RAIL License  |\n",
    "| 2023/04/24 | falcon_7b | 13.45 GB  | No | YES | https://huggingface.co/tiiuae/falcon-7b | 7 B | 2048 | 1.5 T | Apache 2.0 |\n",
    "| 2024/05/04 | redpajama_7b | 12.90 GB | No | No | https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base | 6.9 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/05/05 | mpt_7b | 12.39 GB | No | YES | https://huggingface.co/mosaicml/mpt-7b | 6.7 B | 2048 | 1 T | Apache-2.0 |\n",
    "| 2023/06/30 | mpt_7b_8k| 12.39 GB | No | YES | https://huggingface.co/mosaicml/mpt-7b-8k | 6.7 B | 8192 | 1.5 T | Apache-2.0 |\n",
    "| 2023/07/06 | openllama2_7b | 12.55 GB | No | No | https://huggingface.co/openlm-research/open_llama_7b_v2 | 7 B | 2048 | 1 T | Apache 2.0 | \n",
    "| 2023/07/18 | llama2_7b | 12.55 GB | YES | No | https://huggingface.co/meta-llama/Llama-2-7b-hf | 7 B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/07/26 | llama2_7b_32k | 12.55 GB | No | YES | https://huggingface.co/togethercomputer/LLaMA-2-7B-32K | 7 B | 32768 | fine-tuned | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/20 | mistral_7b | 13.49 GB | No | No | https://huggingface.co/mistralai/Mistral-7B-v0.1 | 7.3 B | 8192 | ?? | Apache 2.0 |\n",
    "| 2023/09/24 | qwen_7b | 14.38 GB | No | YES | https://huggingface.co/Qwen/Qwen-7B | 7 B | 8192 | 2.4 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_6b | 11.29 GB | No | No | https://huggingface.co/01-ai/Yi-6B | 6 B | 4096 | 3 T | Yi Series Models Community License Agreement |\n",
    "| 2023/12/10 | decilm_7b | 13.12 GB | No | YES | https://huggingface.co/Deci/DeciLM-7B | 7 B | 8192 | ?? | Apache 2.0 |\n",
    "\n",
    "### 13B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/06/15 | openllama1_13b | 24.24 GB | No | No | https://huggingface.co/openlm-research/open_llama_13b | 13 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/07/18 | llama2_13b | 24.25 GB | YES | No | https://huggingface.co/meta-llama/Llama-2-13b-hf | 13B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/24 | qwen_14b | 26.39 GB | No | YES | https://huggingface.co/Qwen/Qwen-14B | 14 B | 2048 | 3 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/12/12 | solar_10b | 19.99 GB | No | No | https://huggingface.co/upstage/SOLAR-10.7B-v1.0 | 10.7 B | 4096 | 3 T | Apache 2.0 |\n",
    "\n",
    "### 30-45B parameters\n",
    "\n",
    "| Date | Name | Disk size | Gated access | Remote code | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/02/24 | llama1_33b | 15.78 GB | No | No | https://huggingface.co/alexl83/LLaMA-33B-HF | 32.5 B | 2048 | 1.4 T | Noncommercial license focused on research use cases |\n",
    "| 2023/05/24 | falcon_40b | 21.00 GB | No | YES | https://huggingface.co/tiiuae/falcon-40b | 40 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/06/20 | mpt_30b | 15.00 GB | No | YES | https://huggingface.co/mosaicml/mpt-30b | 30 B | 8192 | 1 T | Apache-2.0 |\n",
    "| 2023/08/24 | codellama_34b | 17.07 GB | No | No | https://huggingface.co/codellama/CodeLlama-34b-hf | 34 B | 16384 | 2.5 T  | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_34b | 17.33 GB | No | No | https://huggingface.co/01-ai/Yi-34B | 34 B | 4096 | 3 T | Yi Series Models Community License Agreement\n",
    "| 2023/12/11 | mixtral_8x7B | 22.18 GB | No | No | https://huggingface.co/mistralai/Mixtral-8x7B-v0.1 | 46.7 B -> 12.9 B | 32768 | ??  | Apache 2.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342144-2d6f-4df1-ae83-13182f2625d6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2ef01b-6443-470a-a8aa-50b9735ed904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T09:31:25.116753Z",
     "iopub.status.busy": "2024-01-07T09:31:25.116654Z",
     "iopub.status.idle": "2024-01-07T09:31:25.121103Z",
     "shell.execute_reply": "2024-01-07T09:31:25.120661Z",
     "shell.execute_reply.started": "2024-01-07T09:31:25.116743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " models = { \n",
    "    \"tinyllama_1b\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\", # 4.10 GB\n",
    "     \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # 15.78 GB https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # 21.00 GB https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # 15.00 GB https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # 17.07 GB https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # 17.33 GB https://huggingface.co/01-ai/Yi-34B    \n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # 22.18 GB https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013cd80-b2c8-4821-a763-adc990e3f8e6",
   "metadata": {},
   "source": [
    "## Models download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deaaa43-17b4-40ff-aa26-76efe13a67e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to be able to access gated HuggingFace repositories:\n",
    "\n",
    "1. Login to your HuggingFace account, go to https://huggingface.co/settings/tokens, create a READ access token and copy it\n",
    "2. Paste your HuggingFace access token in the local file /workspace/hftoken\n",
    "3. Load it in Python with the the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18c8a66-c1ff-4eac-9964-4d0a3855453b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T09:31:35.417159Z",
     "iopub.status.busy": "2024-01-07T09:31:35.416846Z",
     "iopub.status.idle": "2024-01-07T09:31:35.429888Z",
     "shell.execute_reply": "2024-01-07T09:31:35.429406Z",
     "shell.execute_reply.started": "2024-01-07T09:31:35.417141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa843-2258-443b-9b83-45b990f25a87",
   "metadata": {},
   "source": [
    "Download all models in HF local cache and measure the model files size on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b6e717e-5686-4fab-b787-bd0a0ce16dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T09:34:14.601681Z",
     "iopub.status.busy": "2024-01-07T09:34:14.601124Z",
     "iopub.status.idle": "2024-01-07T09:34:14.605450Z",
     "shell.execute_reply": "2024-01-07T09:34:14.604931Z",
     "shell.execute_reply.started": "2024-01-07T09:34:14.601663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "    print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "    AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "    try:            \n",
    "        AutoModelForCausalLM.from_pretrained(pretrained_model_id, device_map=\"meta\", **kwargs)\n",
    "    except Exception as e:\n",
    "        print(\"Ignored exceptions while loading the model in memory: not the goal here\")\n",
    "        print(e)\n",
    "    path,size = get_model_path_and_size_on_disk(pretrained_model_id)\n",
    "    print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"--> stored in directory: {path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fb4f5-8adf-46c0-b402-2a79a1e3e172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx,model_key in enumerate(models):\n",
    "    download_in_local_cache(models[model_key], use_safetensors=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d20b10-c7d2-4b09-8eb1-358a14357140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T11:21:47.498129Z",
     "iopub.status.busy": "2024-01-07T11:21:47.497602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model openlm-research/open_llama_13b in local cache ...\n"
     ]
    }
   ],
   "source": [
    "model_name = list(models)[18]\n",
    "download_in_local_cache(models[model_name], use_safetensors=True)#, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe2d86-a765-499c-b738-cf24fa52caf3",
   "metadata": {},
   "source": [
    "/models/huggingface/transformers/models--togethercomputer--RedPajama-INCITE-7B-Base\n",
    "/models/huggingface/transformers/models--mosaicml--mpt-7b\n",
    "/models/huggingface/transformers/models--mosaicml--mpt-7b-8k\n",
    "/models/huggingface/transformers/models--openlm-research--open_llama_7b_v2\n",
    "/models/huggingface/transformers/models--togethercomputer--LLaMA-2-7B-32K\n",
    "/models/huggingface/transformers/models--mistralai--Mistral-7B-v0.1\n",
    "/models/huggingface/transformers/models--Qwen--Qwen-7B\n",
    "/models/huggingface/transformers/models--01-ai--Yi-6B\n",
    "\n",
    "/models/huggingface/transformers/models--openlm-research--open_llama_13b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa186-370c-4984-b8f3-5d663e3248a7",
   "metadata": {},
   "source": [
    "linkname=$(find . -name \"*.bin\")\n",
    "target=$(readlink -f $linkname)\n",
    "rm \"$target\"\n",
    "rm \"$linkname\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdf56e-a1cb-47d0-b6da-25c7868c713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find . -name \"*.bin\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
