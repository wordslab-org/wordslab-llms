{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d483674f-429c-4679-bca0-eeaf59a184c4",
   "metadata": {},
   "source": [
    "# Small models from july 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b729d-a1a0-4e09-8c62-116d3e4da7e9",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebfaa9-103a-4c9b-9780-346313cd7d12",
   "metadata": {},
   "source": [
    "**Important :** this will only work if **cuda 12.4.0** and Pytorch 2.4.0 are installed int the underlying conda environment.\n",
    "\n",
    "> conda install -y cuda -c nvidia/label/cuda-12.4.0\n",
    "> \n",
    "> conda install -y pytorch=2.4.0 torchvision=0.19.0 torchaudio=2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia/label/cuda-12.4.0\n",
    "\n",
    "The FP8 kernels are not supported in cuda 12.1.0 required by pytorch 2.3.1.\n",
    "\n",
    "vllm will currently install pytorch 2.3.1 in the virtual environment on top of this, but it seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aea41d4-0891-4472-9099-887e67993121",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-30T23:38:16.560069Z",
     "shell.execute_reply": "2024-07-30T23:38:16.559597Z",
     "shell.execute_reply.started": "2024-07-30T23:38:02.636625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed torch-2.3.1 vllm-0.5.3.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6839c4-f252-43bf-934e-06138254d7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:35.607332Z",
     "iopub.status.busy": "2024-07-30T23:38:35.606899Z",
     "iopub.status.idle": "2024-07-30T23:38:35.612127Z",
     "shell.execute_reply": "2024-07-30T23:38:35.611764Z",
     "shell.execute_reply.started": "2024-07-30T23:38:35.607310Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fd2026-16b5-414c-909d-58df9cad9995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:38.681544Z",
     "iopub.status.busy": "2024-07-30T23:38:38.681291Z",
     "iopub.status.idle": "2024-07-30T23:38:38.686377Z",
     "shell.execute_reply": "2024-07-30T23:38:38.686033Z",
     "shell.execute_reply.started": "2024-07-30T23:38:38.681535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cd0082-9bd8-48d7-a136-671ecbd1a3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:39.583071Z",
     "iopub.status.busy": "2024-07-30T23:38:39.582391Z",
     "iopub.status.idle": "2024-07-30T23:38:39.589571Z",
     "shell.execute_reply": "2024-07-30T23:38:39.588977Z",
     "shell.execute_reply.started": "2024-07-30T23:38:39.583048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.43.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedc9c78-8559-4aec-b455-32003407281a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:40.183099Z",
     "iopub.status.busy": "2024-07-30T23:38:40.182818Z",
     "iopub.status.idle": "2024-07-30T23:38:40.188775Z",
     "shell.execute_reply": "2024-07-30T23:38:40.188265Z",
     "shell.execute_reply.started": "2024-07-30T23:38:40.183081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.3.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131852f7-39be-483f-b0fe-55e65294c8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:40.436533Z",
     "iopub.status.busy": "2024-07-30T23:38:40.435908Z",
     "iopub.status.idle": "2024-07-30T23:38:40.440392Z",
     "shell.execute_reply": "2024-07-30T23:38:40.439942Z",
     "shell.execute_reply.started": "2024-07-30T23:38:40.436516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.9.post1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm-flash-attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d2f39-aa24-49e9-ab66-8e40d1a742e3",
   "metadata": {},
   "source": [
    "## Mistral Nemo 12b\n",
    "\n",
    "https://mistral.ai/fr/news/mistral-nemo/\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\n",
    "\n",
    "https://huggingface.co/neuralmagic/Mistral-Nemo-Instruct-2407-FP8\n",
    "\n",
    "**LICENSE :** Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b777ddf-0707-47b9-a1e7-edd82ba83b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:38:46.309936Z",
     "iopub.status.busy": "2024-07-30T23:38:46.309694Z",
     "iopub.status.idle": "2024-07-30T23:40:23.681540Z",
     "shell.execute_reply": "2024-07-30T23:40:23.679253Z",
     "shell.execute_reply.started": "2024-07-30T23:38:46.309927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 01:38:48 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='neuralmagic/Mistral-Nemo-Instruct-2407-FP8', speculative_config=None, tokenizer='neuralmagic/Mistral-Nemo-Instruct-2407-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Mistral-Nemo-Instruct-2407-FP8, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 07-31 01:38:48 utils.py:569] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 07-31 01:38:49 model_runner.py:680] Starting to load model neuralmagic/Mistral-Nemo-Instruct-2407-FP8...\n",
      "WARNING 07-31 01:38:49 fp8.py:39] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 07-31 01:38:50 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dd1933671349858e675d3c991273c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 01:39:53 model_runner.py:692] Loading model weights took 12.9013 GB\n",
      "INFO 07-31 01:39:56 gpu_executor.py:102] # GPU blocks: 2922, # CPU blocks: 1638\n",
      "INFO 07-31 01:40:12 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-31 01:40:12 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-31 01:40:22 model_runner.py:1181] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:01<00:00,  1.36s/it, est. speed input: 16.14 toks/s, output: 38.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr matey! I be Cap'n Chat, the scurviest pirate chatbot to ever sail the digital seas. I be here to share tales, riddles, and grog recipes with ye. What be yer name, landlubber?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Mistral-Nemo-Instruct-2407-FP8\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.3, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b1050c-bd5f-4d8f-9f8a-519411cd3d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:44:48.801744Z",
     "iopub.status.busy": "2024-07-30T23:44:48.801411Z",
     "iopub.status.idle": "2024-07-30T23:44:59.793579Z",
     "shell.execute_reply": "2024-07-30T23:44:59.793177Z",
     "shell.execute_reply.started": "2024-07-30T23:44:48.801734Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:10<00:00, 10.99s/it, est. speed input: 2.82 toks/s, output: 32.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis désolé, mais je ne suis pas un grand philosophe français du siècle des lumières. Je suis un modèle de langage développé grâce à l'intelligence artificielle. Cependant, je peux vous donner quelques idées sur le sens de la vie en m'inspirant de la philosophie du XVIIIe siècle.\n",
      "\n",
      "1. La recherche du bonheur : Les philosophes des Lumières ont souvent mis l'accent sur la recherche du bonheur comme but ultime de la vie humaine. Selon eux, chaque individu devrait chercher à atteindre le bonheur en menant une vie morale et raisonnable.\n",
      "2. La liberté et l'égalité : Les philosophes des Lumières ont également défendu les idées de liberté et d'égalité. Selon eux, chaque individu devrait avoir les mêmes droits et les mêmes opportunités, et devrait être libre de poursuivre ses propres objectifs dans la vie.\n",
      "3. L'éducation et la raison : Les philosophes des Lumières ont mis l'accent sur l'importance de l'éducation et de la raison dans la vie humaine. Selon eux, l'éducation permet aux individus de développer leur raison et de comprendre le monde qui les entoure, ce qui les aide à prendre des décisions éclairées dans leur vie.\n",
      "4. Le progrès et l'amélioration de soi : Les philosophes des Lumières ont également mis l'accent sur l'importance du progrès et de l'amélioration de soi. Selon eux, chaque individu devrait chercher à s'améliorer constamment et à progresser dans sa vie.\n",
      "5. La responsabilité sociale : Enfin, les philosophes des Lumières ont souligné l'importance de la responsabilité sociale. Selon eux, chaque individu a une responsabilité envers la société dans son ensemble et devrait chercher à contribuer au bien commun.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq idées ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "sampling_params.max_tokens=1024\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf59fbe-27b6-4402-bca6-91979bdf33e8",
   "metadata": {},
   "source": [
    "## Llama 3.1 8b\n",
    "\n",
    "https://llama.meta.com/\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\n",
    "\n",
    "**LICENSE :** https://llama.meta.com/llama3_1/license/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f0db3b-262a-4211-a7fa-188e112b681e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:49:48.131746Z",
     "iopub.status.busy": "2024-07-30T23:49:48.131440Z",
     "iopub.status.idle": "2024-07-30T23:50:08.404984Z",
     "shell.execute_reply": "2024-07-30T23:50:08.404508Z",
     "shell.execute_reply.started": "2024-07-30T23:49:48.131735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-31 01:49:50 config.py:246] compressed-tensors quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 07-31 01:49:50 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 07-31 01:49:50 utils.py:569] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 07-31 01:49:51 model_runner.py:680] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic...\n",
      "INFO 07-31 01:49:51 weight_utils.py:223] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720be63fb76942ccb97e408cd0b6a2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 01:49:58 model_runner.py:692] Loading model weights took 8.4939 GB\n",
      "INFO 07-31 01:49:59 gpu_executor.py:102] # GPU blocks: 5758, # CPU blocks: 2048\n",
      "INFO 07-31 01:50:00 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-31 01:50:00 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-31 01:50:07 model_runner.py:1181] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 26.74 toks/s, output: 61.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me hearty! I be a swashbucklin' pirate chatbot, at yer service! Me name be Captain Chatbeard, and I be sailin' the seven seas o' conversation, plunderin' knowledge and treasures o' wisdom fer ye landlubbers! What be bringin' ye to these waters today, matey?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29beac5f-86ec-4d58-9858-0133b67466ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T23:50:12.682753Z",
     "iopub.status.busy": "2024-07-30T23:50:12.682177Z",
     "iopub.status.idle": "2024-07-30T23:50:19.678817Z",
     "shell.execute_reply": "2024-07-30T23:50:19.678270Z",
     "shell.execute_reply.started": "2024-07-30T23:50:12.682712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:06<00:00,  6.99s/it, est. speed input: 6.73 toks/s, output: 70.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ma chère question ! Comme je l'ai souvent écrit, la vie est un mystère qui nous dépasse, mais voici cinq idées qui pourraient nous aider à comprendre son sens :\n",
      "\n",
      "1. **La recherche du bonheur** : La vie est un voyage vers la recherche du bonheur, mais pas un bonheur égoïste, plutôt un bonheur qui se trouve dans la satisfaction des besoins fondamentaux de l'homme, comme la liberté, l'égalité et la fraternité. Le bonheur est une condition nécessaire pour que l'homme puisse vivre dignement.\n",
      "2. **La poursuite de la raison** : La vie est un chemin qui nous amène à découvrir la raison, à comprendre le monde et notre place dans lui. La raison est la faculté qui nous permet de comprendre la nature, de reconnaître les vérités éternelles et de vivre en harmonie avec l'univers.\n",
      "3. **La liberté et la responsabilité** : La vie est une liberté qui nous est donnée pour que nous puissions choisir notre chemin, notre destin. Mais cette liberté est accompagnée de responsabilité, car nous sommes responsables de nos actes, de nos décisions et de leurs conséquences.\n",
      "4. **L'amour et la compassion** : La vie est un don qui nous permet de nous aimer les uns les autres, de nous soutenir et de nous comprendre. L'amour et la compassion sont les sentiments les plus nobles qui nous permettent de vivre en harmonie avec les autres et avec nous-mêmes.\n",
      "5. **La recherche de la vérité** : La vie est un voyage vers la découverte de la vérité, qui est la base de toute connaissance et de toute compréhension. La recherche de la vérité est une condition nécessaire pour que nous puissions vivre avec conscience et responsabilité.\n",
      "\n",
      "Voilà, ma chère question, cinq idées qui pourraient nous aider à comprendre le sens de la vie. Mais, comme je l'ai souvent dit, \"la vie est un problème qui n'a pas de solution\", car elle est un mystère qui nous dépasse et qui nous oblige à la recherche et à la découverte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"vous êtes un grand philosophe français du siècle des lumières\"},\n",
    "    {\"role\": \"user\", \"content\": \"quel est le sens de la vie, résumé en cinq idées ?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "sampling_params.max_tokens=1024\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fc4e6-2f76-4b36-9b0e-00b7854415a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T21:56:28.347023Z",
     "iopub.status.busy": "2024-07-30T21:56:28.346440Z",
     "iopub.status.idle": "2024-07-30T21:56:28.350612Z",
     "shell.execute_reply": "2024-07-30T21:56:28.350074Z",
     "shell.execute_reply.started": "2024-07-30T21:56:28.347003Z"
    }
   },
   "source": [
    "## Gemma 2 9b\n",
    "\n",
    "https://ai.google.dev/gemma/docs/model_card_2\n",
    "\n",
    "https://huggingface.co/google/gemma-2-9b-it\n",
    "\n",
    "https://huggingface.co/neuralmagic/gemma-2-9b-it-FP8\n",
    "\n",
    "**LICENSE :** https://ai.google.dev/gemma/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfbe97-ccca-4291-9424-6c16aee26f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/gemma-2-9b-it-FP8\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d006928-0bb3-4f01-9447-f4699992e533",
   "metadata": {},
   "source": [
    "## Phi-3 mini 128k 3.8b\n",
    "\n",
    "https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Phi-3-mini-128k-instruct-FP8\n",
    "\n",
    "**LICENSE :** MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d73530-90f1-416e-bac3-6d8ac5d92af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Phi-3-mini-128k-instruct-FP8\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Remember to respond in pirate speak!\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e713d-e00f-454c-8dad-488af75fabd2",
   "metadata": {},
   "source": [
    "## Phi-3 medium 128k 14b\n",
    "\n",
    "https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3-medium-128k-instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/Phi-3-medium-128k-instruct-FP8\n",
    "\n",
    "**LICENSE :** MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79ed72-2cbb-4cfc-9414-e462d8dedc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Phi-3-medium-128k-instruct-FP8\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Remember to respond in pirate speak!\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "llm = LLM(model=model_id)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22e39a-2313-442d-b91d-97e4dedf2ad1",
   "metadata": {},
   "source": [
    "# Deepseek coder v2 Lite 16b\n",
    "\n",
    "https://arxiv.org/abs/2406.11931\n",
    "\n",
    "https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\n",
    "\n",
    "https://huggingface.co/neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8\n",
    "\n",
    "**LICENSE :** https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca1406-06bd-4561-954a-546f9d03da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "llm = LLM(model=model_id, trust_remote_code=True, max_model_len=4096)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fe7ff-97ec-40b0-b944-2f52792c46a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
