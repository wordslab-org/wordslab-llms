{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7249409e-3e87-4e64-875c-624a78033472",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51875db1-1156-4474-8e52-de81161a4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e11987-23f3-44e8-8184-b83679f0ee02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:42.975674Z",
     "iopub.status.busy": "2024-09-21T13:14:42.975587Z",
     "iopub.status.idle": "2024-09-21T13:14:42.983783Z",
     "shell.execute_reply": "2024-09-21T13:14:42.983473Z",
     "shell.execute_reply.started": "2024-09-21T13:14:42.975666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad3d9bc-c371-4bf1-8445-70bc48da5986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:52.053603Z",
     "iopub.status.busy": "2024-09-21T13:14:52.053249Z",
     "iopub.status.idle": "2024-09-21T13:14:52.060368Z",
     "shell.execute_reply": "2024-09-21T13:14:52.060090Z",
     "shell.execute_reply.started": "2024-09-21T13:14:52.053580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('triton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55718c-4310-4aa9-b363-acae522d8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459e66c0-05d6-4651-b62b-e486f5b0a961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:45.123335Z",
     "iopub.status.busy": "2024-09-21T13:14:45.122378Z",
     "iopub.status.idle": "2024-09-21T13:14:45.130853Z",
     "shell.execute_reply": "2024-09-21T13:14:45.130581Z",
     "shell.execute_reply.started": "2024-09-21T13:14:45.123300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.44.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc846be-72e4-447e-ab93-094e7e07d7fa",
   "metadata": {},
   "source": [
    "## vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72e89a-3941-4ee1-97cf-18c2e15b1842",
   "metadata": {},
   "source": [
    "https://docs.vllm.ai/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35b0e6-a186-447d-a035-9af71e3358af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876ca82b-1cd6-4aae-a964-2b047a9fc4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:46.321506Z",
     "iopub.status.busy": "2024-09-21T13:14:46.321142Z",
     "iopub.status.idle": "2024-09-21T13:14:46.329698Z",
     "shell.execute_reply": "2024-09-21T13:14:46.328821Z",
     "shell.execute_reply.started": "2024-09-21T13:14:46.321479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.1.post2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f3783-6061-4f60-b968-e0380e60a99f",
   "metadata": {},
   "source": [
    "## SGLang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874cffb-8745-418f-9194-0c2f04060567",
   "metadata": {},
   "source": [
    "https://sglang.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56019c95-b5b3-4fe9-8170-2d3873db36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade \"sglang[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e703628-1345-49f5-b310-b4b93b36c620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:50.712384Z",
     "iopub.status.busy": "2024-09-21T13:14:50.711986Z",
     "iopub.status.idle": "2024-09-21T13:14:50.720997Z",
     "shell.execute_reply": "2024-09-21T13:14:50.720538Z",
     "shell.execute_reply.started": "2024-09-21T13:14:50.712356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('sglang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea3fca-704a-4b5e-a8c0-80496ce179f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb9d3f5-621d-4e1f-b30e-c616ecb2df83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:14:54.202295Z",
     "iopub.status.busy": "2024-09-21T13:14:54.201912Z",
     "iopub.status.idle": "2024-09-21T13:14:54.209965Z",
     "shell.execute_reply": "2024-09-21T13:14:54.209702Z",
     "shell.execute_reply.started": "2024-09-21T13:14:54.202267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.6+cu124torch2.4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('flashinfer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7128d-8f14-4bde-977d-aec6c1793a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:07:11.161321Z",
     "iopub.status.busy": "2024-09-22T10:07:11.160622Z",
     "iopub.status.idle": "2024-09-22T10:07:11.176874Z",
     "shell.execute_reply": "2024-09-22T10:07:11.176187Z",
     "shell.execute_reply.started": "2024-09-22T10:07:11.161295Z"
    }
   },
   "source": [
    "## ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efede01d-b968-4690-ab82-dc016014738e",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cac0e71-e0a2-482e-92d5-69a81768e166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T20:25:03.127960Z",
     "iopub.status.busy": "2024-09-21T20:25:03.127560Z",
     "iopub.status.idle": "2024-09-21T20:26:45.368164Z",
     "shell.execute_reply": "2024-09-21T20:26:45.367508Z",
     "shell.execute_reply.started": "2024-09-21T20:25:03.127947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   117  100   117    0     0    566      0 --:--:-- --:--:-- --:--:--   567\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1583M  100 1583M    0     0  17.1M      0  0:01:32  0:01:32 --:--:-- 16.9M\n"
     ]
    }
   ],
   "source": [
    "!mkdir ollama && curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz && tar -C ./ollama -xzf ollama-linux-amd64.tgz && rm ollama-linux-amd64.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c7e6d-8825-46f9-ac49-64b39fc82971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T20:28:36.847325Z",
     "iopub.status.busy": "2024-09-21T20:28:36.846900Z",
     "iopub.status.idle": "2024-09-21T20:28:36.885093Z",
     "shell.execute_reply": "2024-09-21T20:28:36.884416Z",
     "shell.execute_reply.started": "2024-09-21T20:28:36.847310Z"
    }
   },
   "source": [
    "```bash\n",
    "./ollama/bin/ollama serve &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2783c62-8361-4e45-9fdc-07a9ced35e81",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama-python\n",
    "\n",
    "https://github.com/ollama/ollama/tree/main/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b56434-da15-42f4-9974-4f24be59902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ac7f1f-81db-4643-90d8-2355f26de1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:13.903286Z",
     "iopub.status.busy": "2024-09-21T13:15:13.903058Z",
     "iopub.status.idle": "2024-09-21T13:15:13.909154Z",
     "shell.execute_reply": "2024-09-21T13:15:13.908722Z",
     "shell.execute_reply.started": "2024-09-21T13:15:13.903269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('ollama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9975774-19fa-4877-9847-dde5548308a8",
   "metadata": {},
   "source": [
    "# 2. Performance testing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750794e4-02f8-4998-b575-7d0a8f3e3c89",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa413fe-b029-4e5c-ba6e-70f5859f83db",
   "metadata": {},
   "source": [
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "https://huggingface.co/collections/neuralmagic/llama-31-quantization-66a3f907f48d07feabb8f300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93cb29b-8c75-4187-846a-832024247f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:52.191413Z",
     "iopub.status.busy": "2024-09-22T12:24:52.191324Z",
     "iopub.status.idle": "2024-09-22T12:24:52.194994Z",
     "shell.execute_reply": "2024-09-22T12:24:52.194550Z",
     "shell.execute_reply.started": "2024-09-22T12:24:52.191405Z"
    }
   },
   "outputs": [],
   "source": [
    "test_models = {                                                                    # OpenLLM leaderboard score\n",
    "    \"llama-3.1\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\",                         # 100.0 %\n",
    "    \"llama-3.1:w8a16\" : \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\",  # 99.8 %    \n",
    "    \"llama-3.1:fp8\" : \"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\",                # 99.5 % - warning the \"FP8-dynamic\" version is MUCH slower on RTX 4090 !\n",
    "    \"llama-3.1:w8a8\" : \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8\",    # 99.4 %\n",
    "    \"llama-3.1:w4a16\" : \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"   # 97.1 %\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c1546-1d5e-4f33-96af-590357639f4d",
   "metadata": {},
   "source": [
    "Note: to create custom quantized model versions -> https://docs.vllm.ai/en/latest/quantization/fp8.html#quantization-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded1cee-67b3-4cc8-b111-012411f79dc6",
   "metadata": {},
   "source": [
    "https://ollama.com/library/llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd6727-9d71-4179-8777-67aeeefe6497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T20:40:36.798116Z",
     "iopub.status.busy": "2024-09-21T20:40:36.797241Z",
     "iopub.status.idle": "2024-09-21T20:40:36.804014Z",
     "shell.execute_reply": "2024-09-21T20:40:36.803150Z",
     "shell.execute_reply.started": "2024-09-21T20:40:36.798087Z"
    }
   },
   "source": [
    "```bash\n",
    "./ollama/bin/ollama pull llama3.1:8b-instruct-fp16\n",
    "./ollama/bin/ollama pull llama3.1:8b-instruct-q8_0\n",
    "./ollama/bin/ollama pull llama3.1:8b-instruct-q4_0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc08f74-bd5a-4601-90ba-e1ec0a2eef3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:52.195630Z",
     "iopub.status.busy": "2024-09-22T12:24:52.195535Z",
     "iopub.status.idle": "2024-09-22T12:24:52.199191Z",
     "shell.execute_reply": "2024-09-22T12:24:52.198821Z",
     "shell.execute_reply.started": "2024-09-22T12:24:52.195622Z"
    }
   },
   "outputs": [],
   "source": [
    "ollama_test_models = {\n",
    "    \"llama-3.1\" : \"llama3.1:8b-instruct-fp16\",\n",
    "    \"llama-3.1:int8\" : \"llama3.1:8b-instruct-q8_0\",\n",
    "    \"llama-3.1:int4\" : \"llama3.1:8b-instruct-q4_0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de95a90-29e7-4dc4-974b-8b601a86c16a",
   "metadata": {},
   "source": [
    "## Test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6570564c-6fbb-4eb2-b716-1a161c7d9787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:52.199767Z",
     "iopub.status.busy": "2024-09-22T12:24:52.199662Z",
     "iopub.status.idle": "2024-09-22T12:24:52.203726Z",
     "shell.execute_reply": "2024-09-22T12:24:52.202919Z",
     "shell.execute_reply.started": "2024-09-22T12:24:52.199760Z"
    }
   },
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages du Crédit Mutuel ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages du Crédit Agricole ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la Société Générale ?\"}\n",
    "],\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant utile et professionnel qui répond toujours en français.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quels sont les avantages de la BNP ?\"}\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a427640-100e-49cb-a2fc-f5fa8c18a6e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:52.204167Z",
     "iopub.status.busy": "2024-09-22T12:24:52.204085Z",
     "iopub.status.idle": "2024-09-22T12:24:53.692978Z",
     "shell.execute_reply": "2024-09-22T12:24:53.692323Z",
     "shell.execute_reply.started": "2024-09-22T12:24:52.204159Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def format_prompt(messages, model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267c07a4-c9f1-4d16-8697-d4a1204e2c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:53.694111Z",
     "iopub.status.busy": "2024-09-22T12:24:53.693665Z",
     "iopub.status.idle": "2024-09-22T12:24:54.061306Z",
     "shell.execute_reply": "2024-09-22T12:24:54.060885Z",
     "shell.execute_reply.started": "2024-09-22T12:24:53.694099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nTu es un assistant utile et professionnel qui répond toujours en français.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuels sont les avantages du Crédit Mutuel ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nTu es un assistant utile et professionnel qui répond toujours en français.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuels sont les avantages du Crédit Agricole ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nTu es un assistant utile et professionnel qui répond toujours en français.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuels sont les avantages de la Société Générale ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nTu es un assistant utile et professionnel qui répond toujours en français.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuels sont les avantages de la BNP ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_prompt(test_messages, test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16d245-b3ef-418e-8d3b-5f494f5e6deb",
   "metadata": {},
   "source": [
    "### vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f997ac1-6001-493b-b118-61ca0ff29449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:54.111027Z",
     "iopub.status.busy": "2024-09-22T12:24:54.110568Z",
     "iopub.status.idle": "2024-09-22T12:24:54.113947Z",
     "shell.execute_reply": "2024-09-22T12:24:54.113497Z",
     "shell.execute_reply.started": "2024-09-22T12:24:54.111012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate VLLM with Huggingface Hub\n",
    "import os\n",
    "\n",
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=myhftoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a8369c-d660-4d64-897d-4d5929200a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:24:54.495920Z",
     "iopub.status.busy": "2024-09-22T12:24:54.495479Z",
     "iopub.status.idle": "2024-09-22T12:24:55.824491Z",
     "shell.execute_reply": "2024-09-22T12:24:55.824041Z",
     "shell.execute_reply.started": "2024-09-22T12:24:54.495909Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def vllm_load(model):    \n",
    "    llm = LLM(model, gpu_memory_utilization=0.99, max_model_len=8192)\n",
    "    llm._model = model\n",
    "    return llm\n",
    "\n",
    "def vllm_generate(messages, llm):    \n",
    "    print(f\"vLLM performance test:\")\n",
    "    \n",
    "    prompts = format_prompt(messages, llm._model)\n",
    "    sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "    # warmup\n",
    "    outputs = llm.generate(prompts[0], sampling_params)\n",
    "    print(f\"Generated text: {outputs[0].outputs[0].text!r}\")\n",
    "    \n",
    "    for batch_size in range(1, len(messages) + 1):\n",
    "        start_time = time.time()  # Record the start time\n",
    "        outputs = llm.generate(prompts[0:batch_size], sampling_params)\n",
    "        end_time = time.time()  # Record the end time\n",
    "            \n",
    "        # Print the outputs.\n",
    "        tokenscount = 0\n",
    "        for output in outputs:\n",
    "            generated_text = output.outputs[0].text\n",
    "            tokenscount = tokenscount + len(output.outputs[0].token_ids)\n",
    "\n",
    "        tokens_per_sec = tokenscount/(end_time-start_time)\n",
    "        print(f\"- batch size {batch_size}: {tokens_per_sec:.2f} tokens/sec ({batch_size} x {tokens_per_sec/batch_size:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2267f691-7b8f-44a7-82c6-7e85598c7a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:23:59.651640Z",
     "iopub.status.busy": "2024-09-22T12:23:59.651455Z",
     "iopub.status.idle": "2024-09-22T12:24:42.314947Z",
     "shell.execute_reply": "2024-09-22T12:24:42.313020Z",
     "shell.execute_reply.started": "2024-09-22T12:23:59.651627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:24:00 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-22 14:24:00 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 14:24:01 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 09-22 14:24:01 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474460ce5e1423cadd36a444b5c44e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:24:17 model_runner.py:1008] Loading model weights took 14.9888 GB\n",
      "INFO 09-22 14:24:19 gpu_executor.py:122] # GPU blocks: 3610, # CPU blocks: 2048\n",
      "INFO 09-22 14:24:31 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 14:24:31 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 14:24:42 model_runner.py:1430] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d938178-167d-4df8-a5db-43aaa6026140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:58.808650Z",
     "iopub.status.busy": "2024-09-21T13:15:58.807243Z",
     "iopub.status.idle": "2024-09-21T13:21:53.704000Z",
     "shell.execute_reply": "2024-09-21T13:21:53.703624Z",
     "shell.execute_reply.started": "2024-09-21T13:15:58.808607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:09<00:00,  9.58s/it, est. speed input: 6.58 toks/s, output: 53.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre plusieurs avantages à ses membres et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus élevés sur les épargnes** : Le Crédit Mutuel propose des taux d'intérêt plus élevés sur les comptes d'épargne que de nombreux autres établissements bancaires.\\n2. **Taux d'emprunt compétitifs** : Les prêts personnels, les prêts immobiliers et les prêts pour la mobilité sont proposés avec des taux d'intérêt attractifs.\\n3. **Services personnalisés** : Le Crédit Mutuel offre des services personnalisés et adaptés aux besoins de ses membres et clients, grâce à une approche relationnelle et à une connaissance approfondie de leurs situations financières.\\n4. **Sécurité et confidentialité** : Le Crédit Mutuel s'engage à protéger les données personnelles et financières de ses membres et clients, conformément aux règles de protection des données.\\n5. **Participation aux décisions** : En tant que membre du Crédit Mutuel, vous avez la possibilité de participer aux assemblées générales et de voter sur les décisions stratégiques de l'institution.\\n6. **Services bancaires complets** : Le Crédit Mutuel propose un large éventail de services bancaires, y compris des comptes courants, des comptes d'épargne, des prêts, des cartes de crédit et des assurances.\\n7. **Aide à la gestion financière** : Le Crédit Mutuel propose des outils et des conseils pour aider ses membres et clients à gérer leur budget, à planifier leur avenir financier et à atteindre leurs objectifs économiques.\\n8. **Prévention de la fraude** : Le Crédit Mutuel met en place des mesures de prévention de la fraude pour protéger les comptes et les données de ses membres et clients.\\n9. **Services numériques** : Le Crédit Mutuel propose des applications mobiles et un site internet sécurisé pour gérer ses comptes et effectuer des opérations bancaires en ligne.\\n10. **Eng\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:09<00:00,  9.30s/it, est. speed input: 6.78 toks/s, output: 55.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 55.06 tokens/sec (1 x 55.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:09<00:00,  4.79s/it, est. speed input: 13.24 toks/s, output: 106.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 106.76 tokens/sec (2 x 53.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:09<00:00,  3.21s/it, est. speed input: 19.84 toks/s, output: 159.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 159.49 tokens/sec (3 x 53.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:09<00:00,  2.41s/it, est. speed input: 26.13 toks/s, output: 209.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 209.11 tokens/sec (4 x 52.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:09<00:00,  1.95s/it, est. speed input: 32.38 toks/s, output: 255.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 254.98 tokens/sec (5 x 51.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:09<00:00,  1.63s/it, est. speed input: 38.78 toks/s, output: 314.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 314.21 tokens/sec (6 x 52.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:09<00:00,  1.40s/it, est. speed input: 45.17 toks/s, output: 365.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 365.28 tokens/sec (7 x 52.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:09<00:00,  1.23s/it, est. speed input: 51.42 toks/s, output: 417.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 417.78 tokens/sec (8 x 52.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:09<00:00,  1.10s/it, est. speed input: 57.23 toks/s, output: 453.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 453.50 tokens/sec (9 x 50.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 10/10 [00:09<00:00,  1.00it/s, est. speed input: 63.26 toks/s, output: 513.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 513.14 tokens/sec (10 x 51.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 11/11 [00:10<00:00,  1.09it/s, est. speed input: 69.12 toks/s, output: 558.33 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 558.14 tokens/sec (11 x 50.74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 12/12 [00:10<00:00,  1.19it/s, est. speed input: 74.86 toks/s, output: 607.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 607.03 tokens/sec (12 x 50.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 13/13 [00:10<00:00,  1.28it/s, est. speed input: 80.73 toks/s, output: 656.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 655.89 tokens/sec (13 x 50.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 14/14 [00:10<00:00,  1.36it/s, est. speed input: 86.00 toks/s, output: 690.02 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 689.78 tokens/sec (14 x 49.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 15/15 [00:10<00:00,  1.47it/s, est. speed input: 92.83 toks/s, output: 748.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 748.58 tokens/sec (15 x 49.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 16/16 [00:10<00:00,  1.56it/s, est. speed input: 98.23 toks/s, output: 791.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 790.84 tokens/sec (16 x 49.43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 17/17 [00:10<00:00,  1.56it/s, est. speed input: 98.22 toks/s, output: 794.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 793.86 tokens/sec (17 x 46.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 18/18 [00:11<00:00,  1.64it/s, est. speed input: 103.14 toks/s, output: 837.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 837.21 tokens/sec (18 x 46.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 19/19 [00:11<00:00,  1.73it/s, est. speed input: 108.98 toks/s, output: 872.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 871.95 tokens/sec (19 x 45.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 20/20 [00:11<00:00,  1.79it/s, est. speed input: 112.62 toks/s, output: 906.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 905.96 tokens/sec (20 x 45.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 21/21 [00:11<00:00,  1.88it/s, est. speed input: 118.43 toks/s, output: 959.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 958.86 tokens/sec (21 x 45.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:11<00:00,  1.96it/s, est. speed input: 123.62 toks/s, output: 1003.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1003.52 tokens/sec (22 x 45.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:11<00:00,  1.97it/s, est. speed input: 124.36 toks/s, output: 1009.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 1008.92 tokens/sec (23 x 43.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:11<00:00,  2.04it/s, est. speed input: 128.40 toks/s, output: 1043.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 1043.09 tokens/sec (24 x 43.46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:11<00:00,  2.12it/s, est. speed input: 133.65 toks/s, output: 1076.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 1076.50 tokens/sec (25 x 43.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:11<00:00,  2.17it/s, est. speed input: 136.85 toks/s, output: 1102.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 1102.26 tokens/sec (26 x 42.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:11<00:00,  2.27it/s, est. speed input: 142.88 toks/s, output: 1157.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 1157.37 tokens/sec (27 x 42.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:11<00:00,  2.34it/s, est. speed input: 147.12 toks/s, output: 1189.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 1189.24 tokens/sec (28 x 42.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 29/29 [00:12<00:00,  2.40it/s, est. speed input: 151.45 toks/s, output: 1223.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 1222.97 tokens/sec (29 x 42.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 30/30 [00:12<00:00,  2.44it/s, est. speed input: 153.90 toks/s, output: 1243.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 1242.63 tokens/sec (30 x 41.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 31/31 [00:12<00:00,  2.53it/s, est. speed input: 159.80 toks/s, output: 1282.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 1282.23 tokens/sec (31 x 41.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 32/32 [00:12<00:00,  2.56it/s, est. speed input: 161.42 toks/s, output: 1308.09 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 1307.55 tokens/sec (32 x 40.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*8, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d1f3d-603b-4cf3-be32-ed0312caf5bc",
   "metadata": {},
   "source": [
    "### SGLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98445cbd-0312-4cf5-8832-53b2da13e83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:23:31.653744Z",
     "iopub.status.busy": "2024-09-21T13:23:31.653203Z",
     "iopub.status.idle": "2024-09-21T13:23:31.665077Z",
     "shell.execute_reply": "2024-09-21T13:23:31.664669Z",
     "shell.execute_reply.started": "2024-09-21T13:23:31.653713Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, time\n",
    "import sglang\n",
    "\n",
    "def sglang_load(model):\n",
    "    runtime = sglang.Runtime(model_path=model)\n",
    "    runtime._model = model\n",
    "    return runtime\n",
    "\n",
    "def sglang_generate(messages, runtime):\n",
    "    print(f\"SGLang performance test:\")\n",
    "    \n",
    "    prompts = format_prompt(messages, runtime._model)\n",
    "    sampling_params = { \"temperature\":0.7, \"top_p\":0.8, \"repetition_penalty\":1.05, \"max_new_tokens\":512 }\n",
    "    # warmup\n",
    "    output = json.loads(runtime.generate(prompt=prompts[0], sampling_params=sampling_params))\n",
    "    print(f\"Generated text: {output['text']!r}\")\n",
    "    \n",
    "    for batch_size in range(1, len(messages) + 1):\n",
    "        start_time = time.time()  # Record the start time\n",
    "        outputs = json.loads(runtime.generate(prompt=prompts[0:batch_size], sampling_params=sampling_params))\n",
    "        end_time = time.time()  # Record the end time\n",
    "            \n",
    "        # Print the outputs.\n",
    "        tokenscount = 0\n",
    "        for output in outputs:\n",
    "            generated_text = output[\"text\"]\n",
    "            tokenscount = tokenscount + output[\"meta_info\"][\"completion_tokens\"]\n",
    "\n",
    "        tokens_per_sec = tokenscount/(end_time-start_time)\n",
    "        print(f\"- batch size {batch_size}: {tokens_per_sec:.2f} tokens/sec ({batch_size} x {tokens_per_sec/batch_size:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fac7f-aa20-4429-b6b1-31a30fc7ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = sglang_load(test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23096e8-950c-48fd-adf7-1bef1b423521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:23:53.873159Z",
     "iopub.status.busy": "2024-09-21T13:23:53.872596Z",
     "iopub.status.idle": "2024-09-21T13:29:57.571364Z",
     "shell.execute_reply": "2024-09-21T13:29:57.570949Z",
     "shell.execute_reply.started": "2024-09-21T13:23:53.873131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang performance test:\n",
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre divers avantages à ses membres et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus élevés sur les dépôts** : Le Crédit Mutuel propose des taux d'intérêt plus élevés que les banques traditionnelles pour les dépôts, ce qui permet aux clients de gagner plus d'argent sur leurs économies.\\n\\n2. **Taux d'emprunt compétitifs** : Les emprunts du Crédit Mutuel sont souvent moins chers que ceux proposés par les banques traditionnelles, ce qui peut aider les clients à se procurer des prêts à des conditions avantageuses.\\n\\n3. **Services personnalisés** : En tant que banque coopérative, le Crédit Mutuel prend en compte les besoins spécifiques de ses clients et offre des services personnalisés pour répondre à leurs attentes.\\n\\n4. **Transparence et sécurité** : Le Crédit Mutuel est connu pour sa transparence financière et sa sécurité, ce qui rassure les clients qui souhaitent faire confiance à leur banque.\\n\\n5. **Services numériques performants** : Le Crédit Mutuel propose des services en ligne et mobiles pratiques, permettant aux clients de gérer leurs comptes et effectuer des opérations bancaires en toute simplicité.\\n\\n6. **Prise en charge des projets locaux** : Le Crédit Mutuel soutient les projets locaux et les initiatives communautaires, ce qui contribue au développement économique et social des régions où il est présent.\\n\\n7. **Éthique et responsabilité sociale** : Le Crédit Mutuel est engagé dans des pratiques éthiques et responsables, ce qui rassure les clients qui souhaitent faire des affaires avec une banque qui partage leurs valeurs.\\n\\n8. **Diversification des produits et services** : Le Crédit Mutuel propose une gamme large de produits et services bancaires, allant des comptes courants et des épargnes aux prêts et aux assurances.\\n\\n9. **Conseil financier personnalisé** : Les conseillers du Crédit Mutuel offrent un accompagnement personnalisé pour aider les\"\n",
      "- batch size 1: 54.40 tokens/sec (1 x 54.40)\n",
      "- batch size 2: 106.28 tokens/sec (2 x 53.14)\n",
      "- batch size 3: 158.87 tokens/sec (3 x 52.96)\n",
      "- batch size 4: 204.87 tokens/sec (4 x 51.22)\n",
      "- batch size 5: 261.97 tokens/sec (5 x 52.39)\n",
      "- batch size 6: 313.69 tokens/sec (6 x 52.28)\n",
      "- batch size 7: 365.71 tokens/sec (7 x 52.24)\n",
      "- batch size 8: 417.00 tokens/sec (8 x 52.13)\n",
      "- batch size 9: 468.16 tokens/sec (9 x 52.02)\n",
      "- batch size 10: 510.35 tokens/sec (10 x 51.04)\n",
      "- batch size 11: 550.32 tokens/sec (11 x 50.03)\n",
      "- batch size 12: 603.20 tokens/sec (12 x 50.27)\n",
      "- batch size 13: 641.52 tokens/sec (13 x 49.35)\n",
      "- batch size 14: 704.31 tokens/sec (14 x 50.31)\n",
      "- batch size 15: 757.44 tokens/sec (15 x 50.50)\n",
      "- batch size 16: 803.96 tokens/sec (16 x 50.25)\n",
      "- batch size 17: 809.71 tokens/sec (17 x 47.63)\n",
      "- batch size 18: 856.48 tokens/sec (18 x 47.58)\n",
      "- batch size 19: 900.94 tokens/sec (19 x 47.42)\n",
      "- batch size 20: 932.80 tokens/sec (20 x 46.64)\n",
      "- batch size 21: 975.06 tokens/sec (21 x 46.43)\n",
      "- batch size 22: 1020.68 tokens/sec (22 x 46.39)\n",
      "- batch size 23: 1063.49 tokens/sec (23 x 46.24)\n",
      "- batch size 24: 1104.37 tokens/sec (24 x 46.02)\n",
      "- batch size 25: 1146.04 tokens/sec (25 x 45.84)\n",
      "- batch size 26: 1202.71 tokens/sec (26 x 46.26)\n",
      "- batch size 27: 1240.36 tokens/sec (27 x 45.94)\n",
      "- batch size 28: 1290.63 tokens/sec (28 x 46.09)\n",
      "- batch size 29: 1332.88 tokens/sec (29 x 45.96)\n",
      "- batch size 30: 1369.35 tokens/sec (30 x 45.64)\n",
      "- batch size 31: 932.32 tokens/sec (31 x 30.07)\n",
      "- batch size 32: 640.79 tokens/sec (32 x 20.02)\n"
     ]
    }
   ],
   "source": [
    "sglang_generate(test_messages*8, runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65080db2-7b9c-4b53-b31a-f3287412935a",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6239a-2e41-4a0c-8a3a-c7d36d96e063",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59930211-acff-404a-b3d1-1d091474cd44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:04:19.266501Z",
     "iopub.status.busy": "2024-09-22T10:04:19.265772Z",
     "iopub.status.idle": "2024-09-22T10:04:19.301264Z",
     "shell.execute_reply": "2024-09-22T10:04:19.300884Z",
     "shell.execute_reply.started": "2024-09-22T10:04:19.266474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'llama3.1:8b-instruct-q4_0',\n",
       "   'model': 'llama3.1:8b-instruct-q4_0',\n",
       "   'modified_at': '2024-09-22T11:43:03.484852591+02:00',\n",
       "   'size': 4661230766,\n",
       "   'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}},\n",
       "  {'name': 'llama3.1:8b-instruct-q8_0',\n",
       "   'model': 'llama3.1:8b-instruct-q8_0',\n",
       "   'modified_at': '2024-09-22T11:33:10.006555339+02:00',\n",
       "   'size': 8540789934,\n",
       "   'digest': 'b158ded76fa05be6bce8a682099ce5df8c5571340a04cf63a2923464679db576',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q8_0'}},\n",
       "  {'name': 'llama3.1:8b-instruct-fp16',\n",
       "   'model': 'llama3.1:8b-instruct-fp16',\n",
       "   'modified_at': '2024-09-21T22:54:30.926572546+02:00',\n",
       "   'size': 16068910253,\n",
       "   'digest': '4aacac4194543ff7f70dab3f2ebc169c132d5319bb36f7a7e99c4ff525ebcc09',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'F16'}}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ace51020-dc87-4cbb-a5fb-686ffbc9ccd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T21:06:14.481788Z",
     "iopub.status.busy": "2024-09-21T21:06:14.481058Z",
     "iopub.status.idle": "2024-09-21T21:06:14.501823Z",
     "shell.execute_reply": "2024-09-21T21:06:14.501524Z",
     "shell.execute_reply.started": "2024-09-21T21:06:14.481752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- if or .System .Tools }}<|start_header_id|>system<|end_header_id|>\n",
      "{{- if .System }}\n",
      "\n",
      "{{ .System }}\n",
      "{{- end }}\n",
      "{{- if .Tools }}\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities.\n",
      "{{- end }}<|eot_id|>\n",
      "{{- end }}\n",
      "{{- range $i, $_ := .Messages }}\n",
      "{{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
      "{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
      "{{- if and $.Tools $last }}\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
      "\n",
      "{{ range $.Tools }}\n",
      "{{- . }}\n",
      "{{ end }}\n",
      "Question: {{ .Content }}<|eot_id|>\n",
      "{{- else }}\n",
      "\n",
      "{{ .Content }}<|eot_id|>\n",
      "{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ end }}\n",
      "{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
      "{{- if .ToolCalls }}\n",
      "{{ range .ToolCalls }}\n",
      "{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n",
      "{{- else }}\n",
      "\n",
      "{{ .Content }}\n",
      "{{- end }}{{ if not $last }}<|eot_id|>{{ end }}\n",
      "{{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ end }}\n",
      "{{- end }}\n",
      "{{- end }}\n"
     ]
    }
   ],
   "source": [
    "print(ollama.show(\"llama3.1:8b-instruct-fp16\")['template'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e8405-b6ed-4ba2-a575-46bddf2f9f8d",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a9f668b-5d74-4b78-8a86-408328fd9978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:37:45.184355Z",
     "iopub.status.busy": "2024-09-22T09:37:45.183990Z",
     "iopub.status.idle": "2024-09-22T09:37:45.191794Z",
     "shell.execute_reply": "2024-09-22T09:37:45.191214Z",
     "shell.execute_reply.started": "2024-09-22T09:37:45.184331Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# ollama keeps models 5 min in memory by default, they are reloaded by a query\n",
    "def ollama_load(model):\n",
    "    sampling_params = { \"num_predict\":1 }\n",
    "    ollama.generate(model=model, prompt=\"load\", raw=True, options=sampling_params, stream=False)\n",
    "    return model\n",
    "\n",
    "# ollama API only supports batch size 1\n",
    "def ollama_generate(messages, model):\n",
    "    print(f\"ollama performance test:\")\n",
    "    \n",
    "    prompts = format_prompt(messages, runtime._model)\n",
    "    sampling_params = { \"temperature\":0.7, \"top_p\":0.8, \"repeat_penalty\":1.05, \"num_predict\":512 }\n",
    "    # warmup\n",
    "    output = ollama.generate(model=model, prompt=prompts[0], raw=True, options=sampling_params, stream=False)\n",
    "    print(f\"Generated text: {output['response']!r}\")\n",
    "    \n",
    "    for msg_index in range(len(messages)):\n",
    "        start_time = time.time()  # Record the start time\n",
    "        output = ollama.generate(model=model, prompt=prompts[msg_index], raw=True, options=sampling_params, stream=False)\n",
    "        end_time = time.time()  # Record the end time\n",
    "            \n",
    "        # Print the outputs.\n",
    "        tokenscount = output['eval_count']\n",
    "        tokens_per_sec = tokenscount/(end_time-start_time)\n",
    "        print(f\"- batch size 1: {tokens_per_sec:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43eb08e5-8f25-4012-8073-0335d02f653c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:37:53.426904Z",
     "iopub.status.busy": "2024-09-22T09:37:53.426458Z",
     "iopub.status.idle": "2024-09-22T09:37:53.741430Z",
     "shell.execute_reply": "2024-09-22T09:37:53.741049Z",
     "shell.execute_reply.started": "2024-09-22T09:37:53.426875Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ollama_load(ollama_test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0346b5a-21ae-4f4a-9243-2e4a14ada961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:38:09.282965Z",
     "iopub.status.busy": "2024-09-22T09:38:09.282620Z",
     "iopub.status.idle": "2024-09-22T09:38:59.041822Z",
     "shell.execute_reply": "2024-09-22T09:38:59.041472Z",
     "shell.execute_reply.started": "2024-09-22T09:38:09.282942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama performance test:\n",
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre divers avantages à ses adhérents. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts réduits sur les prêts** : Le Crédit Mutuel propose des taux d'intérêt compétitifs pour les prêts personnels, hypothécaires et professionnels.\\n2. **Avantages fiscaux** : Les adhérents du Crédit Mutuel peuvent bénéficier de réductions fiscales sur leurs prêts et épargnes, en fonction de leur situation financière et de leurs impôts.\\n3. **Assistance financière** : Le Crédit Mutuel offre des services d'assistance financière pour aider ses adhérents à gérer leurs finances et à atteindre leurs objectifs économiques.\\n4. **Produits diversifiés** : Le Crédit Mutuel propose une gamme complète de produits bancaires, tels que les comptes courants, les livrets d'épargne, les cartes de crédit, les assurances et les investissements.\\n5. **Services en ligne** : Les adhérents du Crédit Mutuel ont accès à un portail en ligne sécurisé pour gérer leurs comptes, effectuer des virements, payer leurs factures et suivre leur épargne.\\n6. **Soutien à l'épargne** : Le Crédit Mutuel encourage l'épargne de ses adhérents grâce à des produits spécifiques, tels que les livrets d'épargne ou les comptes d'épargne.\\n7. **Protection et assurance** : Le Crédit Mutuel propose des contrats d'assurance pour protéger les biens et les personnes de ses adhérents contre divers risques (incendie, vol, accident, décès).\\n8. **Soutien à l'entrepreneuriat** : Le Crédit Mutuel offre des prêts et des services spécifiques pour les entrepreneurs et les PME.\\n9. **Fidélité et récompenses** : Les adhérents du Crédit Mutuel peuvent bénéficier de récompenses et de primes en fonction de leur comportement bancaire (\"\n",
      "- batch size 1: 51.97 tokens/sec\n",
      "- batch size 1: 52.16 tokens/sec\n",
      "- batch size 1: 52.33 tokens/sec\n",
      "- batch size 1: 52.55 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "ollama_generate(test_messages, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e81e75-2592-413b-b1ac-99f9a7e273e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:16:52.786635Z",
     "iopub.status.busy": "2024-09-22T10:16:52.786354Z",
     "iopub.status.idle": "2024-09-22T10:16:52.788783Z",
     "shell.execute_reply": "2024-09-22T10:16:52.788498Z",
     "shell.execute_reply.started": "2024-09-22T10:16:52.786624Z"
    }
   },
   "source": [
    "# 3. Performance tests on RTX 4090"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e1d8a-ad1c-454b-a7b6-65c35ba8cc3d",
   "metadata": {},
   "source": [
    "## vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04602b22-efc0-4bd8-bbc1-280ef1be558e",
   "metadata": {},
   "source": [
    "### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c545d0-98c8-4ffe-8af4-48285ce273a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:18:56.380565Z",
     "iopub.status.busy": "2024-09-22T10:18:56.379872Z",
     "iopub.status.idle": "2024-09-22T10:19:18.485222Z",
     "shell.execute_reply": "2024-09-22T10:19:18.484869Z",
     "shell.execute_reply.started": "2024-09-22T10:18:56.380534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:18:56 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-22 12:18:57 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 12:18:57 model_runner.py:997] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 09-22 12:18:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b6ac65bd14f0fb9f0e270863a6f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:19:07 model_runner.py:1008] Loading model weights took 14.9888 GB\n",
      "INFO 09-22 12:19:08 gpu_executor.py:122] # GPU blocks: 3610, # CPU blocks: 2048\n",
      "INFO 09-22 12:19:08 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 12:19:08 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 12:19:18 model_runner.py:1430] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9b791a-343f-4e3f-acd9-a67fa153f6d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:19:41.643872Z",
     "iopub.status.busy": "2024-09-22T10:19:41.643423Z",
     "iopub.status.idle": "2024-09-22T10:29:41.318722Z",
     "shell.execute_reply": "2024-09-22T10:29:41.318342Z",
     "shell.execute_reply.started": "2024-09-22T10:19:41.643840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:09<00:00,  9.62s/it, est. speed input: 6.55 toks/s, output: 53.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre plusieurs avantages à ses membres et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus élevés sur les épargnes** : Le Crédit Mutuel propose des taux d'intérêt plus élevés sur les comptes d'épargne que de nombreux autres établissements bancaires.\\n2. **Taux d'emprunt compétitifs** : Les prêts personnels, les prêts immobiliers et les prêts pour la mobilité sont proposés avec des taux d'intérêt attractifs.\\n3. **Services personnalisés** : Le Crédit Mutuel offre des services personnalisés et adaptés aux besoins de ses membres et clients, grâce à une approche relationnelle et à une connaissance approfondie de leurs situations financières.\\n4. **Sécurité et confidentialité** : Le Crédit Mutuel s'engage à protéger les données personnelles et financières de ses membres et clients, conformément aux règles de protection des données.\\n5. **Participation aux décisions** : En tant que membre du Crédit Mutuel, vous avez la possibilité de participer aux assemblées générales et de voter sur les décisions stratégiques de l'institution.\\n6. **Services bancaires complets** : Le Crédit Mutuel propose un large éventail de services bancaires, y compris des comptes courants, des comptes d'épargne, des prêts, des cartes de crédit et des assurances.\\n7. **Aide à la gestion financière** : Le Crédit Mutuel propose des outils et des conseils pour aider ses membres et clients à gérer leur budget, à planifier leur avenir financier et à atteindre leurs objectifs économiques.\\n8. **Prévention de la fraude** : Le Crédit Mutuel met en place des mesures de prévention de la fraude pour protéger les comptes et les données de ses membres et clients.\\n9. **Services numériques** : Le Crédit Mutuel propose des applications mobiles et un site internet sécurisé pour gérer ses comptes et effectuer des opérations bancaires en ligne.\\n10. **Eng\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:09<00:00,  9.29s/it, est. speed input: 6.78 toks/s, output: 55.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 55.10 tokens/sec (1 x 55.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:09<00:00,  4.79s/it, est. speed input: 13.26 toks/s, output: 106.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 106.89 tokens/sec (2 x 53.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:09<00:00,  3.20s/it, est. speed input: 19.89 toks/s, output: 159.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 159.94 tokens/sec (3 x 53.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:09<00:00,  2.41s/it, est. speed input: 26.12 toks/s, output: 209.04 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 209.00 tokens/sec (4 x 52.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:09<00:00,  1.95s/it, est. speed input: 32.36 toks/s, output: 254.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 254.81 tokens/sec (5 x 50.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:09<00:00,  1.63s/it, est. speed input: 38.83 toks/s, output: 314.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 314.63 tokens/sec (6 x 52.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:09<00:00,  1.40s/it, est. speed input: 45.15 toks/s, output: 365.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 365.23 tokens/sec (7 x 52.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:09<00:00,  1.23s/it, est. speed input: 51.20 toks/s, output: 416.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 416.02 tokens/sec (8 x 52.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:09<00:00,  1.10s/it, est. speed input: 57.18 toks/s, output: 453.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 453.06 tokens/sec (9 x 50.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 10/10 [00:09<00:00,  1.01it/s, est. speed input: 63.46 toks/s, output: 514.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 514.74 tokens/sec (10 x 51.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 11/11 [00:09<00:00,  1.10it/s, est. speed input: 69.57 toks/s, output: 561.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 561.78 tokens/sec (11 x 51.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 12/12 [00:10<00:00,  1.19it/s, est. speed input: 75.24 toks/s, output: 610.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 610.03 tokens/sec (12 x 50.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 13/13 [00:10<00:00,  1.28it/s, est. speed input: 80.76 toks/s, output: 656.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 656.12 tokens/sec (13 x 50.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 14/14 [00:10<00:00,  1.36it/s, est. speed input: 86.09 toks/s, output: 690.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 690.56 tokens/sec (14 x 49.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 15/15 [00:10<00:00,  1.47it/s, est. speed input: 92.65 toks/s, output: 747.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 747.14 tokens/sec (15 x 49.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 16/16 [00:10<00:00,  1.56it/s, est. speed input: 98.35 toks/s, output: 792.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 791.76 tokens/sec (16 x 49.49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 17/17 [00:10<00:00,  1.55it/s, est. speed input: 97.86 toks/s, output: 791.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 790.86 tokens/sec (17 x 46.52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 18/18 [00:11<00:00,  1.63it/s, est. speed input: 102.61 toks/s, output: 833.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 832.88 tokens/sec (18 x 46.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 19/19 [00:11<00:00,  1.73it/s, est. speed input: 108.87 toks/s, output: 871.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 871.01 tokens/sec (19 x 45.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 20/20 [00:11<00:00,  1.79it/s, est. speed input: 112.79 toks/s, output: 907.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 907.32 tokens/sec (20 x 45.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 21/21 [00:11<00:00,  1.88it/s, est. speed input: 118.24 toks/s, output: 957.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 957.29 tokens/sec (21 x 45.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:11<00:00,  1.96it/s, est. speed input: 123.82 toks/s, output: 1005.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1005.15 tokens/sec (22 x 45.69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:11<00:00,  1.96it/s, est. speed input: 123.65 toks/s, output: 1003.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 1003.09 tokens/sec (23 x 43.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:11<00:00,  2.03it/s, est. speed input: 127.89 toks/s, output: 1039.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 1038.99 tokens/sec (24 x 43.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:11<00:00,  2.09it/s, est. speed input: 131.69 toks/s, output: 1061.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 1060.58 tokens/sec (25 x 42.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:11<00:00,  2.18it/s, est. speed input: 137.70 toks/s, output: 1109.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 1109.19 tokens/sec (26 x 42.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:11<00:00,  2.26it/s, est. speed input: 142.36 toks/s, output: 1153.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 1153.22 tokens/sec (27 x 42.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:12<00:00,  2.33it/s, est. speed input: 146.82 toks/s, output: 1187.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 1186.84 tokens/sec (28 x 42.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 29/29 [00:12<00:00,  2.40it/s, est. speed input: 151.49 toks/s, output: 1223.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 1223.40 tokens/sec (29 x 42.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 30/30 [00:12<00:00,  2.44it/s, est. speed input: 154.00 toks/s, output: 1243.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 1243.38 tokens/sec (30 x 41.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 31/31 [00:12<00:00,  2.53it/s, est. speed input: 159.80 toks/s, output: 1282.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 1282.13 tokens/sec (31 x 41.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 32/32 [00:12<00:00,  2.59it/s, est. speed input: 163.12 toks/s, output: 1321.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 1321.37 tokens/sec (32 x 41.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 33/33 [00:12<00:00,  2.61it/s, est. speed input: 164.50 toks/s, output: 1331.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 33: 1330.54 tokens/sec (33 x 40.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 34/34 [00:12<00:00,  2.68it/s, est. speed input: 169.09 toks/s, output: 1366.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 34: 1365.97 tokens/sec (34 x 40.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 35/35 [00:12<00:00,  2.75it/s, est. speed input: 173.34 toks/s, output: 1405.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 35: 1405.09 tokens/sec (35 x 40.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 36/36 [00:12<00:00,  2.81it/s, est. speed input: 177.34 toks/s, output: 1431.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 36: 1430.91 tokens/sec (36 x 39.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 37/37 [00:12<00:00,  2.85it/s, est. speed input: 179.79 toks/s, output: 1452.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 37: 1451.85 tokens/sec (37 x 39.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 38/38 [00:13<00:00,  2.91it/s, est. speed input: 183.41 toks/s, output: 1478.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 38: 1478.29 tokens/sec (38 x 38.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 39/39 [00:13<00:00,  2.99it/s, est. speed input: 188.84 toks/s, output: 1533.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 39: 1532.73 tokens/sec (39 x 39.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 40/40 [00:13<00:00,  3.03it/s, est. speed input: 190.64 toks/s, output: 1518.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 40: 1518.18 tokens/sec (40 x 37.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 41/41 [00:13<00:00,  3.11it/s, est. speed input: 195.87 toks/s, output: 1575.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 41: 1574.79 tokens/sec (41 x 38.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 42/42 [00:13<00:00,  3.16it/s, est. speed input: 199.45 toks/s, output: 1613.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 42: 1612.86 tokens/sec (42 x 38.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 43/43 [00:13<00:00,  3.18it/s, est. speed input: 200.46 toks/s, output: 1625.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 43: 1624.96 tokens/sec (43 x 37.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 44/44 [00:13<00:00,  3.27it/s, est. speed input: 205.80 toks/s, output: 1659.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 44: 1658.89 tokens/sec (44 x 37.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 45/45 [00:21<00:00,  2.09it/s, est. speed input: 131.66 toks/s, output: 1064.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 45: 1064.15 tokens/sec (45 x 23.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 46/46 [00:21<00:00,  2.10it/s, est. speed input: 132.23 toks/s, output: 1071.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 46: 1070.79 tokens/sec (46 x 23.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 47/47 [00:22<00:00,  2.13it/s, est. speed input: 134.17 toks/s, output: 1087.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 47: 1087.21 tokens/sec (47 x 23.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 48/48 [00:22<00:00,  2.13it/s, est. speed input: 134.46 toks/s, output: 1085.58 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 48: 1085.12 tokens/sec (48 x 22.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*12, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12fe53-aff4-4011-a4c4-42c75a1a0e70",
   "metadata": {},
   "source": [
    "### w8a16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe30855-d044-44cd-8c26-d1fa906d376e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T10:47:04.141090Z",
     "iopub.status.busy": "2024-09-22T10:47:04.140661Z",
     "iopub.status.idle": "2024-09-22T10:47:19.006399Z",
     "shell.execute_reply": "2024-09-22T10:47:19.005934Z",
     "shell.execute_reply.started": "2024-09-22T10:47:04.141076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:47:04 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-22 12:47:04 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 12:47:05 model_runner.py:997] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16...\n",
      "INFO 09-22 12:47:05 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50628679523742e29c2ef094a4b4017a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 12:47:08 model_runner.py:1008] Loading model weights took 8.4927 GB\n",
      "INFO 09-22 12:47:09 gpu_executor.py:122] # GPU blocks: 6716, # CPU blocks: 2048\n",
      "INFO 09-22 12:47:09 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 12:47:09 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 12:47:18 model_runner.py:1430] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1:w8a16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08425c2e-00b1-4ce6-a386-dfe38fb14545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T11:02:36.653838Z",
     "iopub.status.busy": "2024-09-22T11:02:36.653443Z",
     "iopub.status.idle": "2024-09-22T11:12:26.033599Z",
     "shell.execute_reply": "2024-09-22T11:12:26.032790Z",
     "shell.execute_reply.started": "2024-09-22T11:02:36.653812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:06<00:00,  6.05s/it, est. speed input: 6.94 toks/s, output: 84.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre de nombreux avantages à ses adhérents et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus élevés** : Le Crédit Mutuel propose des taux d'intérêt plus élevés que les banques traditionnelles, notamment pour les comptes courants et les prêts.\\n2. **Services personnalisés** : En tant que banque coopérative, le Crédit Mutuel met l'accent sur la proximité et la personnalisation de ses services. Les clients ont accès à des conseillers financiers compétents qui leur proposent des solutions adaptées à leurs besoins.\\n3. **Transparence et sécurité** : Le Crédit Mutuel est connu pour sa transparence dans les tarifs et les conditions de prêt. Les clients sont également protégés par des garanties de sécurité renforcées.\\n4. **Épargne et placement** : Le Crédit Mutuel propose une gamme de produits d'épargne et de placement attractifs, notamment des comptes d'épargne, des livrets et des fonds communs de placement.\\n5. **Prêts et financements** : Le Crédit Mutuel offre une variété de prêts et de financements pour les particuliers et les entreprises, notamment des prêts immobiliers, des prêts personnels et des financements pour les entrepreneurs.\\n6. **Services bancaires en ligne** : Le Crédit Mutuel propose des services bancaires en ligne performants, permettant aux clients de gérer leurs comptes et d'effectuer des transactions en toute sécurité.\\n7. **Réseau de distributeurs** : Le Crédit Mutuel dispose d'un réseau de distributeurs étendu, ce qui facilite l'accès aux services bancaires pour les clients.\\n8. **Engagement social et environnemental** : Le Crédit Mutuel s'engage à des pratiques sociales et environnementales responsables, ce qui correspond aux valeurs de ses adhérents et clients.\\n9. **Adaptabilité et innovation** : Le Crédit Mutuel est constamment en train d'innover et d'améliorer ses services pour répondre aux besoins évoluant des clients\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:05<00:00,  5.82s/it, est. speed input: 7.21 toks/s, output: 87.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 87.89 tokens/sec (1 x 87.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:05<00:00,  2.98s/it, est. speed input: 14.25 toks/s, output: 171.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 171.58 tokens/sec (2 x 85.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:05<00:00,  1.99s/it, est. speed input: 21.48 toks/s, output: 257.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 257.67 tokens/sec (3 x 85.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:06<00:00,  1.50s/it, est. speed input: 27.99 toks/s, output: 335.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 335.81 tokens/sec (4 x 83.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:06<00:00,  1.21s/it, est. speed input: 34.65 toks/s, output: 413.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 413.13 tokens/sec (5 x 82.63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:06<00:00,  1.02s/it, est. speed input: 41.37 toks/s, output: 502.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 502.13 tokens/sec (6 x 83.69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:06<00:00,  1.14it/s, est. speed input: 48.05 toks/s, output: 563.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 562.86 tokens/sec (7 x 80.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:06<00:00,  1.29it/s, est. speed input: 54.08 toks/s, output: 658.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 658.07 tokens/sec (8 x 82.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:06<00:00,  1.44it/s, est. speed input: 60.28 toks/s, output: 734.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 734.43 tokens/sec (9 x 81.60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 10/10 [00:06<00:00,  1.59it/s, est. speed input: 66.78 toks/s, output: 806.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 806.50 tokens/sec (10 x 80.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 11/11 [00:06<00:00,  1.69it/s, est. speed input: 71.30 toks/s, output: 857.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 856.78 tokens/sec (11 x 77.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 12/12 [00:06<00:00,  1.87it/s, est. speed input: 78.49 toks/s, output: 956.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 955.85 tokens/sec (12 x 79.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 13/13 [00:06<00:00,  2.01it/s, est. speed input: 84.51 toks/s, output: 1017.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 1017.28 tokens/sec (13 x 78.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 14/14 [00:06<00:00,  2.15it/s, est. speed input: 90.58 toks/s, output: 1087.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 1087.24 tokens/sec (14 x 77.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 15/15 [00:06<00:00,  2.29it/s, est. speed input: 96.63 toks/s, output: 1162.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 1162.08 tokens/sec (15 x 77.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 16/16 [00:06<00:00,  2.46it/s, est. speed input: 103.28 toks/s, output: 1235.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 1235.28 tokens/sec (16 x 77.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 17/17 [00:07<00:00,  2.37it/s, est. speed input: 99.74 toks/s, output: 1199.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 1198.74 tokens/sec (17 x 70.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 18/18 [00:07<00:00,  2.48it/s, est. speed input: 104.09 toks/s, output: 1259.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 1258.70 tokens/sec (18 x 69.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 19/19 [00:07<00:00,  2.61it/s, est. speed input: 109.93 toks/s, output: 1317.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 1317.21 tokens/sec (19 x 69.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 20/20 [00:07<00:00,  2.67it/s, est. speed input: 112.21 toks/s, output: 1365.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 1364.70 tokens/sec (20 x 68.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 21/21 [00:07<00:00,  2.86it/s, est. speed input: 119.95 toks/s, output: 1436.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 1436.10 tokens/sec (21 x 68.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:07<00:00,  2.98it/s, est. speed input: 125.10 toks/s, output: 1498.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1498.23 tokens/sec (22 x 68.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:07<00:00,  3.09it/s, est. speed input: 130.16 toks/s, output: 1557.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 1557.03 tokens/sec (23 x 67.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:07<00:00,  3.21it/s, est. speed input: 134.87 toks/s, output: 1614.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 1613.82 tokens/sec (24 x 67.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:07<00:00,  3.34it/s, est. speed input: 140.42 toks/s, output: 1693.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 1692.18 tokens/sec (25 x 67.69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:07<00:00,  3.39it/s, est. speed input: 142.59 toks/s, output: 1714.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 1713.37 tokens/sec (26 x 65.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:07<00:00,  3.57it/s, est. speed input: 150.38 toks/s, output: 1784.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 1783.18 tokens/sec (27 x 66.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:07<00:00,  3.64it/s, est. speed input: 152.80 toks/s, output: 1857.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 1856.88 tokens/sec (28 x 66.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 29/29 [00:07<00:00,  3.73it/s, est. speed input: 156.48 toks/s, output: 1885.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 1884.27 tokens/sec (29 x 64.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 30/30 [00:07<00:00,  3.85it/s, est. speed input: 161.69 toks/s, output: 1931.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 1930.94 tokens/sec (30 x 64.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 31/31 [00:08<00:00,  3.87it/s, est. speed input: 162.87 toks/s, output: 1967.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 1966.60 tokens/sec (31 x 63.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 32/32 [00:07<00:00,  4.06it/s, est. speed input: 170.60 toks/s, output: 2071.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 2070.27 tokens/sec (32 x 64.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 33/33 [00:08<00:00,  3.99it/s, est. speed input: 167.64 toks/s, output: 2013.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 33: 2012.02 tokens/sec (33 x 60.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 34/34 [00:08<00:00,  4.06it/s, est. speed input: 170.75 toks/s, output: 2057.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 34: 2055.94 tokens/sec (34 x 60.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 35/35 [00:08<00:00,  4.08it/s, est. speed input: 171.43 toks/s, output: 2072.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 35: 2071.45 tokens/sec (35 x 59.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 36/36 [00:08<00:00,  4.22it/s, est. speed input: 177.45 toks/s, output: 2139.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 36: 2138.32 tokens/sec (36 x 59.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 37/37 [00:08<00:00,  4.32it/s, est. speed input: 181.51 toks/s, output: 2183.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 37: 2182.29 tokens/sec (37 x 58.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 38/38 [00:08<00:00,  4.40it/s, est. speed input: 184.81 toks/s, output: 2239.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 38: 2238.40 tokens/sec (38 x 58.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 39/39 [00:08<00:00,  4.45it/s, est. speed input: 186.98 toks/s, output: 2260.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 39: 2259.25 tokens/sec (39 x 57.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 40/40 [00:08<00:00,  4.62it/s, est. speed input: 193.99 toks/s, output: 2339.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 40: 2338.13 tokens/sec (40 x 58.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 41/41 [00:08<00:00,  4.67it/s, est. speed input: 196.33 toks/s, output: 2359.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 41: 2358.16 tokens/sec (41 x 57.52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 42/42 [00:08<00:00,  4.72it/s, est. speed input: 198.40 toks/s, output: 2396.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 42: 2395.28 tokens/sec (42 x 57.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 43/43 [00:09<00:00,  4.71it/s, est. speed input: 198.19 toks/s, output: 2383.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 43: 2382.54 tokens/sec (43 x 55.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 44/44 [00:09<00:00,  4.88it/s, est. speed input: 204.93 toks/s, output: 2442.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 44: 2440.99 tokens/sec (44 x 55.48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 45/45 [00:09<00:00,  4.94it/s, est. speed input: 207.50 toks/s, output: 2503.28 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 45: 2501.75 tokens/sec (45 x 55.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 46/46 [00:09<00:00,  4.95it/s, est. speed input: 207.94 toks/s, output: 2512.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 46: 2510.82 tokens/sec (46 x 54.58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 47/47 [00:09<00:00,  5.10it/s, est. speed input: 214.43 toks/s, output: 2596.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 47: 2594.32 tokens/sec (47 x 55.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 48/48 [00:09<00:00,  5.20it/s, est. speed input: 218.22 toks/s, output: 2615.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 48: 2613.30 tokens/sec (48 x 54.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 49/49 [00:09<00:00,  5.39it/s, est. speed input: 226.31 toks/s, output: 2720.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 49: 2718.71 tokens/sec (49 x 55.48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 50/50 [00:09<00:00,  5.55it/s, est. speed input: 233.29 toks/s, output: 2790.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 50: 2788.50 tokens/sec (50 x 55.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 51/51 [00:09<00:00,  5.17it/s, est. speed input: 217.31 toks/s, output: 2616.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 51: 2615.17 tokens/sec (51 x 51.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 52/52 [00:09<00:00,  5.32it/s, est. speed input: 223.62 toks/s, output: 2702.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 52: 2700.85 tokens/sec (52 x 51.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 53/53 [00:09<00:00,  5.41it/s, est. speed input: 227.09 toks/s, output: 2724.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 53: 2722.50 tokens/sec (53 x 51.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 54/54 [00:09<00:00,  5.40it/s, est. speed input: 227.07 toks/s, output: 2739.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 54: 2738.10 tokens/sec (54 x 50.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 55/55 [00:09<00:00,  5.52it/s, est. speed input: 231.88 toks/s, output: 2794.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 55: 2792.75 tokens/sec (55 x 50.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 56/56 [00:10<00:00,  5.50it/s, est. speed input: 230.98 toks/s, output: 2791.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 56: 2789.30 tokens/sec (56 x 49.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 57/57 [00:10<00:00,  5.62it/s, est. speed input: 236.13 toks/s, output: 2836.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 57: 2834.65 tokens/sec (57 x 49.73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 58/58 [00:10<00:00,  5.71it/s, est. speed input: 239.74 toks/s, output: 2868.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 58: 2866.93 tokens/sec (58 x 49.43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 59/59 [00:10<00:00,  5.69it/s, est. speed input: 239.28 toks/s, output: 2891.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 59: 2890.12 tokens/sec (59 x 48.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 60/60 [00:10<00:00,  5.85it/s, est. speed input: 245.72 toks/s, output: 2945.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 60: 2943.27 tokens/sec (60 x 49.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 61/61 [00:10<00:00,  5.85it/s, est. speed input: 245.85 toks/s, output: 2951.12 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 61: 2949.07 tokens/sec (61 x 48.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 62/62 [00:10<00:00,  5.93it/s, est. speed input: 249.18 toks/s, output: 2990.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 62: 2988.63 tokens/sec (62 x 48.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 63/63 [00:10<00:00,  5.92it/s, est. speed input: 248.93 toks/s, output: 2987.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 63: 2985.80 tokens/sec (63 x 47.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 64/64 [00:10<00:00,  5.93it/s, est. speed input: 249.17 toks/s, output: 2987.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 64: 2985.45 tokens/sec (64 x 46.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 65/65 [00:13<00:00,  4.89it/s, est. speed input: 205.33 toks/s, output: 2475.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 65: 2474.07 tokens/sec (65 x 38.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 66/66 [00:13<00:00,  4.85it/s, est. speed input: 203.57 toks/s, output: 2432.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 66: 2431.67 tokens/sec (66 x 36.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 67/67 [00:13<00:00,  4.87it/s, est. speed input: 204.60 toks/s, output: 2458.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 67: 2457.61 tokens/sec (67 x 36.68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 68/68 [00:14<00:00,  4.75it/s, est. speed input: 199.54 toks/s, output: 2413.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 68: 2412.29 tokens/sec (68 x 35.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                     | 0/69 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvllm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_messages\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mvllm_generate\u001b[0;34m(messages, llm)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the start time\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Print the outputs.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/utils.py:1036\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1033\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[0;32m-> 1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:348\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\u001b[0m\n\u001b[1;32m    339\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    342\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    343\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    344\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    345\u001b[0m     prompt_adapter_request\u001b[38;5;241m=\u001b[39mprompt_adapter_request,\n\u001b[1;32m    346\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request)\n\u001b[0;32m--> 348\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:715\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    713\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 715\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1223\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1220\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1221\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:130\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 130\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/worker_base.py:327\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    324\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 327\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    114\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner.py:1589\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     model_input\u001b[38;5;241m.\u001b[39masync_callback()\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1589\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1596\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:466\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    463\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    464\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 466\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:278\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    275\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m maybe_deferred_sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:968\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    949\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    950\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleReturnType:\n\u001b[1;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:855\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    843\u001b[0m maybe_deferred_args \u001b[38;5;241m=\u001b[39m SampleResultArgsType(\n\u001b[1;32m    844\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    845\u001b[0m     sample_metadata\u001b[38;5;241m=\u001b[39msample_metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     beam_search_logprobs\u001b[38;5;241m=\u001b[39mbeam_search_logprobs,\n\u001b[1;32m    849\u001b[0m     sample_results_dict\u001b[38;5;241m=\u001b[39msample_results_dict)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata\u001b[38;5;241m.\u001b[39mskip_sampler_cpu_output:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    861\u001b[0m         maybe_deferred_args,\n\u001b[1;32m    862\u001b[0m         sampled_token_ids_tensor,\n\u001b[1;32m    863\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:720\u001b[0m, in \u001b[0;36mget_pythonized_sample_results\u001b[0;34m(sample_result_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 720\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    723\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups,\n\u001b[1;32m    724\u001b[0m                                          beam_search_logprobs)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:519\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, random_samples)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    521\u001b[0m results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*18, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d7488-2a64-42b4-8ec0-441447683004",
   "metadata": {},
   "source": [
    "### fp8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd068c8-186b-431f-9067-ae82d15c507a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T11:15:41.261539Z",
     "iopub.status.busy": "2024-09-22T11:15:41.259827Z",
     "iopub.status.idle": "2024-09-22T11:24:26.358892Z",
     "shell.execute_reply": "2024-09-22T11:24:26.358399Z",
     "shell.execute_reply.started": "2024-09-22T11:15:41.261502Z"
    }
   },
   "source": [
    "Note: the \"FP8-dynamic\" version is much slower that the \"FP8\" version.\n",
    "- batch size 16: 61 tokens/sec vs 77 tokens/sec\n",
    "- batch size 32: 42 tokens/sec vs 62 tokens/sec\n",
    "- batch size 48: 03 tokens/sec vs 52 tokens/sec !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5650e783-f639-4c9d-9096-35a6678e91c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T11:39:32.759261Z",
     "iopub.status.busy": "2024-09-22T11:39:32.758586Z",
     "iopub.status.idle": "2024-09-22T11:48:07.880587Z",
     "shell.execute_reply": "2024-09-22T11:48:07.880145Z",
     "shell.execute_reply.started": "2024-09-22T11:39:32.759228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db6daf92370484daf11edee49ca36f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 13:39:33 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc0dc720717411da9b7647bcc042579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd11eaca40342f188dbfb1bc83612cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09962e03bbae49af81f0bd548f54f7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/325 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d997f4197e5847dd9ec3438328611585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-22 13:39:35 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 13:39:35 model_runner.py:997] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8...\n",
      "INFO 09-22 13:39:36 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9163b8f1f63041598e319c582cb695bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d87778d0a544aaae84ede3e66dbbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089eeaf99ba47789d6f40347aaf2836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d27652eebd34392a219e3f6e7f1f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 13:47:57 model_runner.py:1008] Loading model weights took 8.4889 GB\n",
      "INFO 09-22 13:47:58 gpu_executor.py:122] # GPU blocks: 6856, # CPU blocks: 2048\n",
      "INFO 09-22 13:47:58 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 13:47:58 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 13:48:07 model_runner.py:1430] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1:fp8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fdeba4d-443b-4537-9e83-0733c5ccfc3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T11:48:07.881768Z",
     "iopub.status.busy": "2024-09-22T11:48:07.881343Z",
     "iopub.status.idle": "2024-09-22T12:01:23.379397Z",
     "shell.execute_reply": "2024-09-22T12:01:23.378580Z",
     "shell.execute_reply.started": "2024-09-22T11:48:07.881758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:06<00:00,  6.11s/it, est. speed input: 10.32 toks/s, output: 83.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre plusieurs avantages à ses membres et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus élevés sur les épargnes** : Le Crédit Mutuel propose des taux d'intérêt plus élevés sur les comptes d'épargne comparés aux banques traditionnelles.\\n2. **Frais de gestion réduits** : Les frais de gestion pour les comptes courants et les prêts sont souvent moins élevés chez le Crédit Mutuel que dans les banques conventionnelles.\\n3. **Prêts personnalisés** : Le Crédit Mutuel prend en compte la situation personnelle et financière de chaque client pour proposer des prêts adaptés à ses besoins.\\n4. **Soutien à l'entrepreneuriat** : Le Crédit Mutuel propose des solutions financières spécifiques pour les entrepreneurs et les PME, notamment des prêts et des services de financement.\\n5. **Services bancaires complets** : Le Crédit Mutuel propose une gamme complète de services bancaires, y compris les comptes courants, les prêts, les crédits immobiliers, les assurances, les placements et les services de gestion de patrimoine.\\n6. **Transparence et sécurité** : Le Crédit Mutuel est une banque coopérative, ce qui signifie que les décisions sont prises collectivement par les membres et que les informations sont transparentes.\\n7. **Services numériques avancés** : Le Crédit Mutuel propose des outils numériques pour gérer ses finances en ligne, tels que des applications mobiles et des sites web sécurisés.\\n8. **Accès à des services supplémentaires** : Les membres du Crédit Mutuel peuvent accéder à des services supplémentaires, comme des conseils financiers, des formations et des ateliers sur la gestion de l'argent et la maîtrise des finances.\\n9. **Participation à la vie associative** : Les membres du Crédit Mutuel peuvent participer à la vie associative et contribuer à la définition de la politique de la banque.\\n10. **Soutien à la communauté** : Le Crédit Mut\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:05<00:00,  5.93s/it, est. speed input: 10.62 toks/s, output: 86.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 86.27 tokens/sec (1 x 86.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:06<00:00,  3.04s/it, est. speed input: 20.88 toks/s, output: 168.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 168.27 tokens/sec (2 x 84.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:06<00:00,  2.03s/it, est. speed input: 31.39 toks/s, output: 252.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 252.33 tokens/sec (3 x 84.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:06<00:00,  1.53s/it, est. speed input: 41.17 toks/s, output: 334.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 334.44 tokens/sec (4 x 83.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:06<00:00,  1.24s/it, est. speed input: 51.01 toks/s, output: 411.35 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 411.19 tokens/sec (5 x 82.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:06<00:00,  1.03s/it, est. speed input: 61.37 toks/s, output: 497.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 497.24 tokens/sec (6 x 82.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:06<00:00,  1.12it/s, est. speed input: 71.09 toks/s, output: 575.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 574.88 tokens/sec (7 x 82.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:06<00:00,  1.28it/s, est. speed input: 80.54 toks/s, output: 654.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 654.25 tokens/sec (8 x 81.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:06<00:00,  1.43it/s, est. speed input: 90.25 toks/s, output: 729.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 729.21 tokens/sec (9 x 81.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 10/10 [00:06<00:00,  1.57it/s, est. speed input: 99.38 toks/s, output: 806.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 805.72 tokens/sec (10 x 80.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 11/11 [00:06<00:00,  1.74it/s, est. speed input: 109.74 toks/s, output: 889.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 888.85 tokens/sec (11 x 80.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 12/12 [00:06<00:00,  1.88it/s, est. speed input: 118.63 toks/s, output: 964.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 963.66 tokens/sec (12 x 80.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 13/13 [00:06<00:00,  2.03it/s, est. speed input: 127.69 toks/s, output: 1026.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 1025.87 tokens/sec (13 x 78.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 14/14 [00:06<00:00,  2.14it/s, est. speed input: 134.68 toks/s, output: 1086.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 1086.05 tokens/sec (14 x 77.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 15/15 [00:06<00:00,  2.29it/s, est. speed input: 144.63 toks/s, output: 1172.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 1172.37 tokens/sec (15 x 78.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 16/16 [00:06<00:00,  2.42it/s, est. speed input: 152.37 toks/s, output: 1232.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 1231.70 tokens/sec (16 x 76.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 17/17 [00:06<00:00,  2.47it/s, est. speed input: 155.67 toks/s, output: 1264.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 1264.05 tokens/sec (17 x 74.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 18/18 [00:06<00:00,  2.61it/s, est. speed input: 164.44 toks/s, output: 1327.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 1326.57 tokens/sec (18 x 73.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 19/19 [00:06<00:00,  2.74it/s, est. speed input: 173.17 toks/s, output: 1394.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 1394.01 tokens/sec (19 x 73.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 20/20 [00:07<00:00,  2.84it/s, est. speed input: 179.14 toks/s, output: 1443.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 1443.12 tokens/sec (20 x 72.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 21/21 [00:06<00:00,  3.02it/s, est. speed input: 190.27 toks/s, output: 1545.87 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 1545.00 tokens/sec (21 x 73.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:07<00:00,  3.13it/s, est. speed input: 197.52 toks/s, output: 1604.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1603.21 tokens/sec (22 x 72.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:07<00:00,  3.26it/s, est. speed input: 205.49 toks/s, output: 1667.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 1666.15 tokens/sec (23 x 72.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:07<00:00,  3.40it/s, est. speed input: 214.05 toks/s, output: 1732.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 1731.86 tokens/sec (24 x 72.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:07<00:00,  3.23it/s, est. speed input: 203.68 toks/s, output: 1641.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 1640.63 tokens/sec (25 x 65.63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:07<00:00,  3.36it/s, est. speed input: 212.02 toks/s, output: 1720.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 1719.50 tokens/sec (26 x 66.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:07<00:00,  3.46it/s, est. speed input: 218.23 toks/s, output: 1758.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 1757.78 tokens/sec (27 x 65.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:07<00:00,  3.57it/s, est. speed input: 225.07 toks/s, output: 1820.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 1819.32 tokens/sec (28 x 64.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 29/29 [00:07<00:00,  3.64it/s, est. speed input: 229.44 toks/s, output: 1856.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 1855.89 tokens/sec (29 x 64.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 30/30 [00:08<00:00,  3.67it/s, est. speed input: 231.08 toks/s, output: 1874.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 1873.13 tokens/sec (30 x 62.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 31/31 [00:08<00:00,  3.80it/s, est. speed input: 239.93 toks/s, output: 1946.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 1945.79 tokens/sec (31 x 62.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 32/32 [00:08<00:00,  3.88it/s, est. speed input: 244.64 toks/s, output: 1988.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 1986.91 tokens/sec (32 x 62.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 33/33 [00:08<00:00,  3.89it/s, est. speed input: 244.79 toks/s, output: 1959.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 33: 1958.11 tokens/sec (33 x 59.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 34/34 [00:08<00:00,  3.96it/s, est. speed input: 249.55 toks/s, output: 2020.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 34: 2019.27 tokens/sec (34 x 59.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 35/35 [00:08<00:00,  4.08it/s, est. speed input: 257.34 toks/s, output: 2081.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 35: 2079.73 tokens/sec (35 x 59.42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 36/36 [00:08<00:00,  4.17it/s, est. speed input: 262.86 toks/s, output: 2136.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 36: 2134.90 tokens/sec (36 x 59.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 37/37 [00:08<00:00,  4.23it/s, est. speed input: 266.38 toks/s, output: 2150.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 37: 2149.06 tokens/sec (37 x 58.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 38/38 [00:08<00:00,  4.36it/s, est. speed input: 274.64 toks/s, output: 2227.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 38: 2226.55 tokens/sec (38 x 58.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 39/39 [00:08<00:00,  4.64it/s, est. speed input: 292.55 toks/s, output: 2370.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 39: 2368.33 tokens/sec (39 x 60.73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 40/40 [00:08<00:00,  4.47it/s, est. speed input: 281.69 toks/s, output: 2281.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 40: 2280.26 tokens/sec (40 x 57.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 41/41 [00:08<00:00,  4.57it/s, est. speed input: 287.97 toks/s, output: 2330.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 41: 2329.16 tokens/sec (41 x 56.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 42/42 [00:09<00:00,  4.61it/s, est. speed input: 290.65 toks/s, output: 2353.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 42: 2351.93 tokens/sec (42 x 56.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 43/43 [00:09<00:00,  4.64it/s, est. speed input: 292.56 toks/s, output: 2369.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 43: 2367.68 tokens/sec (43 x 55.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 44/44 [00:09<00:00,  4.75it/s, est. speed input: 299.25 toks/s, output: 2423.04 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 44: 2421.30 tokens/sec (44 x 55.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 45/45 [00:09<00:00,  4.75it/s, est. speed input: 299.00 toks/s, output: 2421.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 45: 2419.48 tokens/sec (45 x 53.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 46/46 [00:09<00:00,  4.84it/s, est. speed input: 305.23 toks/s, output: 2474.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 46: 2472.53 tokens/sec (46 x 53.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 47/47 [00:09<00:00,  4.92it/s, est. speed input: 310.48 toks/s, output: 2512.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 47: 2510.48 tokens/sec (47 x 53.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 48/48 [00:09<00:00,  4.90it/s, est. speed input: 309.00 toks/s, output: 2507.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 48: 2505.45 tokens/sec (48 x 52.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 49/49 [00:10<00:00,  4.81it/s, est. speed input: 303.06 toks/s, output: 2448.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 49: 2446.60 tokens/sec (49 x 49.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 50/50 [00:10<00:00,  4.81it/s, est. speed input: 302.88 toks/s, output: 2459.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 50: 2458.17 tokens/sec (50 x 49.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 51/51 [00:10<00:00,  4.89it/s, est. speed input: 308.39 toks/s, output: 2497.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 51: 2496.01 tokens/sec (51 x 48.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 52/52 [00:10<00:00,  5.09it/s, est. speed input: 320.59 toks/s, output: 2598.08 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 52: 2596.31 tokens/sec (52 x 49.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 53/53 [00:10<00:00,  4.95it/s, est. speed input: 311.58 toks/s, output: 2519.97 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 53: 2518.22 tokens/sec (53 x 47.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 54/54 [00:10<00:00,  5.08it/s, est. speed input: 319.88 toks/s, output: 2592.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 54: 2590.62 tokens/sec (54 x 47.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 55/55 [00:10<00:00,  5.10it/s, est. speed input: 321.73 toks/s, output: 2603.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 55: 2602.18 tokens/sec (55 x 47.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 56/56 [00:28<00:00,  1.99it/s, est. speed input: 125.12 toks/s, output: 1013.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 56: 1012.78 tokens/sec (56 x 18.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 57/57 [00:29<00:00,  1.96it/s, est. speed input: 123.31 toks/s, output: 997.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 57: 997.55 tokens/sec (57 x 17.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 58/58 [00:30<00:00,  1.90it/s, est. speed input: 119.90 toks/s, output: 971.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 58: 971.53 tokens/sec (58 x 16.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 59/59 [00:31<00:00,  1.90it/s, est. speed input: 119.68 toks/s, output: 970.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 59: 970.21 tokens/sec (59 x 16.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 60/60 [00:31<00:00,  1.90it/s, est. speed input: 119.84 toks/s, output: 972.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 60: 972.13 tokens/sec (60 x 16.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 61/61 [00:32<00:00,  1.91it/s, est. speed input: 120.05 toks/s, output: 974.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 61: 973.83 tokens/sec (61 x 15.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 62/62 [00:32<00:00,  1.91it/s, est. speed input: 120.49 toks/s, output: 973.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 62: 973.11 tokens/sec (62 x 15.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 63/63 [00:32<00:00,  1.94it/s, est. speed input: 122.35 toks/s, output: 989.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 63: 989.27 tokens/sec (63 x 15.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 64/64 [00:33<00:00,  1.94it/s, est. speed input: 121.93 toks/s, output: 986.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 64: 986.53 tokens/sec (64 x 15.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 65/65 [00:34<00:00,  1.91it/s, est. speed input: 120.07 toks/s, output: 972.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 65: 972.07 tokens/sec (65 x 14.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 66/66 [00:34<00:00,  1.92it/s, est. speed input: 120.88 toks/s, output: 978.95 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 66: 978.71 tokens/sec (66 x 14.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                     | 0/67 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvllm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_messages\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mvllm_generate\u001b[0;34m(messages, llm)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the start time\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Print the outputs.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/utils.py:1036\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1033\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[0;32m-> 1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:348\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\u001b[0m\n\u001b[1;32m    339\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    342\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    343\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    344\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    345\u001b[0m     prompt_adapter_request\u001b[38;5;241m=\u001b[39mprompt_adapter_request,\n\u001b[1;32m    346\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request)\n\u001b[0;32m--> 348\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:715\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    713\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 715\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1223\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1220\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1221\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:130\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 130\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/worker_base.py:327\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    324\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 327\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    114\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner.py:1589\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     model_input\u001b[38;5;241m.\u001b[39masync_callback()\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1589\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1596\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:466\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    463\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    464\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 466\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:278\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    275\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m maybe_deferred_sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:968\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    949\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    950\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleReturnType:\n\u001b[1;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:855\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    843\u001b[0m maybe_deferred_args \u001b[38;5;241m=\u001b[39m SampleResultArgsType(\n\u001b[1;32m    844\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    845\u001b[0m     sample_metadata\u001b[38;5;241m=\u001b[39msample_metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     beam_search_logprobs\u001b[38;5;241m=\u001b[39mbeam_search_logprobs,\n\u001b[1;32m    849\u001b[0m     sample_results_dict\u001b[38;5;241m=\u001b[39msample_results_dict)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata\u001b[38;5;241m.\u001b[39mskip_sampler_cpu_output:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    861\u001b[0m         maybe_deferred_args,\n\u001b[1;32m    862\u001b[0m         sampled_token_ids_tensor,\n\u001b[1;32m    863\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:720\u001b[0m, in \u001b[0;36mget_pythonized_sample_results\u001b[0;34m(sample_result_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 720\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    723\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups,\n\u001b[1;32m    724\u001b[0m                                          beam_search_logprobs)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:519\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, random_samples)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    521\u001b[0m results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*16, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47267753-baff-4dc5-ae32-f421c5b1614a",
   "metadata": {},
   "source": [
    "### w8a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3def90-d426-413f-8dd5-08d10454a26f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:08:29.729799Z",
     "iopub.status.busy": "2024-09-22T12:08:29.728806Z",
     "iopub.status.idle": "2024-09-22T12:16:57.143620Z",
     "shell.execute_reply": "2024-09-22T12:16:57.143191Z",
     "shell.execute_reply.started": "2024-09-22T12:08:29.729765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46228e5837846cb901f16d194877e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:08:30 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f854165b57bf411daceee4d1dd065e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df49cd37bee436d89799ee2262c1a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f096cc73db41f89d2cbf0978d116d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/325 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ecd7d33a574b2b964fa1bafb4b326e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-22 14:08:32 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 14:08:33 model_runner.py:997] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a8...\n",
      "INFO 09-22 14:08:33 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1d8cc8cce46949d1d4cac9406c5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bb6048a027476c92410f6ecd1e5965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed24cf2d54e401b97518f7a76ee0307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/43.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8d2aff09e841699de627de89f355c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:16:47 model_runner.py:1008] Loading model weights took 8.4939 GB\n",
      "INFO 09-22 14:16:48 gpu_executor.py:122] # GPU blocks: 6759, # CPU blocks: 2048\n",
      "INFO 09-22 14:16:48 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 14:16:48 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 14:16:57 model_runner.py:1430] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1:w8a8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbfd8d3-8269-4646-b9c8-61d7d41e00ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:17:09.523309Z",
     "iopub.status.busy": "2024-09-22T12:17:09.522946Z",
     "iopub.status.idle": "2024-09-22T12:23:34.095871Z",
     "shell.execute_reply": "2024-09-22T12:23:34.095274Z",
     "shell.execute_reply.started": "2024-09-22T12:17:09.523262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:06<00:00,  6.90s/it, est. speed input: 6.09 toks/s, output: 74.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque coopérative française qui offre plusieurs avantages à ses membres et clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts plus faibles** : Le Crédit Mutuel propose des taux d'intérêt compétitifs pour les prêts, les comptes courants et les épargnes.\\n2. **Accès à des services bancaires complets** : Le Crédit Mutuel offre une gamme complète de services bancaires, y compris des prêts, des comptes courants, des épargnes, des cartes de crédit, des assurances et des investissements.\\n3. **Conseils personnalisés** : Les conseillers du Crédit Mutuel peuvent vous aider à gérer votre patrimoine, à élaborer un plan financier et à atteindre vos objectifs économiques.\\n4. **Transparence et sécurité** : Le Crédit Mutuel est une banque coopérative, ce qui signifie que les décisions sont prises collectivement par les membres et non par des actionnaires. Cela assure une transparence et une sécurité supplémentaires.\\n5. **Services en ligne performants** : Le Crédit Mutuel propose des outils en ligne permettant de gérer son compte, de consulter son historique de transactions et de réaliser des opérations bancaires en toute sécurité.\\n6. **Réseau de guichets et d'agences** : Le Crédit Mutuel dispose d'un réseau de guichets et d'agences réparties sur tout le territoire français, ce qui facilite l'accès aux services bancaires.\\n7. **Participation aux bénéfices** : Les membres du Crédit Mutuel participent aux bénéfices de la banque, ce qui peut entraîner des dividendes et des avantages supplémentaires.\\n8. **Soutien à l'économie locale** : Le Crédit Mutuel soutient l'économie locale en proposant des services bancaires adaptés aux besoins des entreprises et des particuliers locaux.\\n9. **Formation et sensibilisation financière** : Le Crédit Mutuel propose des formations et des ressources pour aider les personnes à mieux comprendre les questions financières et à prendre des décisions éclair\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████| 1/1 [00:06<00:00,  6.63s/it, est. speed input: 6.33 toks/s, output: 77.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 77.18 tokens/sec (1 x 77.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:06<00:00,  3.39s/it, est. speed input: 12.53 toks/s, output: 149.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 149.67 tokens/sec (2 x 74.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:06<00:00,  2.27s/it, est. speed input: 18.83 toks/s, output: 221.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 221.12 tokens/sec (3 x 73.71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:06<00:00,  1.70s/it, est. speed input: 24.69 toks/s, output: 277.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 277.82 tokens/sec (4 x 69.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:06<00:00,  1.38s/it, est. speed input: 30.48 toks/s, output: 360.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 360.46 tokens/sec (5 x 72.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:06<00:00,  1.16s/it, est. speed input: 36.45 toks/s, output: 442.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 441.98 tokens/sec (6 x 73.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:06<00:00,  1.00it/s, est. speed input: 42.40 toks/s, output: 494.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 493.99 tokens/sec (7 x 70.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:06<00:00,  1.14it/s, est. speed input: 48.02 toks/s, output: 580.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 580.67 tokens/sec (8 x 72.58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:07<00:00,  1.27it/s, est. speed input: 53.51 toks/s, output: 649.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 648.93 tokens/sec (9 x 72.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 10/10 [00:07<00:00,  1.42it/s, est. speed input: 59.71 toks/s, output: 715.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 715.38 tokens/sec (10 x 71.54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 11/11 [00:07<00:00,  1.55it/s, est. speed input: 65.46 toks/s, output: 785.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 784.85 tokens/sec (11 x 71.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 12/12 [00:07<00:00,  1.68it/s, est. speed input: 70.58 toks/s, output: 849.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 849.42 tokens/sec (12 x 70.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 13/13 [00:07<00:00,  1.81it/s, est. speed input: 75.96 toks/s, output: 914.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 914.05 tokens/sec (13 x 70.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 14/14 [00:07<00:00,  1.90it/s, est. speed input: 79.95 toks/s, output: 952.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 952.35 tokens/sec (14 x 68.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 15/15 [00:07<00:00,  2.05it/s, est. speed input: 86.48 toks/s, output: 1028.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 1028.55 tokens/sec (15 x 68.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 16/16 [00:07<00:00,  2.17it/s, est. speed input: 91.31 toks/s, output: 1107.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 1106.96 tokens/sec (16 x 69.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 17/17 [00:07<00:00,  2.25it/s, est. speed input: 94.65 toks/s, output: 1123.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 1122.60 tokens/sec (17 x 66.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 18/18 [00:07<00:00,  2.40it/s, est. speed input: 100.96 toks/s, output: 1194.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 1194.09 tokens/sec (18 x 66.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 19/19 [00:07<00:00,  2.51it/s, est. speed input: 105.88 toks/s, output: 1262.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 1262.16 tokens/sec (19 x 66.43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 20/20 [00:07<00:00,  2.61it/s, est. speed input: 109.41 toks/s, output: 1300.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 1299.72 tokens/sec (20 x 64.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 21/21 [00:07<00:00,  2.75it/s, est. speed input: 115.49 toks/s, output: 1377.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 1376.91 tokens/sec (21 x 65.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:07<00:00,  2.84it/s, est. speed input: 119.28 toks/s, output: 1429.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1428.64 tokens/sec (22 x 64.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:07<00:00,  2.95it/s, est. speed input: 124.36 toks/s, output: 1456.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 1455.72 tokens/sec (23 x 63.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:07<00:00,  3.09it/s, est. speed input: 129.65 toks/s, output: 1500.97 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 1500.26 tokens/sec (24 x 62.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:07<00:00,  3.20it/s, est. speed input: 134.22 toks/s, output: 1593.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 1592.90 tokens/sec (25 x 63.72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:07<00:00,  3.26it/s, est. speed input: 137.04 toks/s, output: 1645.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 1644.59 tokens/sec (26 x 63.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:07<00:00,  3.42it/s, est. speed input: 143.74 toks/s, output: 1724.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 1723.58 tokens/sec (27 x 63.84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:07<00:00,  3.52it/s, est. speed input: 147.78 toks/s, output: 1722.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 1721.85 tokens/sec (28 x 61.49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 29/29 [00:18<00:00,  1.54it/s, est. speed input: 64.72 toks/s, output: 759.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 759.69 tokens/sec (29 x 26.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 30/30 [00:31<00:00,  1.06s/it, est. speed input: 39.75 toks/s, output: 467.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 467.67 tokens/sec (30 x 15.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 31/31 [00:46<00:00,  1.49s/it, est. speed input: 28.29 toks/s, output: 337.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 337.58 tokens/sec (31 x 10.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████| 32/32 [00:59<00:00,  1.87s/it, est. speed input: 22.40 toks/s, output: 269.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 269.92 tokens/sec (32 x 8.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                     | 0/33 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvllm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_messages\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mvllm_generate\u001b[0;34m(messages, llm)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     19\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the start time\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Print the outputs.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/utils.py:1036\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1033\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m         )\n\u001b[0;32m-> 1036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:348\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\u001b[0m\n\u001b[1;32m    339\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    342\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    343\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[1;32m    344\u001b[0m     lora_request\u001b[38;5;241m=\u001b[39mlora_request,\n\u001b[1;32m    345\u001b[0m     prompt_adapter_request\u001b[38;5;241m=\u001b[39mprompt_adapter_request,\n\u001b[1;32m    346\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request)\n\u001b[0;32m--> 348\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:715\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    713\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 715\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1223\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1220\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1221\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:130\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 130\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/worker_base.py:327\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    324\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 327\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner_base.py:112\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    114\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/worker/model_runner.py:1589\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     model_input\u001b[38;5;241m.\u001b[39masync_callback()\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1589\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1596\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:466\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    463\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    464\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 466\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wordslab-notebooks/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:278\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    275\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m maybe_deferred_sample_results, maybe_sampled_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_gpu_probs_tensor:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:968\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    949\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    950\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleReturnType:\n\u001b[1;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:855\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    843\u001b[0m maybe_deferred_args \u001b[38;5;241m=\u001b[39m SampleResultArgsType(\n\u001b[1;32m    844\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    845\u001b[0m     sample_metadata\u001b[38;5;241m=\u001b[39msample_metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     beam_search_logprobs\u001b[38;5;241m=\u001b[39mbeam_search_logprobs,\n\u001b[1;32m    849\u001b[0m     sample_results_dict\u001b[38;5;241m=\u001b[39msample_results_dict)\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata\u001b[38;5;241m.\u001b[39mskip_sampler_cpu_output:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    861\u001b[0m         maybe_deferred_args,\n\u001b[1;32m    862\u001b[0m         sampled_token_ids_tensor,\n\u001b[1;32m    863\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:720\u001b[0m, in \u001b[0;36mget_pythonized_sample_results\u001b[0;34m(sample_result_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 720\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    723\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups,\n\u001b[1;32m    724\u001b[0m                                          beam_search_logprobs)\n",
      "File \u001b[0;32m/workspace/wordslab-llms/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:519\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, random_samples)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;124;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m random_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    521\u001b[0m results: SampleResultType \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*18, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79accf-d5f8-44e9-9338-2434e8c5fa56",
   "metadata": {},
   "source": [
    "### w4a16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221d45d8-315e-42cf-8adb-69959c7dc12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:25:07.253635Z",
     "iopub.status.busy": "2024-09-22T12:25:07.252675Z",
     "iopub.status.idle": "2024-09-22T12:30:40.390307Z",
     "shell.execute_reply": "2024-09-22T12:30:40.389909Z",
     "shell.execute_reply.started": "2024-09-22T12:25:07.253604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a9ea6700b146bc854b3be1339d3f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:25:08 gptq_marlin.py:108] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 09-22 14:25:08 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b28b7fbab294772ba0aeff26beb8fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a58069105c64b4b81826f55148b6bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75751a767bfb420c9298192a76c2cc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-22 14:25:10 utils.py:727] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-22 14:25:10 model_runner.py:997] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16...\n",
      "INFO 09-22 14:25:11 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb0e8c4276f489eb68caec8d5440453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:30:25 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40dad918e004484adea7b1e227fca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 14:30:26 model_runner.py:1008] Loading model weights took 5.3812 GB\n",
      "INFO 09-22 14:30:28 gpu_executor.py:122] # GPU blocks: 8325, # CPU blocks: 2048\n",
      "INFO 09-22 14:30:30 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 14:30:30 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 14:30:40 model_runner.py:1430] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = vllm_load(test_models[\"llama-3.1:w4a16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f3ba5a-1ff7-4260-ad56-2f2d90edb218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T12:30:45.862281Z",
     "iopub.status.busy": "2024-09-22T12:30:45.861790Z",
     "iopub.status.idle": "2024-09-22T12:39:33.032810Z",
     "shell.execute_reply": "2024-09-22T12:39:33.032392Z",
     "shell.execute_reply.started": "2024-09-22T12:30:45.862253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM performance test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████| 1/1 [00:04<00:00,  4.38s/it, est. speed input: 9.58 toks/s, output: 116.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Le Crédit Mutuel est une banque mutuelle française qui offre divers avantages à ses clients. Voici quelques-uns des principaux avantages :\\n\\n1. **Intérêts sur les comptes courants** : Le Crédit Mutuel offre des intérêts sur les comptes courants, ce qui permet aux clients de gagner de l'argent sans avoir à investir leurs fonds.\\n2. **Prêt à usage** : Le Crédit Mutuel propose des prêts à usage avec des taux d'intérêt compétitifs, ce qui peut aider les clients à financer leurs besoins ou leurs projets.\\n3. **Assurance de prêt** : Le Crédit Mutuel offre une assurance de prêt qui couvre les risques de défaut de remboursement, ce qui donne aux emprunteurs une sécurité supplémentaire.\\n4. **Services bancaires complets** : Le Crédit Mutuel propose une gamme complète de services bancaires, y compris des cartes de crédit, des chèques, des virements et des paiements en ligne.\\n5. **Sécurité et protection** : Le Crédit Mutuel met en place des mesures de sécurité pour protéger les comptes et les données de ses clients contre les cyberattentats et les autres menaces.\\n6. **Conseils financiers** : Le Crédit Mutuel propose des conseils financiers et des recommandations pour aider ses clients à gérer leur argent et à atteindre leurs objectifs financiers.\\n7. **Réseau de distributeurs** : Le Crédit Mutuel dispose d'un réseau de distributeurs dans tout le pays, ce qui permet aux clients de retirer de l'argent ou de faire des dépôts facilement.\\n8. **Application mobile** : Le Crédit Mutuel propose une application mobile qui permet aux clients de gérer leurs comptes, de vérifier leurs soldes et de réaliser des opérations bancaires en ligne.\\n9. **Participation aux bénéfices** : En tant que banque mutuelle, le Crédit Mutuel redistribue une partie de ses bénéfices aux associés, ce qui peut être considéré comme un avantage pour les clients.\\n10. **Système de récompenses** : Le Crédit Mutuel propose un système de ré\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 1/1 [00:04<00:00,  4.08s/it, est. speed input: 10.30 toks/s, output: 125.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 1: 125.55 tokens/sec (1 x 125.55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 2/2 [00:04<00:00,  2.12s/it, est. speed input: 20.02 toks/s, output: 241.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 2: 241.06 tokens/sec (2 x 120.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 3/3 [00:04<00:00,  1.40s/it, est. speed input: 30.41 toks/s, output: 355.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 3: 355.46 tokens/sec (3 x 118.49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 4/4 [00:04<00:00,  1.06s/it, est. speed input: 39.45 toks/s, output: 478.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 4: 478.33 tokens/sec (4 x 119.58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 5/5 [00:04<00:00,  1.16it/s, est. speed input: 48.76 toks/s, output: 570.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 5: 569.82 tokens/sec (5 x 113.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 6/6 [00:04<00:00,  1.39it/s, est. speed input: 58.42 toks/s, output: 697.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 6: 697.43 tokens/sec (6 x 116.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 7/7 [00:04<00:00,  1.59it/s, est. speed input: 67.28 toks/s, output: 787.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 7: 787.10 tokens/sec (7 x 112.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 8/8 [00:04<00:00,  1.80it/s, est. speed input: 75.68 toks/s, output: 881.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 8: 881.11 tokens/sec (8 x 110.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████| 9/9 [00:04<00:00,  2.00it/s, est. speed input: 84.02 toks/s, output: 988.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 9: 987.65 tokens/sec (9 x 109.74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 10/10 [00:04<00:00,  2.20it/s, est. speed input: 92.81 toks/s, output: 1067.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 10: 1066.43 tokens/sec (10 x 106.64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 11/11 [00:04<00:00,  2.39it/s, est. speed input: 100.90 toks/s, output: 1190.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 11: 1189.65 tokens/sec (11 x 108.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 12/12 [00:04<00:00,  2.60it/s, est. speed input: 109.12 toks/s, output: 1288.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 12: 1288.05 tokens/sec (12 x 107.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 13/13 [00:04<00:00,  2.78it/s, est. speed input: 116.89 toks/s, output: 1381.02 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 13: 1380.25 tokens/sec (13 x 106.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 14/14 [00:04<00:00,  2.91it/s, est. speed input: 122.61 toks/s, output: 1435.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 14: 1434.50 tokens/sec (14 x 102.46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 15/15 [00:04<00:00,  3.15it/s, est. speed input: 132.66 toks/s, output: 1549.33 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 15: 1548.31 tokens/sec (15 x 103.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 16/16 [00:04<00:00,  3.34it/s, est. speed input: 140.13 toks/s, output: 1636.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 16: 1634.87 tokens/sec (16 x 102.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 17/17 [00:05<00:00,  3.16it/s, est. speed input: 132.65 toks/s, output: 1581.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 17: 1580.32 tokens/sec (17 x 92.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 18/18 [00:05<00:00,  3.29it/s, est. speed input: 138.55 toks/s, output: 1630.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 18: 1629.18 tokens/sec (18 x 90.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 19/19 [00:05<00:00,  3.50it/s, est. speed input: 147.55 toks/s, output: 1711.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 19: 1710.76 tokens/sec (19 x 90.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 20/20 [00:05<00:00,  3.70it/s, est. speed input: 155.47 toks/s, output: 1786.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 20: 1785.25 tokens/sec (20 x 89.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 21/21 [00:05<00:00,  3.77it/s, est. speed input: 158.44 toks/s, output: 1844.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 21: 1843.56 tokens/sec (21 x 87.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 22/22 [00:05<00:00,  3.96it/s, est. speed input: 166.44 toks/s, output: 1955.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 22: 1954.32 tokens/sec (22 x 88.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 23/23 [00:05<00:00,  4.12it/s, est. speed input: 173.21 toks/s, output: 2013.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 23: 2011.92 tokens/sec (23 x 87.47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 24/24 [00:05<00:00,  4.24it/s, est. speed input: 178.23 toks/s, output: 2090.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 24: 2088.91 tokens/sec (24 x 87.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 25/25 [00:05<00:00,  4.43it/s, est. speed input: 186.27 toks/s, output: 2175.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 25: 2174.08 tokens/sec (25 x 86.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 26/26 [00:05<00:00,  4.47it/s, est. speed input: 188.00 toks/s, output: 2199.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 26: 2198.47 tokens/sec (26 x 84.56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 27/27 [00:05<00:00,  4.69it/s, est. speed input: 197.45 toks/s, output: 2314.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 27: 2313.17 tokens/sec (27 x 85.67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 28/28 [00:05<00:00,  4.84it/s, est. speed input: 203.44 toks/s, output: 2359.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 28: 2357.53 tokens/sec (28 x 84.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 29/29 [00:05<00:00,  4.91it/s, est. speed input: 206.42 toks/s, output: 2453.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 29: 2452.02 tokens/sec (29 x 84.55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 30/30 [00:06<00:00,  4.96it/s, est. speed input: 208.42 toks/s, output: 2441.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 30: 2439.96 tokens/sec (30 x 81.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 31/31 [00:05<00:00,  5.17it/s, est. speed input: 217.64 toks/s, output: 2538.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 31: 2536.60 tokens/sec (31 x 81.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 32/32 [00:06<00:00,  5.30it/s, est. speed input: 222.50 toks/s, output: 2605.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 32: 2604.17 tokens/sec (32 x 81.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 33/33 [00:06<00:00,  5.14it/s, est. speed input: 215.97 toks/s, output: 2520.17 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 33: 2518.36 tokens/sec (33 x 76.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 34/34 [00:06<00:00,  5.15it/s, est. speed input: 216.40 toks/s, output: 2504.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 34: 2503.17 tokens/sec (34 x 73.62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 35/35 [00:06<00:00,  5.24it/s, est. speed input: 220.19 toks/s, output: 2595.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 35: 2593.18 tokens/sec (35 x 74.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 36/36 [00:06<00:00,  5.36it/s, est. speed input: 225.03 toks/s, output: 2650.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 36: 2648.47 tokens/sec (36 x 73.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 37/37 [00:06<00:00,  5.45it/s, est. speed input: 229.04 toks/s, output: 2696.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 37: 2694.60 tokens/sec (37 x 72.83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 38/38 [00:06<00:00,  5.50it/s, est. speed input: 231.22 toks/s, output: 2670.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 38: 2668.55 tokens/sec (38 x 70.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 39/39 [00:06<00:00,  5.69it/s, est. speed input: 239.27 toks/s, output: 2815.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 39: 2813.51 tokens/sec (39 x 72.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 40/40 [00:06<00:00,  5.84it/s, est. speed input: 245.40 toks/s, output: 2855.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 40: 2848.50 tokens/sec (40 x 71.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 41/41 [00:07<00:00,  5.80it/s, est. speed input: 243.61 toks/s, output: 2846.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 41: 2843.88 tokens/sec (41 x 69.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 42/42 [00:07<00:00,  5.96it/s, est. speed input: 250.50 toks/s, output: 2936.79 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 42: 2934.41 tokens/sec (42 x 69.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 43/43 [00:07<00:00,  6.09it/s, est. speed input: 255.91 toks/s, output: 2955.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 43: 2952.75 tokens/sec (43 x 68.67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 44/44 [00:07<00:00,  6.00it/s, est. speed input: 251.98 toks/s, output: 2956.01 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 44: 2953.65 tokens/sec (44 x 67.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 45/45 [00:07<00:00,  6.20it/s, est. speed input: 260.36 toks/s, output: 3044.16 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 45: 3041.90 tokens/sec (45 x 67.60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 46/46 [00:07<00:00,  6.26it/s, est. speed input: 263.08 toks/s, output: 3097.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 46: 3095.31 tokens/sec (46 x 67.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 47/47 [00:07<00:00,  6.23it/s, est. speed input: 261.82 toks/s, output: 3090.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 47: 3036.79 tokens/sec (47 x 64.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 48/48 [00:07<00:00,  6.43it/s, est. speed input: 270.23 toks/s, output: 3176.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 48: 3173.57 tokens/sec (48 x 66.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 49/49 [00:08<00:00,  5.85it/s, est. speed input: 245.66 toks/s, output: 2914.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 49: 2912.62 tokens/sec (49 x 59.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 50/50 [00:08<00:00,  5.91it/s, est. speed input: 248.53 toks/s, output: 2905.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 50: 2903.08 tokens/sec (50 x 58.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 51/51 [00:09<00:00,  5.66it/s, est. speed input: 238.12 toks/s, output: 2783.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 51: 2781.90 tokens/sec (51 x 54.55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 52/52 [00:09<00:00,  5.62it/s, est. speed input: 235.93 toks/s, output: 2762.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 52: 2760.68 tokens/sec (52 x 53.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 53/53 [00:09<00:00,  5.71it/s, est. speed input: 239.64 toks/s, output: 2830.30 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 53: 2828.41 tokens/sec (53 x 53.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 54/54 [00:09<00:00,  5.73it/s, est. speed input: 240.64 toks/s, output: 2820.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 54: 2818.25 tokens/sec (54 x 52.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 55/55 [00:09<00:00,  5.77it/s, est. speed input: 242.54 toks/s, output: 2883.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 55: 2881.63 tokens/sec (55 x 52.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 56/56 [00:09<00:00,  5.78it/s, est. speed input: 242.64 toks/s, output: 2857.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 56: 2855.51 tokens/sec (56 x 50.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 57/57 [00:09<00:00,  5.80it/s, est. speed input: 243.43 toks/s, output: 2879.73 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 57: 2877.61 tokens/sec (57 x 50.48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 58/58 [00:09<00:00,  5.91it/s, est. speed input: 248.43 toks/s, output: 2923.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 58: 2921.28 tokens/sec (58 x 50.37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 59/59 [00:09<00:00,  5.93it/s, est. speed input: 249.22 toks/s, output: 2939.09 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 59: 2937.12 tokens/sec (59 x 49.78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 60/60 [00:09<00:00,  6.09it/s, est. speed input: 255.81 toks/s, output: 2973.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 60: 2971.65 tokens/sec (60 x 49.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 61/61 [00:10<00:00,  5.99it/s, est. speed input: 251.65 toks/s, output: 2963.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 61: 2961.58 tokens/sec (61 x 48.55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 62/62 [00:09<00:00,  6.22it/s, est. speed input: 261.29 toks/s, output: 3035.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 62: 3033.71 tokens/sec (62 x 48.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 63/63 [00:10<00:00,  6.29it/s, est. speed input: 264.39 toks/s, output: 3088.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 63: 3085.80 tokens/sec (63 x 48.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 64/64 [00:10<00:00,  6.22it/s, est. speed input: 261.25 toks/s, output: 3032.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 64: 3030.35 tokens/sec (64 x 47.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 65/65 [00:11<00:00,  5.62it/s, est. speed input: 236.04 toks/s, output: 2794.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 65: 2792.49 tokens/sec (65 x 42.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 66/66 [00:11<00:00,  5.80it/s, est. speed input: 243.59 toks/s, output: 2826.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 66: 2824.80 tokens/sec (66 x 42.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 67/67 [00:11<00:00,  5.71it/s, est. speed input: 239.81 toks/s, output: 2794.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 67: 2792.58 tokens/sec (67 x 41.68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 68/68 [00:11<00:00,  5.79it/s, est. speed input: 243.15 toks/s, output: 2831.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 68: 2829.74 tokens/sec (68 x 41.61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 69/69 [00:11<00:00,  5.84it/s, est. speed input: 245.27 toks/s, output: 2880.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 69: 2878.64 tokens/sec (69 x 41.72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 70/70 [00:11<00:00,  5.84it/s, est. speed input: 245.45 toks/s, output: 2876.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 70: 2874.84 tokens/sec (70 x 41.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 71/71 [00:11<00:00,  5.99it/s, est. speed input: 251.64 toks/s, output: 2927.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 71: 2925.52 tokens/sec (71 x 41.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 72/72 [00:12<00:00,  5.91it/s, est. speed input: 248.35 toks/s, output: 2909.87 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch size 72: 2907.97 tokens/sec (72 x 40.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vllm_generate(test_messages*18, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790d59b-8cfb-4c42-b14a-55a0cc6cfe30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
