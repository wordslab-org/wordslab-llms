{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e795d02-d492-41f5-bb89-4e7448c4da71",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058ff6-f379-41b0-8646-25b9cf380087",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925de6e-6681-48b3-8763-134fa1038b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2179c2-e0a1-4dc9-aaee-772fdc22333d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T15:17:50.704982Z",
     "iopub.status.busy": "2023-12-23T15:17:50.704466Z",
     "iopub.status.idle": "2023-12-23T15:17:50.712616Z",
     "shell.execute_reply": "2023-12-23T15:17:50.711623Z",
     "shell.execute_reply.started": "2023-12-23T15:17:50.704952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2318616-103c-4ffd-881d-ff6d07dc73fd",
   "metadata": {},
   "source": [
    "## Models dictionary\n",
    "\n",
    "### 3B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/05/04 | redpajama_3b | https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1 | 2.8 B | 2048 | 800 B | Apache 2.0 |\n",
    "| 2023/07/14 | btlm_3b | https://huggingface.co/cerebras/btlm-3b-8k-base | 3 B | 8192 | 627 B | Apache 2.0 |\n",
    "| 2023/07/16 | openllama_3b | https://huggingface.co/openlm-research/open_llama_3b_v2 | 3 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/09/29 | stablelm_3b | https://huggingface.co/stabilityai/stablelm-3b-4e1t | 3 B | 4096 | 1 T | CC BY-SA-4.0 |\n",
    "| 2023/12/13 | phi2_3b | https://huggingface.co/microsoft/phi-2 | 2.7 B | 2048 | 1.4 T | MICROSOFT RESEARCH LICENSE|\n",
    "\n",
    "### 7B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2022/10/09 | bloomz_7b | https://huggingface.co/bigscience/bloomz-7b1-mt | 7.1 B | 2048 | 350 B | BigScience RAIL License  |\n",
    "| 2023/04/24 | falcon_7b | https://huggingface.co/tiiuae/falcon-7b | 7 B | 2048 | 1.5 T | Apache 2.0 |\n",
    "| 2024/05/04 | redpajama_7b | https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base | 6.9 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/05/05 | mpt_7b | https://huggingface.co/mosaicml/mpt-7b | 6.7 B | 2048 | 1 T | Apache-2.0 |\n",
    "| 2023/06/30 | mpt_7b_8k| https://huggingface.co/mosaicml/mpt-7b-8k | 6.7 B | 8192 | 1.5 T | Apache-2.0 |\n",
    "| 2023/07/06 | openllama_7b | https://huggingface.co/openlm-research/open_llama_7b_v2 | 7 B | 2048 | 1 T | Apache 2.0 | \n",
    "| 2023/07/18 | llama2_7b | https://huggingface.co/meta-llama/Llama-2-7b-hf | 7 B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/07/26 | llama2_7b_32k | https://huggingface.co/togethercomputer/LLaMA-2-7B-32K | 7 B | 32768 | fine-tuned | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/20 | mistral_7b | https://huggingface.co/mistralai/Mistral-7B-v0.1 | 7.3 B | 8192 | ?? | Apache 2.0 |\n",
    "| 2023/09/24 | qwen_7b | https://huggingface.co/Qwen/Qwen-7B | 7 B | 8192 | 2.4 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_6b | https://huggingface.co/01-ai/Yi-6B | 6 B | 4096 | 3 T | Yi Series Models Community License Agreement |\n",
    "| 2023/12/10 | decilm_7b | https://huggingface.co/Deci/DeciLM-7B | 7 B | 8192 | ?? | Apache 2.0 |\n",
    "\n",
    "### 13B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/12/12 | solar_10b | https://huggingface.co/upstage/SOLAR-10.7B-v1.0 | 10.7 B | 4096 | 3 T | Apache 2.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342144-2d6f-4df1-ae83-13182f2625d6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed2ef01b-6443-470a-a8aa-50b9735ed904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T21:00:49.796370Z",
     "iopub.status.busy": "2023-12-23T21:00:49.795022Z",
     "iopub.status.idle": "2023-12-23T21:00:49.801086Z",
     "shell.execute_reply": "2023-12-23T21:00:49.800458Z",
     "shell.execute_reply.started": "2023-12-23T21:00:49.796345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " models = { \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"openllama_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "    \n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"openllama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\",\n",
    "    \n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\",    \n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"openllama_13b\" : \"openlm-research/open_llama_13b\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "    \n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\",\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\",\n",
    "    \"mixtral_8x7B\" : \"heBloke/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013cd80-b2c8-4821-a763-adc990e3f8e6",
   "metadata": {},
   "source": [
    "## Models download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deaaa43-17b4-40ff-aa26-76efe13a67e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to be able to access gated HuggingFace repositories:\n",
    "\n",
    "1. Login to your HuggingFace account, go to https://huggingface.co/settings/tokens, create a READ access token and copy it\n",
    "2. Paste your HuggingFace access token in the local file /workspace/hftoken\n",
    "3. Load it in Python with the the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f18c8a66-c1ff-4eac-9964-4d0a3855453b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T21:11:51.861118Z",
     "iopub.status.busy": "2023-12-23T21:11:51.860212Z",
     "iopub.status.idle": "2023-12-23T21:11:51.875399Z",
     "shell.execute_reply": "2023-12-23T21:11:51.874897Z",
     "shell.execute_reply.started": "2023-12-23T21:11:51.861083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa843-2258-443b-9b83-45b990f25a87",
   "metadata": {},
   "source": [
    "Download all models in HF local cache and measure the model files size on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6e717e-5686-4fab-b787-bd0a0ce16dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T20:46:09.381722Z",
     "iopub.status.busy": "2023-12-23T20:46:09.380949Z",
     "iopub.status.idle": "2023-12-23T20:46:09.388831Z",
     "shell.execute_reply": "2023-12-23T20:46:09.388093Z",
     "shell.execute_reply.started": "2023-12-23T20:46:09.381688Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_model_id, device_map=\"meta\", **kwargs)\n",
    "        path,size = get_model_path_and_size_on_disk(model)\n",
    "        print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"--> stored in directory: {path}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fb4f5-8adf-46c0-b402-2a79a1e3e172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_key in models:\n",
    "    download_in_local_cache(models[model_key], trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a5f38-67b3-4e78-9525-5358f48650c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
