{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf000ad0-ee0a-44bc-b3a1-9fd82d454939",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "https://github.com/turboderp/exllamav2\n",
    "\n",
    "https://github.com/theroyallab/tabbyAPI\n",
    "\n",
    "https://github.com/turboderp/exui\n",
    "\n",
    "Vidéo and example:\n",
    "\n",
    "https://www.youtube.com/watch?v=N5lDUZRI8sc\n",
    "\n",
    "https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/exllamaV2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c567cc-3e99-46c3-9359-02b3e2f186a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T14:13:50.092931Z",
     "iopub.status.busy": "2024-04-07T14:13:50.089730Z",
     "iopub.status.idle": "2024-04-07T14:13:50.148652Z",
     "shell.execute_reply": "2024-04-07T14:13:50.145676Z",
     "shell.execute_reply.started": "2024-04-07T14:13:50.092731Z"
    },
    "tags": []
   },
   "source": [
    "## Quantized models repositories\n",
    "\n",
    "https://huggingface.co/turboderp\n",
    "\n",
    "https://huggingface.co/LoneStriker\n",
    "\n",
    "https://huggingface.co/bartowski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08510c-208e-4dff-82c9-3cc6aad5b4da",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924999c-ae09-4e8d-8c8d-8c0d8c2c4500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade exllamav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0425c2b-52d2-40b9-88c1-411716d59938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:09:55.734899Z",
     "iopub.status.busy": "2024-04-06T21:09:55.733865Z",
     "iopub.status.idle": "2024-04-06T21:09:55.752281Z",
     "shell.execute_reply": "2024-04-06T21:09:55.751840Z",
     "shell.execute_reply.started": "2024-04-06T21:09:55.734861Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0.dev20231217+cu121'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307d4cbc-4d82-4300-8a17-9fdc7b484277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T21:10:31.705238Z",
     "iopub.status.busy": "2024-04-06T21:10:31.704556Z",
     "iopub.status.idle": "2024-04-06T21:10:31.712674Z",
     "shell.execute_reply": "2024-04-06T21:10:31.712038Z",
     "shell.execute_reply.started": "2024-04-06T21:10:31.705210Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.17'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"exllamav2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58806929-1974-465f-9577-e0cd327f7766",
   "metadata": {},
   "source": [
    "Open a terminal:\n",
    "\n",
    "```bash\n",
    "source ../test-pytorch/.venv/bin/activate\n",
    "git clone https://github.com/turboderp/exllamav2\n",
    "cd exllamav2\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4e6e2-c70f-4e5c-8696-bd0b821d1c9c",
   "metadata": {},
   "source": [
    "## Download a model and test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eeca56-bd51-4d54-8779-ebdcdf81550c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T14:17:41.096110Z",
     "iopub.status.busy": "2024-04-07T14:17:41.074157Z",
     "iopub.status.idle": "2024-04-07T14:17:41.127066Z",
     "shell.execute_reply": "2024-04-07T14:17:41.122618Z",
     "shell.execute_reply.started": "2024-04-07T14:17:41.095961Z"
    },
    "tags": []
   },
   "source": [
    "Open a terminal:\n",
    "\n",
    "1. Download an exl2 model:    \n",
    "    \n",
    "```bash\n",
    "mkdir /models/huggingface/exllama2\n",
    "cd /models/huggingface/exllama2\n",
    "git clone --branch 4.0bpw --single-branch --depth 1 https://huggingface.co/turboderp/command-r-v01-35B-exl2\n",
    "rm -rf command-r-v01-35B-exl2/.git\n",
    "```\n",
    "\n",
    "2. Test inference:\n",
    "\n",
    "```bash\n",
    "cd /workspace/wordslab-llms/exllamav2/\n",
    "python test_inference.py -m \"/models/huggingface/exllama2/command-r-v01-35B-exl2\" -p \"Les avantages du Crédit Mutuel sont \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b41b9e-a524-487d-91fa-007287aedf09",
   "metadata": {},
   "source": [
    "## Usage in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d92a2-bab2-4d31-8da0-e06443233e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/content/exllamav2')\n",
    "\n",
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "# Input prompts\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "prompts = \\\n",
    "[\n",
    "    \"How do I open a can of beans?\",\n",
    "    \"How do I open a can of soup?\",\n",
    "    \"How do I open a can of strawberry jam?\",\n",
    "    \"How do I open a can of raspberry jam?\",\n",
    "    \"What's the tallest building in Paris?\",\n",
    "    \"What's the most populous nation on Earth?\",\n",
    "    \"What's the most populous nation on Mars?\",\n",
    "    \"What do the Mole People actually want and how can we best appease them?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"Where is Waldo?\",\n",
    "    \"Who is Waldo?\",\n",
    "    \"Why is Waldo?\",\n",
    "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
    "    \"Is it legal to base jump into a volcano?\",\n",
    "    \"Why are cats better than dogs?\",\n",
    "    \"Why is the Hulk so angry all the time?\",\n",
    "    \"How do I build a time machine?\",\n",
    "    \"Is it legal to grow your own catnip?\"\n",
    "]\n",
    "\n",
    "# Sort by length to minimize padding\n",
    "\n",
    "s_prompts = sorted(prompts, key = len)\n",
    "\n",
    "# Apply prompt format\n",
    "\n",
    "def format_prompt(sp, p):\n",
    "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
    "\n",
    "system_prompt = \"Answer the question to the best of your ability.\"\n",
    "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
    "\n",
    "# Split into batches\n",
    "\n",
    "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "# Initialize model and cache\n",
    "\n",
    "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
    "\n",
    "config = ExLlamaV2Config()\n",
    "config.model_dir = model_directory\n",
    "config.prepare()\n",
    "\n",
    "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
    "\n",
    "model = ExLlamaV2(config)\n",
    "print(\"Loading model: \" + model_directory)\n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
    "model.load_autosplit(cache)\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Sampling settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.05\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
    "\n",
    "# Generate for each batch\n",
    "\n",
    "collected_outputs = []\n",
    "for b, batch in enumerate(batches):\n",
    "\n",
    "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
    "\n",
    "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
    "\n",
    "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
    "    collected_outputs += trimmed_outputs\n",
    "\n",
    "# Print the results\n",
    "\n",
    "for q, a in zip(s_prompts, collected_outputs):\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Q: \" + q)\n",
    "    print(\"A: \" + a)\n",
    "\n",
    "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcddcd-5341-4822-ad03-28e4cfc1a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2StreamingGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Settings\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.top_a = 0.0\n",
    "settings.token_repetition_penalty = 1.05\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "prompt_tokens = input_ids.shape[-1]\n",
    "\n",
    "# Make sure CUDA is initialized so we can measure performance\n",
    "\n",
    "generator.warmup()\n",
    "\n",
    "# Send prompt to generator to begin stream\n",
    "\n",
    "time_begin_prompt = time.time()\n",
    "\n",
    "print (prompt, end = \"\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "generator.set_stop_conditions([])\n",
    "generator.begin_stream(input_ids, settings)\n",
    "\n",
    "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
    "# consoles won't update partial lines without it.\n",
    "\n",
    "time_begin_stream = time.time()\n",
    "generated_tokens = 0\n",
    "\n",
    "while True:\n",
    "    chunk, eos, _ = generator.stream()\n",
    "    generated_tokens += 1\n",
    "    print (chunk, end = \"\")\n",
    "    sys.stdout.flush()\n",
    "    if eos or generated_tokens == max_new_tokens: break\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "time_prompt = time_begin_stream - time_begin_prompt\n",
    "time_tokens = time_end - time_begin_stream\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
    "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-pytorch",
   "language": "python",
   "name": "test-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
