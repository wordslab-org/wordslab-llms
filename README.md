# wordslab-llms - Efficient inference and fine tuning of popular open source llms

## Efficient local LLM inference with the vLLM library

1. Open the notebook:

> 01_vllm-efficient-inference.ipynb

2. Make sure the kernel 'wordslab-llms' is selected, then execute all cells from the notebook and follow the instructions
