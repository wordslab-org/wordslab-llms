{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e795d02-d492-41f5-bb89-4e7448c4da71",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058ff6-f379-41b0-8646-25b9cf380087",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925de6e-6681-48b3-8763-134fa1038b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2179c2-e0a1-4dc9-aaee-772fdc22333d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-23T15:17:50.704982Z",
     "iopub.status.busy": "2023-12-23T15:17:50.704466Z",
     "iopub.status.idle": "2023-12-23T15:17:50.712616Z",
     "shell.execute_reply": "2023-12-23T15:17:50.711623Z",
     "shell.execute_reply.started": "2023-12-23T15:17:50.704952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255816-433e-4ba7-9793-9c61ba2eea8d",
   "metadata": {},
   "source": [
    "For Qwen/Qwen-7B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c814e-348f-4055-bec7-0ea461515b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203ea47-e9f0-40f6-8810-bb8a777c9826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers_stream_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42929969-266f-427c-bd10-376aeb4d4920",
   "metadata": {},
   "source": [
    "For 01-ai/Yi-6B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017470c-95a2-41fe-9e9d-f0aaea0a02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160b3b9-9b74-4254-8abb-36afc07e3f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models dictionary\n",
    "\n",
    "### 3B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/05/04 | redpajama_3b | https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1 | 2.8 B | 2048 | 800 B | Apache 2.0 |\n",
    "| 2023/07/14 | btlm_3b | https://huggingface.co/cerebras/btlm-3b-8k-base | 3 B | 8192 | 627 B | Apache 2.0 |\n",
    "| 2023/07/16 | openllama2_3b | https://huggingface.co/openlm-research/open_llama_3b_v2 | 3 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/09/29 | stablelm_3b | https://huggingface.co/stabilityai/stablelm-3b-4e1t | 3 B | 4096 | 1 T | CC BY-SA-4.0 |\n",
    "| 2023/12/13 | phi2_3b | https://huggingface.co/microsoft/phi-2 | 2.7 B | 2048 | 1.4 T | MICROSOFT RESEARCH LICENSE|\n",
    "\n",
    "### 7B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2022/10/09 | bloomz_7b | https://huggingface.co/bigscience/bloomz-7b1-mt | 7.1 B | 2048 | 350 B | BigScience RAIL License  |\n",
    "| 2023/04/24 | falcon_7b | https://huggingface.co/tiiuae/falcon-7b | 7 B | 2048 | 1.5 T | Apache 2.0 |\n",
    "| 2024/05/04 | redpajama_7b | https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base | 6.9 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/05/05 | mpt_7b | https://huggingface.co/mosaicml/mpt-7b | 6.7 B | 2048 | 1 T | Apache-2.0 |\n",
    "| 2023/06/30 | mpt_7b_8k| https://huggingface.co/mosaicml/mpt-7b-8k | 6.7 B | 8192 | 1.5 T | Apache-2.0 |\n",
    "| 2023/07/06 | openllama2_7b | https://huggingface.co/openlm-research/open_llama_7b_v2 | 7 B | 2048 | 1 T | Apache 2.0 | \n",
    "| 2023/07/18 | llama2_7b | https://huggingface.co/meta-llama/Llama-2-7b-hf | 7 B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/07/26 | llama2_7b_32k | https://huggingface.co/togethercomputer/LLaMA-2-7B-32K | 7 B | 32768 | fine-tuned | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/20 | mistral_7b | https://huggingface.co/mistralai/Mistral-7B-v0.1 | 7.3 B | 8192 | ?? | Apache 2.0 |\n",
    "| 2023/09/24 | qwen_7b | https://huggingface.co/Qwen/Qwen-7B | 7 B | 8192 | 2.4 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_6b | https://huggingface.co/01-ai/Yi-6B | 6 B | 4096 | 3 T | Yi Series Models Community License Agreement |\n",
    "| 2023/12/10 | decilm_7b | https://huggingface.co/Deci/DeciLM-7B | 7 B | 8192 | ?? | Apache 2.0 |\n",
    "\n",
    "### 13B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/06/15 | openllama1_13b | https://huggingface.co/openlm-research/open_llama_13b | 13 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/07/18 | llama2_13b | https://huggingface.co/meta-llama/Llama-2-13b-hf | 13B  | 4096 | 2 T | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/09/24 | qwen_14b | https://huggingface.co/Qwen/Qwen-14B | 14 B | 2048 | 3 T | Tongyi Qianwen LICENSE AGREEMENT |\n",
    "| 2023/12/12 | solar_10b | https://huggingface.co/upstage/SOLAR-10.7B-v1.0 | 10.7 B | 4096 | 3 T | Apache 2.0 |\n",
    "\n",
    "### 30-45B parameters\n",
    "\n",
    "| Date | Name | URL | Params | Context | Train tokens | License |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 2023/02/24 | llama1_33b | https://huggingface.co/alexl83/LLaMA-33B-HF | 32.5 B | 2048 | 1.4 T | Noncommercial license focused on research use cases |\n",
    "| 2023/05/24 | falcon_40b | https://huggingface.co/tiiuae/falcon-40b | 40 B | 2048 | 1 T | Apache 2.0 |\n",
    "| 2023/06/20 | mpt_30b | https://huggingface.co/mosaicml/mpt-30b | 30 B | 8192 | 1 T | Apache-2.0 |\n",
    "| 2023/08/24 | codellama_34b | https://huggingface.co/codellama/CodeLlama-34b-hf | 34 B | 16384 | 2.5 T  | LLAMA 2 COMMUNITY LICENSE AGREEMENT |\n",
    "| 2023/11/01 | yi_34b | https://huggingface.co/01-ai/Yi-34B | 34 B | 4096 | 3 T | Yi Series Models Community License Agreement\n",
    "| 2023/12/11 | mixtral_8x7B | https://huggingface.co/mistralai/Mixtral-8x7B-v0.1 | 46.7 B -> 12.9 B | 32768 | ??  | Apache 2.0 |\n",
    "    \n",
    "**!!! ADD GATED !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342144-2d6f-4df1-ae83-13182f2625d6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2ef01b-6443-470a-a8aa-50b9735ed904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-24T10:17:50.258210Z",
     "iopub.status.busy": "2023-12-24T10:17:50.257856Z",
     "iopub.status.idle": "2023-12-24T10:17:50.267184Z",
     "shell.execute_reply": "2023-12-24T10:17:50.266473Z",
     "shell.execute_reply.started": "2023-12-24T10:17:50.258188Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " models = { \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "    \n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # https://huggingface.co/01-ai/Yi-34B\n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013cd80-b2c8-4821-a763-adc990e3f8e6",
   "metadata": {},
   "source": [
    "## Models download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deaaa43-17b4-40ff-aa26-76efe13a67e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you want to be able to access gated HuggingFace repositories:\n",
    "\n",
    "1. Login to your HuggingFace account, go to https://huggingface.co/settings/tokens, create a READ access token and copy it\n",
    "2. Paste your HuggingFace access token in the local file /workspace/hftoken\n",
    "3. Load it in Python with the the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18c8a66-c1ff-4eac-9964-4d0a3855453b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-24T07:43:35.400641Z",
     "iopub.status.busy": "2023-12-24T07:43:35.400411Z",
     "iopub.status.idle": "2023-12-24T07:43:35.404158Z",
     "shell.execute_reply": "2023-12-24T07:43:35.403715Z",
     "shell.execute_reply.started": "2023-12-24T07:43:35.400631Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa843-2258-443b-9b83-45b990f25a87",
   "metadata": {},
   "source": [
    "Download all models in HF local cache and measure the model files size on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6e717e-5686-4fab-b787-bd0a0ce16dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-24T07:43:49.357863Z",
     "iopub.status.busy": "2023-12-24T07:43:49.357621Z",
     "iopub.status.idle": "2023-12-24T07:43:51.130908Z",
     "shell.execute_reply": "2023-12-24T07:43:51.130201Z",
     "shell.execute_reply.started": "2023-12-24T07:43:49.357847Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def download_in_local_cache(pretrained_model_id, **kwargs):\n",
    "        print(f\"Loading model {pretrained_model_id} in local cache ...\")\n",
    "        AutoTokenizer.from_pretrained(pretrained_model_id, **kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(pretrained_model_id, device_map=\"meta\", **kwargs)\n",
    "        path,size = get_model_path_and_size_on_disk(model)\n",
    "        print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"--> stored in directory: {path}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fb4f5-8adf-46c0-b402-2a79a1e3e172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-24T10:17:58.625622Z",
     "iopub.status.busy": "2023-12-24T10:17:58.624970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq in local cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-30b-instruct:\n",
      "- configuration_mpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-30b-instruct:\n",
      "- modeling_mpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07170a3e3944f209afc840bedc7dd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beb8f1a696b4fbdb527acc1d662b01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00006.bin:   0%|          | 0.00/3.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx,model_key in enumerate(models):\n",
    "    if idx<=20: continue\n",
    "    download_in_local_cache(models[model_key], trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a5f38-67b3-4e78-9525-5358f48650c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
