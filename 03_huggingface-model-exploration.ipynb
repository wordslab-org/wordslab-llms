{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c14562f-e893-4edd-a575-4a0c59837251",
   "metadata": {},
   "source": [
    "# Huggingface model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1d990-feaf-45f3-8e0d-4eae92b7fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93f81b-a2ff-4269-b617-5893d84250df",
   "metadata": {},
   "source": [
    "## Choose a model you want to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69da06b6-3b2c-48df-9c48-45edd4d1dab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:12:13.473040Z",
     "iopub.status.busy": "2025-11-16T13:12:13.472913Z",
     "iopub.status.idle": "2025-11-16T13:12:13.476945Z",
     "shell.execute_reply": "2025-11-16T13:12:13.476158Z",
     "shell.execute_reply.started": "2025-11-16T13:12:13.473030Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a10e9-3fd6-4f9b-8607-69649fac72c8",
   "metadata": {},
   "source": [
    "## Load the model and generate one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f8ea27-5129-4987-b022-726e1b67e6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:12:13.601416Z",
     "iopub.status.busy": "2025-11-16T13:12:13.601205Z",
     "iopub.status.idle": "2025-11-16T13:12:30.955433Z",
     "shell.execute_reply": "2025-11-16T13:12:30.955070Z",
     "shell.execute_reply.started": "2025-11-16T13:12:13.601402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Qwen/Qwen3-0.6B\n",
      "\n",
      "PROMPT TEMPLATE:\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "ANSWER:\n",
      "<think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by recalling what I know about them. Large language models are AI systems designed to understand and generate human language. They're used in various applications like chatbots, translation, and content generation.\n",
      "\n",
      "I should mention their core capabilities. Maybe talk about understanding context, generating text, and adapting to different languages. Also, highlight their applications. But wait, I need to keep it concise. The user might be looking for a brief overview, so I need to make sure not to get too technical. \n",
      "\n",
      "Wait, should I include something about their training data and how they process information? That could be useful. But the user asked for a short introduction, so maybe just the main points. Let me check if there are any key features I'm missing. The introduction should be engaging and informative. \n",
      "\n",
      "Also, think about the audience. Are they students, professionals, or someone new to the field? The introduction should be accessible. Avoid jargon. Use simple terms. Make sure to mention that they can understand and generate text in different languages. \n",
      "\n",
      "Wait, maybe start with a hook. \"Large language models (LLMs) are AI systems designed to understand and generate human language.\" Then list their key features. Applications like chatbots, translation, content generation. Conclude with their significance. \n",
      "\n",
      "I should check if there's any specific aspect the user wants emphasized. Since they didn't specify, stick to the basics. Make sure the introduction flows well and covers the main points without being too long. Alright, time to put it all together.\n",
      "</think>\n",
      "\n",
      "Large language models (LLMs) are AI systems designed to understand and generate human language, enabling them to process context, generate text, and adapt to various languages. These models are used in applications like chatbots, translation, and content creation, making them versatile tools for communication and information retrieval.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare the model input\n",
    "prompt = \"Give me a short introduction to large language models\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# Decode and display the output\n",
    "output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(f\"MODEL: {model_name}\\n\")\n",
    "print(\"PROMPT TEMPLATE:\")\n",
    "print(text)\n",
    "print(\"ANSWER:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34165896-556a-4b62-9dae-bba11a3b2fd3",
   "metadata": {},
   "source": [
    "## Explore the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41c0e0-9869-4548-8058-fcfbb134660b",
   "metadata": {},
   "source": [
    "A tokenizer converts raw text into smaller units called tokens, which a language model can process. LLMs cannot directly understand characters or words as humans do, so the tokenizer maps text to numeric IDs that the model was trained on. \n",
    "\n",
    "Instead of using full words only, modern tokenizers often use subwords, which are pieces of words (like un, break, ##able) that help handle rare, new, or misspelled terms more efficiently. This allows the model to understand and generate language flexibly, without needing every possible word in its vocabulary. \n",
    "\n",
    "In short, the tokenizer forms the bridge between human text and the model’s internal numeric representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef5266-70b7-4568-9c79-c55b4d532092",
   "metadata": {},
   "source": [
    "### Main characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971239f-4286-425d-9116-9ce96106672b",
   "metadata": {},
   "source": [
    "**1- Tokenizer type**\n",
    "\n",
    "Hugging Face tokenizers can have two implementations: slow (Python) and fast (Rust-backed). Both follow the same conceptual processing pipeline, but fast tokenizers are significantly more efficient and provide richer features (like offset mapping).\n",
    "\n",
    "| Feature                    | Slow Tokenizer (Python) | Fast Tokenizer (Rust)            |\n",
    "| -------------------------- | ----------------------- | -------------------------------- |\n",
    "| Language backend           | Pure Python             | Rust (`tokenizers` library)      |\n",
    "| Speed                      | Slower                  | 10–100× faster                   |\n",
    "| Offset mapping             | ❌ often missing         | ✅ available                      |\n",
    "| Consistency with HF models | Good                    | Best / canonical implementations |\n",
    "| Best for custom logic      | Easier to modify        | More restrictive                 |\n",
    "| Unicode + splitting        | Python-based            | Optimized Rust implementation    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dff3dfc-5c37-43e3-a86d-ee8f4d50c97f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:42:12.719995Z",
     "iopub.status.busy": "2025-11-15T13:42:12.719825Z",
     "iopub.status.idle": "2025-11-15T13:42:12.722883Z",
     "shell.execute_reply": "2025-11-15T13:42:12.722430Z",
     "shell.execute_reply.started": "2025-11-15T13:42:12.719986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-0.6B'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0aa3f0ae-a577-4a56-b8ff-7b6adbee835b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:30:48.817594Z",
     "iopub.status.busy": "2025-11-15T13:30:48.817159Z",
     "iopub.status.idle": "2025-11-15T13:30:48.823465Z",
     "shell.execute_reply": "2025-11-15T13:30:48.822917Z",
     "shell.execute_reply.started": "2025-11-15T13:30:48.817566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1eb66e1b-f43e-4d94-b9ec-130299579a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:34:20.188066Z",
     "iopub.status.busy": "2025-11-15T13:34:20.187920Z",
     "iopub.status.idle": "2025-11-15T13:34:20.192710Z",
     "shell.execute_reply": "2025-11-15T13:34:20.192375Z",
     "shell.execute_reply.started": "2025-11-15T13:34:20.188057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ebea613-720a-440b-b303-e0f09a9a23d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:44:42.563252Z",
     "iopub.status.busy": "2025-11-15T13:44:42.562849Z",
     "iopub.status.idle": "2025-11-15T13:44:42.569078Z",
     "shell.execute_reply": "2025-11-15T13:44:42.568148Z",
     "shell.execute_reply.started": "2025-11-15T13:44:42.563225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.slow_tokenizer_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad48d96-8d43-4746-84b2-892fa0afd821",
   "metadata": {},
   "source": [
    "**2. Tokenization Strategy / Model**\n",
    "\n",
    "Defines how text gets broken into tokens. Common strategies include:\n",
    "- BPE (Byte Pair Encoding) — merges frequent byte or character pairs (e.g., GPT-2, RoBERTa)\n",
    "- WordPiece — uses subwords prefixed with ## for continuation (e.g., BERT)\n",
    "- Unigram / SentencePiece — probabilistic subword model (e.g., T5, ALBERT)\n",
    "- Character/Byte-level — tokens represent characters or raw bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adb3dbea-b65e-417f-ad3a-21ceec969642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:57:35.993559Z",
     "iopub.status.busy": "2025-11-15T13:57:35.993402Z",
     "iopub.status.idle": "2025-11-15T13:57:36.039107Z",
     "shell.execute_reply": "2025-11-15T13:57:36.038754Z",
     "shell.execute_reply.started": "2025-11-15T13:57:35.993549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\", fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab ...'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(tokenizer.backend_tokenizer.model)[:150] + \" ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53842-7900-43f2-be70-ad69e7a382e7",
   "metadata": {},
   "source": [
    "**3. Vocabulary**\n",
    "\n",
    "The set of allowed tokens and their numeric IDs:\n",
    "- Vocabulary size (e.g., 50k tokens)\n",
    "- Special tokens (e.g., <pad>, <s>, </s>, <unk>, <mask>)\n",
    "- Token format (e.g., ##sub in WordPiece, Ġword in BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "701f815a-bb7e-4cff-84ce-ffb5d5ec411a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:45:49.923697Z",
     "iopub.status.busy": "2025-11-15T13:45:49.923264Z",
     "iopub.status.idle": "2025-11-15T13:45:49.929299Z",
     "shell.execute_reply": "2025-11-15T13:45:49.928742Z",
     "shell.execute_reply.started": "2025-11-15T13:45:49.923668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97055092-7e06-453b-b25d-0484612d6887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:26:11.948624Z",
     "iopub.status.busy": "2025-11-15T13:26:11.948201Z",
     "iopub.status.idle": "2025-11-15T13:26:11.953849Z",
     "shell.execute_reply": "2025-11-15T13:26:11.953011Z",
     "shell.execute_reply.started": "2025-11-15T13:26:11.948596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token -> <|im_end|>\n",
      "pad_token -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_name in tokenizer.special_tokens_map.keys():\n",
    "    if token_name != \"additional_special_tokens\":\n",
    "        print(f\"{token_name} -> {tokenizer.special_tokens_map[token_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a56b37b4-f0e1-4d13-a7ee-6442bd33ffc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:53:54.001414Z",
     "iopub.status.busy": "2025-11-15T13:53:54.000959Z",
     "iopub.status.idle": "2025-11-15T13:53:54.008213Z",
     "shell.execute_reply": "2025-11-15T13:53:54.007544Z",
     "shell.execute_reply.started": "2025-11-15T13:53:54.001384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|>',\n",
       " '<|endoftext|>',\n",
       " '<|im_start|>',\n",
       " '<|object_ref_start|>',\n",
       " '<|object_ref_end|>',\n",
       " '<|box_start|>',\n",
       " '<|box_end|>',\n",
       " '<|quad_start|>',\n",
       " '<|quad_end|>',\n",
       " '<|vision_start|>',\n",
       " '<|vision_end|>',\n",
       " '<|vision_pad|>',\n",
       " '<|image_pad|>',\n",
       " '<|video_pad|>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2453e2b0-e5fe-4f19-888d-e4008edd2001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:53:22.936536Z",
     "iopub.status.busy": "2025-11-15T13:53:22.936339Z",
     "iopub.status.idle": "2025-11-15T13:53:22.939896Z",
     "shell.execute_reply": "2025-11-15T13:53:22.939506Z",
     "shell.execute_reply.started": "2025-11-15T13:53:22.936525Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 151643,\n",
       " '<|im_start|>': 151644,\n",
       " '<|im_end|>': 151645,\n",
       " '<|object_ref_start|>': 151646,\n",
       " '<|object_ref_end|>': 151647,\n",
       " '<|box_start|>': 151648,\n",
       " '<|box_end|>': 151649,\n",
       " '<|quad_start|>': 151650,\n",
       " '<|quad_end|>': 151651,\n",
       " '<|vision_start|>': 151652,\n",
       " '<|vision_end|>': 151653,\n",
       " '<|vision_pad|>': 151654,\n",
       " '<|image_pad|>': 151655,\n",
       " '<|video_pad|>': 151656,\n",
       " '<tool_call>': 151657,\n",
       " '</tool_call>': 151658,\n",
       " '<|fim_prefix|>': 151659,\n",
       " '<|fim_middle|>': 151660,\n",
       " '<|fim_suffix|>': 151661,\n",
       " '<|fim_pad|>': 151662,\n",
       " '<|repo_name|>': 151663,\n",
       " '<|file_sep|>': 151664,\n",
       " '<tool_response>': 151665,\n",
       " '</tool_response>': 151666,\n",
       " '<think>': 151667,\n",
       " '</think>': 151668}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5eb39a2-5869-4037-b2b6-33e0649626c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:03:08.686508Z",
     "iopub.status.busy": "2025-11-15T14:03:08.686075Z",
     "iopub.status.idle": "2025-11-15T14:03:08.692106Z",
     "shell.execute_reply": "2025-11-15T14:03:08.691660Z",
     "shell.execute_reply.started": "2025-11-15T14:03:08.686481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĠĠ,ĠĠĠĠ,in,Ġt,ĠĠĠĠĠĠĠĠ,er,ĠĠĠ,on,Ġa,re,at,st,en,or,Ġth,ĊĊ,Ġc,le,Ġs,it,an,ar,al,Ġthe,;Ċ,Ġp,Ġf,ou,Ġ=,is,ĠĠĠĠĠĠĠ,ing,es,Ġw,ion,ed,ic,Ġb,Ġd,et,Ġm,Ġo,ĉĉ,ro,as,el,ct,nd,Ġin,Ġh,ent,id,Ġn,am,ĠĠĠĠĠĠĠĠĠĠĠ,Ġto,Ġre,--,Ġ{,Ġof,om,);Ċ,im,čĊ,Ġ(,il,//,Ġand,ur,se,Ġl,ex,ĠS,ad,Ġ\",ch,ut,if,**,Ġ},em,ol,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,th,)Ċ,Ġ{Ċ,Ġg,ig,iv,,Ċ,ce,od,Ġv,ate,ĠT,ag,ay,Ġ*,ot,us,ĠC,Ġst,ĠI,un,ul,ue,ĠA,ow,Ġ\\',ew,Ġ<,ation,(),Ġfor,ab,ort,um,ame,Ġis,pe,tr,ck,âĢ,Ġy,ist,----,.ĊĊ,he,Ġe,lo,ĠM,Ġbe,ers,Ġon,Ġcon,ap,ub,ĠP,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,ass,int,>Ċ,ly,urn,Ġ$,;ĊĊ,av,port,ir,->,nt,ction,end,Ġde,ith,out,turn,our,ĠĠĠĠĠ,lic,res,pt,==,Ġthis,Ġwh,Ġif,ĠD,ver,age,ĠB,ht,ext,=\",Ġthat,****,ĠR,Ġit,ess,ĠF,Ġr,os,and,Ġas,ect,ke,rom,Ġ//,con,ĠL,(\",qu,lass,Ġwith,iz,de,ĠN,Ġal,op,up,get,Ġ}Ċ,ile,Ġan,ata,ore,ri,Ġpro,;čĊ,ĉĉĉĉ,ter,ain,ĠW,ĠE,Ġcom,Ġreturn,art,ĠH,ack,import,ublic,Ġor,est,ment,ĠG,able,Ġ-,ine,ill,ind,ere,::,ity,Ġ+,Ġtr,elf,ight,(\\',orm,ult,str,..,\",,Ġyou,ype,pl,Ġnew,Ġj,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,Ġfrom,Ġex,ĠO,ld,Ġ[,oc,:Ċ,Ġse'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(tokenizer.convert_ids_to_tokens([id for id in range(256,512)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc313306-ace5-43ba-9b56-505fd99798ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:04:13.261078Z",
     "iopub.status.busy": "2025-11-15T14:04:13.260575Z",
     "iopub.status.idle": "2025-11-15T14:04:13.269249Z",
     "shell.execute_reply": "2025-11-15T14:04:13.268173Z",
     "shell.execute_reply.started": "2025-11-15T14:04:13.261049Z"
    }
   },
   "source": [
    "**4. Language Model interface**\n",
    "\n",
    "- Huggingface model input properties names\n",
    "- Language model maximum sequence length\n",
    "- Chat template that should be applied to the text before tokenization to match the instruct model training format, including special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb8a360e-f370-4934-adf1-1ac7fa8841c0",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-15T13:41:53.455458Z",
     "iopub.status.busy": "2025-11-15T13:41:53.455314Z",
     "iopub.status.idle": "2025-11-15T13:41:53.458310Z",
     "shell.execute_reply": "2025-11-15T13:41:53.457738Z",
     "shell.execute_reply.started": "2025-11-15T13:41:53.455449Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3621fcf-fe61-4b06-b04a-be7bb5ca1f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:42:01.765257Z",
     "iopub.status.busy": "2025-11-15T13:42:01.764840Z",
     "iopub.status.idle": "2025-11-15T13:42:01.770949Z",
     "shell.execute_reply": "2025-11-15T13:42:01.770296Z",
     "shell.execute_reply.started": "2025-11-15T13:42:01.765228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "808a4ea8-3690-4f36-b5fa-623cf4291e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:22:59.025074Z",
     "iopub.status.busy": "2025-11-15T13:22:59.024893Z",
     "iopub.status.idle": "2025-11-15T13:22:59.028192Z",
     "shell.execute_reply": "2025-11-15T13:22:59.027762Z",
     "shell.execute_reply.started": "2025-11-15T13:22:59.025064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```jinja\n",
       "{%- if tools %}\n",
       "    {{- '<|im_start|>system\\n' }}\n",
       "    {%- if messages[0].role == 'system' %}\n",
       "        {{- messages[0].content + '\\n\\n' }}\n",
       "    {%- endif %}\n",
       "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
       "    {%- for tool in tools %}\n",
       "        {{- \"\\n\" }}\n",
       "        {{- tool | tojson }}\n",
       "    {%- endfor %}\n",
       "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
       "{%- else %}\n",
       "    {%- if messages[0].role == 'system' %}\n",
       "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
       "    {%- endif %}\n",
       "{%- endif %}\n",
       "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
       "{%- for message in messages[::-1] %}\n",
       "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
       "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
       "        {%- set ns.multi_step_tool = false %}\n",
       "        {%- set ns.last_query_index = index %}\n",
       "    {%- endif %}\n",
       "{%- endfor %}\n",
       "{%- for message in messages %}\n",
       "    {%- if message.content is string %}\n",
       "        {%- set content = message.content %}\n",
       "    {%- else %}\n",
       "        {%- set content = '' %}\n",
       "    {%- endif %}\n",
       "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
       "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
       "    {%- elif message.role == \"assistant\" %}\n",
       "        {%- set reasoning_content = '' %}\n",
       "        {%- if message.reasoning_content is string %}\n",
       "            {%- set reasoning_content = message.reasoning_content %}\n",
       "        {%- else %}\n",
       "            {%- if '</think>' in content %}\n",
       "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
       "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
       "            {%- endif %}\n",
       "        {%- endif %}\n",
       "        {%- if loop.index0 > ns.last_query_index %}\n",
       "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
       "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
       "            {%- else %}\n",
       "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
       "            {%- endif %}\n",
       "        {%- else %}\n",
       "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
       "        {%- endif %}\n",
       "        {%- if message.tool_calls %}\n",
       "            {%- for tool_call in message.tool_calls %}\n",
       "                {%- if (loop.first and content) or (not loop.first) %}\n",
       "                    {{- '\\n' }}\n",
       "                {%- endif %}\n",
       "                {%- if tool_call.function %}\n",
       "                    {%- set tool_call = tool_call.function %}\n",
       "                {%- endif %}\n",
       "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
       "                {{- tool_call.name }}\n",
       "                {{- '\", \"arguments\": ' }}\n",
       "                {%- if tool_call.arguments is string %}\n",
       "                    {{- tool_call.arguments }}\n",
       "                {%- else %}\n",
       "                    {{- tool_call.arguments | tojson }}\n",
       "                {%- endif %}\n",
       "                {{- '}\\n</tool_call>' }}\n",
       "            {%- endfor %}\n",
       "        {%- endif %}\n",
       "        {{- '<|im_end|>\\n' }}\n",
       "    {%- elif message.role == \"tool\" %}\n",
       "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
       "            {{- '<|im_start|>user' }}\n",
       "        {%- endif %}\n",
       "        {{- '\\n<tool_response>\\n' }}\n",
       "        {{- content }}\n",
       "        {{- '\\n</tool_response>' }}\n",
       "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
       "            {{- '<|im_end|>\\n' }}\n",
       "        {%- endif %}\n",
       "    {%- endif %}\n",
       "{%- endfor %}\n",
       "{%- if add_generation_prompt %}\n",
       "    {{- '<|im_start|>assistant\\n' }}\n",
       "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
       "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
       "    {%- endif %}\n",
       "{%- endif %}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"```jinja\\n{tokenizer.chat_template}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3bbc3-ffea-484e-b328-a84dd255ae9a",
   "metadata": {},
   "source": [
    "### Tokenization Pipeline\n",
    "\n",
    "Regardless of slow or fast implementation, the tokenizer generally performs steps such as:\n",
    "\n",
    "1. Normalization\n",
    "2. Pre-tokenization\n",
    "3. Subword tokenization / Model step\n",
    "4. Post-processing (special tokens, padding/truncation, etc.)\n",
    "5. Conversion to IDs\n",
    "6. Output formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f18dc8-b7ab-4a80-b381-e92a6e800f10",
   "metadata": {},
   "source": [
    "**1️- Normalization**\n",
    "\n",
    "Transforms the input string into a standard form.\n",
    "\n",
    "Common operations:\n",
    "- Lowercasing (if model is uncased)\n",
    "- Unicode normalization (NFD/NFC, etc.)\n",
    "- Stripping accents\n",
    "- Replacing special characters\n",
    "- Handling control characters or whitespace cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68d26fb4-5d61-4976-bda9-cba5f18e5374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:46:50.802245Z",
     "iopub.status.busy": "2025-11-15T12:46:50.802100Z",
     "iopub.status.idle": "2025-11-15T12:46:50.805244Z",
     "shell.execute_reply": "2025-11-15T12:46:50.804912Z",
     "shell.execute_reply.started": "2025-11-15T12:46:50.802237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NFC()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94a764-a325-4db0-8e62-40fec37f8473",
   "metadata": {},
   "source": [
    "**2- Pre-tokenization**\n",
    "\n",
    "Splits text into basic token units before subword encoding.\n",
    "\n",
    "Examples depending on tokenizer:\n",
    "- Whitespace splitting\n",
    "- Punctuation splitting (\"hello,\" → [\"hello\", \",\"])\n",
    "- Byte-level (GPT/BPE), where raw bytes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8123ebbe-b1d7-4e87-ac3c-0a423f6aeb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:49:13.624528Z",
     "iopub.status.busy": "2025-11-15T12:49:13.624106Z",
     "iopub.status.idle": "2025-11-15T12:49:13.628247Z",
     "shell.execute_reply": "2025-11-15T12:49:13.627828Z",
     "shell.execute_reply.started": "2025-11-15T12:49:13.624506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(pretokenizers=[Split(pattern=Regex(\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"), behavior=Isolated, invert=False), ByteLevel(add_prefix_space=False, trim_offsets=False, use_regex=False)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddddb0-5c0f-4e70-9374-8e87eef9c022",
   "metadata": {},
   "source": [
    "**3- Subword Tokenization (Model step)**\n",
    "\n",
    "Applies the vocabulary and subword rules (depending on model type).\n",
    "\n",
    "Examples:\n",
    "- BPE (GPT-2, RoBERTa): Merges frequent byte-pairs into subwords\n",
    "- WordPiece (BERT): Uses \"##\" to join subwords\n",
    "- SentencePiece/Unigram (T5, ALBERT): Probabilistic, language-agnostic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14b848a5-6af5-40ea-b190-6e0d39ba5c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:57:35.993559Z",
     "iopub.status.busy": "2025-11-15T13:57:35.993402Z",
     "iopub.status.idle": "2025-11-15T13:57:36.039107Z",
     "shell.execute_reply": "2025-11-15T13:57:36.038754Z",
     "shell.execute_reply.started": "2025-11-15T13:57:35.993549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\", fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab ...'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(tokenizer.backend_tokenizer.model)[:150] + \" ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc46d8-8f37-493d-8131-e5a749b0ed24",
   "metadata": {},
   "source": [
    "**4- Post-processing**\n",
    "\n",
    "Adds model-specific special tokens.\n",
    "\n",
    "Examples:\n",
    "- BERT: [CLS] tokens [SEP]\n",
    "- GPT-2: no explicit BOS/EOS by default\n",
    "- T5: <pad> token, etc.\n",
    "\n",
    "Other applied options:\n",
    "- Truncation (max length cutting)\n",
    "- Padding (pad to max or dynamic length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce4ff6d-7f83-49da-8c3b-1dc421ff8635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:46:35.550139Z",
     "iopub.status.busy": "2025-11-15T12:46:35.549720Z",
     "iopub.status.idle": "2025-11-15T12:46:35.557391Z",
     "shell.execute_reply": "2025-11-15T12:46:35.556939Z",
     "shell.execute_reply.started": "2025-11-15T12:46:35.550117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteLevel(add_prefix_space=False, trim_offsets=False, use_regex=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc823df-07e3-4bf4-9116-debe12d705fe",
   "metadata": {},
   "source": [
    "**5- Convert Tokens -> IDs**\n",
    "\n",
    "Maps tokens/subwords to integer ids using the vocabulary.\n",
    "\n",
    "Example: [\"[CLS]\", \"hello\", \"world\", \"[SEP]\"] → [101, 7592, 2088, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c42b239a-4ee7-4d66-abaa-d6b6f2a28810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:16:24.829904Z",
     "iopub.status.busy": "2025-11-15T14:16:24.829720Z",
     "iopub.status.idle": "2025-11-15T14:16:24.832666Z",
     "shell.execute_reply": "2025-11-15T14:16:24.832202Z",
     "shell.execute_reply.started": "2025-11-15T14:16:24.829894Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_tokens(text):\n",
    "    enc = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
    "    offsets = enc[\"offset_mapping\"]\n",
    "    \n",
    "    html = \"\"\n",
    "    for tok, (start, end) in zip(tokens, offsets):\n",
    "        part = text[start:end]\n",
    "        html += f\"<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>{tok}</span>\"\n",
    "    \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "932f12de-7085-4f98-98e8-ef3b1abbe1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:16:34.802433Z",
     "iopub.status.busy": "2025-11-15T14:16:34.802277Z",
     "iopub.status.idle": "2025-11-15T14:16:34.806270Z",
     "shell.execute_reply": "2025-11-15T14:16:34.805754Z",
     "shell.execute_reply.started": "2025-11-15T14:16:34.802424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Give</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġme</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġa</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġshort</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġintroduction</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġto</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlarge</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlanguage</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġmodels</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cafbdc2e-7d7f-4eec-9f71-5fb04f69aacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:17:01.262621Z",
     "iopub.status.busy": "2025-11-15T14:17:01.262222Z",
     "iopub.status.idle": "2025-11-15T14:17:01.268920Z",
     "shell.execute_reply": "2025-11-15T14:17:01.268352Z",
     "shell.execute_reply.started": "2025-11-15T14:17:01.262593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Don</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>ne</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>-m</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>oi</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġune</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġintroduction</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġaux</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġgrands</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġmod</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ã¨les</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġde</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlang</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>age</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(\"Donne-moi une introduction aux grands modèles de langage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75f2f56e-e14b-46c0-a354-98abc3a5788b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:18:06.812144Z",
     "iopub.status.busy": "2025-11-15T14:18:06.811313Z",
     "iopub.status.idle": "2025-11-15T14:18:06.817984Z",
     "shell.execute_reply": "2025-11-15T14:18:06.817445Z",
     "shell.execute_reply.started": "2025-11-15T14:18:06.812090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>9</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>8</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>2</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>5</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>4</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>.</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>7</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġ+</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġ</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>4</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>,</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>7</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(\"198254.17 + 14,76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21accf-f4d3-4dd2-95ea-278274419fb6",
   "metadata": {},
   "source": [
    "**6- Format Output**\n",
    "\n",
    "Returns a dictionary like:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'input_ids': [...],\n",
    "  'attention_mask': [...],\n",
    "  'token_type_ids': [...],   # for some models (e.g., BERT)\n",
    "  'offset_mapping': [...]    # fast tokenizers only\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0831d568-8abb-407d-ac32-31e09f06b883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:12:00.388702Z",
     "iopub.status.busy": "2025-11-15T14:12:00.388548Z",
     "iopub.status.idle": "2025-11-15T14:12:00.391947Z",
     "shell.execute_reply": "2025-11-15T14:12:00.391483Z",
     "shell.execute_reply.started": "2025-11-15T14:12:00.388694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [35127, 752, 264, 2805, 16800, 311, 3460, 4128, 4119], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 4), (4, 7), (7, 9), (9, 15), (15, 28), (28, 31), (31, 37), (37, 46), (46, 53)]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompt, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ec8ab-2a52-427c-808a-bd4eaf70be79",
   "metadata": {},
   "source": [
    "## Tokenizer performance in 4 languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1f93f-a29c-4c3e-af23-c381894c1222",
   "metadata": {},
   "source": [
    "Load real world datasets containing common business and finance vocabulary, scraped from the web in 2023-2024 in 4 languages : english, french, german, spanish.\n",
    "\n",
    "https://huggingface.co/datasets/frenchtext/bank-en-2401\n",
    "\n",
    "https://huggingface.co/datasets/frenchtext/banque-fr-2311\n",
    "\n",
    "https://huggingface.co/datasets/frenchtext/bank-de-2401\n",
    "\n",
    "https://huggingface.co/datasets/frenchtext/bank-es-2401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2927c5c-bb82-4414-b190-90997195702a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:19:36.079079Z",
     "iopub.status.busy": "2025-11-16T07:19:36.078590Z",
     "iopub.status.idle": "2025-11-16T07:19:37.081141Z",
     "shell.execute_reply": "2025-11-16T07:19:37.080683Z",
     "shell.execute_reply.started": "2025-11-16T07:19:36.079050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a33d7080bd4efa8f10656086f92496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85677286d922412daf7e401b725c0fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42198cee9cfd4a8fb3a930121aa1c931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb85e8da101f4efb97cf2788fd1b1be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d650c9676e4b738ab5249a79f6b461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88608e3e0c0246b58f17ae2fff9e13e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_en = load_dataset(\"frenchtext/bank-en-2401\",  split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60f2f4c8-5af4-4f6d-8e78-ed01aaf5c4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:20:04.573290Z",
     "iopub.status.busy": "2025-11-16T07:20:04.572998Z",
     "iopub.status.idle": "2025-11-16T07:20:05.567269Z",
     "shell.execute_reply": "2025-11-16T07:20:05.566885Z",
     "shell.execute_reply.started": "2025-11-16T07:20:04.573272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc259608ec54df5b7e38ee9d447bc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c067760ab3c04a1986fa3866a212b428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484013af13424b80a2c3b2820ab7ba0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cfa9dd209b4d6c92125259ecd42f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98e8dbada68427c9e1f7c74542d6c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafe3a38f5b84bce8056a245908e6d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_fr = load_dataset(\"frenchtext/banque-fr-2311\",  split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2181153b-f3dc-4f87-983a-c6b958f79db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:20:21.023260Z",
     "iopub.status.busy": "2025-11-16T07:20:21.023088Z",
     "iopub.status.idle": "2025-11-16T07:20:22.017519Z",
     "shell.execute_reply": "2025-11-16T07:20:22.017217Z",
     "shell.execute_reply.started": "2025-11-16T07:20:21.023251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215df2a6567744209541d11f66c37c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f7aaa8007f418f965f05b89736289a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e290a2fa609d4811a4bfc802f985b3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a53e1924f45d3b2c022e4598d697f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cc476e3bfe4775a949e6509f53f9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f7043ad02845f3903a4b2b9e67ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_de = load_dataset(\"frenchtext/bank-de-2401\",  split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd6342e7-bcf7-4637-8a5f-c679abbc0969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:22:35.910405Z",
     "iopub.status.busy": "2025-11-16T07:22:35.910214Z",
     "iopub.status.idle": "2025-11-16T07:22:36.924822Z",
     "shell.execute_reply": "2025-11-16T07:22:36.924452Z",
     "shell.execute_reply.started": "2025-11-16T07:22:35.910394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6d40ac362e435eac2abece0683fb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc0be8861514e08b0ec91fa4c94e6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aa541bfe654d7c8476dc971b0261cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b56640100c4acba3f62461567ad1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9649a245e34abbb29516430bc4d6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00c2e4fdf1441fa010afd70938e958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_es = load_dataset(\"frenchtext/bank-es-2401\",  split=\"train+valid+test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b83e912c-8682-4054-81a7-d87c88e85575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T07:29:55.190963Z",
     "iopub.status.busy": "2025-11-16T07:29:55.190519Z",
     "iopub.status.idle": "2025-11-16T07:29:55.197932Z",
     "shell.execute_reply": "2025-11-16T07:29:55.197150Z",
     "shell.execute_reply.started": "2025-11-16T07:29:55.190934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Uri': Value('string'),\n",
       " 'ExtractedFromPDF': Value('bool'),\n",
       " 'Timestamp': Value('string'),\n",
       " 'Lang': Value('string'),\n",
       " 'Title': Value('string'),\n",
       " 'Text': Value('string'),\n",
       " 'Words': Value('int32'),\n",
       " 'AvgWordsLength': Value('int32'),\n",
       " 'Chars': Value('int32'),\n",
       " 'LetterChars': Value('int32'),\n",
       " 'NumberChars': Value('int32'),\n",
       " 'OtherChars': Value('int32')}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0af59984-9f19-456f-b993-ee99bdc940f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:53:10.388722Z",
     "iopub.status.busy": "2025-11-16T09:53:10.388406Z",
     "iopub.status.idle": "2025-11-16T09:53:10.396805Z",
     "shell.execute_reply": "2025-11-16T09:53:10.396310Z",
     "shell.execute_reply.started": "2025-11-16T09:53:10.388702Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def tokenize(dataitem):\n",
    "    return tokenizer(dataitem[\"Text\"])\n",
    "\n",
    "def dataset_stats(dataset):    \n",
    "    print(f\"Analyzing dataset {dataset.info.dataset_name}\")\n",
    "    documents_count = len(dataset)\n",
    "    print(f\"- number of documents: {documents_count}\")\n",
    "    print()\n",
    "    print(\"... tokenizing all documents ...\")\n",
    "    dataset.cleanup_cache_files()\n",
    "    start = time.time()\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    end = time.time()\n",
    "    tokenize_duration = end-start\n",
    "    print(f\"--> done in {tokenize_duration:.3f} sec\")\n",
    "    print()\n",
    "    print(\"... computing all statistics ...\")\n",
    "    start = time.time()\n",
    "    dataset.total_tokens_count = 0\n",
    "    dataset.avg_tokens_count = 0\n",
    "    dataset.min_tokens_count = 2**31\n",
    "    dataset.max_tokens_count = 0\n",
    "    dataset.total_words_count = 0\n",
    "    dataset.avg_words_count = 0\n",
    "    dataset.min_words_count = 2**31\n",
    "    dataset.max_words_count = 0\n",
    "    dataset.total_chars_count = 0\n",
    "    dataset.avg_chars_count = 0\n",
    "    dataset.min_chars_count = 2**31\n",
    "    dataset.max_chars_count = 0\n",
    "    for dataitem in dataset:\n",
    "        tokens = len(dataitem['input_ids'])\n",
    "        words = dataitem['Words']\n",
    "        chars = dataitem['Chars'] \n",
    "        dataset.total_tokens_count = dataset.total_tokens_count + tokens\n",
    "        if tokens < dataset.min_tokens_count:\n",
    "            dataset.min_tokens_count = tokens\n",
    "        if tokens > dataset.max_tokens_count:\n",
    "            dataset.max_tokens_count = tokens\n",
    "        dataset.total_words_count = dataset.total_words_count + words\n",
    "        if words < dataset.min_words_count:\n",
    "            dataset.min_words_count = words\n",
    "        if words > dataset.max_words_count:\n",
    "            dataset.max_words_count = words\n",
    "        dataset.total_chars_count = dataset.total_chars_count + chars\n",
    "        if chars < dataset.min_chars_count:\n",
    "            dataset.min_chars_count = chars\n",
    "        if chars > dataset.max_chars_count:\n",
    "            dataset.max_chars_count = chars\n",
    "    dataset.avg_tokens_count = dataset.total_tokens_count / documents_count                                                            \n",
    "    dataset.avg_words_count = dataset.total_words_count / documents_count\n",
    "    dataset.avg_chars_count = dataset.total_chars_count / documents_count\n",
    "    end = time.time()\n",
    "    stats_duration = end-start\n",
    "    print(f\"--> done in {stats_duration:.3f} sec\")\n",
    "    print()\n",
    "    print(f\"- tokens: total={dataset.total_tokens_count:_}, avg={dataset.avg_tokens_count:_.2f}, min={dataset.min_tokens_count}, max={dataset.max_tokens_count:_}\".replace(\"_\",\" \"))\n",
    "    print(f\"- words: total={dataset.total_words_count:_}, avg={dataset.avg_words_count:_.2f}, min={dataset.min_words_count}, max={dataset.max_words_count:_}\".replace(\"_\",\" \"))\n",
    "    print(f\"- chars: total={dataset.total_chars_count:_}, avg={dataset.avg_chars_count:_.2f}, min={dataset.min_chars_count}, max={dataset.max_chars_count:_}\".replace(\"_\",\" \"))\n",
    "    print()\n",
    "    print(f\"- chars per word: {dataset.total_chars_count/dataset.total_words_count:.2f}\")\n",
    "    print(f\"- chars per token: {dataset.total_chars_count/dataset.total_tokens_count:.2f}\")\n",
    "    print()\n",
    "    print(f\"- tokens per word: {dataset.total_tokens_count/dataset.total_words_count:.2f}\")\n",
    "    print(f\"- tokens per second: {dataset.total_tokens_count/tokenize_duration:_.2f}\".replace(\"_\",\" \"))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "59e7586c-cd49-4b39-8851-8e0aecc3aea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:53:13.120294Z",
     "iopub.status.busy": "2025-11-16T09:53:13.120000Z",
     "iopub.status.idle": "2025-11-16T09:54:15.388841Z",
     "shell.execute_reply": "2025-11-16T09:54:15.388329Z",
     "shell.execute_reply.started": "2025-11-16T09:53:13.120270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset bank-en-2401\n",
      "- number of documents: 25585\n",
      "\n",
      "... tokenizing all documents ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce928379e04291bf352a6d26a7ff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> done in 35.439 sec\n",
      "\n",
      "... computing all statistics ...\n",
      "--> done in 26.766 sec\n",
      "\n",
      "- tokens: total=89 411 161, avg=3 494.67, min=0, max=944 733\n",
      "- words: total=57 605 232, avg=2 251.52, min=0, max=609 542\n",
      "- chars: total=367 496 436, avg=14 363.75, min=0, max=3 866 079\n",
      "\n",
      "- chars per word: 6.38\n",
      "- chars per token: 4.11\n",
      "\n",
      "- tokens per word: 1.55\n",
      "- tokens per second: 2 522 947.96\n"
     ]
    }
   ],
   "source": [
    "dataset_en = dataset_stats(dataset_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b6b2d546-b4c2-41a4-9890-15f648df4356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:54:15.389623Z",
     "iopub.status.busy": "2025-11-16T09:54:15.389511Z",
     "iopub.status.idle": "2025-11-16T09:55:37.424741Z",
     "shell.execute_reply": "2025-11-16T09:55:37.424279Z",
     "shell.execute_reply.started": "2025-11-16T09:54:15.389615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset banque-fr-2311\n",
      "- number of documents: 85229\n",
      "\n",
      "... tokenizing all documents ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afe3826a4b64d9f81e3687af5234456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> done in 41.897 sec\n",
      "\n",
      "... computing all statistics ...\n",
      "--> done in 40.069 sec\n",
      "\n",
      "- tokens: total=126 572 248, avg=1 485.08, min=1, max=565 555\n",
      "- words: total=67 061 556, avg=786.84, min=0, max=271 697\n",
      "- chars: total=427 224 314, avg=5 012.66, min=2, max=1 724 199\n",
      "\n",
      "- chars per word: 6.37\n",
      "- chars per token: 3.38\n",
      "\n",
      "- tokens per word: 1.89\n",
      "- tokens per second: 3 021 066.65\n"
     ]
    }
   ],
   "source": [
    "dataset_fr = dataset_stats(dataset_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "17b2e0f8-fc44-4af0-9b30-ac64d0513082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:55:37.425186Z",
     "iopub.status.busy": "2025-11-16T09:55:37.425087Z",
     "iopub.status.idle": "2025-11-16T09:56:30.423223Z",
     "shell.execute_reply": "2025-11-16T09:56:30.422779Z",
     "shell.execute_reply.started": "2025-11-16T09:55:37.425178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset bank-de-2401\n",
      "- number of documents: 29745\n",
      "\n",
      "... tokenizing all documents ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dda3d0f28f466d86a334f87a4e1055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> done in 26.994 sec\n",
      "\n",
      "... computing all statistics ...\n",
      "--> done in 25.957 sec\n",
      "\n",
      "- tokens: total=87 282 651, avg=2 934.36, min=0, max=434 883\n",
      "- words: total=37 262 822, avg=1 252.74, min=0, max=158 911\n",
      "- chars: total=282 155 923, avg=9 485.83, min=0, max=1 281 290\n",
      "\n",
      "- chars per word: 7.57\n",
      "- chars per token: 3.23\n",
      "\n",
      "- tokens per word: 2.34\n",
      "- tokens per second: 3 233 463.64\n"
     ]
    }
   ],
   "source": [
    "dataset_de = dataset_stats(dataset_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "99ee876e-b4e2-4c99-b89c-3023476d8a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:56:30.423954Z",
     "iopub.status.busy": "2025-11-16T09:56:30.423843Z",
     "iopub.status.idle": "2025-11-16T09:57:03.957606Z",
     "shell.execute_reply": "2025-11-16T09:57:03.957137Z",
     "shell.execute_reply.started": "2025-11-16T09:56:30.423946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset bank-es-2401\n",
      "- number of documents: 25455\n",
      "\n",
      "... tokenizing all documents ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a66edad39af4c848da60eb1b4400366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> done in 17.253 sec\n",
      "\n",
      "... computing all statistics ...\n",
      "--> done in 16.248 sec\n",
      "\n",
      "- tokens: total=54 677 880, avg=2 148.02, min=1, max=231 497\n",
      "- words: total=29 180 953, avg=1 146.37, min=0, max=118 731\n",
      "- chars: total=185 519 177, avg=7 288.12, min=2, max=776 947\n",
      "\n",
      "- chars per word: 6.36\n",
      "- chars per token: 3.39\n",
      "\n",
      "- tokens per word: 1.87\n",
      "- tokens per second: 3 169 201.46\n"
     ]
    }
   ],
   "source": [
    "dataset_es = dataset_stats(dataset_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fd0fb6c5-684d-4460-bbe7-b77701214ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T09:57:59.130107Z",
     "iopub.status.busy": "2025-11-16T09:57:59.129766Z",
     "iopub.status.idle": "2025-11-16T09:57:59.139226Z",
     "shell.execute_reply": "2025-11-16T09:57:59.138862Z",
     "shell.execute_reply.started": "2025-11-16T09:57:59.130089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language compression comparison for the tokenizer: Qwen/Qwen3-0.6B\n",
      "- french/english: 21.6 % additional tokens\n",
      "- german/english: 50.9 % additional tokens\n",
      "- spanish/english: 20.7 % additional tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Language compression comparison for the tokenizer: {model_name}\")\n",
    "print(f\"- french/english: {((dataset_fr.total_tokens_count/dataset_fr.total_words_count)/(dataset_en.total_tokens_count/dataset_en.total_words_count)-1)*100:.1f} % additional tokens\")\n",
    "print(f\"- german/english: {((dataset_de.total_tokens_count/dataset_de.total_words_count)/(dataset_en.total_tokens_count/dataset_en.total_words_count)-1)*100:.1f} % additional tokens\")\n",
    "print(f\"- spanish/english: {((dataset_es.total_tokens_count/dataset_es.total_words_count)/(dataset_en.total_tokens_count/dataset_en.total_words_count)-1)*100:.1f} % additional tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa44557-8c50-46c2-b332-9c796556a6d5",
   "metadata": {},
   "source": [
    "## Explore the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbf12f-56b4-46e3-8618-1e0aa9893173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:16:52.352790Z",
     "iopub.status.busy": "2025-11-16T10:16:52.352263Z",
     "iopub.status.idle": "2025-11-16T10:16:52.357924Z",
     "shell.execute_reply": "2025-11-16T10:16:52.356864Z",
     "shell.execute_reply.started": "2025-11-16T10:16:52.352761Z"
    }
   },
   "source": [
    "### Main characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c3fd5850-f9f6-464b-85b1-013e34037392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:54:46.329737Z",
     "iopub.status.busy": "2025-11-16T10:54:46.329307Z",
     "iopub.status.idle": "2025-11-16T10:54:46.336346Z",
     "shell.execute_reply": "2025-11-16T10:54:46.335696Z",
     "shell.execute_reply.started": "2025-11-16T10:54:46.329710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-0.6B', ['Qwen3ForCausalLM'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name_or_path, model.config.architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c3ff82b8-1005-4b12-9bd7-8679f149adb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:57:58.462930Z",
     "iopub.status.busy": "2025-11-16T10:57:58.462524Z",
     "iopub.status.idle": "2025-11-16T10:57:58.469361Z",
     "shell.execute_reply": "2025-11-16T10:57:58.468765Z",
     "shell.execute_reply.started": "2025-11-16T10:57:58.462904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('596 049 920 parameters', torch.bfloat16)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{model.num_parameters():_} parameters\".replace(\"_\",\" \"), model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bb33884c-cd88-4aab-b02c-03cde7c49d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:53:11.730682Z",
     "iopub.status.busy": "2025-11-16T10:53:11.730258Z",
     "iopub.status.idle": "2025-11-16T10:53:11.852996Z",
     "shell.execute_reply": "2025-11-16T10:53:11.852188Z",
     "shell.execute_reply.started": "2025-11-16T10:53:11.730653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Size in memory: 1.110 GB', 'Size on disk: 0.700 GB')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import model_info\n",
    "info = model_info(model_name)\n",
    "f\"Size in memory: {model.get_memory_footprint()/1024**3:.3f} GB\", f\"Size on disk: {info.safetensors.total/1024**3:.3f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f09f69c8-46a9-48a2-9070-cce92f8a39d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:56:48.635137Z",
     "iopub.status.busy": "2025-11-16T10:56:48.634807Z",
     "iopub.status.idle": "2025-11-16T10:56:48.639008Z",
     "shell.execute_reply": "2025-11-16T10:56:48.638503Z",
     "shell.execute_reply.started": "2025-11-16T10:56:48.635125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151936, 131072, True)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab_size, tokenizer.model_max_length, model.config.tie_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "718ac3ee-5d80-45ac-8d2f-a01f5f9899d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:27:47.647913Z",
     "iopub.status.busy": "2025-11-16T10:27:47.647751Z",
     "iopub.status.idle": "2025-11-16T10:27:47.651833Z",
     "shell.execute_reply": "2025-11-16T10:27:47.651506Z",
     "shell.execute_reply.started": "2025-11-16T10:27:47.647904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (embed_tokens): Embedding(151936, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen3DecoderLayer(\n",
       "      (self_attn): Qwen3Attention(\n",
       "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "      )\n",
       "      (mlp): Qwen3MLP(\n",
       "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "  (rotary_emb): Qwen3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "16cbd920-915a-4dc9-80dc-d10e6bd2539b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:36:55.754322Z",
     "iopub.status.busy": "2025-11-16T10:36:55.754089Z",
     "iopub.status.idle": "2025-11-16T10:36:55.757828Z",
     "shell.execute_reply": "2025-11-16T10:36:55.757506Z",
     "shell.execute_reply.started": "2025-11-16T10:36:55.754309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, {'full_attention'}, None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_hidden_layers, set(model.config.layer_types), model.config.sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bb6d4fb3-83cc-4792-a0ec-4b958beca4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:59:24.866031Z",
     "iopub.status.busy": "2025-11-16T10:59:24.865809Z",
     "iopub.status.idle": "2025-11-16T10:59:24.869545Z",
     "shell.execute_reply": "2025-11-16T10:59:24.869045Z",
     "shell.execute_reply.started": "2025-11-16T10:59:24.866019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 3072, 'silu')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size, model.config.intermediate_size, model.config.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b130fda-f6c9-4ed4-932b-1a1444ac2ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:40:18.032125Z",
     "iopub.status.busy": "2025-11-16T10:40:18.031900Z",
     "iopub.status.idle": "2025-11-16T10:40:18.037830Z",
     "shell.execute_reply": "2025-11-16T10:40:18.037476Z",
     "shell.execute_reply.started": "2025-11-16T10:40:18.032110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16, 128)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_key_value_heads, model.config.num_attention_heads, model.config.head_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f22e2-8ef5-4dac-920a-efe75df948f2",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237dd4ec-f5fa-4337-8a53-15b03307457b",
   "metadata": {},
   "source": [
    "You dont' need to understand the details the code below: it is a utility function which displays the details of a Pytroch model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "af13814e-68b8-4df4-9ced-8ef5f5e9fc7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:15.294141Z",
     "iopub.status.busy": "2025-11-16T10:03:15.293520Z",
     "iopub.status.idle": "2025-11-16T10:03:15.303331Z",
     "shell.execute_reply": "2025-11-16T10:03:15.302930Z",
     "shell.execute_reply.started": "2025-11-16T10:03:15.294102Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "88bde8d8-a504-48ec-9af0-ead5eaf620b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T10:03:59.800824Z",
     "iopub.status.busy": "2025-11-16T10:03:59.800647Z",
     "iopub.status.idle": "2025-11-16T10:03:59.810467Z",
     "shell.execute_reply": "2025-11-16T10:03:59.810074Z",
     "shell.execute_reply.started": "2025-11-16T10:03:59.800814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-0.6B architecture description\n",
      "---------------------\n",
      "Qwen3ForCausalLM\n",
      "> submodules\n",
      "- model: Qwen3Model\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#Qwen3Model\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: Qwen3RMSNorm\n",
      "  - rotary_emb: Qwen3RotaryEmbedding\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [151936, 1024] (296.8 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..27: 28X Qwen3DecoderLayer\n",
      "      ---------------------\n",
      "      0..27#Qwen3DecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: Qwen3Attention\n",
      "      - mlp: Qwen3MLP\n",
      "      - input_layernorm: Qwen3RMSNorm\n",
      "      - post_attention_layernorm: Qwen3RMSNorm\n",
      "        ---------------------\n",
      "        self_attn#Qwen3Attention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - q_norm: Qwen3RMSNorm\n",
      "        - k_norm: Qwen3RMSNorm\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [2048, 1024] (4.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 1024] (2.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 1024] (2.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 2048] (4.0 MB)\n",
      "          ---------------------\n",
      "          q_norm#Qwen3RMSNorm\n",
      "          > parameters\n",
      "          - weight: bfloat16 [128] (0.0 MB)\n",
      "          ---------------------\n",
      "          k_norm#Qwen3RMSNorm\n",
      "          > parameters\n",
      "          - weight: bfloat16 [128] (0.0 MB)\n",
      "        ---------------------\n",
      "        mlp#Qwen3MLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [3072, 1024] (6.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [3072, 1024] (6.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 3072] (6.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#Qwen3RMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [1024] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#Qwen3RMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [1024] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#Qwen3RMSNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [1024] (0.0 MB)\n",
      "    ---------------------\n",
      "    rotary_emb#Qwen3RotaryEmbedding\n",
      "    > buffers\n",
      "    - inv_freq: float32 [64] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [151936, 1024] (296.8 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "Qwen3ForCausalLM.forward()\n",
      "---------------------\n",
      "    @can_return_tuple\n",
      "    @auto_docstring\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
      "        **kwargs: Unpack[TransformersKwargs],\n",
      "    ) -> CausalLMOutputWithPast:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM\n",
      "\n",
      "        >>> model = Qwen3ForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\")\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "        outputs: BaseModelOutputWithPast = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs.last_hidden_state\n",
      "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
      "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
      "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Qwen3Model.forward()\n",
      "---------------------\n",
      "    @check_model_inputs\n",
      "    @auto_docstring\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[TransformersKwargs],\n",
      "    ) -> BaseModelOutputWithPast:\n",
      "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
      "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if use_cache and past_key_values is None:\n",
      "            past_key_values = DynamicCache(config=self.config)\n",
      "\n",
      "        if cache_position is None:\n",
      "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
      "            cache_position = torch.arange(\n",
      "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
      "            )\n",
      "\n",
      "        if position_ids is None:\n",
      "            position_ids = cache_position.unsqueeze(0)\n",
      "\n",
      "        # It may already have been prepared by e.g. `generate`\n",
      "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
      "            # Prepare mask arguments\n",
      "            mask_kwargs = {\n",
      "                \"config\": self.config,\n",
      "                \"input_embeds\": inputs_embeds,\n",
      "                \"attention_mask\": attention_mask,\n",
      "                \"cache_position\": cache_position,\n",
      "                \"past_key_values\": past_key_values,\n",
      "                \"position_ids\": position_ids,\n",
      "            }\n",
      "            # Create the masks\n",
      "            causal_mask_mapping = {\n",
      "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
      "            }\n",
      "            # The sliding window alternating layers are not always activated depending on the config\n",
      "            if self.has_sliding_layers:\n",
      "                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        # create position embeddings to be shared across the decoder layers\n",
      "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
      "\n",
      "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
      "            hidden_states = decoder_layer(\n",
      "                hidden_states,\n",
      "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
      "                position_ids=position_ids,\n",
      "                past_key_values=past_key_values,\n",
      "                use_cache=use_cache,\n",
      "                cache_position=cache_position,\n",
      "                position_embeddings=position_embeddings,\n",
      "                **kwargs,\n",
      "            )\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=past_key_values if use_cache else None,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input,\n",
      "            self.weight,\n",
      "            self.padding_idx,\n",
      "            self.max_norm,\n",
      "            self.norm_type,\n",
      "            self.scale_grad_by_freq,\n",
      "            self.sparse,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Qwen3DecoderLayer.forward()\n",
      "---------------------\n",
      "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "        **kwargs: Unpack[TransformersKwargs],\n",
      "    ) -> torch.Tensor:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "        # Self Attention\n",
      "        hidden_states, _ = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            position_embeddings=position_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "        return hidden_states\n",
      "\n",
      "---------------------\n",
      "Qwen3Attention.forward()\n",
      "---------------------\n",
      "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "\n",
      "        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n",
      "        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != \"eager\":\n",
      "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "\n",
      "        attn_output, attn_weights = attention_interface(\n",
      "            self,\n",
      "            query_states,\n",
      "            key_states,\n",
      "            value_states,\n",
      "            attention_mask,\n",
      "            dropout=0.0 if not self.training else self.attention_dropout,\n",
      "            scaling=self.scaling,\n",
      "            sliding_window=self.sliding_window,  # diff with Llama\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return attn_output, attn_weights\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "Qwen3RMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n",
      "---------------------\n",
      "Qwen3MLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        return down_proj\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "Qwen3RotaryEmbedding.forward()\n",
      "---------------------\n",
      "    @torch.no_grad()\n",
      "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
      "    def forward(self, x, position_ids):\n",
      "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
      "        position_ids_expanded = position_ids[:, None, :].float()\n",
      "\n",
      "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
      "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
      "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "            emb = torch.cat((freqs, freqs), dim=-1)\n",
      "            cos = emb.cos() * self.attention_scaling\n",
      "            sin = emb.sin() * self.attention_scaling\n",
      "\n",
      "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} architecture description\")\n",
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d38911-8790-4147-8eb7-e73ef8dc9c88",
   "metadata": {},
   "source": [
    "### Execute the model step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a11872-7d9b-4085-9307-421b475a8e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T21:06:34.787468Z",
     "iopub.status.busy": "2025-11-16T21:06:34.787211Z",
     "iopub.status.idle": "2025-11-16T21:06:34.792053Z",
     "shell.execute_reply": "2025-11-16T21:06:34.791620Z",
     "shell.execute_reply.started": "2025-11-16T21:06:34.787452Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_tokens_probs(prompt, max_new_tokens=50):\n",
    "    # Remove previous hooks on the model to be able to re-execute this cell in case of exception\n",
    "    if 'hook_handles' not in vars():\n",
    "        hook_handles = []\n",
    "    else:\n",
    "        for hook_handle in hook_handles:\n",
    "            hook_handle.remove()\n",
    "            hook_handles = []\n",
    "    \n",
    "    # Functions to decode input and output tokens\n",
    "    def get_input_tokens(inpTensor):\n",
    "        return \"[\" + \"] [\".join([tokenizer.decode(element.item()).replace('\\n',\"\\\\n\") for element in inpTensor]) + \"]\"\n",
    "    \n",
    "    def get_output_tokens(outTensor):\n",
    "        preds = torch.softmax(outTensor.float(), dim=1)\n",
    "        next_token_ids = torch.topk(preds[-1,:], k=5)\n",
    "        return [(tokenizer.decode(token_id), f\"{preds[-1,token_id].item():.3f}\") for token_id in next_token_ids.indices]\n",
    "    \n",
    "    # Display input and output tokens\n",
    "    def print_embed_in(module, input, output):\n",
    "        inpTensor = input[0].squeeze(dim=0)\n",
    "        print(f\">> Input : {get_input_tokens(inpTensor)}\")\n",
    "        \n",
    "    def print_embed_out(module, input, output):\n",
    "        outTensor = output[0]\n",
    "        print(f\">> Output:  {get_output_tokens(outTensor)}\")\n",
    "    \n",
    "    # Register hooks to the input embedding and output lm head modules\n",
    "    hook_handles.append(model.get_input_embeddings().register_forward_hook(print_embed_in))\n",
    "    hook_handles.append(model.get_output_embeddings().register_forward_hook(print_embed_out))\n",
    "    \n",
    "    # Prepare the model input\n",
    "    prompt = prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate an answer with the hooks in place\n",
    "    model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    # Remove the hooks\n",
    "    for hook_handle in hook_handles:\n",
    "        hook_handle.remove()\n",
    "    hook_handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cc72435-256b-419c-a720-2254c2feb048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T21:07:11.694957Z",
     "iopub.status.busy": "2025-11-16T21:07:11.694550Z",
     "iopub.status.idle": "2025-11-16T21:07:12.445627Z",
     "shell.execute_reply": "2025-11-16T21:07:12.445233Z",
     "shell.execute_reply.started": "2025-11-16T21:07:11.694932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Input : [<|im_start|>] [user] [\\n] [Why] [ is] [ the] [ sky] [ blue] [ ?] [<|im_end|>] [\\n] [<|im_start|>] [assistant] [\\n] [<think>] [\\n\\n] [</think>] [\\n\\n]\n",
      ">> Output:  [('The', '1.000'), ('It', '0.000'), ('There', '0.000'), ('Actually', '0.000'), ('Well', '0.000')]\n",
      ">> Input : [The]\n",
      ">> Output:  [(' sky', '0.995'), (' color', '0.003'), (' blue', '0.000'), (' **', '0.000'), (' reason', '0.000')]\n",
      ">> Input : [ sky]\n",
      ">> Output:  [(' appears', '0.892'), (' is', '0.107'), (' appearing', '0.000'), (' looks', '0.000'), (' seems', '0.000')]\n",
      ">> Input : [ appears]\n",
      ">> Output:  [(' blue', '1.000'), (' **', '0.000'), (' to', '0.000'), ('蓝色', '0.000'), (' bl', '0.000')]\n",
      ">> Input : [ blue]\n",
      ">> Output:  [(' because', '0.605'), (' due', '0.324'), (' primarily', '0.044'), (' in', '0.011'), (' for', '0.004')]\n",
      ">> Input : [ because]\n",
      ">> Output:  [(' of', '0.572'), (' it', '0.186'), (' **', '0.164'), (' the', '0.047'), (' sunlight', '0.010')]\n",
      ">> Input : [ of]\n",
      ">> Output:  [(' the', '0.865'), (' **', '0.117'), (' a', '0.007'), (' how', '0.002'), (' Ray', '0.002')]\n",
      ">> Input : [ the]\n",
      ">> Output:  [(' way', '0.752'), (' scattering', '0.131'), (' **', '0.070'), (' interaction', '0.011'), (' presence', '0.006')]\n",
      ">> Input : [ way]\n",
      ">> Output:  [(' light', '0.540'), (' sunlight', '0.371'), (' **', '0.016'), (' the', '0.012'), (' water', '0.009')]\n",
      ">> Input : [ light]\n",
      ">> Output:  [(' interacts', '0.966'), (' is', '0.018'), (' reflects', '0.006'), (' behaves', '0.003'), (' travels', '0.003')]\n",
      ">> Input : [ interacts]\n",
      ">> Output:  [(' with', '1.000'), (' in', '0.000'), (' within', '0.000'), (' and', '0.000'), ('.', '0.000')]\n",
      ">> Input : [ with]\n",
      ">> Output:  [(' the', '0.876'), (' water', '0.063'), (' air', '0.039'), (' Earth', '0.007'), (' molecules', '0.004')]\n",
      ">> Input : [ the]\n",
      ">> Output:  [(' Earth', '0.627'), (' atmosphere', '0.336'), (' molecules', '0.013'), (' water', '0.011'), (' air', '0.007')]\n",
      ">> Input : [ Earth]\n",
      ">> Output:  [(\"'s\", '0.958'), ('’s', '0.042'), (' and', '0.000'), ('.', '0.000'), (',', '0.000')]\n",
      ">> Input : ['s]\n",
      ">> Output:  [(' atmosphere', '0.981'), (' surface', '0.018'), (' air', '0.001'), (' atmospheric', '0.000'), (' water', '0.000')]\n",
      ">> Input : [ atmosphere]\n",
      ">> Output:  [('.', '0.956'), (' and', '0.033'), (',', '0.011'), ('.\\n\\n', '0.000'), (' at', '0.000')]\n",
      ">> Input : [.]\n",
      ">> Output:  [(' Here', '0.786'), (' When', '0.175'), (' Let', '0.027'), (' The', '0.002'), (' Light', '0.002')]\n",
      ">> Input : [ Here]\n",
      ">> Output:  [(\"'s\", '0.974'), ('’s', '0.026'), (' is', '0.000'), (' are', '0.000'), (' goes', '0.000')]\n",
      ">> Input : ['s]\n",
      ">> Output:  [(' a', '0.965'), (' the', '0.014'), (' why', '0.011'), (' how', '0.008'), (' an', '0.001')]\n",
      ">> Input : [ a]\n",
      ">> Output:  [(' simplified', '0.707'), (' simple', '0.123'), (' breakdown', '0.074'), (' concise', '0.045'), (' simpler', '0.021')]\n",
      ">> Input : [ simplified]\n",
      ">> Output:  [(' explanation', '0.930'), (' breakdown', '0.041'), (' version', '0.015'), (' scientific', '0.006'), (' and', '0.003')]\n",
      ">> Input : [ explanation]\n",
      ">> Output:  [(':\\n\\n', '0.989'), (' of', '0.007'), (':', '0.002'), (' to', '0.001'), (' based', '0.001')]\n",
      ">> Input : [:\\n\\n]\n",
      ">> Output:  [('1', '0.965'), ('###', '0.014'), ('-', '0.014'), ('When', '0.005'), ('**', '0.002')]\n",
      ">> Input : [1]\n",
      ">> Output:  [('.', '1.000'), ('️', '0.000'), (')', '0.000'), (':', '0.000'), (' �', '0.000')]\n",
      ">> Input : [.]\n",
      ">> Output:  [(' **', '0.999'), (' When', '0.001'), (' Light', '0.000'), (' The', '0.000'), (' Sun', '0.000')]\n"
     ]
    }
   ],
   "source": [
    "show_tokens_probs(\"Why is the sky blue ?\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba55753b-4fb3-4ba0-862b-d4779a66ff04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T21:07:30.810509Z",
     "iopub.status.busy": "2025-11-16T21:07:30.810328Z",
     "iopub.status.idle": "2025-11-16T21:07:31.629228Z",
     "shell.execute_reply": "2025-11-16T21:07:31.628855Z",
     "shell.execute_reply.started": "2025-11-16T21:07:30.810498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Input : [<|im_start|>] [user] [\\n] [Tell] [ me] [ a] [ funny] [ story] [<|im_end|>] [\\n] [<|im_start|>] [assistant] [\\n] [<think>] [\\n\\n] [</think>] [\\n\\n]\n",
      ">> Output:  [('Sure', '0.886'), ('Ah', '0.039'), ('Certainly', '0.021'), ('Here', '0.011'), ('Oh', '0.009')]\n",
      ">> Input : [Sure]\n",
      ">> Output:  [('!', '0.940'), (',', '0.060'), (' here', '0.000'), ('...', '0.000'), ('!*', '0.000')]\n",
      ">> Input : [!]\n",
      ">> Output:  [(' Here', '0.985'), (' Let', '0.005'), (' I', '0.005'), (' here', '0.002'), (' �', '0.001')]\n",
      ">> Input : [ Here]\n",
      ">> Output:  [(\"'s\", '0.893'), ('’s', '0.107'), (' is', '0.000'), (' it', '0.000'), (' a', '0.000')]\n",
      ">> Input : ['s]\n",
      ">> Output:  [(' a', '1.000'), (' an', '0.000'), (' my', '0.000'), (' one', '0.000'), (' **', '0.000')]\n",
      ">> Input : [ a]\n",
      ">> Output:  [(' funny', '0.818'), (' **', '0.098'), (' fun', '0.025'), (' hilarious', '0.013'), (' light', '0.012')]\n",
      ">> Input : [ funny]\n",
      ">> Output:  [(' story', '0.849'), (' and', '0.115'), (' one', '0.029'), (' tale', '0.002'), (' short', '0.001')]\n",
      ">> Input : [ story]\n",
      ">> Output:  [(' for', '0.926'), (':\\n\\n', '0.041'), (' to', '0.022'), (':', '0.006'), (' I', '0.001')]\n",
      ">> Input : [ for]\n",
      ">> Output:  [(' you', '0.999'), (' your', '0.000'), (' ya', '0.000'), (' a', '0.000'), (' the', '0.000')]\n",
      ">> Input : [ you]\n",
      ">> Output:  [(':\\n\\n', '0.920'), (':', '0.076'), ('...\\n\\n', '0.001'), ('...', '0.001'), (' —', '0.001')]\n",
      ">> Input : [:\\n\\n]\n",
      ">> Output:  [('Once', '0.795'), ('**', '0.095'), ('---\\n\\n', '0.065'), ('One', '0.008'), ('There', '0.007')]\n",
      ">> Input : [Once]\n",
      ">> Output:  [(' upon', '0.969'), (' there', '0.023'), (',', '0.005'), (' on', '0.002'), (' in', '0.000')]\n",
      ">> Input : [ upon]\n",
      ">> Output:  [(' a', '0.998'), (' an', '0.002'), (' time', '0.000'), (' the', '0.000'), (' there', '0.000')]\n",
      ">> Input : [ a]\n",
      ">> Output:  [(' time', '1.000'), (' **', '0.000'), (' midnight', '0.000'), (' fairy', '0.000'), (' lonely', '0.000')]\n",
      ">> Input : [ time]\n",
      ">> Output:  [(',', '0.958'), (' in', '0.042'), (' there', '0.000'), (' on', '0.000'), ('，', '0.000')]\n",
      ">> Input : [,]\n",
      ">> Output:  [(' there', '0.762'), (' in', '0.193'), (' a', '0.033'), (' two', '0.004'), (' three', '0.002')]\n",
      ">> Input : [ there]\n",
      ">> Output:  [(' was', '0.933'), (' lived', '0.060'), (' were', '0.007'), (\"'s\", '0.000'), ('’s', '0.000')]\n",
      ">> Input : [ was]\n",
      ">> Output:  [(' a', '0.996'), (' an', '0.004'), (' this', '0.000'), (' Mr', '0.000'), (' someone', '0.000')]\n",
      ">> Input : [ a]\n",
      ">> Output:  [(' cat', '0.167'), (' man', '0.147'), (' baker', '0.061'), (' character', '0.058'), (' little', '0.054')]\n",
      ">> Input : [ cat]\n",
      ">> Output:  [(' named', '0.937'), (' who', '0.025'), (' and', '0.025'), (' with', '0.007'), (' whose', '0.002')]\n",
      ">> Input : [ named]\n",
      ">> Output:  [(' Wh', '0.343'), (' Max', '0.092'), (' M', '0.041'), (' P', '0.023'), (' Tim', '0.021')]\n",
      ">> Input : [ Wh]\n",
      ">> Output:  [('isk', '0.997'), ('isp', '0.000'), ('im', '0.000'), ('ims', '0.000'), ('opper', '0.000')]\n",
      ">> Input : [isk]\n",
      ">> Output:  [('ers', '0.972'), ('er', '0.026'), ('.', '0.001'), ('ly', '0.000'), (',', '0.000')]\n",
      ">> Input : [ers]\n",
      ">> Output:  [(' who', '0.417'), (' and', '0.287'), ('.', '0.253'), (',', '0.039'), (' from', '0.001')]\n",
      ">> Input : [.]\n",
      ">> Output:  [(' One', '0.697'), (' He', '0.176'), (' Wh', '0.083'), (' Every', '0.016'), (' She', '0.005')]\n"
     ]
    }
   ],
   "source": [
    "show_tokens_probs(\"Tell me a funny story\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814093b-247a-4f6f-b796-b21aa8e41a6b",
   "metadata": {},
   "source": [
    "### Profile the GPU compute and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df086f17-4a75-4028-9abf-37af276572c6",
   "metadata": {},
   "source": [
    "WARNING: for the benchmark below to be meaningful, you should restart the notebook kernel, rexecute only the first cell with to define the model_name, then execute the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f081bb9-bfec-40a9-b300-40c69f544ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T11:35:10.252085Z",
     "iopub.status.busy": "2025-11-16T11:35:10.251877Z",
     "iopub.status.idle": "2025-11-16T11:35:11.295274Z",
     "shell.execute_reply": "2025-11-16T11:35:11.294783Z",
     "shell.execute_reply.started": "2025-11-16T11:35:10.252071Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model):    \n",
    "    model_config_file = cached_file(model.name_or_path, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            total_size += os.path.getsize(full_entry_path)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def get_used_cpu_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    process_memory = process.memory_info().rss\n",
    "    return process_memory\n",
    "\n",
    "def get_used_and_max_gpu_memory():\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    return used_memory,max_used_memory\n",
    "\n",
    "def reset_max_gpu_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbeb334-62fc-4a43-b66d-f04df19e0b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T11:35:11.295915Z",
     "iopub.status.busy": "2025-11-16T11:35:11.295777Z",
     "iopub.status.idle": "2025-11-16T11:35:12.489679Z",
     "shell.execute_reply": "2025-11-16T11:35:12.489218Z",
     "shell.execute_reply.started": "2025-11-16T11:35:11.295907Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000\n",
    "\n",
    "class ModelBenchmark:   \n",
    "    \n",
    "    def __init__(self, pretrained_model_id):\n",
    "        self.pretrained_model_id = pretrained_model_id\n",
    "        self.tokenizer = None \n",
    "        self.model = None\n",
    "        \n",
    "        self.model_path = None\n",
    "        self.model_size_on_disk = 0\n",
    "        self.tokenizer_load_time_ns = 0\n",
    "        self.tokenizer_cpu_memory = 0\n",
    "        self.model_load_time_ns = 0\n",
    "        self.model_cpu_memory = 0\n",
    "        self.model_gpu_memory = 0\n",
    "        self.model_load_max_gpu_memory = 0\n",
    "        \n",
    "    def trace_load_from_cache(self, **kwargs):\n",
    "        cpu_memory_before = get_used_cpu_memory()\n",
    "        gpu_memory_before = get_used_and_max_gpu_memory()[0]\n",
    "        reset_max_gpu_memory()        \n",
    "        time_before = perf_counter_ns()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_tokenizer = get_used_cpu_memory()\n",
    "        time_tokenizer = perf_counter_ns()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.pretrained_model_id, **kwargs)\n",
    "        cpu_memory_model = get_used_cpu_memory()\n",
    "        gpu_memory_model,max_gpu_memory_model = get_used_and_max_gpu_memory()     \n",
    "        time_model = perf_counter_ns()\n",
    "        \n",
    "        self.model_path,self.model_size_on_disk = get_model_path_and_size_on_disk(self.model)\n",
    "        self.tokenizer_load_time_ns = time_tokenizer-time_before\n",
    "        self.tokenizer_cpu_memory = cpu_memory_tokenizer-cpu_memory_before\n",
    "        self.model_load_time_ns = time_model-time_tokenizer\n",
    "        self.model_cpu_memory = cpu_memory_model-cpu_memory_tokenizer\n",
    "        self.model_gpu_memory = gpu_memory_model-gpu_memory_before\n",
    "        self.model_load_max_gpu_memory = max_gpu_memory_model\n",
    "        \n",
    "        print(f\"Model files: {(self.model_size_on_disk/1024/1024/1024):.2f} GB on disk\")\n",
    "        print(\"\"f\"(cache path: {self.model_path})\")\n",
    "        print()\n",
    "        print(f\"Tokenizer load time : {(self.tokenizer_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Tokenizer CPU memory: {(self.tokenizer_cpu_memory/memory_unit_mb):.2f} MB\")\n",
    "        print()\n",
    "        print(f\"Model load time : {(self.model_load_time_ns/time_unit_ms):.2f} ms\")\n",
    "        print(f\"Model CPU memory: {(self.model_cpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Model GPU memory: {(self.model_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print(f\"Max   GPU memory: {(self.model_load_max_gpu_memory/memory_unit_gb):.2f} GB\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1dbbfe-4329-4da5-80c5-a7fd927ac3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T11:35:12.490670Z",
     "iopub.status.busy": "2025-11-16T11:35:12.490246Z",
     "iopub.status.idle": "2025-11-16T11:35:12.493641Z",
     "shell.execute_reply": "2025-11-16T11:35:12.492873Z",
     "shell.execute_reply.started": "2025-11-16T11:35:12.490636Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark = ModelBenchmark(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a2ed84-c0b5-4610-8556-4427a74b3f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T11:35:12.978371Z",
     "iopub.status.busy": "2025-11-16T11:35:12.978166Z",
     "iopub.status.idle": "2025-11-16T11:35:14.529930Z",
     "shell.execute_reply": "2025-11-16T11:35:14.529468Z",
     "shell.execute_reply.started": "2025-11-16T11:35:12.978359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 1.41 GB on disk\n",
      "(cache path: /home/models/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca)\n",
      "\n",
      "Tokenizer load time : 499.47 ms\n",
      "Tokenizer CPU memory: 112.12 MB\n",
      "\n",
      "Model load time : 986.08 ms\n",
      "Model CPU memory: 0.10 GB\n",
      "Model GPU memory: 2.22 GB\n",
      "Max   GPU memory: 2.80 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark.trace_load_from_cache(device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e67955-2d84-4205-801b-f385e2081ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T11:44:50.577187Z",
     "iopub.status.busy": "2025-11-16T11:44:50.576279Z",
     "iopub.status.idle": "2025-11-16T11:44:53.179444Z",
     "shell.execute_reply": "2025-11-16T11:44:53.179021Z",
     "shell.execute_reply.started": "2025-11-16T11:44:50.577123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens: 100\n",
      "Time: 2.2130 sec\n",
      "Tokens/sec: 45.19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"Write a short novel about a large language model\"\n",
    "inputs = benchmark.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Warmup\n",
    "_ = benchmark.model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "# Benchmark\n",
    "max_new_tokens = 100\n",
    "start = time.perf_counter()\n",
    "output = benchmark.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "end = time.perf_counter()\n",
    "\n",
    "# Count only the generated tokens (not including prompt)\n",
    "generated_tokens = output.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "tokens_per_second = generated_tokens / (end - start)\n",
    "\n",
    "print(f\"Generated tokens: {generated_tokens}\")\n",
    "print(f\"Time: {end - start:.4f} sec\")\n",
    "print(f\"Tokens/sec: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3dcdb39-075f-4753-981f-fe5e4a30c03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:37:24.880823Z",
     "iopub.status.busy": "2025-11-16T13:37:24.880659Z",
     "iopub.status.idle": "2025-11-16T13:37:24.887379Z",
     "shell.execute_reply": "2025-11-16T13:37:24.886891Z",
     "shell.execute_reply.started": "2025-11-16T13:37:24.880814Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import ProfilerActivity\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def add_call_stacks(event):\n",
    "    filtered_stack = []\n",
    "    torch_calls = []\n",
    "    for frame in event.stack:\n",
    "        if \"profile_forward\" in frame:\n",
    "            break\n",
    "        elif not frame.startswith(\"<built-in\") and not frame.startswith(\"torch/\"):\n",
    "            function = frame.split(\": \")[1]\n",
    "            if function!=\"_call_impl\":\n",
    "                filtered_stack.append(function)\n",
    "        elif frame.startswith(\"<built-in method\"):\n",
    "            frame_words = frame.split(\" \")\n",
    "            torch_calls.append(frame_words[2])\n",
    "            torch_calls.append(frame_words[4])\n",
    "        elif frame.startswith(\"<built-in function\"):\n",
    "            frame_words = frame.split(\" \")\n",
    "            torch_calls.append(frame_words[2][:-1])\n",
    "    filtered_stack.reverse()    \n",
    "    event.call_stack = \".\".join(filtered_stack)\n",
    "    torch_calls.reverse()\n",
    "    event.torch_stack = \".\".join(torch_calls)\n",
    "\n",
    "def profile_forward(model, coalesce_layers, batch_size=1, seq_length=None, percent_threshold=0.2):\n",
    "    # Execute one forward pass\n",
    "    if seq_length is None: seq_length = model.config.max_position_embeddings\n",
    "    input_ids = torch.randint(low=0, high=32000, size=(batch_size,seq_length), dtype=torch.int64).to(model.device)\n",
    "    attention_mask = torch.ones(batch_size,seq_length).to(model.device)\n",
    "    model.eval()\n",
    "    with torch.profiler.profile(activities=[ProfilerActivity.CPU,ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True, with_flops=True, with_modules=True, experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        with torch.profiler.record_function(\"MODEL INFERENCE\"):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "    # Analyze profiling events\n",
    "    events = prof.events()\n",
    "    coalesced_events = []\n",
    "    first_layer0_event_index = 0\n",
    "    layer0_events_count = 0\n",
    "    layer_index = 0\n",
    "    event_index_in_layer = 0\n",
    "    for event in events:\n",
    "        if event.cpu_parent is not None and event.cpu_parent.id == events[0].id:\n",
    "            add_call_stacks(event)            \n",
    "            key = event.call_stack\n",
    "            start_index = key.find(coalesce_layers)\n",
    "            if start_index >= 0:\n",
    "                dot_index = key.find('.', start_index)\n",
    "                current_layer_index = int(key[start_index+len(coalesce_layers)+1:dot_index])\n",
    "    \n",
    "                if first_layer0_event_index == 0:\n",
    "                    first_layer0_event_index = len(coalesced_events)\n",
    "                if layer0_events_count == 0 and current_layer_index == 1:\n",
    "                    layer0_events_count = len(coalesced_events) - first_layer0_event_index\n",
    "                \n",
    "                if current_layer_index > layer_index:\n",
    "                    layer_index = current_layer_index \n",
    "                    if event_index_in_layer != layer0_events_count:\n",
    "                        print(f\"ERROR at layer {layer_index}: number of events {event_index_in_layer} different of layer 0 events count: {layer0_events_count}\")\n",
    "                        break\n",
    "                    event_index_in_layer = 0                        \n",
    "                \n",
    "                if layer_index == 0:\n",
    "                    event.layers_count = 1\n",
    "                    event.layers_cpu_time = event.cpu_time\n",
    "                    event.layers_cuda_time = event.device_time\n",
    "                    coalesced_events.append(event)\n",
    "                else:\n",
    "                    first_event = coalesced_events[first_layer0_event_index + event_index_in_layer]\n",
    "                    first_event.layers_count += 1\n",
    "                    first_event.layers_cpu_time += event.cpu_time\n",
    "                    first_event.layers_cuda_time += event.device_time    \n",
    "                event_index_in_layer += 1\n",
    "            else:\n",
    "                coalesced_events.append(event)\n",
    "\n",
    "    # Display profiling results\n",
    "    table =  \"| Cuda time (µs) | Cuda time (%) | Calls | Stack | PyTorch | Function |\\n\" \n",
    "    table += \"| -------------- | ------------- | ----- | ----- | ------- | -------- |\\n\" \n",
    "    for event in coalesced_events:\n",
    "        if getattr(event, \"layers_count\", 0) > 0:  \n",
    "            percent_cuda_time = event.layers_cuda_time/events[0].device_time*100\n",
    "            if percent_cuda_time >= percent_threshold:\n",
    "                table += f\"| {int(event.layers_cuda_time)} | {percent_cuda_time:.2f} | {event.layers_count} | {event.call_stack} | {event.torch_stack} | {event.name} |\\n\"\n",
    "        else:\n",
    "            percent_cuda_time = event.device_time/events[0].device_time*100\n",
    "            if percent_cuda_time >= percent_threshold:\n",
    "                table += f\"| {int(event.device_time)} | {percent_cuda_time:.2f} | 1 | {event.call_stack} | {event.torch_stack} | {event.name} |\\n\"\n",
    "    display(Markdown(table.replace(\"__\",\"\\\\_\\\\_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72d53bf0-fdad-4786-bf75-16cb4a168246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:40:45.614791Z",
     "iopub.status.busy": "2025-11-16T13:40:45.614304Z",
     "iopub.status.idle": "2025-11-16T13:40:47.042159Z",
     "shell.execute_reply": "2025-11-16T13:40:47.041581Z",
     "shell.execute_reply.started": "2025-11-16T13:40:45.614763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Cuda time (µs) | Cuda time (%) | Calls | Stack | PyTorch | Function |\n",
       "| -------------- | ------------- | ----- | ----- | ------- | -------- |\n",
       "| 400 | 0.74 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3RMSNorm_0.forward |  | aten::mul |\n",
       "| 4161 | 7.66 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.Linear_0 | linear | aten::linear |\n",
       "| 2325 | 4.28 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.Linear_1 | linear | aten::linear |\n",
       "| 2309 | 4.25 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.Linear_2 | linear | aten::linear |\n",
       "| 313 | 0.58 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.apply_rotary_pos_emb.rotate_half | type.cat | aten::cat |\n",
       "| 1047 | 1.93 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.sdpa_attention_forward | scaled_dot_product_attention | aten::scaled_dot_product_attention |\n",
       "| 4247 | 7.82 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3Attention_0.wrapped_func.forward.Linear_3 | linear | aten::linear |\n",
       "| 396 | 0.73 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3RMSNorm_3.forward |  | aten::mul |\n",
       "| 6136 | 11.30 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3MLP_0.forward.Linear_4 | linear | aten::linear |\n",
       "| 6096 | 11.23 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3MLP_0.forward.Linear_5 | linear | aten::linear |\n",
       "| 6592 | 12.14 | 28 | Qwen3ForCausalLM_0.wrapper.forward.Qwen3Model_0.wrapper.forward.\\_\\_call\\_\\_.Qwen3DecoderLayer_0.wrapped_func.forward.Qwen3MLP_0.forward.Linear_6 | linear | aten::linear |\n",
       "| 14569 | 26.83 | 1 | Qwen3ForCausalLM_0.wrapper.forward.Linear_196 | linear | aten::linear |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile_forward(model, coalesce_layers = \"Qwen3DecoderLayer\", batch_size=1, seq_length=200, percent_threshold=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
