{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c14562f-e893-4edd-a575-4a0c59837251",
   "metadata": {},
   "source": [
    "# Huggingface model exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc1d990-feaf-45f3-8e0d-4eae92b7fa3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:21:28.510639Z",
     "iopub.status.busy": "2025-11-15T12:21:28.510540Z",
     "iopub.status.idle": "2025-11-15T12:21:28.635815Z",
     "shell.execute_reply": "2025-11-15T12:21:28.635375Z",
     "shell.execute_reply.started": "2025-11-15T12:21:28.510630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/jupyterlab/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m258 packages\u001b[0m \u001b[2min 0.57ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m153 packages\u001b[0m \u001b[2min 0.99ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93f81b-a2ff-4269-b617-5893d84250df",
   "metadata": {},
   "source": [
    "## Choose a model you want to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69da06b6-3b2c-48df-9c48-45edd4d1dab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:22:16.399369Z",
     "iopub.status.busy": "2025-11-15T12:22:16.398727Z",
     "iopub.status.idle": "2025-11-15T12:22:16.403442Z",
     "shell.execute_reply": "2025-11-15T12:22:16.402480Z",
     "shell.execute_reply.started": "2025-11-15T12:22:16.399338Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a10e9-3fd6-4f9b-8607-69649fac72c8",
   "metadata": {},
   "source": [
    "## Load the model and generate one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f8ea27-5129-4987-b022-726e1b67e6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:31:44.904660Z",
     "iopub.status.busy": "2025-11-15T12:31:44.904304Z",
     "iopub.status.idle": "2025-11-15T12:31:52.311331Z",
     "shell.execute_reply": "2025-11-15T12:31:52.310895Z",
     "shell.execute_reply.started": "2025-11-15T12:31:44.904636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Qwen/Qwen3-0.6B\n",
      "\n",
      "PROMPT TEMPLATE:\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "ANSWER:\n",
      "<think>\n",
      "Okay, the user is asking for a short introduction to large language models. Let me start by recalling what I know about them. First, I should mention that they are big language models, which are AI systems designed to understand and generate human language.\n",
      "\n",
      "I need to highlight their capabilities, like understanding complex texts and generating creative content. Also, their training data and how they learn from it. Maybe mention the different types, like GPT series or others. Oh, and their applications, like in various fields.\n",
      "\n",
      "Wait, should I include something about their training process? Like how they are trained on massive datasets. Also, their ability to handle multiple languages. Oh, and maybe their use cases in different industries. Let me check if I'm covering all key points without being too technical. Keep it concise but informative. Make sure the introduction flows well, starting with the general definition, then their main features, training methods, and applications.\n",
      "</think>\n",
      "\n",
      "A large language model (LLM) is a type of artificial intelligence designed to understand and generate human language. These models are trained on vast datasets, allowing them to comprehend complex texts, produce creative content, and interact naturally with users. They are widely used across industries for tasks like writing, translation, and content generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare the model input\n",
    "prompt = \"Give me a short introduction to large language models\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# Decode and display the output\n",
    "output = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(f\"MODEL: {model_name}\\n\")\n",
    "print(\"PROMPT TEMPLATE:\")\n",
    "print(text)\n",
    "print(\"ANSWER:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34165896-556a-4b62-9dae-bba11a3b2fd3",
   "metadata": {},
   "source": [
    "## Explore the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971239f-4286-425d-9116-9ce96106672b",
   "metadata": {},
   "source": [
    "A tokenizer converts raw text into smaller units called tokens, which a language model can process. LLMs cannot directly understand characters or words as humans do, so the tokenizer maps text to numeric IDs that the model was trained on. \n",
    "\n",
    "Instead of using full words only, modern tokenizers often use subwords, which are pieces of words (like un, break, ##able) that help handle rare, new, or misspelled terms more efficiently. This allows the model to understand and generate language flexibly, without needing every possible word in its vocabulary. \n",
    "\n",
    "In short, the tokenizer forms the bridge between human text and the model’s internal numeric representation.\n",
    "\n",
    "**Main characteristics**\n",
    "\n",
    "**1- Tokenizer type**\n",
    "\n",
    "Hugging Face tokenizers can have two implementations: slow (Python) and fast (Rust-backed). Both follow the same conceptual processing pipeline, but fast tokenizers are significantly more efficient and provide richer features (like offset mapping).\n",
    "\n",
    "| Feature                    | Slow Tokenizer (Python) | Fast Tokenizer (Rust)            |\n",
    "| -------------------------- | ----------------------- | -------------------------------- |\n",
    "| Language backend           | Pure Python             | Rust (`tokenizers` library)      |\n",
    "| Speed                      | Slower                  | 10–100× faster                   |\n",
    "| Offset mapping             | ❌ often missing         | ✅ available                      |\n",
    "| Consistency with HF models | Good                    | Best / canonical implementations |\n",
    "| Best for custom logic      | Easier to modify        | More restrictive                 |\n",
    "| Unicode + splitting        | Python-based            | Optimized Rust implementation    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dff3dfc-5c37-43e3-a86d-ee8f4d50c97f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:42:12.719995Z",
     "iopub.status.busy": "2025-11-15T13:42:12.719825Z",
     "iopub.status.idle": "2025-11-15T13:42:12.722883Z",
     "shell.execute_reply": "2025-11-15T13:42:12.722430Z",
     "shell.execute_reply.started": "2025-11-15T13:42:12.719986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-0.6B'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0aa3f0ae-a577-4a56-b8ff-7b6adbee835b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:30:48.817594Z",
     "iopub.status.busy": "2025-11-15T13:30:48.817159Z",
     "iopub.status.idle": "2025-11-15T13:30:48.823465Z",
     "shell.execute_reply": "2025-11-15T13:30:48.822917Z",
     "shell.execute_reply.started": "2025-11-15T13:30:48.817566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1eb66e1b-f43e-4d94-b9ec-130299579a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:34:20.188066Z",
     "iopub.status.busy": "2025-11-15T13:34:20.187920Z",
     "iopub.status.idle": "2025-11-15T13:34:20.192710Z",
     "shell.execute_reply": "2025-11-15T13:34:20.192375Z",
     "shell.execute_reply.started": "2025-11-15T13:34:20.188057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ebea613-720a-440b-b303-e0f09a9a23d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:44:42.563252Z",
     "iopub.status.busy": "2025-11-15T13:44:42.562849Z",
     "iopub.status.idle": "2025-11-15T13:44:42.569078Z",
     "shell.execute_reply": "2025-11-15T13:44:42.568148Z",
     "shell.execute_reply.started": "2025-11-15T13:44:42.563225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.slow_tokenizer_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad48d96-8d43-4746-84b2-892fa0afd821",
   "metadata": {},
   "source": [
    "**2. Tokenization Strategy / Model**\n",
    "\n",
    "Defines how text gets broken into tokens. Common strategies include:\n",
    "- BPE (Byte Pair Encoding) — merges frequent byte or character pairs (e.g., GPT-2, RoBERTa)\n",
    "- WordPiece — uses subwords prefixed with ## for continuation (e.g., BERT)\n",
    "- Unigram / SentencePiece — probabilistic subword model (e.g., T5, ALBERT)\n",
    "- Character/Byte-level — tokens represent characters or raw bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adb3dbea-b65e-417f-ad3a-21ceec969642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:57:35.993559Z",
     "iopub.status.busy": "2025-11-15T13:57:35.993402Z",
     "iopub.status.idle": "2025-11-15T13:57:36.039107Z",
     "shell.execute_reply": "2025-11-15T13:57:36.038754Z",
     "shell.execute_reply.started": "2025-11-15T13:57:35.993549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\", fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab ...'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(tokenizer.backend_tokenizer.model)[:150] + \" ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a53842-7900-43f2-be70-ad69e7a382e7",
   "metadata": {},
   "source": [
    "**3. Vocabulary**\n",
    "\n",
    "The set of allowed tokens and their numeric IDs:\n",
    "- Vocabulary size (e.g., 50k tokens)\n",
    "- Special tokens (e.g., <pad>, <s>, </s>, <unk>, <mask>)\n",
    "- Token format (e.g., ##sub in WordPiece, Ġword in BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "701f815a-bb7e-4cff-84ce-ffb5d5ec411a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:45:49.923697Z",
     "iopub.status.busy": "2025-11-15T13:45:49.923264Z",
     "iopub.status.idle": "2025-11-15T13:45:49.929299Z",
     "shell.execute_reply": "2025-11-15T13:45:49.928742Z",
     "shell.execute_reply.started": "2025-11-15T13:45:49.923668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97055092-7e06-453b-b25d-0484612d6887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:26:11.948624Z",
     "iopub.status.busy": "2025-11-15T13:26:11.948201Z",
     "iopub.status.idle": "2025-11-15T13:26:11.953849Z",
     "shell.execute_reply": "2025-11-15T13:26:11.953011Z",
     "shell.execute_reply.started": "2025-11-15T13:26:11.948596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token -> <|im_end|>\n",
      "pad_token -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_name in tokenizer.special_tokens_map.keys():\n",
    "    if token_name != \"additional_special_tokens\":\n",
    "        print(f\"{token_name} -> {tokenizer.special_tokens_map[token_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a56b37b4-f0e1-4d13-a7ee-6442bd33ffc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:53:54.001414Z",
     "iopub.status.busy": "2025-11-15T13:53:54.000959Z",
     "iopub.status.idle": "2025-11-15T13:53:54.008213Z",
     "shell.execute_reply": "2025-11-15T13:53:54.007544Z",
     "shell.execute_reply.started": "2025-11-15T13:53:54.001384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|>',\n",
       " '<|endoftext|>',\n",
       " '<|im_start|>',\n",
       " '<|object_ref_start|>',\n",
       " '<|object_ref_end|>',\n",
       " '<|box_start|>',\n",
       " '<|box_end|>',\n",
       " '<|quad_start|>',\n",
       " '<|quad_end|>',\n",
       " '<|vision_start|>',\n",
       " '<|vision_end|>',\n",
       " '<|vision_pad|>',\n",
       " '<|image_pad|>',\n",
       " '<|video_pad|>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2453e2b0-e5fe-4f19-888d-e4008edd2001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:53:22.936536Z",
     "iopub.status.busy": "2025-11-15T13:53:22.936339Z",
     "iopub.status.idle": "2025-11-15T13:53:22.939896Z",
     "shell.execute_reply": "2025-11-15T13:53:22.939506Z",
     "shell.execute_reply.started": "2025-11-15T13:53:22.936525Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 151643,\n",
       " '<|im_start|>': 151644,\n",
       " '<|im_end|>': 151645,\n",
       " '<|object_ref_start|>': 151646,\n",
       " '<|object_ref_end|>': 151647,\n",
       " '<|box_start|>': 151648,\n",
       " '<|box_end|>': 151649,\n",
       " '<|quad_start|>': 151650,\n",
       " '<|quad_end|>': 151651,\n",
       " '<|vision_start|>': 151652,\n",
       " '<|vision_end|>': 151653,\n",
       " '<|vision_pad|>': 151654,\n",
       " '<|image_pad|>': 151655,\n",
       " '<|video_pad|>': 151656,\n",
       " '<tool_call>': 151657,\n",
       " '</tool_call>': 151658,\n",
       " '<|fim_prefix|>': 151659,\n",
       " '<|fim_middle|>': 151660,\n",
       " '<|fim_suffix|>': 151661,\n",
       " '<|fim_pad|>': 151662,\n",
       " '<|repo_name|>': 151663,\n",
       " '<|file_sep|>': 151664,\n",
       " '<tool_response>': 151665,\n",
       " '</tool_response>': 151666,\n",
       " '<think>': 151667,\n",
       " '</think>': 151668}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5eb39a2-5869-4037-b2b6-33e0649626c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:03:08.686508Z",
     "iopub.status.busy": "2025-11-15T14:03:08.686075Z",
     "iopub.status.idle": "2025-11-15T14:03:08.692106Z",
     "shell.execute_reply": "2025-11-15T14:03:08.691660Z",
     "shell.execute_reply.started": "2025-11-15T14:03:08.686481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĠĠ,ĠĠĠĠ,in,Ġt,ĠĠĠĠĠĠĠĠ,er,ĠĠĠ,on,Ġa,re,at,st,en,or,Ġth,ĊĊ,Ġc,le,Ġs,it,an,ar,al,Ġthe,;Ċ,Ġp,Ġf,ou,Ġ=,is,ĠĠĠĠĠĠĠ,ing,es,Ġw,ion,ed,ic,Ġb,Ġd,et,Ġm,Ġo,ĉĉ,ro,as,el,ct,nd,Ġin,Ġh,ent,id,Ġn,am,ĠĠĠĠĠĠĠĠĠĠĠ,Ġto,Ġre,--,Ġ{,Ġof,om,);Ċ,im,čĊ,Ġ(,il,//,Ġand,ur,se,Ġl,ex,ĠS,ad,Ġ\",ch,ut,if,**,Ġ},em,ol,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,th,)Ċ,Ġ{Ċ,Ġg,ig,iv,,Ċ,ce,od,Ġv,ate,ĠT,ag,ay,Ġ*,ot,us,ĠC,Ġst,ĠI,un,ul,ue,ĠA,ow,Ġ\\',ew,Ġ<,ation,(),Ġfor,ab,ort,um,ame,Ġis,pe,tr,ck,âĢ,Ġy,ist,----,.ĊĊ,he,Ġe,lo,ĠM,Ġbe,ers,Ġon,Ġcon,ap,ub,ĠP,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,ass,int,>Ċ,ly,urn,Ġ$,;ĊĊ,av,port,ir,->,nt,ction,end,Ġde,ith,out,turn,our,ĠĠĠĠĠ,lic,res,pt,==,Ġthis,Ġwh,Ġif,ĠD,ver,age,ĠB,ht,ext,=\",Ġthat,****,ĠR,Ġit,ess,ĠF,Ġr,os,and,Ġas,ect,ke,rom,Ġ//,con,ĠL,(\",qu,lass,Ġwith,iz,de,ĠN,Ġal,op,up,get,Ġ}Ċ,ile,Ġan,ata,ore,ri,Ġpro,;čĊ,ĉĉĉĉ,ter,ain,ĠW,ĠE,Ġcom,Ġreturn,art,ĠH,ack,import,ublic,Ġor,est,ment,ĠG,able,Ġ-,ine,ill,ind,ere,::,ity,Ġ+,Ġtr,elf,ight,(\\',orm,ult,str,..,\",,Ġyou,ype,pl,Ġnew,Ġj,ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ,Ġfrom,Ġex,ĠO,ld,Ġ[,oc,:Ċ,Ġse'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(tokenizer.convert_ids_to_tokens([id for id in range(256,512)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc313306-ace5-43ba-9b56-505fd99798ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:04:13.261078Z",
     "iopub.status.busy": "2025-11-15T14:04:13.260575Z",
     "iopub.status.idle": "2025-11-15T14:04:13.269249Z",
     "shell.execute_reply": "2025-11-15T14:04:13.268173Z",
     "shell.execute_reply.started": "2025-11-15T14:04:13.261049Z"
    }
   },
   "source": [
    "**4. Language Model interface**\n",
    "\n",
    "- Huggingface model input properties names\n",
    "- Language model maximum sequence length\n",
    "- Chat template that should be applied to the text before tokenization to match the instruct model training format, including special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb8a360e-f370-4934-adf1-1ac7fa8841c0",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-15T13:41:53.455458Z",
     "iopub.status.busy": "2025-11-15T13:41:53.455314Z",
     "iopub.status.idle": "2025-11-15T13:41:53.458310Z",
     "shell.execute_reply": "2025-11-15T13:41:53.457738Z",
     "shell.execute_reply.started": "2025-11-15T13:41:53.455449Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3621fcf-fe61-4b06-b04a-be7bb5ca1f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:42:01.765257Z",
     "iopub.status.busy": "2025-11-15T13:42:01.764840Z",
     "iopub.status.idle": "2025-11-15T13:42:01.770949Z",
     "shell.execute_reply": "2025-11-15T13:42:01.770296Z",
     "shell.execute_reply.started": "2025-11-15T13:42:01.765228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "808a4ea8-3690-4f36-b5fa-623cf4291e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:22:59.025074Z",
     "iopub.status.busy": "2025-11-15T13:22:59.024893Z",
     "iopub.status.idle": "2025-11-15T13:22:59.028192Z",
     "shell.execute_reply": "2025-11-15T13:22:59.027762Z",
     "shell.execute_reply.started": "2025-11-15T13:22:59.025064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```jinja\n",
       "{%- if tools %}\n",
       "    {{- '<|im_start|>system\\n' }}\n",
       "    {%- if messages[0].role == 'system' %}\n",
       "        {{- messages[0].content + '\\n\\n' }}\n",
       "    {%- endif %}\n",
       "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
       "    {%- for tool in tools %}\n",
       "        {{- \"\\n\" }}\n",
       "        {{- tool | tojson }}\n",
       "    {%- endfor %}\n",
       "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
       "{%- else %}\n",
       "    {%- if messages[0].role == 'system' %}\n",
       "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
       "    {%- endif %}\n",
       "{%- endif %}\n",
       "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
       "{%- for message in messages[::-1] %}\n",
       "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
       "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
       "        {%- set ns.multi_step_tool = false %}\n",
       "        {%- set ns.last_query_index = index %}\n",
       "    {%- endif %}\n",
       "{%- endfor %}\n",
       "{%- for message in messages %}\n",
       "    {%- if message.content is string %}\n",
       "        {%- set content = message.content %}\n",
       "    {%- else %}\n",
       "        {%- set content = '' %}\n",
       "    {%- endif %}\n",
       "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
       "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
       "    {%- elif message.role == \"assistant\" %}\n",
       "        {%- set reasoning_content = '' %}\n",
       "        {%- if message.reasoning_content is string %}\n",
       "            {%- set reasoning_content = message.reasoning_content %}\n",
       "        {%- else %}\n",
       "            {%- if '</think>' in content %}\n",
       "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
       "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
       "            {%- endif %}\n",
       "        {%- endif %}\n",
       "        {%- if loop.index0 > ns.last_query_index %}\n",
       "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
       "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
       "            {%- else %}\n",
       "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
       "            {%- endif %}\n",
       "        {%- else %}\n",
       "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
       "        {%- endif %}\n",
       "        {%- if message.tool_calls %}\n",
       "            {%- for tool_call in message.tool_calls %}\n",
       "                {%- if (loop.first and content) or (not loop.first) %}\n",
       "                    {{- '\\n' }}\n",
       "                {%- endif %}\n",
       "                {%- if tool_call.function %}\n",
       "                    {%- set tool_call = tool_call.function %}\n",
       "                {%- endif %}\n",
       "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
       "                {{- tool_call.name }}\n",
       "                {{- '\", \"arguments\": ' }}\n",
       "                {%- if tool_call.arguments is string %}\n",
       "                    {{- tool_call.arguments }}\n",
       "                {%- else %}\n",
       "                    {{- tool_call.arguments | tojson }}\n",
       "                {%- endif %}\n",
       "                {{- '}\\n</tool_call>' }}\n",
       "            {%- endfor %}\n",
       "        {%- endif %}\n",
       "        {{- '<|im_end|>\\n' }}\n",
       "    {%- elif message.role == \"tool\" %}\n",
       "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
       "            {{- '<|im_start|>user' }}\n",
       "        {%- endif %}\n",
       "        {{- '\\n<tool_response>\\n' }}\n",
       "        {{- content }}\n",
       "        {{- '\\n</tool_response>' }}\n",
       "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
       "            {{- '<|im_end|>\\n' }}\n",
       "        {%- endif %}\n",
       "    {%- endif %}\n",
       "{%- endfor %}\n",
       "{%- if add_generation_prompt %}\n",
       "    {{- '<|im_start|>assistant\\n' }}\n",
       "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
       "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
       "    {%- endif %}\n",
       "{%- endif %}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"```jinja\\n{tokenizer.chat_template}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f18dc8-b7ab-4a80-b381-e92a6e800f10",
   "metadata": {},
   "source": [
    "**Overall Tokenization Pipeline**\n",
    "\n",
    "Regardless of slow or fast implementation, the tokenizer generally performs steps such as:\n",
    "\n",
    "1. Normalization\n",
    "2. Pre-tokenization\n",
    "3. Subword tokenization / Model step\n",
    "4. Post-processing (special tokens, padding/truncation, etc.)\n",
    "5. Conversion to IDs\n",
    "6. Output formatting\n",
    "\n",
    "**1️- Normalization**\n",
    "\n",
    "Transforms the input string into a standard form.\n",
    "\n",
    "Common operations:\n",
    "- Lowercasing (if model is uncased)\n",
    "- Unicode normalization (NFD/NFC, etc.)\n",
    "- Stripping accents\n",
    "- Replacing special characters\n",
    "- Handling control characters or whitespace cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68d26fb4-5d61-4976-bda9-cba5f18e5374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:46:50.802245Z",
     "iopub.status.busy": "2025-11-15T12:46:50.802100Z",
     "iopub.status.idle": "2025-11-15T12:46:50.805244Z",
     "shell.execute_reply": "2025-11-15T12:46:50.804912Z",
     "shell.execute_reply.started": "2025-11-15T12:46:50.802237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NFC()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94a764-a325-4db0-8e62-40fec37f8473",
   "metadata": {},
   "source": [
    "**2- Pre-tokenization**\n",
    "\n",
    "Splits text into basic token units before subword encoding.\n",
    "\n",
    "Examples depending on tokenizer:\n",
    "- Whitespace splitting\n",
    "- Punctuation splitting (\"hello,\" → [\"hello\", \",\"])\n",
    "- Byte-level (GPT/BPE), where raw bytes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8123ebbe-b1d7-4e87-ac3c-0a423f6aeb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:49:13.624528Z",
     "iopub.status.busy": "2025-11-15T12:49:13.624106Z",
     "iopub.status.idle": "2025-11-15T12:49:13.628247Z",
     "shell.execute_reply": "2025-11-15T12:49:13.627828Z",
     "shell.execute_reply.started": "2025-11-15T12:49:13.624506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(pretokenizers=[Split(pattern=Regex(\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"), behavior=Isolated, invert=False), ByteLevel(add_prefix_space=False, trim_offsets=False, use_regex=False)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ddddb0-5c0f-4e70-9374-8e87eef9c022",
   "metadata": {},
   "source": [
    "**3- Subword Tokenization (Model step)**\n",
    "\n",
    "Applies the vocabulary and subword rules (depending on model type).\n",
    "\n",
    "Examples:\n",
    "- BPE (GPT-2, RoBERTa): Merges frequent byte-pairs into subwords\n",
    "- WordPiece (BERT): Uses \"##\" to join subwords\n",
    "- SentencePiece/Unigram (T5, ALBERT): Probabilistic, language-agnostic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14b848a5-6af5-40ea-b190-6e0d39ba5c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T13:57:35.993559Z",
     "iopub.status.busy": "2025-11-15T13:57:35.993402Z",
     "iopub.status.idle": "2025-11-15T13:57:36.039107Z",
     "shell.execute_reply": "2025-11-15T13:57:36.038754Z",
     "shell.execute_reply.started": "2025-11-15T13:57:35.993549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\", fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab ...'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(tokenizer.backend_tokenizer.model)[:150] + \" ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc46d8-8f37-493d-8131-e5a749b0ed24",
   "metadata": {},
   "source": [
    "**4- Post-processing**\n",
    "\n",
    "Adds model-specific special tokens.\n",
    "\n",
    "Examples:\n",
    "- BERT: [CLS] tokens [SEP]\n",
    "- GPT-2: no explicit BOS/EOS by default\n",
    "- T5: <pad> token, etc.\n",
    "\n",
    "Other applied options:\n",
    "- Truncation (max length cutting)\n",
    "- Padding (pad to max or dynamic length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce4ff6d-7f83-49da-8c3b-1dc421ff8635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T12:46:35.550139Z",
     "iopub.status.busy": "2025-11-15T12:46:35.549720Z",
     "iopub.status.idle": "2025-11-15T12:46:35.557391Z",
     "shell.execute_reply": "2025-11-15T12:46:35.556939Z",
     "shell.execute_reply.started": "2025-11-15T12:46:35.550117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteLevel(add_prefix_space=False, trim_offsets=False, use_regex=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc823df-07e3-4bf4-9116-debe12d705fe",
   "metadata": {},
   "source": [
    "**5- Convert Tokens -> IDs**\n",
    "\n",
    "Maps tokens/subwords to integer ids using the vocabulary.\n",
    "\n",
    "Example: [\"[CLS]\", \"hello\", \"world\", \"[SEP]\"] → [101, 7592, 2088, 102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c42b239a-4ee7-4d66-abaa-d6b6f2a28810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:16:24.829904Z",
     "iopub.status.busy": "2025-11-15T14:16:24.829720Z",
     "iopub.status.idle": "2025-11-15T14:16:24.832666Z",
     "shell.execute_reply": "2025-11-15T14:16:24.832202Z",
     "shell.execute_reply.started": "2025-11-15T14:16:24.829894Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_tokens(text):\n",
    "    enc = tokenizer(text, return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
    "    offsets = enc[\"offset_mapping\"]\n",
    "    \n",
    "    html = \"\"\n",
    "    for tok, (start, end) in zip(tokens, offsets):\n",
    "        part = text[start:end]\n",
    "        html += f\"<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>{tok}</span>\"\n",
    "    \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "932f12de-7085-4f98-98e8-ef3b1abbe1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:16:34.802433Z",
     "iopub.status.busy": "2025-11-15T14:16:34.802277Z",
     "iopub.status.idle": "2025-11-15T14:16:34.806270Z",
     "shell.execute_reply": "2025-11-15T14:16:34.805754Z",
     "shell.execute_reply.started": "2025-11-15T14:16:34.802424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Give</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġme</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġa</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġshort</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġintroduction</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġto</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlarge</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlanguage</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġmodels</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cafbdc2e-7d7f-4eec-9f71-5fb04f69aacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:17:01.262621Z",
     "iopub.status.busy": "2025-11-15T14:17:01.262222Z",
     "iopub.status.idle": "2025-11-15T14:17:01.268920Z",
     "shell.execute_reply": "2025-11-15T14:17:01.268352Z",
     "shell.execute_reply.started": "2025-11-15T14:17:01.262593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Don</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>ne</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>-m</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>oi</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġune</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġintroduction</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġaux</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġgrands</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġmod</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ã¨les</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġde</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġlang</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>age</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(\"Donne-moi une introduction aux grands modèles de langage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75f2f56e-e14b-46c0-a354-98abc3a5788b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:18:06.812144Z",
     "iopub.status.busy": "2025-11-15T14:18:06.811313Z",
     "iopub.status.idle": "2025-11-15T14:18:06.817984Z",
     "shell.execute_reply": "2025-11-15T14:18:06.817445Z",
     "shell.execute_reply.started": "2025-11-15T14:18:06.812090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>9</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>8</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>2</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>5</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>4</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>.</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>7</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġ+</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>Ġ</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>1</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>4</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>,</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>7</span><span style='background:#cce5ff; padding:2px; margin:2px; border-radius:3px;'>6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tokens(\"198254.17 + 14,76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21accf-f4d3-4dd2-95ea-278274419fb6",
   "metadata": {},
   "source": [
    "**6- Format Output**\n",
    "\n",
    "Returns a dictionary like:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'input_ids': [...],\n",
    "  'attention_mask': [...],\n",
    "  'token_type_ids': [...],   # for some models (e.g., BERT)\n",
    "  'offset_mapping': [...]    # fast tokenizers only\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0831d568-8abb-407d-ac32-31e09f06b883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T14:12:00.388702Z",
     "iopub.status.busy": "2025-11-15T14:12:00.388548Z",
     "iopub.status.idle": "2025-11-15T14:12:00.391947Z",
     "shell.execute_reply": "2025-11-15T14:12:00.391483Z",
     "shell.execute_reply.started": "2025-11-15T14:12:00.388694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [35127, 752, 264, 2805, 16800, 311, 3460, 4128, 4119], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 4), (4, 7), (7, 9), (9, 15), (15, 28), (28, 31), (31, 37), (37, 46), (46, 53)]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompt, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ec8ab-2a52-427c-808a-bd4eaf70be79",
   "metadata": {},
   "source": [
    "## Measure the tokenizer performance in 4 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2927c5c-bb82-4414-b190-90997195702a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
