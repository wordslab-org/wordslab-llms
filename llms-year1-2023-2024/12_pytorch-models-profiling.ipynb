{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39fda68-aa5a-4224-b27f-a6b149dbbc48",
   "metadata": {},
   "source": [
    "# Pytorch models profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5519d1-ba59-4ea4-a9aa-f3b9e23bc41f",
   "metadata": {},
   "source": [
    "## Native pytorch profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9ecc80-06bf-4459-920c-c901d42809f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:44:20.359356Z",
     "iopub.status.busy": "2024-02-24T17:44:20.355357Z",
     "iopub.status.idle": "2024-02-24T17:44:20.391673Z",
     "shell.execute_reply": "2024-02-24T17:44:20.390651Z",
     "shell.execute_reply.started": "2024-02-24T17:44:20.359356Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae1024-9758-4eff-bfe7-a8726d51e488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d41de-2ab7-4f0f-ae9b-be9b09e9e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217b137-498d-4c9d-b9c0-2c19abf26492",
   "metadata": {},
   "source": [
    "**Pytorch profiler Implementation**\n",
    "\n",
    "https://github.com/pytorch/pytorch/blob/487ebcac3bc10b4b4b0631dafe2a12ddb0852f2a/torch/csrc/profiler/python/init.cpp\n",
    "\n",
    "https://github.com/pytorch/kineto/tree/main/libkineto/src\n",
    "\n",
    "**Profiler log level**\n",
    "\n",
    "  VERBOSE = 0,\n",
    "  INFO = 1,\n",
    "  WARNING = 2,\n",
    "  ERROR = 3,\n",
    "  STAGE = 4,\n",
    "  ENUM_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1db81e-e4ad-4168-b5fa-15e84b838d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T06:06:36.357725Z",
     "iopub.status.busy": "2024-02-25T06:06:36.356723Z",
     "iopub.status.idle": "2024-02-25T06:06:36.362362Z",
     "shell.execute_reply": "2024-02-25T06:06:36.362362Z",
     "shell.execute_reply.started": "2024-02-25T06:06:36.357725Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KINETO_LOG_LEVEL\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248dfee8-5446-43eb-80fd-9b627afe7962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T06:06:40.703118Z",
     "iopub.status.busy": "2024-02-25T06:06:40.702118Z",
     "iopub.status.idle": "2024-02-25T06:06:49.921509Z",
     "shell.execute_reply": "2024-02-25T06:06:49.921509Z",
     "shell.execute_reply.started": "2024-02-25T06:06:40.703118Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8b107be1fb43ffb2fc2117aa8c97ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1ed36c2d994e019f212ecb99bfc761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\miniconda3\\workspace\\wordslab-llms\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laure\\miniconda3\\workspace\\wordslab-llms\\models\\huggingface\\hub\\models--croissantllm--CroissantLLMBase. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"croissantllm/CroissantLLMBase\", use_safetensors=False, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799f1fb7-b75d-421d-bc95-6bb9f0537828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T06:06:58.258489Z",
     "iopub.status.busy": "2024-02-25T06:06:58.257488Z",
     "iopub.status.idle": "2024-02-25T06:07:10.392160Z",
     "shell.execute_reply": "2024-02-25T06:07:10.392160Z",
     "shell.execute_reply.started": "2024-02-25T06:06:58.258489Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "              model_inference         3.76%     208.695ms       100.00%        5.550s        5.550s      17.441ms         0.16%       10.949s       10.949s             1  \n",
      "              aten::embedding         0.02%       1.310ms         1.29%      71.408ms      71.408ms     951.000us         0.01%      98.468ms      98.468ms             1  \n",
      "                aten::reshape         0.08%       4.467ms         0.11%       6.235ms      25.764us       2.578ms         0.02%      15.751ms      65.087us           242  \n",
      "                   aten::view         0.04%       2.011ms         0.04%       2.011ms       6.384us      13.420ms         0.12%      13.420ms      42.603us           315  \n",
      "           aten::index_select         1.23%      68.293ms         1.25%      69.314ms      69.314ms      80.560ms         0.74%      96.860ms      96.860ms             1  \n",
      "                  aten::empty         0.01%     407.000us         0.01%     407.000us       8.140us     493.000us         0.00%     493.000us       9.860us            50  \n",
      "                aten::resize_         0.02%     905.000us         0.02%     905.000us     452.500us      16.177ms         0.15%      16.177ms       8.088ms             2  \n",
      "                 aten::arange         0.49%      27.072ms         0.97%      53.811ms      26.905ms     262.000us         0.00%     516.000us     258.000us             2  \n",
      "              aten::unsqueeze         0.59%      32.891ms         0.60%      33.257ms     274.851us       2.921ms         0.03%       3.960ms      32.727us           121  \n",
      "             aten::as_strided         0.04%       2.003ms         0.04%       2.003ms       2.866us       6.430ms         0.06%       6.430ms       9.199us           699  \n",
      "                     aten::eq         0.23%      12.501ms         0.23%      12.501ms      12.501ms      12.126ms         0.11%      12.126ms      12.126ms             1  \n",
      "                    aten::any         0.26%      14.430ms         0.26%      14.433ms      14.433ms      13.972ms         0.13%      13.975ms      13.975ms             1  \n",
      "                   aten::item         0.01%     296.000us         0.02%     934.000us     934.000us      58.000us         0.00%     706.000us     706.000us             1  \n",
      "    aten::_local_scalar_dense         0.01%     638.000us         0.01%     638.000us     638.000us     648.000us         0.01%     648.000us     648.000us             1  \n",
      "                     aten::to         2.44%     135.357ms         8.85%     490.887ms       2.012ms       1.976ms         0.02%        4.787s      19.618ms           244  \n",
      "               aten::_to_copy         1.50%      83.380ms         6.41%     355.530ms       2.079ms       2.643ms         0.02%        4.785s      27.981ms           171  \n",
      "          aten::empty_strided         4.27%     237.063ms         4.27%     237.063ms       1.216ms        3.774s        34.47%        3.774s      19.355ms           195  \n",
      "                  aten::copy_         0.64%      35.439ms         0.64%      35.439ms     207.246us        1.008s         9.21%        1.008s       5.895ms           171  \n",
      "                    aten::pow         1.56%      86.464ms         1.56%      86.496ms       1.765ms     155.945ms         1.42%     156.120ms       3.186ms            49  \n",
      "            aten::result_type         0.00%      26.000us         0.00%      26.000us       0.531us      92.000us         0.00%      92.000us       1.878us            49  \n",
      "                   aten::mean         0.95%      52.834ms         0.95%      52.834ms       1.078ms      70.338ms         0.64%      70.338ms       1.435ms            49  \n",
      "                    aten::add         0.98%      54.322ms         0.98%      54.322ms     374.634us     145.252ms         1.33%     145.252ms       1.002ms           145  \n",
      "                  aten::rsqrt         0.33%      18.534ms         0.33%      18.534ms     378.245us      17.429ms         0.16%      17.429ms     355.694us            49  \n",
      "                    aten::mul         0.32%      18.008ms         0.32%      18.008ms      82.606us     372.524ms         3.40%     372.524ms       1.709ms           218  \n",
      "                 aten::linear         9.19%     510.037ms        25.51%        1.416s       8.378ms       1.117ms         0.01%        4.870s      28.819ms           169  \n",
      "                      aten::t         0.09%       4.956ms         0.98%      54.516ms     322.580us       9.666ms         0.09%      10.584ms      62.627us           169  \n",
      "              aten::transpose         6.94%     385.189ms         6.95%     385.912ms       1.145ms       1.949ms         0.02%       6.058ms      17.976us           337  \n",
      "                 aten::matmul         2.73%     151.735ms        28.14%        1.562s       8.092ms       4.006ms         0.04%        4.864s      25.202ms           193  \n",
      "                     aten::mm        12.61%     699.991ms        12.61%     699.991ms       4.142ms        4.842s        44.23%        4.842s      28.653ms           169  \n",
      "           aten::_unsafe_view         0.01%     773.000us         0.01%     773.000us       4.005us     523.000us         0.00%     523.000us       2.710us           193  \n",
      "                  aten::slice        28.65%        1.590s        28.66%        1.591s       9.468ms       1.377ms         0.01%       2.119ms      12.613us           168  \n",
      "                 aten::expand        11.95%     663.241ms        11.95%     663.354ms       9.213ms       1.431ms         0.01%       1.968ms      27.333us            72  \n",
      "                    aten::bmm         0.75%      41.837ms         0.75%      41.837ms       1.743ms     769.000us         0.01%     769.000us      32.042us            24  \n",
      "                    aten::cat         0.24%      13.406ms         0.24%      13.406ms     186.194us      98.474ms         0.90%      98.474ms       1.368ms            72  \n",
      "                    aten::cos         0.07%       4.044ms         0.07%       4.044ms     168.500us       3.764ms         0.03%       3.764ms     156.833us            24  \n",
      "                    aten::sin         0.07%       3.649ms         0.07%       3.649ms     152.042us       3.565ms         0.03%       3.565ms     148.542us            24  \n",
      "                    aten::neg         5.19%     288.084ms         5.19%     288.084ms       6.002ms      28.745ms         0.26%      28.745ms     598.854us            48  \n",
      "                FlashAttnFunc         0.53%      29.160ms         0.54%      30.216ms       1.259ms     169.240ms         1.55%     170.217ms       7.092ms            24  \n",
      "             aten::empty_like         0.01%     444.000us         0.01%     796.000us      33.167us     468.000us         0.00%     614.000us      25.583us            24  \n",
      "                   aten::silu         1.19%      65.987ms         1.19%      65.987ms       2.749ms      65.303ms         0.60%      65.303ms       2.721ms            24  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.550s\n",
      "Self CUDA time total: 10.949s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "input_ids = torch.randint(low=0, high=32000, size=(50,1024), dtype=torch.int64).to(model.device)\n",
    "attention_mask = torch.ones(50,1024).to(model.device)\n",
    "model.eval()\n",
    "\n",
    "with profile(\n",
    "  activities=[ProfilerActivity.CPU,ProfilerActivity.CUDA],\n",
    "  with_stack=True,\n",
    "  experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False) \n",
    "\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cab5a-333f-4248-b5c0-654aaacffee0",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bcef82-e07e-49c6-962f-76d1d14fd300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:11:48.629446Z",
     "iopub.status.busy": "2024-02-25T07:11:48.629446Z",
     "iopub.status.idle": "2024-02-25T07:11:48.662137Z",
     "shell.execute_reply": "2024-02-25T07:11:48.661123Z",
     "shell.execute_reply.started": "2024-02-25T07:11:48.629446Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import inspect\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "\n",
    "def display_modules(module, name_prefix=None, depth=0, max_depth=99, forward_methods=None):\n",
    "    if forward_methods is None:\n",
    "        forward_methods = {}\n",
    "    header = module.__class__.__name__\n",
    "    if name_prefix is not None:\n",
    "        header = f\"{name_prefix}#{header}\" \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+header)\n",
    "    if len(list(module.named_parameters(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> parameters\")\n",
    "        for name,parameter in module.named_parameters(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(parameter)}\")\n",
    "    if len(list(module.named_buffers(recurse=False))) > 0:\n",
    "        print(depth_prefix+\"> buffers\")\n",
    "        for name,buffer in module.named_buffers(recurse=False):\n",
    "            print(depth_prefix+f\"- {name}: {get_tensor_description(buffer)}\")\n",
    "    if len(list(module.named_children())) > 0:\n",
    "        print(depth_prefix+\"> submodules\")\n",
    "        for name,submodule in module.named_children():\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")\n",
    "    source_code = inspect.getsource(module.forward)\n",
    "    forward_methods[module.__class__.__name__] = source_code\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in module.named_children():\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "    if depth==0:\n",
    "        print()\n",
    "        print()\n",
    "        for module_type,source_code in forward_methods.items():\n",
    "            print(\"---------------------\")\n",
    "            print(f\"{module_type}.forward()\")\n",
    "            print(\"---------------------\")\n",
    "            print(source_code)\n",
    "            \n",
    "def display_module_list(module_list, name_prefix=None, depth=0, max_depth=1, forward_methods=None):\n",
    "    # ------------------------------\n",
    "    # Detect repeated layers in ModuleList: code inspired from Pytorch: ModuleList.__repr__    \n",
    "    list_of_reprs = [repr(item) for item in module_list]\n",
    "    if len(list_of_reprs) == 0:\n",
    "        return\n",
    "\n",
    "    start_end_indices = [[0, 0]]\n",
    "    repeated_blocks = [list_of_reprs[0]]\n",
    "    for i, r in enumerate(list_of_reprs[1:], 1):\n",
    "        if r == repeated_blocks[-1]:\n",
    "            start_end_indices[-1][1] += 1\n",
    "            continue\n",
    "\n",
    "        start_end_indices.append([i, i])\n",
    "        repeated_blocks.append(r)\n",
    "    # -------------------------------\n",
    "    \n",
    "    depth_prefix = \"  \"*depth\n",
    "    print(depth_prefix+\"---------------------\")\n",
    "    print(depth_prefix+f\"{name_prefix}#ModuleList\")\n",
    "    print(depth_prefix+\"> submodules\")\n",
    "    named_submodules = []\n",
    "    for (start_id, end_id) in start_end_indices:\n",
    "        submodule = module_list[start_id]\n",
    "        if start_id != end_id:      \n",
    "            name = f\"{start_id}..{end_id}\"\n",
    "            print(depth_prefix+f\"- {name}: {(end_id-start_id+1)}X {submodule.__class__.__name__}\")\n",
    "        else:\n",
    "            name = str(start_id)\n",
    "            print(depth_prefix+f\"- {name}: {submodule.__class__.__name__}\")        \n",
    "        named_submodules.append((name,submodule))\n",
    "    if depth < max_depth:\n",
    "        for name,submodule in named_submodules:\n",
    "            if isinstance(submodule, ModuleList):\n",
    "                display_module_list(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "            else:\n",
    "                display_modules(submodule, name_prefix=name, depth=depth+1, max_depth=max_depth, forward_methods=forward_methods)\n",
    "\n",
    "def get_tensor_description(t):\n",
    "    dtype = str(t.dtype)[6:]\n",
    "    dimensions = str(t.size())[11:-1]\n",
    "    total_byte_size = t.numel() * t.element_size()\n",
    "    return f\"{dtype} {dimensions} ({(total_byte_size/memory_unit_mb):.1f} MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd6a8b7-3130-418b-98b0-14970aa096f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:12:19.408198Z",
     "iopub.status.busy": "2024-02-25T07:12:19.408198Z",
     "iopub.status.idle": "2024-02-25T07:12:19.477713Z",
     "shell.execute_reply": "2024-02-25T07:12:19.477713Z",
     "shell.execute_reply.started": "2024-02-25T07:12:19.408198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "LlamaForCausalLM\n",
      "> submodules\n",
      "- model: LlamaModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#LlamaModel\n",
      "  > buffers\n",
      "  - causal_mask: int64 [2048, 2048] (32.0 MB)\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: LlamaRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: float16 [32000, 2048] (125.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..23: 24X LlamaDecoderLayer\n",
      "      ---------------------\n",
      "      0..23#LlamaDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: LlamaFlashAttention2\n",
      "      - mlp: LlamaMLP\n",
      "      - input_layernorm: LlamaRMSNorm\n",
      "      - post_attention_layernorm: LlamaRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#LlamaFlashAttention2\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: LlamaRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [2048, 2048] (8.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [2048, 2048] (8.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [2048, 2048] (8.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [2048, 2048] (8.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#LlamaRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "        ---------------------\n",
      "        mlp#LlamaMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLU\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [5504, 2048] (21.5 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [5504, 2048] (21.5 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: float16 [2048, 5504] (21.5 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLU\n",
      "        ---------------------\n",
      "        input_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2048] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#LlamaRMSNorm\n",
      "        > parameters\n",
      "        - weight: float16 [2048] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#LlamaRMSNorm\n",
      "    > parameters\n",
      "    - weight: float16 [2048] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: float16 [32000, 2048] (125.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "LlamaForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
      "\n",
      "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "            cache_position=cache_position,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
      "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
      "            logits = torch.cat(logits, dim=-1)\n",
      "        else:\n",
      "            logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "LlamaModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
      "            raise ValueError(\n",
      "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
      "            )\n",
      "\n",
      "        if self.gradient_checkpointing and self.training and use_cache:\n",
      "            logger.warning_once(\n",
      "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
      "            )\n",
      "            use_cache = False\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        past_seen_tokens = 0\n",
      "        if use_cache:  # kept for BC (cache positions)\n",
      "            if not isinstance(past_key_values, StaticCache):\n",
      "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
      "            past_seen_tokens = past_key_values.get_seq_length()\n",
      "\n",
      "        if cache_position is None:\n",
      "            cache_position = torch.arange(\n",
      "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
      "            )\n",
      "\n",
      "        if position_ids is None:\n",
      "            position_ids = cache_position.unsqueeze(0)\n",
      "\n",
      "        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)\n",
      "\n",
      "        # embed positions\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = None\n",
      "\n",
      "        for decoder_layer in self.layers:\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    causal_mask,\n",
      "                    position_ids,\n",
      "                    past_key_values,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                    cache_position,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=causal_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_values,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                    cache_position=cache_position,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = None\n",
      "        if use_cache:\n",
      "            next_cache = (\n",
      "                next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
      "            )\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "LlamaDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*):\n",
      "                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n",
      "                query_sequence_length, key_sequence_length)` if default attention is used.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "LlamaFlashAttention2.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.LongTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        output_attentions = False\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        # Flash attention requires the input to have the shape\n",
      "        # batch_size x seq_length x head_dim x hidden_dim\n",
      "        # therefore we just need to keep the original shape\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n",
      "        # to be able to avoid many of these transpose/reshape/view.\n",
      "        query_states = query_states.transpose(1, 2)\n",
      "        key_states = key_states.transpose(1, 2)\n",
      "        value_states = value_states.transpose(1, 2)\n",
      "\n",
      "        dropout_rate = self.attention_dropout if self.training else 0.0\n",
      "\n",
      "        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n",
      "        # therefore the input hidden states gets silently casted in float32. Hence, we need\n",
      "        # cast them back in the correct dtype just to be sure everything works as expected.\n",
      "        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n",
      "        # in fp32. (LlamaRMSNorm handles it correctly)\n",
      "\n",
      "        input_dtype = query_states.dtype\n",
      "        if input_dtype == torch.float32:\n",
      "            if torch.is_autocast_enabled():\n",
      "                target_dtype = torch.get_autocast_gpu_dtype()\n",
      "            # Handle the case where the model is quantized\n",
      "            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n",
      "                target_dtype = self.config._pre_quantization_dtype\n",
      "            else:\n",
      "                target_dtype = self.q_proj.weight.dtype\n",
      "\n",
      "            logger.warning_once(\n",
      "                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n",
      "                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n",
      "                f\" {target_dtype}.\"\n",
      "            )\n",
      "\n",
      "            query_states = query_states.to(target_dtype)\n",
      "            key_states = key_states.to(target_dtype)\n",
      "            value_states = value_states.to(target_dtype)\n",
      "\n",
      "        attn_output = self._flash_attention_forward(\n",
      "            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "LlamaRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, position_ids, seq_len=None):\n",
      "        if seq_len is not None:\n",
      "            logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.40.\")\n",
      "\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
      "        position_ids_expanded = position_ids[:, None, :].float()\n",
      "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
      "        emb = torch.cat((freqs, freqs), dim=-1)\n",
      "        cos = emb.cos().to(dtype=x.dtype)\n",
      "        sin = emb.sin().to(dtype=x.dtype)\n",
      "        # backwards compatibility\n",
      "        self._cos_cached = cos\n",
      "        self._sin_cached = sin\n",
      "        return cos, sin\n",
      "\n",
      "---------------------\n",
      "LlamaMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        if self.config.pretraining_tp > 1:\n",
      "            slice = self.intermediate_size // self.config.pretraining_tp\n",
      "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
      "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
      "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
      "\n",
      "            gate_proj = torch.cat(\n",
      "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
      "            )\n",
      "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
      "\n",
      "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
      "            down_proj = [\n",
      "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
      "            ]\n",
      "            down_proj = sum(down_proj)\n",
      "        else:\n",
      "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "        return down_proj\n",
      "\n",
      "---------------------\n",
      "SiLU.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.silu(input, inplace=self.inplace)\n",
      "\n",
      "---------------------\n",
      "LlamaRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed9053-21b1-4cc1-acc6-26313a30b862",
   "metadata": {},
   "source": [
    "## Inference profiling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "182a5b20-dcf0-49d2-aae2-351801233acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T08:06:23.425690Z",
     "iopub.status.busy": "2024-02-25T08:06:23.425690Z",
     "iopub.status.idle": "2024-02-25T08:06:23.442734Z",
     "shell.execute_reply": "2024-02-25T08:06:23.441072Z",
     "shell.execute_reply.started": "2024-02-25T08:06:23.425690Z"
    }
   },
   "outputs": [],
   "source": [
    "def profile_forward(model, batch_size=1, seq_length=None):\n",
    "    if seq_length is None: seq_length = model.config.max_position_embeddings\n",
    "    input_ids = torch.randint(low=0, high=32000, size=(batch_size,seq_length), dtype=torch.int64).to(model.device)\n",
    "    attention_mask = torch.ones(batch_size,seq_length).to(model.device)\n",
    "    model.eval()\n",
    "    with torch.profiler.profile(activities=[ProfilerActivity.CPU,ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True, with_flops=True, with_modules=True, experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True)) as prof:\n",
    "        with torch.profiler.record_function(\"MODEL INFERENCE\"):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)        \n",
    "\n",
    "    return prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b87d307d-637b-492b-9018-8e9b1183ea41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T08:06:24.540531Z",
     "iopub.status.busy": "2024-02-25T08:06:24.540531Z",
     "iopub.status.idle": "2024-02-25T08:06:29.127912Z",
     "shell.execute_reply": "2024-02-25T08:06:29.126404Z",
     "shell.execute_reply.started": "2024-02-25T08:06:24.540531Z"
    }
   },
   "outputs": [],
   "source": [
    "prof = profile_forward(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9f34508-fc8d-4740-bbcb-917275f4b0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:31:23.192266Z",
     "iopub.status.busy": "2024-02-25T09:31:23.192266Z",
     "iopub.status.idle": "2024-02-25T09:31:23.547641Z",
     "shell.execute_reply": "2024-02-25T09:31:23.547081Z",
     "shell.execute_reply.started": "2024-02-25T09:31:23.192266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "              MODEL INFERENCE        29.18%     224.864ms       100.00%     770.521ms     770.521ms      30.171ms         3.85%     782.769ms     782.769ms           0 b           0 b     250.00 Mb      -8.50 Gb             1            --  \n",
      "              aten::embedding         0.03%     226.000us         0.08%     648.000us     648.000us     119.000us         0.02%       1.653ms       1.653ms           0 b           0 b       8.00 Mb           0 b             1            --  \n",
      "                aten::reshape         2.06%      15.881ms         2.39%      18.440ms      76.198us       8.794ms         1.12%      18.695ms      77.252us           0 b           0 b           0 b           0 b           242            --  \n",
      "                   aten::view         0.40%       3.100ms         0.40%       3.100ms       9.841us      10.641ms         1.36%      10.641ms      33.781us           0 b           0 b           0 b           0 b           315            --  \n",
      "           aten::index_select         0.03%     252.000us         0.04%     301.000us     301.000us       1.301ms         0.17%       1.394ms       1.394ms           0 b           0 b       8.00 Mb           0 b             1            --  \n",
      "                  aten::empty         0.11%     860.000us         0.11%     860.000us      17.200us       1.732ms         0.22%       1.732ms      34.640us           0 b           0 b       3.01 Mb       3.01 Mb            50            --  \n",
      "                aten::resize_         0.00%      34.000us         0.00%      34.000us      17.000us      40.000us         0.01%      40.000us      20.000us           0 b           0 b       8.02 Mb       8.02 Mb             2            --  \n",
      "                 aten::arange         0.02%     125.000us         0.03%     216.000us     108.000us     103.000us         0.01%     205.000us     102.500us           0 b           0 b      32.00 Kb           0 b             2            --  \n",
      "              aten::unsqueeze         1.03%       7.931ms         1.12%       8.631ms      71.331us       5.813ms         0.74%       9.231ms      76.289us           0 b           0 b           0 b           0 b           121            --  \n",
      "             aten::as_strided         0.58%       4.461ms         0.58%       4.461ms       6.382us      12.432ms         1.59%      12.432ms      17.785us           0 b           0 b           0 b           0 b           699            --  \n",
      "                     aten::eq         0.01%      89.000us         0.01%      89.000us      89.000us      56.000us         0.01%      56.000us      56.000us           0 b           0 b       2.00 Kb       2.00 Kb             1            --  \n",
      "                    aten::any         0.02%     163.000us         0.02%     179.000us     179.000us     219.000us         0.03%     223.000us     223.000us           0 b           0 b         512 b         512 b             1            --  \n",
      "                   aten::item         0.01%      54.000us         0.07%     533.000us     533.000us      26.000us         0.00%     163.000us     163.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "    aten::_local_scalar_dense         0.06%     479.000us         0.06%     479.000us     479.000us     137.000us         0.02%     137.000us     137.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                     aten::to         1.74%      13.435ms        12.07%      93.034ms     381.287us       4.653ms         0.59%      54.985ms     225.348us           0 b           0 b       1.42 Gb           0 b           244            --  \n",
      "               aten::_to_copy         8.45%      65.138ms        10.33%      79.599ms     465.491us       5.882ms         0.75%      50.332ms     294.339us           0 b           0 b       1.42 Gb           0 b           171            --  \n",
      "          aten::empty_strided         0.50%       3.864ms         0.50%       3.864ms      19.815us       5.043ms         0.64%       5.043ms      25.862us           0 b           0 b       1.60 Gb       1.60 Gb           195            --  \n",
      "                  aten::copy_         1.48%      11.438ms         1.48%      11.438ms      66.889us      40.143ms         5.13%      40.143ms     234.754us           0 b           0 b           0 b           0 b           171            --  \n",
      "                    aten::pow         2.44%      18.778ms         2.48%      19.107ms     389.939us      20.210ms         2.58%      20.869ms     425.898us           0 b           0 b     784.00 Mb     784.00 Mb            49            --  \n",
      "            aten::result_type         0.02%     164.000us         0.02%     164.000us       3.347us     285.000us         0.04%     285.000us       5.816us           0 b           0 b           0 b           0 b            49            --  \n",
      "                   aten::mean         0.49%       3.772ms         0.49%       3.772ms      76.980us      12.651ms         1.62%      12.651ms     258.184us           0 b           0 b     392.00 Kb     392.00 Kb            49            --  \n",
      "                    aten::add         1.42%      10.936ms         1.42%      10.936ms      75.421us      27.607ms         3.53%      27.607ms     190.393us           0 b           0 b     768.38 Mb     768.38 Mb           145       402.754  \n",
      "                  aten::rsqrt         0.36%       2.763ms         0.36%       2.763ms      56.388us       1.147ms         0.15%       1.147ms      23.408us           0 b           0 b     392.00 Kb     392.00 Kb            49            --  \n",
      "                    aten::mul         2.15%      16.569ms         2.15%      16.569ms      76.005us      71.212ms         9.10%      71.212ms     326.661us           0 b           0 b       2.40 Gb       2.40 Gb           218       878.807  \n",
      "                 aten::linear         7.89%      60.756ms        21.01%     161.899ms     957.982us       2.634ms         0.34%     437.379ms       2.588ms           0 b           0 b       2.07 Gb           0 b           169            --  \n",
      "                      aten::t         1.77%      13.602ms         3.30%      25.429ms     150.467us       8.597ms         1.10%      11.639ms      68.870us           0 b           0 b           0 b           0 b           169            --  \n",
      "              aten::transpose         4.82%      37.152ms         5.10%      39.294ms     116.599us       8.111ms         1.04%      12.629ms      37.475us           0 b           0 b           0 b           0 b           337            --  \n",
      "                 aten::matmul         5.66%      43.607ms        18.48%     142.426ms     737.959us      12.235ms         1.56%     436.403ms       2.261ms           0 b           0 b       2.08 Gb           0 b           193            --  \n",
      "                     aten::mm         3.63%      28.008ms         3.63%      28.008ms     165.728us     400.436ms        51.16%     400.436ms       2.369ms           0 b           0 b       2.07 Gb       2.07 Gb           169   5242007.585  \n",
      "           aten::_unsafe_view         0.17%       1.291ms         0.17%       1.291ms       6.689us       1.199ms         0.15%       1.199ms       6.212us           0 b           0 b           0 b           0 b           193            --  \n",
      "                  aten::slice        12.53%      96.542ms        12.69%      97.791ms     582.089us       6.084ms         0.78%       9.075ms      54.018us           0 b           0 b           0 b           0 b           168            --  \n",
      "                 aten::expand         4.58%      35.291ms         4.63%      35.645ms     495.069us       2.937ms         0.38%       4.438ms      61.639us           0 b           0 b           0 b           0 b            72            --  \n",
      "                    aten::bmm         2.43%      18.726ms         2.43%      18.726ms     780.250us       1.177ms         0.15%       1.177ms      49.042us           0 b           0 b      12.00 Mb      12.00 Mb            24         6.291  \n",
      "                    aten::cat         0.87%       6.737ms         0.87%       6.737ms      93.569us      15.339ms         1.96%      15.339ms     213.042us           0 b           0 b     408.00 Mb     408.00 Mb            72            --  \n",
      "                    aten::cos         0.27%       2.057ms         0.27%       2.057ms      85.708us       1.398ms         0.18%       1.398ms      58.250us           0 b           0 b      24.00 Mb      24.00 Mb            24            --  \n",
      "                    aten::sin         0.20%       1.518ms         0.20%       1.518ms      63.250us       1.676ms         0.21%       1.676ms      69.833us           0 b           0 b      24.00 Mb      24.00 Mb            24            --  \n",
      "                    aten::neg         0.45%       3.447ms         0.45%       3.447ms      71.812us       7.769ms         0.99%       7.769ms     161.854us           0 b           0 b     192.00 Mb     192.00 Mb            48            --  \n",
      "                FlashAttnFunc         1.58%      12.159ms         2.13%      16.431ms     684.625us      40.155ms         5.13%      44.055ms       1.836ms           0 b           0 b     192.00 Mb      -3.01 Mb            24            --  \n",
      "             aten::empty_like         0.34%       2.603ms         0.45%       3.444ms     143.500us       1.493ms         0.19%       2.229ms      92.875us           0 b           0 b     192.00 Mb           0 b            24            --  \n",
      "                   aten::silu         0.21%       1.649ms         0.21%       1.649ms      68.708us      11.112ms         1.42%      11.112ms     463.000us           0 b           0 b     516.00 Mb     516.00 Mb            24            --  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 770.521ms\n",
      "Self CUDA time total: 782.769ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036991bd-71a5-4f35-ad25-dc7a15d45437",
   "metadata": {},
   "source": [
    "## CUDA calls analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71b59884-93d9-4a41-b4a1-44026e2c9ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T08:06:37.052937Z",
     "iopub.status.busy": "2024-02-25T08:06:37.052937Z",
     "iopub.status.idle": "2024-02-25T08:06:37.059791Z",
     "shell.execute_reply": "2024-02-25T08:06:37.058474Z",
     "shell.execute_reply.started": "2024-02-25T08:06:37.052937Z"
    }
   },
   "outputs": [],
   "source": [
    "events = prof.events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "215aaa36-1cb8-4ac9-8d9b-12f9997d528f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:04:17.119864Z",
     "iopub.status.busy": "2024-02-25T10:04:17.119864Z",
     "iopub.status.idle": "2024-02-25T10:04:17.138723Z",
     "shell.execute_reply": "2024-02-25T10:04:17.137714Z",
     "shell.execute_reply.started": "2024-02-25T10:04:17.119864Z"
    }
   },
   "outputs": [],
   "source": [
    "module_file = __import__(model.__module__).__file__\n",
    "packages_dir = \"site-packages\"\n",
    "index = module_file.find(packages_dir)\n",
    "packages_path = module_file[:index + len(packages_dir)+1]\n",
    "packages_path\n",
    "\n",
    "\n",
    "def add_call_stacks(event):\n",
    "    filtered_stack = []\n",
    "    torch_calls = []\n",
    "    for frame in event.stack:\n",
    "        if frame.endswith(\"profile_forward\"):\n",
    "            break\n",
    "        elif not frame.startswith(\"<built-in\") and not frame.startswith(\"torch/\"):\n",
    "            function = frame.split(\": \")[1]\n",
    "            if function!=\"_call_impl\":\n",
    "                filtered_stack.append(function)\n",
    "        elif frame.startswith(\"<built-in method\"):\n",
    "            frame_words = frame.split(\" \")\n",
    "            torch_calls.append(frame_words[2])\n",
    "            torch_calls.append(frame_words[4])\n",
    "        elif frame.startswith(\"<built-in function\"):\n",
    "            frame_words = frame.split(\" \")\n",
    "            torch_calls.append(frame_words[2][:-1])\n",
    "    filtered_stack.reverse()    \n",
    "    event.call_stack = \".\".join(filtered_stack)\n",
    "    torch_calls.reverse()\n",
    "    event.torch_stack = \".\".join(torch_calls)\n",
    "\n",
    "def print_profiler_event(event, show_code_lines=False):\n",
    "    csindex = 0\n",
    "    call_site = event.stack[csindex].split(\": \")[0]\n",
    "    while call_site.startswith(\"<built-in\") or call_site.startswith(\"torch/\"):\n",
    "        csindex += 1\n",
    "        call_site = event.stack[csindex].split(\": \")[0]\n",
    "        \n",
    "    if show_code_lines:\n",
    "        base_dir = packages_path\n",
    "        relative_path = call_site.split(\"(\")[0]\n",
    "        line_number = int(call_site.split(\"(\")[1][:-1])\n",
    "        try:\n",
    "            with open(base_dir+relative_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                call_line = \"\"\n",
    "                for idx,line in enumerate(file_content.splitlines()):\n",
    "                    if idx>=line_number-1:\n",
    "                        call_line += f\"{idx} {line}\\n\"\n",
    "                        if line.strip().startswith(\"return \"):\n",
    "                            break\n",
    "        except:\n",
    "            call_line = \"\"\n",
    "    \n",
    "    add_call_stacks(event)\n",
    "    \n",
    "    filtered_inputs = []\n",
    "    for input_shape in event.input_shapes:\n",
    "        if len(input_shape)>0:\n",
    "            filtered_inputs.append(input_shape)\n",
    "    \n",
    "    print(f\"- call stack : {event.call_stack}\")\n",
    "    print(f\"- torch stack: {event.torch_stack}\")\n",
    "    print(f\"- kernel     : {event.name}\")\n",
    "    print(f\"- inputs     : {filtered_inputs}\")\n",
    "    print(f\"- cpu time   : {event.cpu_time/1000:.2f} ms\")\n",
    "    print(f\"- cpu memory : {event.cpu_memory_usage/1024/1024:.2f} MB\")\n",
    "    print(f\"- gpu time   : {event.cuda_time/1000:.2f} ms\")\n",
    "    print(f\"- gpu memory : {event.cuda_memory_usage/1024/1024:.2f} MB\")\n",
    "    print(f\"- flops      : {event.flops}\")    \n",
    "    if show_code_lines:\n",
    "        print(f\"- func src   : {call_site}\")\n",
    "        print(call_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1bc1bcb3-47bb-4580-b902-bac35492b074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T10:04:18.060562Z",
     "iopub.status.busy": "2024-02-25T10:04:18.056553Z",
     "iopub.status.idle": "2024-02-25T10:04:18.069642Z",
     "shell.execute_reply": "2024-02-25T10:04:18.069036Z",
     "shell.execute_reply.started": "2024-02-25T10:04:18.060562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- call stack : LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_6.forward.LlamaFlashAttention2_6.forward.LlamaRotaryEmbedding_6.forward\n",
      "- torch stack: Tensor.float\n",
      "- kernel     : aten::_to_copy\n",
      "- inputs     : [[1, 1, 2048]]\n",
      "- cpu time   : 0.17 ms\n",
      "- cpu memory : 0.00 MB\n",
      "- gpu time   : 0.16 ms\n",
      "- gpu memory : 0.01 MB\n",
      "- flops      : 0\n",
      "- func src   : transformers\\models\\llama\\modeling_llama.py(119)\n",
      "118     def forward(self, x, position_ids, seq_len=None):\n",
      "119         if seq_len is not None:\n",
      "120             logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.40.\")\n",
      "121 \n",
      "122         # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "123         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
      "124         position_ids_expanded = position_ids[:, None, :].float()\n",
      "125         freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
      "126         emb = torch.cat((freqs, freqs), dim=-1)\n",
      "127         cos = emb.cos().to(dtype=x.dtype)\n",
      "128         sin = emb.sin().to(dtype=x.dtype)\n",
      "129         # backwards compatibility\n",
      "130         self._cos_cached = cos\n",
      "131         self._sin_cached = sin\n",
      "132         return cos, sin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_profiler_event(events[1203], show_code_lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2539ca60-e154-477b-b513-8175b496e917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:35:43.540491Z",
     "iopub.status.busy": "2024-02-25T09:35:43.540491Z",
     "iopub.status.idle": "2024-02-25T09:35:43.561695Z",
     "shell.execute_reply": "2024-02-25T09:35:43.561695Z",
     "shell.execute_reply.started": "2024-02-25T09:35:43.540491Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770521.0, 770521.0)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = 0\n",
    "for event in events:\n",
    "    if event.cpu_parent is not None and event.cpu_parent.id == events[0].id:\n",
    "        total_time += event.cpu_time\n",
    "events[0].cpu_time ,total_time + events[0].self_cpu_time_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f2e58728-c5c9-4088-b9b2-6f523961ca0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:38:04.532422Z",
     "iopub.status.busy": "2024-02-25T09:38:04.532422Z",
     "iopub.status.idle": "2024-02-25T09:38:04.540237Z",
     "shell.execute_reply": "2024-02-25T09:38:04.539080Z",
     "shell.execute_reply.started": "2024-02-25T09:38:04.532422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = events[0].cuda_time/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9541e1e8-540d-43c3-8f0e-41b4f7ac98e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:39:46.811320Z",
     "iopub.status.busy": "2024-02-25T09:39:46.810087Z",
     "iopub.status.idle": "2024-02-25T09:39:46.830328Z",
     "shell.execute_reply": "2024-02-25T09:39:46.830328Z",
     "shell.execute_reply.started": "2024-02-25T09:39:46.811320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 16.62735849056604, 79.96982506972044)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = 0\n",
    "count_events = 0\n",
    "count_threshold = 0\n",
    "for event in events:\n",
    "    if event.cpu_parent is not None and event.cpu_parent.id == events[0].id:\n",
    "        count_events += 1\n",
    "        if event.cuda_time>=threshold:\n",
    "            count_threshold += 1\n",
    "            total_time += event.cuda_time\n",
    "count_threshold, count_threshold/count_events*100, total_time/events[0].cuda_time*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1a3ef4e2-e099-404d-9936-e2b35a9f455f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T15:19:59.769049Z",
     "iopub.status.busy": "2024-02-25T15:19:59.769049Z",
     "iopub.status.idle": "2024-02-25T15:19:59.780789Z",
     "shell.execute_reply": "2024-02-25T15:19:59.779113Z",
     "shell.execute_reply.started": "2024-02-25T15:19:59.769049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer.forward.LlamaRMSNorm_0.forward 0\n"
     ]
    }
   ],
   "source": [
    "coalesce_layers = \"LlamaDecoderLayer\"\n",
    "\n",
    "call_stack = \"LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward\"\n",
    "start_index = call_stack.find(coalesce_layers)\n",
    "if start_index >= 0:\n",
    "    dot_index = call_stack.find('.', start_index)\n",
    "    layer_index = int(call_stack[start_index+len(coalesce_layers)+1:dot_index])\n",
    "    call_stack = call_stack[0:start_index+len(coalesce_layers)]+call_stack[dot_index:]\n",
    "print(call_stack,layer_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4a28689f-0008-4a2b-bdf0-713bb27a8872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T17:16:26.800272Z",
     "iopub.status.busy": "2024-02-25T17:16:26.800272Z",
     "iopub.status.idle": "2024-02-25T17:16:26.865292Z",
     "shell.execute_reply": "2024-02-25T17:16:26.865292Z",
     "shell.execute_reply.started": "2024-02-25T17:16:26.800272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layers 0 event index: 6\n",
      "layers 0 events count: 70, events length: 76\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Cuda time (s) | Cuda time (%) | Calls | Stack | PyTorch | Function |\n",
       "| -------------- | ------------- | ----- | ----- | ------- | -------- |\n",
       "| 1653 | 0.21 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.Embedding_0.forward.embedding | type.embedding | aten::embedding |\n",
       "| 111 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward | type.arange | aten::arange |\n",
       "| 38 | 0.00 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward | Tensor.unsqueeze | aten::unsqueeze |\n",
       "| 56 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward._update_causal_mask.\\_\\_contains\\_\\_ |  | aten::eq |\n",
       "| 223 | 0.03 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward._update_causal_mask.\\_\\_contains\\_\\_ | Tensor.any | aten::any |\n",
       "| 163 | 0.02 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward._update_causal_mask.\\_\\_contains\\_\\_ | Tensor.item | aten::item |\n",
       "| 10961 | 1.40 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward | Tensor.to | aten::to |\n",
       "| 10873 | 1.39 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward | Tensor.pow | aten::pow |\n",
       "| 6732 | 0.86 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward | Tensor.mean | aten::mean |\n",
       "| 289 | 0.04 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward |  | aten::add |\n",
       "| 285 | 0.04 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward | type.rsqrt | aten::rsqrt |\n",
       "| 10428 | 1.33 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward |  | aten::mul |\n",
       "| 9258 | 1.18 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward | Tensor.to | aten::to |\n",
       "| 6025 | 0.77 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_0.forward |  | aten::mul |\n",
       "| 45574 | 5.82 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.Linear_0.forward | linear | aten::linear |\n",
       "| 46018 | 5.88 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.Linear_1.forward | linear | aten::linear |\n",
       "| 45519 | 5.82 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.Linear_2.forward | linear | aten::linear |\n",
       "| 56 | 0.01 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.view | aten::view |\n",
       "| 169 | 0.02 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 93 | 0.01 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.view | aten::view |\n",
       "| 1437 | 0.18 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 588 | 0.08 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.view | aten::view |\n",
       "| 1569 | 0.20 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 1166 | 0.15 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::unsqueeze |\n",
       "| 1555 | 0.20 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::slice |\n",
       "| 1689 | 0.22 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::unsqueeze |\n",
       "| 203 | 0.03 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.float | aten::to |\n",
       "| 1475 | 0.19 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.expand | aten::expand |\n",
       "| 1439 | 0.18 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::slice |\n",
       "| 2316 | 0.30 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::unsqueeze |\n",
       "| 1595 | 0.20 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::slice |\n",
       "| 5464 | 0.70 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.float | aten::to |\n",
       "| 13297 | 1.70 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward |  | aten::matmul |\n",
       "| 1824 | 0.23 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 1630 | 0.21 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | type.cat | aten::cat |\n",
       "| 1398 | 0.18 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.cos | aten::cos |\n",
       "| 4817 | 0.62 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.to | aten::to |\n",
       "| 1676 | 0.21 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.sin | aten::sin |\n",
       "| 4618 | 0.59 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.LlamaRotaryEmbedding_0.forward | Tensor.to | aten::to |\n",
       "| 2237 | 0.29 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb | Tensor.unsqueeze | aten::unsqueeze |\n",
       "| 1785 | 0.23 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb | Tensor.unsqueeze | aten::unsqueeze |\n",
       "| 6993 | 0.89 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::mul |\n",
       "| 1026 | 0.13 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::slice |\n",
       "| 1107 | 0.14 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::slice |\n",
       "| 3849 | 0.49 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::neg |\n",
       "| 7671 | 0.98 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half | type.cat | aten::cat |\n",
       "| 5780 | 0.74 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::mul |\n",
       "| 6947 | 0.89 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::add |\n",
       "| 4993 | 0.64 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::mul |\n",
       "| 1142 | 0.15 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::slice |\n",
       "| 1211 | 0.15 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::slice |\n",
       "| 3920 | 0.50 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half |  | aten::neg |\n",
       "| 6038 | 0.77 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb.rotate_half | type.cat | aten::cat |\n",
       "| 4992 | 0.64 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::mul |\n",
       "| 6954 | 0.89 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.apply_rotary_pos_emb |  | aten::add |\n",
       "| 917 | 0.12 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 1030 | 0.13 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 2641 | 0.34 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.transpose | aten::transpose |\n",
       "| 44055 | 5.63 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward._flash_attention_forward.flash_attn_func.apply | FunctionMeta.apply | FlashAttnFunc |\n",
       "| 165 | 0.02 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward | Tensor.reshape | aten::reshape |\n",
       "| 41821 | 5.34 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaFlashAttention2_0.forward.Linear_3.forward | linear | aten::linear |\n",
       "| 6378 | 0.81 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward |  | aten::add |\n",
       "| 8433 | 1.08 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward | Tensor.to | aten::to |\n",
       "| 9885 | 1.26 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward | Tensor.pow | aten::pow |\n",
       "| 5839 | 0.75 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward | Tensor.mean | aten::mean |\n",
       "| 770 | 0.10 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward |  | aten::add |\n",
       "| 815 | 0.10 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward | type.rsqrt | aten::rsqrt |\n",
       "| 9109 | 1.16 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward |  | aten::mul |\n",
       "| 9236 | 1.18 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward | Tensor.to | aten::to |\n",
       "| 5497 | 0.70 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaRMSNorm_1.forward |  | aten::mul |\n",
       "| 83935 | 10.72 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaMLP_0.forward.Linear_4.forward | linear | aten::linear |\n",
       "| 11112 | 1.42 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaMLP_0.forward.SiLU_0.forward.silu | silu | aten::silu |\n",
       "| 80307 | 10.26 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaMLP_0.forward.Linear_5.forward | linear | aten::linear |\n",
       "| 17147 | 2.19 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaMLP_0.forward |  | aten::mul |\n",
       "| 84765 | 10.83 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward.LlamaMLP_0.forward.Linear_6.forward | linear | aten::linear |\n",
       "| 6261 | 0.80 | 24 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaDecoderLayer_0.forward |  | aten::add |\n",
       "| 353 | 0.05 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward | Tensor.to | aten::to |\n",
       "| 111 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward | Tensor.pow | aten::pow |\n",
       "| 80 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward | Tensor.mean | aten::mean |\n",
       "| 8 | 0.00 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward |  | aten::add |\n",
       "| 47 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward | type.rsqrt | aten::rsqrt |\n",
       "| 183 | 0.02 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward |  | aten::mul |\n",
       "| 102 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward | Tensor.to | aten::to |\n",
       "| 65 | 0.01 | 1 | LlamaForCausalLM_0.forward.LlamaModel_0.forward.LlamaRMSNorm_48.forward |  | aten::mul |\n",
       "| 9440 | 1.21 | 1 | LlamaForCausalLM_0.forward.Linear_168.forward | linear | aten::linear |\n",
       "| 1166 | 0.15 | 1 | LlamaForCausalLM_0.forward | Tensor.float | aten::to |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "coalesce_layers = \"LlamaDecoderLayer\"\n",
    "\n",
    "coalesced_events = []\n",
    "first_layer0_event_index = 0\n",
    "layer0_events_count = 0\n",
    "layer_index = 0\n",
    "event_index_in_layer = 0\n",
    "for event in events:\n",
    "    if event.cpu_parent is not None and event.cpu_parent.id == events[0].id:\n",
    "        add_call_stacks(event)\n",
    "        \n",
    "        key = event.call_stack\n",
    "        start_index = key.find(coalesce_layers)\n",
    "        if start_index >= 0:\n",
    "            dot_index = key.find('.', start_index)\n",
    "            current_layer_index = int(key[start_index+len(coalesce_layers)+1:dot_index])\n",
    "\n",
    "            if first_layer0_event_index == 0:\n",
    "                first_layer0_event_index = len(coalesced_events)\n",
    "                print(f\"first layers 0 event index: {first_layer0_event_index}\")\n",
    "            if layer0_events_count == 0 and current_layer_index == 1:\n",
    "                layer0_events_count = len(coalesced_events) - first_layer0_event_index\n",
    "                print(f\"layers 0 events count: {layer0_events_count}, events length: {len(coalesced_events)}\")\n",
    "            \n",
    "            if current_layer_index > layer_index:\n",
    "                layer_index = current_layer_index \n",
    "                if event_index_in_layer != layer0_events_count:\n",
    "                    print(f\"ERROR at layer {layer_index}: number of events {event_index_in_layer} different of layer 0 events count: {layer0_events_count}\")\n",
    "                    break\n",
    "                event_index_in_layer = 0                        \n",
    "            \n",
    "            if layer_index == 0:\n",
    "                event.layers_count = 1\n",
    "                event.layers_cpu_time = event.cpu_time\n",
    "                event.layers_cuda_time = event.cuda_time\n",
    "                coalesced_events.append(event)\n",
    "            else:\n",
    "                first_event = coalesced_events[first_layer0_event_index + event_index_in_layer]\n",
    "                first_event.layers_count += 1\n",
    "                first_event.layers_cpu_time += event.cpu_time\n",
    "                first_event.layers_cuda_time += event.cuda_time\n",
    "\n",
    "            event_index_in_layer += 1\n",
    "        else:\n",
    "            coalesced_events.append(event)\n",
    "\n",
    "table =  \"| Cuda time (s) | Cuda time (%) | Calls | Stack | PyTorch | Function |\\n\" \n",
    "table += \"| -------------- | ------------- | ----- | ----- | ------- | -------- |\\n\" \n",
    "for event in coalesced_events:\n",
    "    if getattr(event, \"layers_count\", 0) > 0:\n",
    "        table += f\"| {int(event.layers_cuda_time)} | {(event.layers_cuda_time/events[0].cuda_time*100):.2f} | {event.layers_count} | {event.call_stack} | {event.torch_stack} | {event.name} |\\n\"\n",
    "    else:\n",
    "        table += f\"| {int(event.cuda_time)} | {(event.cuda_time/events[0].cuda_time*100):.2f} | 1 | {event.call_stack} | {event.torch_stack} | {event.name} |\\n\"\n",
    "\n",
    "display(Markdown(table.replace(\"__\",\"\\\\_\\\\_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "29b22dc2-1225-4c88-83f7-56f0431e71cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T09:41:16.079537Z",
     "iopub.status.busy": "2024-02-25T09:41:16.079537Z",
     "iopub.status.idle": "2024-02-25T09:41:16.129781Z",
     "shell.execute_reply": "2024-02-25T09:41:16.129274Z",
     "shell.execute_reply.started": "2024-02-25T09:41:16.079537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::embedding 1653 0.21 %\n",
      "- aten::index_select 1394 84.33 %\n",
      "aten::to 1929 0.25 %\n",
      "- aten::_to_copy 1895 98.24 %\n",
      "  - aten::copy_ 1782 92.38 %\n",
      "aten::pow 2127 0.27 %\n",
      "aten::mean 1261 0.16 %\n",
      "aten::mul 2072 0.26 %\n",
      "aten::to 1765 0.23 %\n",
      "- aten::_to_copy 1758 99.60 %\n",
      "  - aten::copy_ 1744 98.81 %\n",
      "aten::mul 1129 0.14 %\n",
      "aten::linear 5832 0.75 %\n",
      "- aten::matmul 5805 99.54 %\n",
      "  - aten::mm 5628 96.50 %\n",
      "aten::linear 5639 0.72 %\n",
      "- aten::matmul 5501 97.55 %\n",
      "  - aten::mm 5394 95.66 %\n",
      "aten::linear 5628 0.72 %\n",
      "- aten::matmul 5501 97.74 %\n",
      "  - aten::mm 5409 96.11 %\n",
      "aten::mul 1187 0.15 %\n",
      "aten::cat 1065 0.14 %\n",
      "aten::mul 1125 0.14 %\n",
      "aten::add 1650 0.21 %\n",
      "aten::mul 1099 0.14 %\n",
      "aten::cat 1081 0.14 %\n",
      "aten::mul 1121 0.14 %\n",
      "aten::add 1643 0.21 %\n",
      "FlashAttnFunc 6576 0.84 %\n",
      "aten::linear 5800 0.74 %\n",
      "- aten::matmul 5658 97.55 %\n",
      "  - aten::mm 5614 96.79 %\n",
      "aten::add 1619 0.21 %\n",
      "aten::to 1744 0.22 %\n",
      "- aten::_to_copy 1734 99.43 %\n",
      "  - aten::copy_ 1720 98.62 %\n",
      "aten::pow 2120 0.27 %\n",
      "aten::mean 1267 0.16 %\n",
      "aten::mul 2098 0.27 %\n",
      "aten::to 1767 0.23 %\n",
      "- aten::_to_copy 1758 99.49 %\n",
      "  - aten::copy_ 1744 98.70 %\n",
      "aten::mul 1154 0.15 %\n",
      "aten::linear 11195 1.43 %\n",
      "- aten::matmul 11167 99.75 %\n",
      "  - aten::mm 11091 99.07 %\n",
      "aten::silu 2782 0.36 %\n",
      "aten::linear 11257 1.44 %\n",
      "- aten::matmul 11229 99.75 %\n",
      "  - aten::mm 11085 98.47 %\n",
      "aten::mul 4350 0.56 %\n",
      "aten::linear 12374 1.58 %\n",
      "- aten::matmul 12346 99.77 %\n",
      "  - aten::mm 12203 98.62 %\n",
      "aten::add 1564 0.20 %\n",
      "aten::to 1754 0.22 %\n",
      "- aten::_to_copy 1747 99.60 %\n",
      "  - aten::copy_ 1732 98.75 %\n",
      "aten::pow 2135 0.27 %\n",
      "aten::mean 1262 0.16 %\n",
      "aten::mul 2038 0.26 %\n",
      "aten::to 1771 0.23 %\n",
      "- aten::_to_copy 1763 99.55 %\n",
      "  - aten::copy_ 1750 98.81 %\n",
      "aten::mul 1135 0.14 %\n",
      "aten::linear 5733 0.73 %\n",
      "- aten::matmul 5704 99.49 %\n",
      "  - aten::mm 5576 97.26 %\n",
      "aten::linear 5740 0.73 %\n",
      "- aten::matmul 5644 98.33 %\n",
      "  - aten::mm 5448 94.91 %\n",
      "aten::linear 5541 0.71 %\n",
      "- aten::matmul 5428 97.96 %\n",
      "  - aten::mm 5316 95.94 %\n",
      "aten::mul 1212 0.15 %\n",
      "aten::cat 1077 0.14 %\n",
      "aten::mul 1140 0.15 %\n",
      "aten::add 1652 0.21 %\n",
      "aten::mul 1118 0.14 %\n",
      "aten::cat 1070 0.14 %\n",
      "aten::mul 1111 0.14 %\n",
      "aten::add 1647 0.21 %\n",
      "FlashAttnFunc 5913 0.76 %\n",
      "aten::linear 5756 0.74 %\n",
      "- aten::matmul 5606 97.39 %\n",
      "  - aten::mm 5504 95.62 %\n",
      "aten::add 1565 0.20 %\n",
      "aten::to 1762 0.23 %\n",
      "- aten::_to_copy 1754 99.55 %\n",
      "  - aten::copy_ 1739 98.69 %\n",
      "aten::pow 2137 0.27 %\n",
      "aten::mean 1254 0.16 %\n",
      "aten::mul 2069 0.26 %\n",
      "aten::to 1764 0.23 %\n",
      "- aten::_to_copy 1755 99.49 %\n",
      "  - aten::copy_ 1739 98.58 %\n",
      "aten::mul 1120 0.14 %\n",
      "aten::linear 11277 1.44 %\n",
      "- aten::matmul 11248 99.74 %\n",
      "  - aten::mm 11062 98.09 %\n",
      "aten::silu 2767 0.35 %\n",
      "aten::linear 11279 1.44 %\n",
      "- aten::matmul 11248 99.73 %\n",
      "  - aten::mm 11031 97.80 %\n",
      "aten::mul 4345 0.56 %\n",
      "aten::linear 12343 1.58 %\n",
      "- aten::matmul 12313 99.76 %\n",
      "  - aten::mm 12086 97.92 %\n",
      "aten::add 1567 0.20 %\n",
      "aten::to 1761 0.22 %\n",
      "- aten::_to_copy 1755 99.66 %\n",
      "  - aten::copy_ 1739 98.75 %\n",
      "aten::pow 2124 0.27 %\n",
      "aten::mean 1268 0.16 %\n",
      "aten::mul 2040 0.26 %\n",
      "aten::to 1764 0.23 %\n",
      "- aten::_to_copy 1757 99.60 %\n",
      "  - aten::copy_ 1742 98.75 %\n",
      "aten::mul 1132 0.14 %\n",
      "aten::linear 5703 0.73 %\n",
      "- aten::matmul 5676 99.53 %\n",
      "  - aten::mm 5500 96.44 %\n",
      "aten::linear 5821 0.74 %\n",
      "- aten::matmul 5728 98.40 %\n",
      "  - aten::mm 5560 95.52 %\n",
      "aten::linear 5774 0.74 %\n",
      "- aten::matmul 5627 97.45 %\n",
      "  - aten::mm 5582 96.67 %\n",
      "aten::mul 1206 0.15 %\n",
      "aten::cat 1075 0.14 %\n",
      "aten::mul 1127 0.14 %\n",
      "aten::add 1645 0.21 %\n",
      "aten::mul 1105 0.14 %\n",
      "aten::cat 1060 0.14 %\n",
      "aten::mul 1111 0.14 %\n",
      "aten::add 1650 0.21 %\n",
      "FlashAttnFunc 5766 0.74 %\n",
      "aten::linear 5731 0.73 %\n",
      "- aten::matmul 5629 98.22 %\n",
      "  - aten::mm 5533 96.55 %\n",
      "aten::add 1601 0.20 %\n",
      "aten::to 1743 0.22 %\n",
      "- aten::_to_copy 1735 99.54 %\n",
      "  - aten::copy_ 1721 98.74 %\n",
      "aten::pow 2134 0.27 %\n",
      "aten::mean 1265 0.16 %\n",
      "aten::mul 2026 0.26 %\n",
      "aten::to 1761 0.22 %\n",
      "- aten::_to_copy 1754 99.60 %\n",
      "  - aten::copy_ 1740 98.81 %\n",
      "aten::mul 1130 0.14 %\n",
      "aten::linear 11370 1.45 %\n",
      "- aten::matmul 11342 99.75 %\n",
      "  - aten::mm 11211 98.60 %\n",
      "aten::silu 2756 0.35 %\n",
      "aten::linear 11266 1.44 %\n",
      "- aten::matmul 11237 99.74 %\n",
      "  - aten::mm 11076 98.31 %\n",
      "aten::mul 4337 0.55 %\n",
      "aten::linear 12428 1.59 %\n",
      "- aten::matmul 12399 99.77 %\n",
      "  - aten::mm 12230 98.41 %\n",
      "aten::add 1563 0.20 %\n",
      "aten::to 1775 0.23 %\n",
      "- aten::_to_copy 1768 99.61 %\n",
      "  - aten::copy_ 1754 98.82 %\n",
      "aten::pow 2126 0.27 %\n",
      "aten::mean 1266 0.16 %\n",
      "aten::mul 2040 0.26 %\n",
      "aten::to 1774 0.23 %\n",
      "- aten::_to_copy 1767 99.61 %\n",
      "  - aten::copy_ 1752 98.76 %\n",
      "aten::mul 1116 0.14 %\n",
      "aten::linear 5728 0.73 %\n",
      "- aten::matmul 5699 99.49 %\n",
      "  - aten::mm 5583 97.47 %\n",
      "aten::linear 5581 0.71 %\n",
      "- aten::matmul 5464 97.90 %\n",
      "  - aten::mm 5376 96.33 %\n",
      "aten::linear 5430 0.69 %\n",
      "- aten::matmul 5355 98.62 %\n",
      "  - aten::mm 5304 97.68 %\n",
      "aten::mul 1208 0.15 %\n",
      "aten::cat 1733 0.22 %\n",
      "aten::mul 804 0.10 %\n",
      "FlashAttnFunc 1100 0.14 %\n",
      "aten::linear 1144 0.15 %\n",
      "- aten::matmul 1030 90.03 %\n",
      "  - aten::mm 902 78.85 %\n",
      "aten::linear 2058 0.26 %\n",
      "- aten::matmul 2042 99.22 %\n",
      "  - aten::mm 1970 95.72 %\n",
      "aten::linear 2043 0.26 %\n",
      "- aten::matmul 2027 99.22 %\n",
      "  - aten::mm 1980 96.92 %\n",
      "aten::linear 2039 0.26 %\n",
      "- aten::matmul 2023 99.22 %\n",
      "  - aten::mm 1947 95.49 %\n",
      "aten::linear 908 0.12 %\n",
      "- aten::matmul 891 98.13 %\n",
      "  - aten::mm 816 89.87 %\n",
      "aten::linear 934 0.12 %\n",
      "- aten::matmul 876 93.79 %\n",
      "  - aten::mm 801 85.76 %\n",
      "aten::linear 933 0.12 %\n",
      "- aten::matmul 880 94.32 %\n",
      "  - aten::mm 807 86.50 %\n",
      "FlashAttnFunc 1024 0.13 %\n",
      "aten::linear 1003 0.13 %\n",
      "- aten::matmul 920 91.72 %\n",
      "  - aten::mm 822 81.95 %\n",
      "aten::linear 2003 0.26 %\n",
      "- aten::matmul 1987 99.20 %\n",
      "  - aten::mm 1887 94.21 %\n",
      "aten::linear 1966 0.25 %\n",
      "- aten::matmul 1950 99.19 %\n",
      "  - aten::mm 1883 95.78 %\n",
      "aten::linear 2089 0.27 %\n",
      "- aten::matmul 2073 99.23 %\n",
      "  - aten::mm 1995 95.50 %\n",
      "aten::linear 1124 0.14 %\n",
      "- aten::matmul 1107 98.49 %\n",
      "  - aten::mm 886 78.83 %\n",
      "aten::linear 1135 0.14 %\n",
      "- aten::matmul 1050 92.51 %\n",
      "  - aten::mm 959 84.49 %\n",
      "aten::linear 1119 0.14 %\n",
      "- aten::matmul 1039 92.85 %\n",
      "  - aten::mm 941 84.09 %\n",
      "FlashAttnFunc 1293 0.17 %\n",
      "aten::linear 1124 0.14 %\n",
      "- aten::matmul 1050 93.42 %\n",
      "  - aten::mm 964 85.77 %\n",
      "aten::linear 2388 0.31 %\n",
      "- aten::matmul 2351 98.45 %\n",
      "  - aten::mm 2180 91.29 %\n",
      "aten::linear 2271 0.29 %\n",
      "- aten::matmul 2252 99.16 %\n",
      "  - aten::mm 2178 95.90 %\n",
      "aten::linear 2337 0.30 %\n",
      "- aten::matmul 2320 99.27 %\n",
      "  - aten::mm 2243 95.98 %\n",
      "aten::linear 1085 0.14 %\n",
      "- aten::matmul 1068 98.43 %\n",
      "  - aten::mm 933 85.99 %\n",
      "aten::linear 1120 0.14 %\n",
      "- aten::matmul 1028 91.79 %\n",
      "  - aten::mm 929 82.95 %\n",
      "aten::linear 1129 0.14 %\n",
      "- aten::matmul 1007 89.19 %\n",
      "  - aten::mm 918 81.31 %\n",
      "FlashAttnFunc 1224 0.16 %\n",
      "aten::linear 1296 0.17 %\n",
      "- aten::matmul 1123 86.65 %\n",
      "  - aten::mm 970 74.85 %\n",
      "aten::linear 2428 0.31 %\n",
      "- aten::matmul 2313 95.26 %\n",
      "  - aten::mm 2115 87.11 %\n",
      "aten::linear 2553 0.33 %\n",
      "- aten::matmul 2354 92.21 %\n",
      "  - aten::mm 2118 82.96 %\n",
      "aten::linear 2357 0.30 %\n",
      "- aten::matmul 2339 99.24 %\n",
      "  - aten::mm 2210 93.76 %\n",
      "aten::linear 1104 0.14 %\n",
      "- aten::matmul 1060 96.01 %\n",
      "  - aten::mm 918 83.15 %\n",
      "aten::linear 1342 0.17 %\n",
      "- aten::matmul 1096 81.67 %\n",
      "  - aten::mm 959 71.46 %\n",
      "aten::linear 1307 0.17 %\n",
      "- aten::matmul 1110 84.93 %\n",
      "  - aten::mm 947 72.46 %\n",
      "aten::matmul 842 0.11 %\n",
      "FlashAttnFunc 1354 0.17 %\n",
      "aten::linear 1103 0.14 %\n",
      "- aten::matmul 995 90.21 %\n",
      "  - aten::mm 902 81.78 %\n",
      "aten::linear 2132 0.27 %\n",
      "- aten::matmul 2116 99.25 %\n",
      "  - aten::mm 2055 96.39 %\n",
      "aten::linear 2238 0.29 %\n",
      "- aten::matmul 2141 95.67 %\n",
      "  - aten::mm 2071 92.54 %\n",
      "aten::linear 2282 0.29 %\n",
      "- aten::matmul 2264 99.21 %\n",
      "  - aten::mm 2135 93.56 %\n",
      "aten::linear 1077 0.14 %\n",
      "- aten::matmul 958 88.95 %\n",
      "  - aten::mm 891 82.73 %\n",
      "aten::linear 1138 0.15 %\n",
      "- aten::matmul 962 84.53 %\n",
      "  - aten::mm 877 77.07 %\n",
      "aten::linear 1058 0.14 %\n",
      "- aten::matmul 935 88.37 %\n",
      "  - aten::mm 854 80.72 %\n",
      "FlashAttnFunc 991 0.13 %\n",
      "aten::linear 1028 0.13 %\n",
      "- aten::matmul 906 88.13 %\n",
      "  - aten::mm 846 82.30 %\n",
      "aten::linear 2040 0.26 %\n",
      "- aten::matmul 2024 99.22 %\n",
      "  - aten::mm 1973 96.72 %\n",
      "aten::linear 2322 0.30 %\n",
      "- aten::matmul 2177 93.76 %\n",
      "  - aten::mm 1973 84.97 %\n",
      "aten::linear 2281 0.29 %\n",
      "- aten::matmul 2265 99.30 %\n",
      "  - aten::mm 2124 93.12 %\n",
      "aten::linear 1207 0.15 %\n",
      "- aten::matmul 1100 91.14 %\n",
      "  - aten::mm 918 76.06 %\n",
      "aten::linear 1253 0.16 %\n",
      "- aten::matmul 1062 84.76 %\n",
      "  - aten::mm 915 73.02 %\n",
      "aten::linear 1201 0.15 %\n",
      "- aten::matmul 1057 88.01 %\n",
      "  - aten::mm 867 72.19 %\n",
      "FlashAttnFunc 1140 0.15 %\n",
      "aten::linear 1264 0.16 %\n",
      "- aten::matmul 1067 84.41 %\n",
      "  - aten::mm 965 76.34 %\n",
      "aten::linear 2351 0.30 %\n",
      "- aten::matmul 2312 98.34 %\n",
      "  - aten::mm 2157 91.75 %\n",
      "aten::linear 2326 0.30 %\n",
      "- aten::matmul 2308 99.23 %\n",
      "  - aten::mm 2186 93.98 %\n",
      "aten::linear 2371 0.30 %\n",
      "- aten::matmul 2353 99.24 %\n",
      "  - aten::mm 2245 94.69 %\n",
      "aten::linear 1133 0.14 %\n",
      "- aten::matmul 1116 98.50 %\n",
      "  - aten::mm 986 87.03 %\n",
      "aten::linear 1262 0.16 %\n",
      "- aten::matmul 1065 84.39 %\n",
      "  - aten::mm 930 73.69 %\n",
      "aten::linear 1424 0.18 %\n",
      "- aten::matmul 1176 82.58 %\n",
      "  - aten::mm 976 68.54 %\n",
      "FlashAttnFunc 1170 0.15 %\n",
      "aten::linear 1252 0.16 %\n",
      "- aten::matmul 1059 84.58 %\n",
      "  - aten::mm 930 74.28 %\n",
      "aten::linear 2243 0.29 %\n",
      "- aten::matmul 2226 99.24 %\n",
      "  - aten::mm 2064 92.02 %\n",
      "aten::linear 2411 0.31 %\n",
      "- aten::matmul 2252 93.41 %\n",
      "  - aten::mm 2104 87.27 %\n",
      "aten::linear 2297 0.29 %\n",
      "- aten::matmul 2280 99.26 %\n",
      "  - aten::mm 2158 93.95 %\n",
      "aten::linear 1132 0.14 %\n",
      "- aten::matmul 1115 98.50 %\n",
      "  - aten::mm 930 82.16 %\n",
      "aten::linear 1201 0.15 %\n",
      "- aten::matmul 998 83.10 %\n",
      "  - aten::mm 897 74.69 %\n",
      "aten::linear 1056 0.13 %\n",
      "- aten::matmul 937 88.73 %\n",
      "  - aten::mm 881 83.43 %\n",
      "FlashAttnFunc 1009 0.13 %\n",
      "aten::linear 994 0.13 %\n",
      "- aten::matmul 918 92.35 %\n",
      "  - aten::mm 876 88.13 %\n",
      "aten::linear 2173 0.28 %\n",
      "- aten::matmul 2084 95.90 %\n",
      "  - aten::mm 2043 94.02 %\n",
      "aten::linear 2411 0.31 %\n",
      "- aten::matmul 2327 96.52 %\n",
      "  - aten::mm 2065 85.65 %\n",
      "aten::linear 2357 0.30 %\n",
      "- aten::matmul 2340 99.28 %\n",
      "  - aten::mm 2121 89.99 %\n",
      "aten::linear 1091 0.14 %\n",
      "- aten::matmul 1075 98.53 %\n",
      "  - aten::mm 922 84.51 %\n",
      "aten::linear 1215 0.16 %\n",
      "- aten::matmul 1081 88.97 %\n",
      "  - aten::mm 931 76.63 %\n",
      "aten::linear 1078 0.14 %\n",
      "- aten::matmul 956 88.68 %\n",
      "  - aten::mm 896 83.12 %\n",
      "FlashAttnFunc 975 0.12 %\n",
      "aten::linear 1120 0.14 %\n",
      "- aten::matmul 931 83.12 %\n",
      "  - aten::mm 872 77.86 %\n",
      "aten::linear 2221 0.28 %\n",
      "- aten::matmul 2121 95.50 %\n",
      "  - aten::mm 2087 93.97 %\n",
      "aten::linear 2159 0.28 %\n",
      "- aten::matmul 2142 99.21 %\n",
      "  - aten::mm 2087 96.67 %\n",
      "aten::linear 2223 0.28 %\n",
      "- aten::matmul 2206 99.24 %\n",
      "  - aten::mm 2148 96.63 %\n",
      "aten::linear 1002 0.13 %\n",
      "- aten::matmul 985 98.30 %\n",
      "  - aten::mm 893 89.12 %\n",
      "aten::linear 1025 0.13 %\n",
      "- aten::matmul 966 94.24 %\n",
      "  - aten::mm 892 87.02 %\n",
      "aten::linear 1303 0.17 %\n",
      "- aten::matmul 1183 90.79 %\n",
      "  - aten::mm 962 73.83 %\n",
      "FlashAttnFunc 977 0.12 %\n",
      "aten::linear 1014 0.13 %\n",
      "- aten::matmul 961 94.77 %\n",
      "  - aten::mm 897 88.46 %\n",
      "aten::linear 2190 0.28 %\n",
      "- aten::matmul 2173 99.22 %\n",
      "  - aten::mm 2110 96.35 %\n",
      "aten::linear 2162 0.28 %\n",
      "- aten::matmul 2146 99.26 %\n",
      "  - aten::mm 2089 96.62 %\n",
      "aten::linear 2331 0.30 %\n",
      "- aten::matmul 2315 99.31 %\n",
      "  - aten::mm 2245 96.31 %\n",
      "aten::linear 1086 0.14 %\n",
      "- aten::matmul 1069 98.43 %\n",
      "  - aten::mm 947 87.20 %\n",
      "aten::linear 1112 0.14 %\n",
      "- aten::matmul 1047 94.15 %\n",
      "  - aten::mm 967 86.96 %\n",
      "aten::linear 1054 0.13 %\n",
      "- aten::matmul 991 94.02 %\n",
      "  - aten::mm 932 88.43 %\n",
      "FlashAttnFunc 1061 0.14 %\n",
      "aten::linear 1034 0.13 %\n",
      "- aten::matmul 971 93.91 %\n",
      "  - aten::mm 930 89.94 %\n",
      "aten::linear 2268 0.29 %\n",
      "- aten::matmul 2250 99.21 %\n",
      "  - aten::mm 2178 96.03 %\n",
      "aten::linear 2256 0.29 %\n",
      "- aten::matmul 2237 99.16 %\n",
      "  - aten::mm 2173 96.32 %\n",
      "aten::linear 2330 0.30 %\n",
      "- aten::matmul 2313 99.27 %\n",
      "  - aten::mm 2262 97.08 %\n",
      "aten::linear 1063 0.14 %\n",
      "- aten::matmul 1045 98.31 %\n",
      "  - aten::mm 953 89.65 %\n",
      "aten::linear 1054 0.13 %\n",
      "- aten::matmul 994 94.31 %\n",
      "  - aten::mm 964 91.46 %\n",
      "aten::linear 1070 0.14 %\n",
      "- aten::matmul 1014 94.77 %\n",
      "  - aten::mm 984 91.96 %\n",
      "FlashAttnFunc 1245 0.16 %\n",
      "aten::linear 1167 0.15 %\n",
      "- aten::matmul 1101 94.34 %\n",
      "  - aten::mm 1001 85.78 %\n",
      "aten::linear 2534 0.32 %\n",
      "- aten::matmul 2515 99.25 %\n",
      "  - aten::mm 2351 92.78 %\n",
      "aten::linear 2446 0.31 %\n",
      "- aten::matmul 2427 99.22 %\n",
      "  - aten::mm 2345 95.87 %\n",
      "aten::linear 2523 0.32 %\n",
      "- aten::matmul 2504 99.25 %\n",
      "  - aten::mm 2425 96.12 %\n",
      "aten::linear 1135 0.14 %\n",
      "- aten::matmul 1117 98.41 %\n",
      "  - aten::mm 1000 88.11 %\n",
      "aten::linear 1154 0.15 %\n",
      "- aten::matmul 1083 93.85 %\n",
      "  - aten::mm 995 86.22 %\n",
      "aten::linear 1204 0.15 %\n",
      "- aten::matmul 1081 89.78 %\n",
      "  - aten::mm 996 82.72 %\n",
      "FlashAttnFunc 1238 0.16 %\n",
      "aten::linear 1163 0.15 %\n",
      "- aten::matmul 1096 94.24 %\n",
      "  - aten::mm 1004 86.33 %\n",
      "aten::linear 2649 0.34 %\n",
      "- aten::matmul 2630 99.28 %\n",
      "  - aten::mm 2345 88.52 %\n",
      "aten::linear 2426 0.31 %\n",
      "- aten::matmul 2408 99.26 %\n",
      "  - aten::mm 2341 96.50 %\n",
      "aten::linear 2628 0.34 %\n",
      "- aten::matmul 2588 98.48 %\n",
      "  - aten::mm 2451 93.26 %\n",
      "aten::linear 1207 0.15 %\n",
      "- aten::matmul 1171 97.02 %\n",
      "  - aten::mm 1002 83.02 %\n",
      "aten::linear 1389 0.18 %\n",
      "- aten::matmul 1166 83.95 %\n",
      "  - aten::mm 1011 72.79 %\n",
      "aten::linear 1391 0.18 %\n",
      "- aten::matmul 1140 81.96 %\n",
      "  - aten::mm 996 71.60 %\n",
      "FlashAttnFunc 1261 0.16 %\n",
      "aten::linear 1388 0.18 %\n",
      "- aten::matmul 1173 84.51 %\n",
      "  - aten::mm 976 70.32 %\n",
      "aten::linear 2459 0.31 %\n",
      "- aten::matmul 2439 99.19 %\n",
      "  - aten::mm 2278 92.64 %\n",
      "aten::linear 2418 0.31 %\n",
      "- aten::matmul 2401 99.30 %\n",
      "  - aten::mm 2241 92.68 %\n",
      "aten::linear 2497 0.32 %\n",
      "- aten::matmul 2480 99.32 %\n",
      "  - aten::mm 2394 95.88 %\n",
      "aten::linear 1215 0.16 %\n",
      "- aten::matmul 1196 98.44 %\n",
      "  - aten::mm 1020 83.95 %\n",
      "aten::linear 1274 0.16 %\n",
      "- aten::matmul 1126 88.38 %\n",
      "  - aten::mm 974 76.45 %\n",
      "aten::linear 1186 0.15 %\n",
      "- aten::matmul 1084 91.40 %\n",
      "  - aten::mm 969 81.70 %\n",
      "FlashAttnFunc 1363 0.17 %\n",
      "aten::linear 1268 0.16 %\n",
      "- aten::matmul 1173 92.51 %\n",
      "  - aten::mm 979 77.21 %\n",
      "aten::linear 2587 0.33 %\n",
      "- aten::matmul 2435 94.12 %\n",
      "  - aten::mm 2162 83.57 %\n",
      "aten::linear 2374 0.30 %\n",
      "- aten::matmul 2356 99.24 %\n",
      "  - aten::mm 2152 90.65 %\n",
      "aten::linear 2358 0.30 %\n",
      "- aten::matmul 2340 99.24 %\n",
      "  - aten::mm 2256 95.67 %\n",
      "aten::linear 1522 0.19 %\n",
      "- aten::matmul 1330 87.39 %\n",
      "  - aten::mm 979 64.32 %\n",
      "aten::linear 1217 0.16 %\n",
      "- aten::matmul 1116 91.70 %\n",
      "  - aten::mm 982 80.69 %\n",
      "aten::linear 1205 0.15 %\n",
      "- aten::matmul 1106 91.78 %\n",
      "  - aten::mm 978 81.16 %\n",
      "FlashAttnFunc 1317 0.17 %\n",
      "aten::linear 1221 0.16 %\n",
      "- aten::matmul 1132 92.71 %\n",
      "  - aten::mm 951 77.89 %\n",
      "aten::linear 2421 0.31 %\n",
      "- aten::matmul 2264 93.52 %\n",
      "  - aten::mm 2091 86.37 %\n",
      "aten::linear 2193 0.28 %\n",
      "- aten::matmul 2177 99.27 %\n",
      "  - aten::mm 2056 93.75 %\n",
      "aten::linear 2301 0.29 %\n",
      "- aten::matmul 2285 99.30 %\n",
      "  - aten::mm 2154 93.61 %\n",
      "aten::linear 1136 0.15 %\n",
      "- aten::matmul 1120 98.59 %\n",
      "  - aten::mm 939 82.66 %\n",
      "aten::linear 1154 0.15 %\n",
      "- aten::matmul 1072 92.89 %\n",
      "  - aten::mm 897 77.73 %\n",
      "aten::linear 1204 0.15 %\n",
      "- aten::matmul 1122 93.19 %\n",
      "  - aten::mm 943 78.32 %\n",
      "FlashAttnFunc 1204 0.15 %\n",
      "aten::linear 1185 0.15 %\n",
      "- aten::matmul 1096 92.49 %\n",
      "  - aten::mm 904 76.29 %\n",
      "aten::linear 2468 0.32 %\n",
      "- aten::matmul 2254 91.33 %\n",
      "  - aten::mm 2102 85.17 %\n",
      "aten::linear 2112 0.27 %\n",
      "- aten::matmul 2096 99.24 %\n",
      "  - aten::mm 1972 93.37 %\n",
      "aten::linear 2237 0.29 %\n",
      "- aten::matmul 2221 99.28 %\n",
      "  - aten::mm 2125 94.99 %\n",
      "aten::linear 1255 0.16 %\n",
      "- aten::matmul 1064 84.78 %\n",
      "  - aten::mm 868 69.16 %\n",
      "aten::linear 1202 0.15 %\n",
      "- aten::matmul 1090 90.68 %\n",
      "  - aten::mm 900 74.88 %\n",
      "aten::linear 1138 0.15 %\n",
      "- aten::matmul 1056 92.79 %\n",
      "  - aten::mm 916 80.49 %\n",
      "aten::to 790 0.10 %\n",
      "FlashAttnFunc 1963 0.25 %\n",
      "aten::linear 1319 0.17 %\n",
      "- aten::matmul 1216 92.19 %\n",
      "  - aten::mm 1057 80.14 %\n",
      "aten::linear 3291 0.42 %\n",
      "- aten::matmul 2536 77.06 %\n",
      "  - aten::mm 1995 60.62 %\n",
      "aten::linear 1881 0.24 %\n",
      "- aten::matmul 1865 99.15 %\n",
      "  - aten::mm 1730 91.97 %\n",
      "aten::linear 2015 0.26 %\n",
      "- aten::matmul 1999 99.21 %\n",
      "  - aten::mm 1860 92.31 %\n",
      "aten::linear 1141 0.15 %\n",
      "- aten::matmul 1010 88.52 %\n",
      "aten::linear 1060 0.14 %\n",
      "- aten::matmul 969 91.42 %\n",
      "  - aten::mm 794 74.91 %\n",
      "aten::linear 1094 0.14 %\n",
      "- aten::matmul 998 91.22 %\n",
      "  - aten::mm 806 73.67 %\n",
      "aten::matmul 2125 0.27 %\n",
      "aten::to 856 0.11 %\n",
      "aten::to 809 0.10 %\n",
      "FlashAttnFunc 1794 0.23 %\n",
      "aten::linear 1451 0.19 %\n",
      "- aten::matmul 1346 92.76 %\n",
      "  - aten::mm 1165 80.29 %\n",
      "aten::linear 3158 0.40 %\n",
      "- aten::matmul 2416 76.50 %\n",
      "  - aten::mm 1881 59.56 %\n",
      "aten::linear 1928 0.25 %\n",
      "- aten::matmul 1912 99.17 %\n",
      "  - aten::mm 1724 89.42 %\n",
      "aten::linear 1920 0.25 %\n",
      "- aten::matmul 1904 99.17 %\n",
      "  - aten::mm 1773 92.34 %\n",
      "aten::linear 955 0.12 %\n",
      "- aten::matmul 941 98.53 %\n",
      "aten::linear 996 0.13 %\n",
      "- aten::matmul 902 90.56 %\n",
      "aten::linear 992 0.13 %\n",
      "- aten::matmul 903 91.03 %\n",
      "aten::matmul 887 0.11 %\n",
      "FlashAttnFunc 1097 0.14 %\n",
      "aten::linear 996 0.13 %\n",
      "- aten::matmul 873 87.65 %\n",
      "aten::linear 2031 0.26 %\n",
      "- aten::matmul 1783 87.79 %\n",
      "  - aten::mm 1598 78.68 %\n",
      "aten::linear 1609 0.21 %\n",
      "- aten::matmul 1592 98.94 %\n",
      "  - aten::mm 1512 93.97 %\n",
      "aten::linear 1847 0.24 %\n",
      "- aten::matmul 1833 99.24 %\n",
      "  - aten::mm 1640 88.79 %\n",
      "aten::linear 9440 1.21 %\n",
      "- aten::matmul 9290 98.41 %\n",
      "  - aten::mm 9035 95.71 %\n",
      "aten::to 1166 0.15 %\n",
      "- aten::_to_copy 1163 99.74 %\n",
      "  - aten::copy_ 1154 98.97 %\n"
     ]
    }
   ],
   "source": [
    "for event in events:\n",
    "    if event.cpu_parent is not None and event.cpu_parent.id == events[0].id:\n",
    "        if event.cuda_time>=threshold:\n",
    "            print(event.name, int(event.cuda_time), f\"{(event.cuda_time/events[0].cuda_time*100):.2f} %\")\n",
    "            if event.cpu_children is not None:\n",
    "                for child_event in event.cpu_children:\n",
    "                    if child_event.cuda_time>=threshold:\n",
    "                        print(\"-\", child_event.name, int(child_event.cuda_time), f\"{(child_event.cuda_time/event.cuda_time*100):.2f} %\")\n",
    "                        if child_event.cpu_children is not None:\n",
    "                            for grandchild_event in child_event.cpu_children:\n",
    "                                if grandchild_event.cuda_time>=threshold:\n",
    "                                    print(\" \",\"-\", grandchild_event.name, int(grandchild_event.cuda_time), f\"{(grandchild_event.cuda_time/event.cuda_time*100):.2f} %\")\n",
    "                                    if grandchild_event.cpu_children is not None:\n",
    "                                        for Grandchild_event in grandchild_event.cpu_children:\n",
    "                                            if Grandchild_event.cuda_time>=threshold:\n",
    "                                                print(\" \",\" \",\"-\", Grandchild_event.name, int(Grandchild_event.cuda_time), f\"{(Grandchild_event.cuda_time/event.cuda_time*100):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b448759-b639-47a5-be2e-1bd1cd81036b",
   "metadata": {},
   "source": [
    "## Install pytorch and cuda with conda - Feb 2024\n",
    "\n",
    "https://twitter.com/jeremyphoward/status/1697435241152127369\n",
    "\n",
    "1. Install miniconda\n",
    "\n",
    "https://github.com/fastai/fastsetup/blob/master/setup-conda.sh\n",
    "\n",
    "2. Find out what CUDA version PyTorch expects by going to their website and seeing what the latest \"compute platform\" version is.\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "> conda install pytorch ... pytorch-cuda=12.1 ...\n",
    "\n",
    "3. Install CUDA\n",
    "\n",
    "```\n",
    "conda install cuda -c nvidia/label/cuda-12.1.0\n",
    "```\n",
    "\n",
    "4.Copy the command to install pytorch from their website, but replace `-c nvidia` with a version specific label, as shown below:\n",
    "\n",
    "```\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia/label/cuda-12.1.0`\n",
    "```\n",
    "\n",
    "**ubuntu version**\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# lsb_release -a\n",
    "No LSB modules are available.\n",
    "Distributor ID: Ubuntu\n",
    "Description:    Ubuntu 22.04.3 LTS\n",
    "Release:        22.04\n",
    "Codename:       jammy\n",
    "\n",
    "root@ac9978bd266c:~# uname -a\n",
    "Linux ac9978bd266c 5.15.0-89-generic #99-Ubuntu SMP Mon Oct 30 20:42:41 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n",
    "\n",
    "root@ac9978bd266c:~# cat /etc/os-release\n",
    "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
    "NAME=\"Ubuntu\"\n",
    "VERSION_ID=\"22.04\"\n",
    "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
    "VERSION_CODENAME=jammy\n",
    "ID=ubuntu\n",
    "ID_LIKE=debian\n",
    "HOME_URL=\"https://www.ubuntu.com/\"\n",
    "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
    "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
    "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
    "UBUNTU_CODENAME=jammy\n",
    "```\n",
    "\n",
    "**container setup**\n",
    "\n",
    "Docker container ID\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# echo $HOSTNAME\n",
    "ac9978bd266c\n",
    "```\n",
    "\n",
    "Docker container entrypoint\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /docker-entrypoint.sh\n",
    "#!/bin/bash\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "conda activate py3.10                   \n",
    "echo \"PasswordAuthentication no\" >> /etc/ssh/sshd_config\n",
    "service ssh start\n",
    "export SHELL=\"/bin/bash\"\n",
    "\n",
    "env HOME=/home code-server --host 0.0.0.0 --port 7007 --auth none&\n",
    "env HOME=/home jupyter lab --ip=0.0.0.0 --NotebookApp.token=$TOKEN  --allow-root --port 8889\n",
    "```\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# ps aux\n",
    "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
    "root           1  0.0  0.0   4360  3392 ?        Ss   08:26   0:00 /bin/bash /docker-entrypoint.sh\n",
    "root          18  0.0  0.0  15428  3732 ?        Ss   08:26   0:00 sshd: /usr/sbin/sshd [listener] 0 of 10-100 startups\n",
    "root          19  0.0  0.0 1238088 64232 ?       Sl   08:26   0:00 /usr/lib/code-server/lib/node /usr/lib/code-server --host 0.0.0.0 --port 7007 --auth none\n",
    "root          20  0.2  0.0 888616 94924 ?        Rl   08:26   0:07 /root/miniconda3/envs/py3.10/bin/python /root/miniconda3/envs/py3.10/bin/jupyter-lab --ip=0.0.0.0 --NotebookApp.token=3vAcOEC8d551ymv0ppLqoR69HaQEfJB-1KEiXP1WwS8WlZbWf0_R7tCOxXoV8T8G --allo\n",
    "root          46  0.0  0.0 1172236 64432 ?       Sl   08:26   0:00 /usr/lib/code-server/lib/node /usr/lib/code-server/out/node/entry\n",
    "```\n",
    "\n",
    "Jarvislab Urls\n",
    "\n",
    "- Jupyterlab: https://ac9978bd266c.notebooksh.jarvislabs.net/\n",
    "- VS Code:    https://ac9978bd266c0.notebooksh.jarvislabs.net/\n",
    "- Any service listening on port 6006 inside the container: https://ac9978bd266c1.notebooksh.jarvislabs.net/\n",
    "\n",
    "**miniconda install**\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:/root/miniconda3# which conda\n",
    "/root/miniconda3/bin/conda\n",
    "\n",
    "root@ac9978bd266c:/root/miniconda3# conda --version\n",
    "conda 23.11.0\n",
    "\n",
    "root@ac9978bd266c:~# which python\n",
    "/root/miniconda3/envs/py3.10/bin/python\n",
    "\n",
    "root@ac9978bd266c:/root/miniconda3# python --version\n",
    "Python 3.10.13\n",
    "```\n",
    "\n",
    "**miniconda environments**\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:/root/miniconda3# conda env list\n",
    "base                     /root/miniconda3\n",
    "py3.10                   /root/miniconda3/envs/py3.10\n",
    "\n",
    "root@ac9978bd266c:/root/miniconda3/envs/py3.10/conda-meta# cat history \n",
    "==> 2024-01-30 10:00:19 <==\n",
    "# cmd: /root/miniconda3/bin/conda create -n py3.10 python=3.10\n",
    "# conda version: 23.11.0\n",
    "```\n",
    "\n",
    "**apt source**\n",
    "\n",
    "https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64 \n",
    "\n",
    "**nvidia packages**\n",
    "\n",
    "```\n",
    "cuda-cccl-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-compat-12-3/unknown,now 545.23.08-1 amd64 [installed]\n",
    "cuda-cudart-12-3/unknown,now 12.3.101-1 amd64 [installed]\n",
    "cuda-cudart-dev-12-3/unknown,now 12.3.101-1 amd64 [installed]\n",
    "cuda-cuobjdump-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-cupti-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-cupti-dev-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-cuxxfilt-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-driver-dev-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-gdb-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-nvdisasm-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-nvml-dev-12-3/unknown,now 12.3.101-1 amd64 [installed]\n",
    "cuda-nvprof-12-3/unknown,now 12.3.101-1 amd64 [installed]\n",
    "cuda-nvprune-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-nvtx-12-3/unknown,now 12.3.101-1 amd64 [installed]\n",
    "cuda-opencl-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-opencl-dev-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-profiler-api-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-sanitizer-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "cuda-toolkit-12-3-config-common/unknown,now 12.3.101-1 all [installed,automatic]\n",
    "cuda-toolkit-12-config-common/unknown,now 12.3.101-1 all [installed,automatic]\n",
    "cuda-toolkit-config-common/unknown,now 12.3.101-1 all [installed,automatic]\n",
    "libcublas-12-3/unknown,now 12.3.4.1-1 amd64 [installed]\n",
    "libcublas-dev-12-3/unknown,now 12.3.4.1-1 amd64 [installed]\n",
    "libcufft-12-3/unknown,now 11.0.12.1-1 amd64 [installed,automatic]\n",
    "libcufft-dev-12-3/unknown,now 11.0.12.1-1 amd64 [installed,automatic]\n",
    "libcufile-12-3/unknown,now 1.8.1.2-1 amd64 [installed,automatic]\n",
    "libcufile-dev-12-3/unknown,now 1.8.1.2-1 amd64 [installed,automatic]\n",
    "libcusolver-12-3/unknown,now 11.5.4.101-1 amd64 [installed,automatic]\n",
    "libcusolver-dev-12-3/unknown,now 11.5.4.101-1 amd64 [installed,automatic]\n",
    "libcusparse-12-3/unknown,now 12.2.0.103-1 amd64 [installed]\n",
    "libcusparse-dev-12-3/unknown,now 12.2.0.103-1 amd64 [installed]\n",
    "libnpp-12-3/unknown,now 12.2.3.2-1 amd64 [installed]\n",
    "libnpp-dev-12-3/unknown,now 12.2.3.2-1 amd64 [installed]\n",
    "libnvjitlink-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "libnvjitlink-dev-12-3/unknown,now 12.3.101-1 amd64 [installed,automatic]\n",
    "libnvjpeg-12-3/unknown,now 12.3.0.81-1 amd64 [installed,automatic]\n",
    "libnvjpeg-dev-12-3/unknown,now 12.3.0.81-1 amd64 [installed,automatic]\n",
    "nsight-compute-2023.3.1/unknown,now 2023.3.1.1-1 amd64 [installed,automatic]\n",
    "```\n",
    "\n",
    "**Environment variables**\n",
    "\n",
    "```\n",
    "NV_LIBCUBLAS_VERSION=12.3.4.1-1\n",
    "NVIDIA_VISIBLE_DEVICES=3\n",
    "NV_NVML_DEV_VERSION=12.3.101-1\n",
    "NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.3\n",
    "NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
    "PYTHON_VERSION=3.10\n",
    "NVIDIA_REQUIRE_CUDA=cuda>=12.3 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\n",
    "NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-3=12.3.4.1-1\n",
    "NV_NVTX_VERSION=12.3.101-1\n",
    "NV_CUDA_CUDART_DEV_VERSION=12.3.101-1\n",
    "NV_LIBCUSPARSE_VERSION=12.2.0.103-1\n",
    "NV_LIBNPP_VERSION=12.2.3.2-1\n",
    "NCCL_VERSION=2.19.3-1\n",
    "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-3=12.3.101-1\n",
    "NV_LIBNPP_PACKAGE=libnpp-12-3=12.2.3.2-1\n",
    "NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
    "NV_LIBCUBLAS_DEV_VERSION=12.3.4.1-1\n",
    "NVIDIA_PRODUCT_NAME=CUDA\n",
    "NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-3\n",
    "NV_CUDA_CUDART_VERSION=12.3.101-1\n",
    "CUDA_VERSION=12.3.1\n",
    "NV_LIBCUBLAS_PACKAGE=libcublas-12-3=12.3.4.1-1\n",
    "NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-3=12.3.1-1\n",
    "PYDEVD_USE_FRAME_EVAL=NO\n",
    "NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-3=12.2.3.2-1\n",
    "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-3\n",
    "NV_LIBNPP_DEV_VERSION=12.2.3.2-1\n",
    "NV_LIBCUSPARSE_DEV_VERSION=12.2.0.103-1\n",
    "LIBRARY_PATH=/usr/local/cuda/lib64/stubs\n",
    "NV_CUDA_LIB_VERSION=12.3.1-1\n",
    "NVARCH=x86_64\n",
    "NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-3\n",
    "NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.3\n",
    "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
    "NV_CUDA_NSIGHT_COMPUTE_VERSION=12.3.1-1\n",
    "NV_NVPROF_VERSION=12.3.101-1\n",
    "PATH=/root/miniconda3/envs/py3.10/bin:/root/miniconda3/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
    "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
    "NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
    "```\n",
    "\n",
    "**python packages**\n",
    "\n",
    "```\n",
    "libpython3-stdlib/jammy-updates,jammy-security,now 3.10.6-1~22.04 amd64 [installed,automatic]\n",
    "libpython3.10-minimal/jammy-updates,jammy-security,now 3.10.12-1~22.04.3 amd64 [installed,automatic]\n",
    "libpython3.10-stdlib/jammy-updates,jammy-security,now 3.10.12-1~22.04.3 amd64 [installed,automatic]\n",
    "libpython3.10/jammy-updates,jammy-security,now 3.10.12-1~22.04.3 amd64 [installed,automatic]\n",
    "python3-dbus/jammy,now 1.2.18-3build1 amd64 [installed,automatic]\n",
    "python3-distro/jammy,now 1.7.0-1 all [installed,automatic]\n",
    "python3-gi/jammy-updates,now 3.42.1-0ubuntu1 amd64 [installed,automatic]\n",
    "python3-minimal/jammy-updates,jammy-security,now 3.10.6-1~22.04 amd64 [installed,automatic]\n",
    "python3.10-minimal/jammy-updates,jammy-security,now 3.10.12-1~22.04.3 amd64 [installed,automatic]\n",
    "python3.10/jammy-updates,jammy-security,now 3.10.12-1~22.04.3 amd64 [installed,automatic]\n",
    "python3/jammy-updates,jammy-security,now 3.10.6-1~22.04 amd64 [installed,automatic]\n",
    "```\n",
    "\n",
    "```\n",
    "print(sys.path)\n",
    "['/home', \n",
    " '/root/miniconda3/envs/py3.10/lib/python310.zip', \n",
    " '/root/miniconda3/envs/py3.10/lib/python3.10', \n",
    " '/root/miniconda3/envs/py3.10/lib/python3.10/lib-dynload', \n",
    " '', \n",
    " '/root/miniconda3/envs/py3.10/lib/python3.10/site-packages']\n",
    "```\n",
    "\n",
    "**Jupyterlab config**\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_notebook_config.d/jupyter-lsp-notebook.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_notebook_config.d/jupyter-lsp-notebook.json\n",
    "{\n",
    "  \"NotebookApp\": {\n",
    "    \"nbserver_extensions\": {\n",
    "      \"jupyter_lsp\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_notebook_config.d/jupyterlab.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_notebook_config.d/jupyterlab.json\n",
    "{\n",
    "  \"NotebookApp\": {\n",
    "    \"nbserver_extensions\": {\n",
    "      \"jupyterlab\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyter-lsp-jupyter-server.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyter-lsp-jupyter-server.json\n",
    "{\n",
    "  \"ServerApp\": {\n",
    "    \"jpserver_extensions\": {\n",
    "      \"jupyter_lsp\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyter_server_terminals.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyter_server_terminals.json\n",
    "{\n",
    "  \"ServerApp\": {\n",
    "    \"jpserver_extensions\": {\n",
    "      \"jupyter_server_terminals\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyterlab.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/jupyterlab.json\n",
    "{\n",
    "  \"ServerApp\": {\n",
    "    \"jpserver_extensions\": {\n",
    "      \"jupyterlab\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/notebook.json\n",
    "\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/notebook.json\n",
    "{\n",
    "  \"ServerApp\": {\n",
    "    \"jpserver_extensions\": {\n",
    "      \"notebook\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/notebook_shim.json\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/jupyter_server_config.d/notebook_shim.json\n",
    "{\n",
    "    \"ServerApp\": {\n",
    "        \"jpserver_extensions\": {\n",
    "            \"notebook_shim\": true\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "/root/miniconda3/envs/py3.10/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json \n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat /root/miniconda3/envs/py3.10/etc/jupyter/nbconfig/notebook.d/widgetsnbextension.json \n",
    "{\n",
    "  \"load_extensions\": {\n",
    "    \"jupyter-js-widgets/extension\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**code-server config**\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# cat ~/.config/code-server/config.yaml\n",
    "\n",
    "> nothing\n",
    "\n",
    "root@ac9978bd266c:~# /usr/lib/code-server/bin/code-server --list-extensions\n",
    "\n",
    "> nothing\n",
    "```\n",
    "\n",
    "**pip list**\n",
    "\n",
    "```\n",
    "diffusers                 0.26.3\n",
    "jupyterlab                4.0.11\n",
    "numpy                     1.26.3\n",
    "nvidia-cublas-cu12        12.1.3.1\n",
    "nvidia-cuda-cupti-cu12    12.1.105\n",
    "nvidia-cuda-nvrtc-cu12    12.1.105\n",
    "nvidia-cuda-runtime-cu12  12.1.105\n",
    "nvidia-cudnn-cu12         8.9.2.26\n",
    "nvidia-cufft-cu12         11.0.2.54\n",
    "nvidia-curand-cu12        10.3.2.106\n",
    "nvidia-cusolver-cu12      11.4.5.107\n",
    "nvidia-cusparse-cu12      12.1.0.106\n",
    "nvidia-nccl-cu12          2.18.1\n",
    "nvidia-nvjitlink-cu12     12.3.101\n",
    "nvidia-nvtx-cu12          12.1.105\n",
    "pandas                    2.2.0\n",
    "spacy                     3.7.4\n",
    "torch                     2.1.2\n",
    "transformers              4.37.2\n",
    "triton                    2.1.0\n",
    "```\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:~# pip show torch\n",
    "Name: torch\n",
    "Version: 2.1.2\n",
    "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
    "Home-page: https://pytorch.org/\n",
    "Author: PyTorch Team\n",
    "Author-email: packages@pytorch.org\n",
    "License: BSD-3\n",
    "Location: /root/miniconda3/envs/py3.10/lib/python3.10/site-packages\n",
    "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
    "Required-by: torchaudio, torchvision\n",
    "```\n",
    "\n",
    "```\n",
    "root@ac9978bd266c:/root/miniconda3# conda list -n py3.10\n",
    "# packages in environment at /root/miniconda3/envs/py3.10:\n",
    "#\n",
    "# Name                    Version                   Build  Channel\n",
    "_libgcc_mutex             0.1                        main  \n",
    "_openmp_mutex             5.1                       1_gnu  \n",
    "annotated-types           0.6.0                    pypi_0    pypi\n",
    "anyio                     4.2.0                    pypi_0    pypi\n",
    "argon2-cffi               23.1.0                   pypi_0    pypi\n",
    "argon2-cffi-bindings      21.2.0                   pypi_0    pypi\n",
    "arrow                     1.3.0                    pypi_0    pypi\n",
    "asttokens                 2.4.1                    pypi_0    pypi\n",
    "async-lru                 2.0.4                    pypi_0    pypi\n",
    "attrs                     23.2.0                   pypi_0    pypi\n",
    "babel                     2.14.0                   pypi_0    pypi\n",
    "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
    "bleach                    6.1.0                    pypi_0    pypi\n",
    "blis                      0.7.11                   pypi_0    pypi\n",
    "bzip2                     1.0.8                h7b6447c_0  \n",
    "ca-certificates           2023.12.12           h06a4308_0  \n",
    "catalogue                 2.0.10                   pypi_0    pypi\n",
    "certifi                   2023.11.17               pypi_0    pypi\n",
    "cffi                      1.16.0                   pypi_0    pypi\n",
    "charset-normalizer        3.3.2                    pypi_0    pypi\n",
    "click                     8.1.7                    pypi_0    pypi\n",
    "cloudpathlib              0.16.0                   pypi_0    pypi\n",
    "comm                      0.2.1                    pypi_0    pypi\n",
    "confection                0.1.4                    pypi_0    pypi\n",
    "cymem                     2.0.8                    pypi_0    pypi\n",
    "debugpy                   1.8.0                    pypi_0    pypi\n",
    "decorator                 5.1.1                    pypi_0    pypi\n",
    "defusedxml                0.7.1                    pypi_0    pypi\n",
    "diffusers                 0.26.3                   pypi_0    pypi\n",
    "exceptiongroup            1.2.0                    pypi_0    pypi\n",
    "executing                 2.0.1                    pypi_0    pypi\n",
    "fastjsonschema            2.19.1                   pypi_0    pypi\n",
    "filelock                  3.13.1                   pypi_0    pypi\n",
    "fqdn                      1.5.1                    pypi_0    pypi\n",
    "fsspec                    2023.12.2                pypi_0    pypi\n",
    "ftfy                      6.1.3                    pypi_0    pypi\n",
    "huggingface-hub           0.20.3                   pypi_0    pypi\n",
    "idna                      3.6                      pypi_0    pypi\n",
    "importlib-metadata        7.0.1                    pypi_0    pypi\n",
    "ipykernel                 6.29.0                   pypi_0    pypi\n",
    "ipython                   8.20.0                   pypi_0    pypi\n",
    "ipywidgets                8.1.1                    pypi_0    pypi\n",
    "isoduration               20.11.0                  pypi_0    pypi\n",
    "jedi                      0.19.1                   pypi_0    pypi\n",
    "jinja2                    3.1.3                    pypi_0    pypi\n",
    "json5                     0.9.14                   pypi_0    pypi\n",
    "jsonpointer               2.4                      pypi_0    pypi\n",
    "jsonschema                4.21.1                   pypi_0    pypi\n",
    "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
    "jupyter-client            8.6.0                    pypi_0    pypi\n",
    "jupyter-core              5.7.1                    pypi_0    pypi\n",
    "jupyter-events            0.9.0                    pypi_0    pypi\n",
    "jupyter-lsp               2.2.2                    pypi_0    pypi\n",
    "jupyter-server            2.12.5                   pypi_0    pypi\n",
    "jupyter-server-terminals  0.5.2                    pypi_0    pypi\n",
    "jupyterlab                4.0.11                   pypi_0    pypi\n",
    "jupyterlab-pygments       0.3.0                    pypi_0    pypi\n",
    "jupyterlab-server         2.25.2                   pypi_0    pypi\n",
    "jupyterlab-widgets        3.0.9                    pypi_0    pypi\n",
    "langcodes                 3.3.0                    pypi_0    pypi\n",
    "ld_impl_linux-64          2.38                 h1181459_1  \n",
    "libffi                    3.4.4                h6a678d5_0  \n",
    "libgcc-ng                 11.2.0               h1234567_1  \n",
    "libgomp                   11.2.0               h1234567_1  \n",
    "libstdcxx-ng              11.2.0               h1234567_1  \n",
    "libuuid                   1.41.5               h5eee18b_0  \n",
    "markupsafe                2.1.4                    pypi_0    pypi\n",
    "matplotlib-inline         0.1.6                    pypi_0    pypi\n",
    "mistune                   3.0.2                    pypi_0    pypi\n",
    "mpmath                    1.3.0                    pypi_0    pypi\n",
    "murmurhash                1.0.10                   pypi_0    pypi\n",
    "nbclient                  0.9.0                    pypi_0    pypi\n",
    "nbconvert                 7.14.2                   pypi_0    pypi\n",
    "nbformat                  5.9.2                    pypi_0    pypi\n",
    "ncurses                   6.4                  h6a678d5_0  \n",
    "nest-asyncio              1.6.0                    pypi_0    pypi\n",
    "networkx                  3.2.1                    pypi_0    pypi\n",
    "notebook                  7.0.7                    pypi_0    pypi\n",
    "notebook-shim             0.2.3                    pypi_0    pypi\n",
    "numpy                     1.26.3                   pypi_0    pypi\n",
    "nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n",
    "nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n",
    "nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n",
    "nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n",
    "nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\n",
    "nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n",
    "nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n",
    "nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n",
    "nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n",
    "nvidia-nccl-cu12          2.18.1                   pypi_0    pypi\n",
    "nvidia-nvjitlink-cu12     12.3.101                 pypi_0    pypi\n",
    "nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n",
    "openssl                   3.0.12               h7f8727e_0  \n",
    "overrides                 7.7.0                    pypi_0    pypi\n",
    "packaging                 23.2                     pypi_0    pypi\n",
    "pandas                    2.2.0                    pypi_0    pypi\n",
    "pandocfilters             1.5.1                    pypi_0    pypi\n",
    "parso                     0.8.3                    pypi_0    pypi\n",
    "pexpect                   4.9.0                    pypi_0    pypi\n",
    "pillow                    10.2.0                   pypi_0    pypi\n",
    "pip                       23.3.2                   pypi_0    pypi\n",
    "platformdirs              4.1.0                    pypi_0    pypi\n",
    "preshed                   3.0.9                    pypi_0    pypi\n",
    "prometheus-client         0.19.0                   pypi_0    pypi\n",
    "prompt-toolkit            3.0.43                   pypi_0    pypi\n",
    "psutil                    5.9.8                    pypi_0    pypi\n",
    "ptyprocess                0.7.0                    pypi_0    pypi\n",
    "pure-eval                 0.2.2                    pypi_0    pypi\n",
    "pycparser                 2.21                     pypi_0    pypi\n",
    "pydantic                  2.6.1                    pypi_0    pypi\n",
    "pydantic-core             2.16.2                   pypi_0    pypi\n",
    "pygments                  2.17.2                   pypi_0    pypi\n",
    "python                    3.10.13              h955ad1f_0  \n",
    "python-dateutil           2.8.2                    pypi_0    pypi\n",
    "python-json-logger        2.0.7                    pypi_0    pypi\n",
    "pytz                      2024.1                   pypi_0    pypi\n",
    "pyyaml                    6.0.1                    pypi_0    pypi\n",
    "pyzmq                     25.1.2                   pypi_0    pypi\n",
    "readline                  8.2                  h5eee18b_0  \n",
    "referencing               0.33.0                   pypi_0    pypi\n",
    "regex                     2023.12.25               pypi_0    pypi\n",
    "requests                  2.31.0                   pypi_0    pypi\n",
    "rfc3339-validator         0.1.4                    pypi_0    pypi\n",
    "rfc3986-validator         0.1.1                    pypi_0    pypi\n",
    "rpds-py                   0.17.1                   pypi_0    pypi\n",
    "safetensors               0.4.2                    pypi_0    pypi\n",
    "send2trash                1.8.2                    pypi_0    pypi\n",
    "setuptools                68.2.2          py310h06a4308_0  \n",
    "six                       1.16.0                   pypi_0    pypi\n",
    "smart-open                6.4.0                    pypi_0    pypi\n",
    "sniffio                   1.3.0                    pypi_0    pypi\n",
    "soupsieve                 2.5                      pypi_0    pypi\n",
    "spacy                     3.7.4                    pypi_0    pypi\n",
    "spacy-legacy              3.0.12                   pypi_0    pypi\n",
    "spacy-loggers             1.0.5                    pypi_0    pypi\n",
    "sqlite                    3.41.2               h5eee18b_0  \n",
    "srsly                     2.4.8                    pypi_0    pypi\n",
    "stack-data                0.6.3                    pypi_0    pypi\n",
    "sympy                     1.12                     pypi_0    pypi\n",
    "terminado                 0.18.0                   pypi_0    pypi\n",
    "thinc                     8.2.3                    pypi_0    pypi\n",
    "tinycss2                  1.2.1                    pypi_0    pypi\n",
    "tk                        8.6.12               h1ccaba5_0  \n",
    "tokenizers                0.15.2                   pypi_0    pypi\n",
    "tomli                     2.0.1                    pypi_0    pypi\n",
    "torch                     2.1.2                    pypi_0    pypi\n",
    "torchaudio                2.1.2                    pypi_0    pypi\n",
    "torchvision               0.16.2                   pypi_0    pypi\n",
    "tornado                   6.4                      pypi_0    pypi\n",
    "tqdm                      4.66.2                   pypi_0    pypi\n",
    "traitlets                 5.14.1                   pypi_0    pypi\n",
    "transformers              4.37.2                   pypi_0    pypi\n",
    "triton                    2.1.0                    pypi_0    pypi\n",
    "typer                     0.9.0                    pypi_0    pypi\n",
    "types-python-dateutil     2.8.19.20240106          pypi_0    pypi\n",
    "typing-extensions         4.9.0                    pypi_0    pypi\n",
    "tzdata                    2024.1                   pypi_0    pypi\n",
    "uri-template              1.3.0                    pypi_0    pypi\n",
    "urllib3                   2.1.0                    pypi_0    pypi\n",
    "wasabi                    1.1.2                    pypi_0    pypi\n",
    "wcwidth                   0.2.13                   pypi_0    pypi\n",
    "weasel                    0.3.4                    pypi_0    pypi\n",
    "webcolors                 1.13                     pypi_0    pypi\n",
    "webencodings              0.5.1                    pypi_0    pypi\n",
    "websocket-client          1.7.0                    pypi_0    pypi\n",
    "wheel                     0.41.2          py310h06a4308_0  \n",
    "widgetsnbextension        4.0.9                    pypi_0    pypi\n",
    "xz                        5.4.5                h5eee18b_0  \n",
    "zipp                      3.17.0                   pypi_0    pypi\n",
    "zlib                      1.2.13               h5eee18b_0  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
