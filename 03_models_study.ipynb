{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38436cd7-6df4-4ce8-a2f1-b6fd2ce6e007",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e76c80-8f87-4a1b-8f4c-d1ceb6d36d12",
   "metadata": {},
   "source": [
    "## Huggingface \"from_pretrained\" model loading\n",
    "\n",
    "Analyzing code from main branch on 30/12/2O23:\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/modeling_utils.py#L2514\n",
    "\n",
    "Goal: loading a configuration (from a json file) and state dictionary (from a single or sharded archive file).\n",
    "\n",
    "3 use cases:\n",
    "- from a model repo on huggingface.co\n",
    "- from a local directory\n",
    "- from function arguments config and state_dict\n",
    "\n",
    "We will analyze only the first use case.\n",
    "\n",
    "Additional librairies used:\n",
    "- safetensors: https://huggingface.co/docs/safetensors/\n",
    "- accelerate: https://huggingface.co/docs/accelerate/\n",
    "- bitasandbytes: https://github.com/TimDettmers/bitsandbytes\n",
    "- flash-attn: https://github.com/Dao-AILab/flash-attention\n",
    "- optimum: https://huggingface.co/docs/optimum/\n",
    "- autogptq: https://github.com/PanQiWei/AutoGPTQ\n",
    "- autoawq: https://github.com/casper-hansen/AutoAWQ\n",
    "\n",
    "Safetensors should be much faster than Pytorch to load the weights on CPU or GPU (no copy). \n",
    "\n",
    "We test this below on this machine:\n",
    "- 200x faster to load 5 GB weights on CPU\n",
    "- 3x faster to load 5 GB weights on GPU\n",
    "\n",
    "**Finding 1**: alway use safetensors ! View zoom, code and experiments below.\n",
    "\n",
    "**Finding 2**: always try to pass use_safetensors=True to the from_pretrained method => if there is no safetensors file in the main branch of the repo, the transformers library will try to load the result of an automatic conversion to safetensors in a PR created by a HF bot.\n",
    "\n",
    "**Finding 3**: always specify explicitly the torch_dtype=\"auto\" to load the weights in the native model data type (read from the config file or the the first weight in the state dict), or torch_dtype=torch.float16 if want to quantize it, because otherwise the default dtype will be float32 !!\n",
    "\n",
    "**Finding 4**: never pass trust_remote_code=True (unless you get an error when loading the model), because you will get a stale and out of date version of the model code from the original repo and not the HF official implementation with the latest features (like FlashAttention).\n",
    "\n",
    "**Finding 5**: all supported quantization algorithms now support serialization => you should always try to load an already quantized model instead of quantizing it yourself.\n",
    "\n",
    "**Finding 6**: use transformers>4.6.0 and make sure to use param \"_fast_init=True\" to disable weights random initialization before loading, and make sure we don't spend an extraodinary amount of time doing useless work. See [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
    "\n",
    "**Finding 7**: always try to pass the argument attn_implementation=\"flash_attention_2\" to from_pretrained(), after installing the flash-attn package, to check wether flash attention 2 is supported by the model architecture, which should greatly speedup operations if available (see zoom below).\n",
    "\n",
    "\n",
    "Step 1: download model config file \"config.json\"\n",
    "\n",
    "```python\n",
    "# We make a call to the config file first (which may be absent)\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "\n",
    "resolved_config_file = cached_file(\n",
    "    pretrained_model_name_or_path,\n",
    "    CONFIG_NAME,\n",
    "    cache_dir=cache_dir,\n",
    "    force_download=force_download,\n",
    "    resume_download=resume_download,\n",
    "    proxies=proxies,\n",
    "    local_files_only=local_files_only,\n",
    "    token=token,\n",
    "    revision=revision,\n",
    "    subfolder=subfolder,\n",
    "    _raise_exceptions_for_missing_entries=False,\n",
    "    _raise_exceptions_for_connection_errors=False,\n",
    ")\n",
    "```\n",
    "\n",
    "Step 2: check and set memory loading parameters\n",
    "\n",
    "- device_map & low_cpu_mem_usage\n",
    "\n",
    "```python\n",
    "if device_map is not None:\n",
    "    low_cpu_mem_usage = True\n",
    "        \n",
    "    if not is_accelerate_available():\n",
    "        raise ImportError(\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate\")\n",
    "```\n",
    "\n",
    "- quantization_config\n",
    "\n",
    "```python\n",
    "if load_in_8bit or load_in_4bit:\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
    "    if not (is_accelerate_available() and is_bitsandbytes_available()):\n",
    "        raise ImportError(\"Using `load_in_8bit=True` requires Accelerate and the latest version of bitsandbytes\")\n",
    "        \n",
    "    if torch_dtype is None:\n",
    "        # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n",
    "        torch_dtype = torch.float16\n",
    "    if device_map is None:\n",
    "        device_map = {\"\": torch.cuda.current_device()}\n",
    "```\n",
    "\n",
    "Step 3: instantiate config object\n",
    "\n",
    "```python\n",
    "config, model_kwargs = cls.config_class.from_pretrained(\n",
    "    config_path,\n",
    "    cache_dir=cache_dir,\n",
    "    return_unused_kwargs=True,\n",
    "    force_download=force_download,\n",
    "    resume_download=resume_download,\n",
    "    proxies=proxies,\n",
    "    local_files_only=local_files_only,\n",
    "    token=token,\n",
    "    revision=revision,\n",
    "    subfolder=subfolder,\n",
    "    _from_auto=from_auto_class,\n",
    "    _from_pipeline=from_pipeline,\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "Note: this config object can itself contain a quantization config object.\n",
    "\n",
    "Step 4: check and set quantization config (from config or/and from params)\n",
    "\n",
    "- GPTQ\n",
    "\n",
    "```python\n",
    "if quantization_method == QuantizationMethod.GPTQ:\n",
    "    if not gptq_supports_cpu and not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\n",
    "    elif not (is_optimum_available() and is_auto_gptq_available()):\n",
    "        raise ImportError(\"Loading a GPTQ quantized model requires optimum and auto-gptq library.\")\n",
    "    elif version.parse(importlib.metadata.version(\"auto_gptq\")) < version.parse(\"0.4.2\"):\n",
    "        raise ImportError(\"You need a version of auto_gptq >= 0.4.2 to use GPTQ: `pip install --upgrade auto-gptq`\")\n",
    "\n",
    "    if torch_dtype is None:\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        logger.info(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with GPTQ.\")\n",
    "```\n",
    "\n",
    "- AWQ\n",
    "\n",
    "```python\n",
    "if quantization_method_from_config == QuantizationMethod.AWQ:\n",
    "     if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"GPU is required to run AWQ quantized model.\")\n",
    "    if not is_auto_awq_available():\n",
    "        raise ImportError(\"Loading an AWQ quantized model requires auto-awq library\")\n",
    "\n",
    "    if not is_accelerate_available():\n",
    "        raise ImportError(\"Loading an AWQ quantized model requires accelerate\")\n",
    "\n",
    "    if torch_dtype is None:\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        logger.info(\"We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\")\n",
    "```\n",
    "\n",
    "- BITS_AND_BYTES (see above)\n",
    "\n",
    "Step 5: \n",
    "\n",
    "1. Locate archive files\n",
    "\n",
    "```python\n",
    "SAFE_WEIGHTS_NAME = \"model.safetensors\"\n",
    "SAFE_WEIGHTS_INDEX_NAME = \"model.safetensors.index.json\"\n",
    "\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "WEIGHTS_INDEX_NAME = \"pytorch_model.bin.index.json\"\n",
    "\n",
    "if use_safetensors is not False:\n",
    "    filename = _add_variant(SAFE_WEIGHTS_NAME, variant)\n",
    "else:\n",
    "    filename = _add_variant(WEIGHTS_NAME, variant)\n",
    "\n",
    "resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
    "\n",
    "if resolved_archive_file is None and filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n",
    "    resolved_archive_file = cached_file(pretrained_model_name_or_path,\n",
    "                            _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant),\n",
    "                            **cached_file_kwargs)\n",
    "    if resolved_archive_file is not None:\n",
    "        is_sharded = True\n",
    "    elif use_safetensors:\n",
    "        if revision == \"main\":\n",
    "        # Here auto_conversion() tries to load the safetensors file from an PR\n",
    "        # pr.author = \"SFConvertBot\"\n",
    "        # pr_title = \"Adding `safetensors` variant of this model\"\n",
    "            resolved_archive_file, revision, is_sharded = auto_conversion(\n",
    "                pretrained_model_name_or_path, **cached_file_kwargs\n",
    "            )\n",
    "    else:\n",
    "        # This repo has no safetensors file of any kind, we switch to PyTorch.\n",
    "        filename = _add_variant(WEIGHTS_NAME, variant)\n",
    "        resolved_archive_file = cached_file(\n",
    "            pretrained_model_name_or_path, filename, **cached_file_kwargs\n",
    "        )\n",
    "\n",
    "if resolved_archive_file is None and filename == _add_variant(WEIGHTS_NAME, variant):\n",
    "    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n",
    "    resolved_archive_file = cached_file(\n",
    "        pretrained_model_name_or_path,\n",
    "        _add_variant(WEIGHTS_INDEX_NAME, variant),\n",
    "        **cached_file_kwargs,\n",
    "    )\n",
    "    if resolved_archive_file is not None:\n",
    "        is_sharded = True\n",
    "            \n",
    "# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\n",
    "if is_sharded:\n",
    "    # resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\n",
    "    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n",
    "        pretrained_model_name_or_path, ....)\n",
    "```\n",
    "\n",
    "2. Load state dict\n",
    "\n",
    "```python\n",
    "if not is_sharded and state_dict is None:\n",
    "    # Time to load the checkpoint\n",
    "    state_dict = load_state_dict(resolved_archive_file)\n",
    "\n",
    "if is_sharded:\n",
    "    loaded_state_dict_keys = sharded_metadata[\"all_checkpoint_keys\"]\n",
    "else:\n",
    "    loaded_state_dict_keys = list(state_dict.keys())\n",
    "  \n",
    "if low_cpu_mem_usage:\n",
    "    state_dict = None\n",
    "    \n",
    "def load_state_dict(checkpoint_file: Union[str, os.PathLike]):\n",
    "    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n",
    "        # Check format of the archive\n",
    "        return safe_load_file(checkpoint_file)\n",
    "    else:\n",
    "        map_location = \"cpu\"\n",
    "        return torch.load(checkpoint_file, map_location=map_location, weights_only=True)\n",
    "```\n",
    "\n",
    "=> we load the state dict once for nothing ??\n",
    "\n",
    "3. Compute torch_dtype\n",
    "\n",
    "```python\n",
    "if torch_dtype == \"auto\":\n",
    "    if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n",
    "        torch_dtype = config.torch_dtype\n",
    "    else:\n",
    "        torch_dtype = get_state_dict_dtype(state_dict)\n",
    "```       \n",
    "\n",
    "Step 6: Instantiate model.\n",
    "\n",
    "```python\n",
    "init_contexts = [no_init_weights(_enable=_fast_init)]\n",
    "\n",
    "if load_in_8bit or load_in_4bit or low_cpu_mem_usage:\n",
    "    init_contexts.append(init_empty_weights())\n",
    "\n",
    "config = cls._autoset_attn_implementation(config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map)\n",
    "\n",
    "with ContextManagers(init_contexts):\n",
    "    # Let's make sure we don't run the init function of buffer modules\n",
    "    model = cls(config, *model_args, **model_kwargs)\n",
    "    \n",
    "\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90d11f-28eb-4868-9f25-456c1b71c663",
   "metadata": {},
   "source": [
    "## Flash attention 2 support\n",
    "\n",
    "Documentation and installation:\n",
    "\n",
    "https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2\n",
    "\n",
    "- you need flash_attn package version to be greater or equal than 2.1.0\n",
    "- pass the argument attn_implementation=\"flash_attention_2\" to from_pretrained()\n",
    "\n",
    "Configuration:\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/modeling_utils.py#L1393\n",
    "\n",
    "- if torch_dtype not in [torch.float16, torch.bfloat16]: Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes\n",
    "- the full model must be loaded on a GPU (no partial offloading to CPU or disk)\n",
    "\n",
    "Implementation: \n",
    "\n",
    "https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/models/mistral/modeling_mistral.py#L715\n",
    "\n",
    "- supported on models classes with the property: cls._supports_flash_attn_2=True\n",
    "\n",
    "Example for Mistral:\n",
    "\n",
    "```python\n",
    "MISTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": MistralAttention,\n",
    "    \"flash_attention_2\": MistralFlashAttention2,\n",
    "    \"sdpa\": MistralSdpaAttention,\n",
    "}\n",
    "\n",
    "class MistralDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: int):\n",
    "        self.self_attn = MISTRAL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
    "        \n",
    "# https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/models/mistral/modeling_mistral.py#L321\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "- FlashAttention-2 does not support computing attention scores with padding tokens: this leads to a significant slowdown for batched generations with padding tokens\n",
    "- you must manually pad/unpad the attention scores for batched inference when the sequence contains padding tokens\n",
    "- to overcome this, you should use FlashAttention-2 without padding tokens in the sequence during training (by packing a dataset or concatenating sequences until reaching the maximum sequence length)\n",
    "\n",
    "List of supported models as of 12/30/2023:\n",
    "- mbart\n",
    "- bark\n",
    "- distilbert\n",
    "- falcon\n",
    "- gpt_bigcode\n",
    "- gpt_neo / gpt_neox\n",
    "- llama / \n",
    "- llava / vipllava\n",
    "- mistral / mixtral\n",
    "- opt\n",
    "- phi\n",
    "- whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c2f2f-504a-48e9-9824-5f23d4b0681d",
   "metadata": {},
   "source": [
    "## Load models from local cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe34e8-0575-4dcb-b6fc-08ec248f2bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12da3c-bfb7-4377-ba73-4fa05b362079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1d4ec-5bdd-4a4c-8221-7a0b38fbec06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b33cd-0556-48e0-b9a1-ef76204761cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28803c9-a2f6-4276-9656-ed6d1ee49ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecd5846e-2f6b-4f5e-91a5-2265958c0e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:53:53.679522Z",
     "iopub.status.busy": "2023-12-30T14:53:53.678951Z",
     "iopub.status.idle": "2023-12-30T14:53:53.694304Z",
     "shell.execute_reply": "2023-12-30T14:53:53.693766Z",
     "shell.execute_reply.started": "2023-12-30T14:53:53.679490Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "705da8a6-ab0b-4679-a96e-a56b1449c7b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:53:54.059001Z",
     "iopub.status.busy": "2023-12-30T14:53:54.057532Z",
     "iopub.status.idle": "2023-12-30T14:53:54.064476Z",
     "shell.execute_reply": "2023-12-30T14:53:54.064038Z",
     "shell.execute_reply.started": "2023-12-30T14:53:54.058960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec0f16ae-eb09-4262-9080-ca42e41c7576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:53:54.281705Z",
     "iopub.status.busy": "2023-12-30T14:53:54.280808Z",
     "iopub.status.idle": "2023-12-30T14:53:54.289332Z",
     "shell.execute_reply": "2023-12-30T14:53:54.288630Z",
     "shell.execute_reply.started": "2023-12-30T14:53:54.281652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.0'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"accelerate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47ec7909-3bb9-4394-9600-44d3504354b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:53:55.293209Z",
     "iopub.status.busy": "2023-12-30T14:53:55.292725Z",
     "iopub.status.idle": "2023-12-30T14:53:55.301999Z",
     "shell.execute_reply": "2023-12-30T14:53:55.301326Z",
     "shell.execute_reply.started": "2023-12-30T14:53:55.293176Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.41.3.post2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1e5510a-2c0a-430c-8983-c610db1e9ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:56:12.770753Z",
     "iopub.status.busy": "2023-12-30T14:56:12.769665Z",
     "iopub.status.idle": "2023-12-30T14:56:12.777891Z",
     "shell.execute_reply": "2023-12-30T14:56:12.777396Z",
     "shell.execute_reply.started": "2023-12-30T14:56:12.770700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.2'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"flash-attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d56ea8-56ec-41f0-a5e6-a6395288b859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:33.664254Z",
     "iopub.status.busy": "2023-12-30T14:04:33.663239Z",
     "iopub.status.idle": "2023-12-30T14:04:33.676793Z",
     "shell.execute_reply": "2023-12-30T14:04:33.675885Z",
     "shell.execute_reply.started": "2023-12-30T14:04:33.664213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # 15.78 GB https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # 21.00 GB https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # 15.00 GB https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # 17.07 GB https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # 17.33 GB https://huggingface.co/01-ai/Yi-34B    \n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # 22.18 GB https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90043bca-8dad-43cd-8b20-e3aa800a2972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:35.163399Z",
     "iopub.status.busy": "2023-12-30T14:04:35.162428Z",
     "iopub.status.idle": "2023-12-30T14:04:35.168634Z",
     "shell.execute_reply": "2023-12-30T14:04:35.167911Z",
     "shell.execute_reply.started": "2023-12-30T14:04:35.163361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "\n",
    "time_unit_µs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35979ae8-9b3d-4e1e-9f29-a52083e6792b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:35.855277Z",
     "iopub.status.busy": "2023-12-30T14:04:35.854212Z",
     "iopub.status.idle": "2023-12-30T14:04:36.753900Z",
     "shell.execute_reply": "2023-12-30T14:04:36.753250Z",
     "shell.execute_reply.started": "2023-12-30T14:04:35.855236Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_gpu_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit_mb):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit_mb):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit_mb):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit_mb):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used (total): {(used_memory/memory_unit_mb):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Used (model): {((used_memory-overhead_memory)/memory_unit_mb):8,.1f} MB - {int((used_memory-overhead_memory)/total_memory*100):3} %\")\n",
    "    print(f\"Max used    : {(max_used_memory/memory_unit_mb):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03afda9-b997-4037-86a0-4ee62a23409b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:36.755187Z",
     "iopub.status.busy": "2023-12-30T14:04:36.754672Z",
     "iopub.status.idle": "2023-12-30T14:04:36.924472Z",
     "shell.execute_reply": "2023-12-30T14:04:36.923919Z",
     "shell.execute_reply.started": "2023-12-30T14:04:36.755172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model_name):    \n",
    "    model_config_file = cached_file(model_name, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            file_size = os.path.getsize(full_entry_path)\n",
    "            if file_size > 1024*1024:\n",
    "                print(f\"- {entry} = {file_size/memory_unit_mb:.1f} MB\")\n",
    "            total_size += file_size\n",
    "    return model_directory,total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0c69b-599c-4f7a-9540-f68cf5245aaf",
   "metadata": {},
   "source": [
    "### Benchmark 1 - Pytorch weights loading on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287b7524-8dfc-4247-aeb5-1f06d558aab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:38.522149Z",
     "iopub.status.busy": "2023-12-30T14:04:38.520895Z",
     "iopub.status.idle": "2023-12-30T14:04:38.531342Z",
     "shell.execute_reply": "2023-12-30T14:04:38.530935Z",
     "shell.execute_reply.started": "2023-12-30T14:04:38.522110Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'togethercomputer/RedPajama-INCITE-Base-3B-v1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "model_name = list(models.values())[idx]\n",
    "idx,model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcfb5b9-08d2-4642-bdcd-bc17910a49db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:38.998017Z",
     "iopub.status.busy": "2023-12-30T14:04:38.997482Z",
     "iopub.status.idle": "2023-12-30T14:04:39.007600Z",
     "shell.execute_reply": "2023-12-30T14:04:39.007058Z",
     "shell.execute_reply.started": "2023-12-30T14:04:38.997953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files size on disk:\n",
      "- pytorch_model.bin = 5422.7 MB\n",
      "- tokenizer.json = 2.0 MB\n",
      "Total size = 5424.7 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Files size on disk:\")\n",
    "path,size = get_model_path_and_size_on_disk(model_name)\n",
    "print(f\"Total size = {size/memory_unit_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46adbd9-695f-4870-a101-a1ba727745be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:39.514313Z",
     "iopub.status.busy": "2023-12-30T14:04:39.513431Z",
     "iopub.status.idle": "2023-12-30T14:04:39.522643Z",
     "shell.execute_reply": "2023-12-30T14:04:39.522134Z",
     "shell.execute_reply.started": "2023-12-30T14:04:39.514279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt_filename = os.path.join(path, \"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4161707-12b1-43b8-8835-2d6f1343a8ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:41.296431Z",
     "iopub.status.busy": "2023-12-30T14:04:41.295314Z",
     "iopub.status.idle": "2023-12-30T14:04:44.450332Z",
     "shell.execute_reply": "2023-12-30T14:04:44.449637Z",
     "shell.execute_reply.started": "2023-12-30T14:04:41.296366Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with Pytorch (cold) = 3.150 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = torch.load(pt_filename, map_location=\"cpu\")\n",
    "duration_pt_cold_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with Pytorch (cold) = {(duration_pt_cold_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c51d27-6e06-49b9-b394-d1d7c15f4b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:44.451424Z",
     "iopub.status.busy": "2023-12-30T14:04:44.451287Z",
     "iopub.status.idle": "2023-12-30T14:04:49.113943Z",
     "shell.execute_reply": "2023-12-30T14:04:49.112906Z",
     "shell.execute_reply.started": "2023-12-30T14:04:44.451415Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with Pytorch (hot) = 4.658 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = torch.load(pt_filename, map_location=\"cpu\")\n",
    "duration_pt_hot_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with Pytorch (hot) = {(duration_pt_hot_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1789d4-8812-49ab-9699-4c42b39d6d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:49.115600Z",
     "iopub.status.busy": "2023-12-30T14:04:49.115195Z",
     "iopub.status.idle": "2023-12-30T14:04:49.119746Z",
     "shell.execute_reply": "2023-12-30T14:04:49.119291Z",
     "shell.execute_reply.started": "2023-12-30T14:04:49.115570Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'stabilityai/stablelm-3b-4e1t')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "model_name = list(models.values())[idx]\n",
    "idx,model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3126e03d-3dc2-4fd8-a129-3bf5ecbf74a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:49.121086Z",
     "iopub.status.busy": "2023-12-30T14:04:49.120817Z",
     "iopub.status.idle": "2023-12-30T14:04:49.143192Z",
     "shell.execute_reply": "2023-12-30T14:04:49.142503Z",
     "shell.execute_reply.started": "2023-12-30T14:04:49.121069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files size on disk:\n",
      "- model.safetensors = 5331.9 MB\n",
      "- tokenizer.json = 2.0 MB\n",
      "Total size = 5334.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Files size on disk:\")\n",
    "path,size = get_model_path_and_size_on_disk(model_name)\n",
    "print(f\"Total size = {size/memory_unit_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9deccc1a-c1ef-4265-b051-32a69537bb34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:49.144262Z",
     "iopub.status.busy": "2023-12-30T14:04:49.143973Z",
     "iopub.status.idle": "2023-12-30T14:04:49.147578Z",
     "shell.execute_reply": "2023-12-30T14:04:49.146848Z",
     "shell.execute_reply.started": "2023-12-30T14:04:49.144244Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_filename = os.path.join(path, \"model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c977fc6b-31b0-4415-ac7a-0bcc7638a1f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:51.942376Z",
     "iopub.status.busy": "2023-12-30T14:04:51.942031Z",
     "iopub.status.idle": "2023-12-30T14:04:51.978389Z",
     "shell.execute_reply": "2023-12-30T14:04:51.977845Z",
     "shell.execute_reply.started": "2023-12-30T14:04:51.942361Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with safetensors (cold) = 0.029 s\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "before_time_ns = perf_counter_ns()\n",
    "weights = load_file(sf_filename, device=\"cpu\")\n",
    "duration_sf_cold_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with safetensors (cold) = {(duration_sf_cold_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4975eec-2167-43a4-9c89-ccf498d0689b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:52.858730Z",
     "iopub.status.busy": "2023-12-30T14:04:52.858217Z",
     "iopub.status.idle": "2023-12-30T14:04:52.870175Z",
     "shell.execute_reply": "2023-12-30T14:04:52.869631Z",
     "shell.execute_reply.started": "2023-12-30T14:04:52.858710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with safetensors (hot) = 0.009 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = load_file(sf_filename, device=\"cpu\")\n",
    "duration_sf_hot_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with safetensors (hot) = {(duration_sf_hot_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "287e08ef-af39-4469-a71f-11f41ed82883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:04:54.522037Z",
     "iopub.status.busy": "2023-12-30T14:04:54.521652Z",
     "iopub.status.idle": "2023-12-30T14:04:54.529371Z",
     "shell.execute_reply": "2023-12-30T14:04:54.528876Z",
     "shell.execute_reply.started": "2023-12-30T14:04:54.522018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on CPU, safetensors is faster than pytorch by: 204.7 X\n"
     ]
    }
   ],
   "source": [
    "print(f\"on CPU, safetensors is faster than pytorch by: {(duration_pt_cold_ns+duration_pt_hot_ns)/(duration_sf_cold_ns+duration_sf_hot_ns):.1f} X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e29921-cd52-4c1d-ade3-2eb9e5f49d39",
   "metadata": {},
   "source": [
    "### Benchmark 2 - Pytorch weights loading on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f337ad8c-1244-4e62-9c3b-fca6ea8e881e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:08:09.652156Z",
     "iopub.status.busy": "2023-12-30T14:08:09.651151Z",
     "iopub.status.idle": "2023-12-30T14:08:09.967527Z",
     "shell.execute_reply": "2023-12-30T14:08:09.966940Z",
     "shell.execute_reply.started": "2023-12-30T14:08:09.652117Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **** IMPORTANT INIT ***\n",
    "\n",
    "# This is required because this feature hasn't been fully verified yet, but \n",
    "# it's been tested on many different environments\n",
    "os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "\n",
    "# CUDA startup out of the measurement\n",
    "torch.zeros((2, 2)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f58c02-4cce-4fac-98e7-08d796815786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:10:05.179410Z",
     "iopub.status.busy": "2023-12-30T14:10:05.178558Z",
     "iopub.status.idle": "2023-12-30T14:10:08.749230Z",
     "shell.execute_reply": "2023-12-30T14:10:08.748657Z",
     "shell.execute_reply.started": "2023-12-30T14:10:05.179378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with Pytorch (cold) = 3.566 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = torch.load(pt_filename, map_location=\"cuda:0\")\n",
    "duration_pt_cold_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with Pytorch (cold) = {(duration_pt_cold_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ac21978-8607-4364-8d6c-d2ad2a4704b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:10:17.996042Z",
     "iopub.status.busy": "2023-12-30T14:10:17.995493Z",
     "iopub.status.idle": "2023-12-30T14:10:20.776893Z",
     "shell.execute_reply": "2023-12-30T14:10:20.776338Z",
     "shell.execute_reply.started": "2023-12-30T14:10:17.996006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with Pytorch (hot) = 2.777 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = torch.load(pt_filename, map_location=\"cuda:0\")\n",
    "duration_pt_hot_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with Pytorch (hot) = {(duration_pt_hot_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df101035-4b66-4bd5-94f3-1714066bfe42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:10:46.240102Z",
     "iopub.status.busy": "2023-12-30T14:10:46.238615Z",
     "iopub.status.idle": "2023-12-30T14:10:47.406485Z",
     "shell.execute_reply": "2023-12-30T14:10:47.405910Z",
     "shell.execute_reply.started": "2023-12-30T14:10:46.240050Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with safetensors (cold) = 1.163 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = load_file(sf_filename, device=\"cuda:0\")\n",
    "duration_sf_cold_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with safetensors (cold) = {(duration_sf_cold_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7782225e-72f2-40bd-b74f-3c9589e84269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:10:55.341027Z",
     "iopub.status.busy": "2023-12-30T14:10:55.340007Z",
     "iopub.status.idle": "2023-12-30T14:10:56.241330Z",
     "shell.execute_reply": "2023-12-30T14:10:56.240832Z",
     "shell.execute_reply.started": "2023-12-30T14:10:55.340988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time with safetensors (hot) = 0.896 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "weights = load_file(sf_filename, device=\"cuda:0\")\n",
    "duration_sf_hot_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time with safetensors (hot) = {(duration_sf_hot_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "409728f2-68e7-4336-bb10-d80f8c8200ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T14:10:59.890375Z",
     "iopub.status.busy": "2023-12-30T14:10:59.889268Z",
     "iopub.status.idle": "2023-12-30T14:10:59.897090Z",
     "shell.execute_reply": "2023-12-30T14:10:59.896031Z",
     "shell.execute_reply.started": "2023-12-30T14:10:59.890334Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on GPU, safetensors is faster than pytorch by: 3.1 X\n"
     ]
    }
   ],
   "source": [
    "print(f\"on GPU, safetensors is faster than pytorch by: {(duration_pt_cold_ns+duration_pt_hot_ns)/(duration_sf_cold_ns+duration_sf_hot_ns):.1f} X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1ac73-3e10-4699-a8b1-0cee561daa27",
   "metadata": {},
   "source": [
    "### Test: convert local weights from Pytorch to safetensor format\n",
    "\n",
    "Let's find out how we can convert models downloaded in the Pytorch format: \n",
    "https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92810caa-1275-48ab-9400-868006269f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(local_model_name)\n",
    "    model_directory = os.path.dirname(cached_file(local_model_name, \"config.json\", local_files_only=True))\n",
    "    filenames = list(os.listdir(model_directory))\n",
    "    \n",
    "    if any(filename.endswith(\".safetensors\") for filename in filenames):\n",
    "        print(f\"Model {local_model_name} is already converted: skipping\")\n",
    "        return\n",
    "    elif \"pytorch_model.bin\" in filenames:\n",
    "        operations, errors = convert_single()\n",
    "    elif \"pytorch_model.bin.index.json\" in filenames:\n",
    "        operations, errors = convert_multi()\n",
    "    else:\n",
    "        print(f\"Model {local_model_name} doesn't seem to be a valid pytorch model: cannot convert\")\n",
    "        return\n",
    "\n",
    "    print(operations)\n",
    "    print(errors)\n",
    "    \n",
    "def convert_multi(\n",
    "    model_id: str, *, revision=Optional[str], folder: str, token: Optional[str], discard_names: List[str]\n",
    ") -> ConversionResult:\n",
    "    filename = hf_hub_download(\n",
    "        repo_id=model_id, revision=revision, filename=\"pytorch_model.bin.index.json\", token=token, cache_dir=folder\n",
    "    )\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filenames = set(data[\"weight_map\"].values())\n",
    "    local_filenames = []\n",
    "    for filename in filenames:\n",
    "        pt_filename = hf_hub_download(repo_id=model_id, filename=filename, token=token, cache_dir=folder)\n",
    "\n",
    "        sf_filename = rename(pt_filename)\n",
    "        sf_filename = os.path.join(folder, sf_filename)\n",
    "        convert_file(pt_filename, sf_filename, discard_names=discard_names)\n",
    "        local_filenames.append(sf_filename)\n",
    "\n",
    "    index = os.path.join(folder, \"model.safetensors.index.json\")\n",
    "    with open(index, \"w\") as f:\n",
    "        newdata = {k: v for k, v in data.items()}\n",
    "        newmap = {k: rename(v) for k, v in data[\"weight_map\"].items()}\n",
    "        newdata[\"weight_map\"] = newmap\n",
    "        json.dump(newdata, f, indent=4)\n",
    "    local_filenames.append(index)\n",
    "\n",
    "    operations = [\n",
    "        CommitOperationAdd(path_in_repo=os.path.basename(local), path_or_fileobj=local) for local in local_filenames\n",
    "    ]\n",
    "    errors: List[Tuple[str, \"Exception\"]] = []\n",
    "\n",
    "    return operations, errors\n",
    "\n",
    "\n",
    "def convert_single(\n",
    "    model_id: str, *, revision: Optional[str], folder: str, token: Optional[str], discard_names: List[str]\n",
    ") -> ConversionResult:\n",
    "    pt_filename = hf_hub_download(\n",
    "        repo_id=model_id, revision=revision, filename=\"pytorch_model.bin\", token=token, cache_dir=folder\n",
    "    )\n",
    "\n",
    "    sf_name = \"model.safetensors\"\n",
    "    sf_filename = os.path.join(folder, sf_name)\n",
    "    convert_file(pt_filename, sf_filename, discard_names)\n",
    "    operations = [CommitOperationAdd(path_in_repo=sf_name, path_or_fileobj=sf_filename)]\n",
    "    errors: List[Tuple[str, \"Exception\"]] = []\n",
    "    return operations, errors\n",
    "\n",
    "\n",
    "def convert_file(\n",
    "    pt_filename: str,\n",
    "    sf_filename: str,\n",
    "    discard_names: List[str],\n",
    "):\n",
    "    loaded = torch.load(pt_filename, map_location=\"cpu\")\n",
    "    if \"state_dict\" in loaded:\n",
    "        loaded = loaded[\"state_dict\"]\n",
    "    to_removes = _remove_duplicate_names(loaded, discard_names=discard_names)\n",
    "\n",
    "    metadata = {\"format\": \"pt\"}\n",
    "    for kept_name, to_remove_group in to_removes.items():\n",
    "        for to_remove in to_remove_group:\n",
    "            if to_remove not in metadata:\n",
    "                metadata[to_remove] = kept_name\n",
    "            del loaded[to_remove]\n",
    "    # Force tensors to be contiguous\n",
    "    loaded = {k: v.contiguous() for k, v in loaded.items()}\n",
    "\n",
    "    dirname = os.path.dirname(sf_filename)\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    save_file(loaded, sf_filename, metadata=metadata)\n",
    "    check_file_size(sf_filename, pt_filename)\n",
    "    reloaded = load_file(sf_filename)\n",
    "    for k in loaded:\n",
    "        pt_tensor = loaded[k]\n",
    "        sf_tensor = reloaded[k]\n",
    "        if not torch.equal(pt_tensor, sf_tensor):\n",
    "            raise RuntimeError(f\"The output tensors do not match for key {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc982-72d1-431e-9741-c8d824ba0f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b8bcd-73eb-4be2-b692-2125bc9131a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a56bba8-ed66-4d1a-9d9a-7e82bb47b369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:30:13.102777Z",
     "iopub.status.busy": "2023-12-30T11:30:13.102026Z",
     "iopub.status.idle": "2023-12-30T11:30:39.707240Z",
     "shell.execute_reply": "2023-12-30T11:30:39.706169Z",
     "shell.execute_reply.started": "2023-12-30T11:30:13.102746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temptensor = torch.rand(int(6536/2),1024,1024,dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5db4efa0-b2d1-4543-878c-c040d9817dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:32:50.991939Z",
     "iopub.status.busy": "2023-12-30T11:32:50.990901Z",
     "iopub.status.idle": "2023-12-30T11:32:52.019556Z",
     "shell.execute_reply": "2023-12-30T11:32:52.019094Z",
     "shell.execute_reply.started": "2023-12-30T11:32:50.991910Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved : 11,026.0 MB -  44 %\n",
      "Free     : 11,982.0 MB -  48 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,903.1 MB -  15 %\n",
      "Max used    : 10,917.1 MB -  44 %\n",
      "\n",
      "- gpu transfer time (cold) = 0.984 s\n",
      "\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,696.5 MB -   6 %\n",
      "Reserved : 17,562.0 MB -  71 %\n",
      "Free     :  5,305.0 MB -  21 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,762.1 MB -  15 %\n",
      "Max used    : 11,994.6 MB -  48 %\n"
     ]
    }
   ],
   "source": [
    "display_gpu_memory()\n",
    "print()\n",
    "\n",
    "before_time_ns = perf_counter_ns()\n",
    "temptensor.to('cuda')\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- gpu transfer time (cold) = {(duration_ns/time_unit_s):.3f} s\")\n",
    "\n",
    "print()\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2103d808-f5e4-4f70-ab46-4af06baba7e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:34:49.581466Z",
     "iopub.status.busy": "2023-12-30T11:34:49.580560Z",
     "iopub.status.idle": "2023-12-30T11:34:50.249826Z",
     "shell.execute_reply": "2023-12-30T11:34:50.249393Z",
     "shell.execute_reply.started": "2023-12-30T11:34:49.581421Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpu transfer time (hot) = 0.663 s\n",
      "\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,696.5 MB -   6 %\n",
      "Reserved : 17,562.0 MB -  71 %\n",
      "Free     :  5,305.0 MB -  21 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,762.1 MB -  15 %\n",
      "Max used    : 11,994.6 MB -  48 %\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "temptensor.to('cuda')\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- gpu transfer time (hot) = {(duration_ns/time_unit_s):.3f} s\")\n",
    "\n",
    "print()\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35abe513-8c25-44a4-80c8-d07c810599ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:00:43.429770Z",
     "iopub.status.busy": "2023-12-30T11:00:43.429423Z",
     "iopub.status.idle": "2023-12-30T11:00:48.204801Z",
     "shell.execute_reply": "2023-12-30T11:00:48.204257Z",
     "shell.execute_reply.started": "2023-12-30T11:00:43.429754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading togethercomputer/RedPajama-INCITE-Base-3B-v1 from disk to GPU:\n",
      "- load time = 4.758 s\n",
      "- GPU memory = 3903.1 MB\n",
      "\n",
      "GPU memory after\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved : 11,026.0 MB -  44 %\n",
      "Free     : 11,982.0 MB -  48 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,903.1 MB -  15 %\n",
      "Max used    : 10,917.1 MB -  44 %\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(f\"Loading {model_name} from disk to GPU:\")\n",
    "\n",
    "before_time_ns = perf_counter_ns()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\") \n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- load time = {(duration_ns/time_unit_s):.3f} s\")\n",
    "free_memory = torch.cuda.mem_get_info()[0]\n",
    "reserved_memory = torch.cuda.memory_reserved(0)\n",
    "overhead_memory = total_memory - free_memory - reserved_memory\n",
    "used_memory = torch.cuda.memory_allocated(0)    \n",
    "model_used_memory = used_memory - overhead_memory\n",
    "print(f\"- GPU memory = {((used_memory-overhead_memory)/memory_unit_mb):.1f} MB\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"GPU memory after\")\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758907cb-c13a-42a5-9625-be57666c10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
