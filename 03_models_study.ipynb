{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38436cd7-6df4-4ce8-a2f1-b6fd2ce6e007",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90d11f-28eb-4868-9f25-456c1b71c663",
   "metadata": {},
   "source": [
    "## Flash attention 2 support\n",
    "\n",
    "Documentation and installation:\n",
    "\n",
    "https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2\n",
    "\n",
    "- you need flash_attn package version to be greater or equal than 2.1.0\n",
    "- pass the argument attn_implementation=\"flash_attention_2\" to from_pretrained()\n",
    "\n",
    "Configuration:\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/modeling_utils.py#L1393\n",
    "\n",
    "- if torch_dtype not in [torch.float16, torch.bfloat16]: Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes\n",
    "- the full model must be loaded on a GPU (no partial offloading to CPU or disk)\n",
    "\n",
    "Implementation: \n",
    "\n",
    "https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/models/mistral/modeling_mistral.py#L715\n",
    "\n",
    "- supported on models classes with the property: cls._supports_flash_attn_2=True\n",
    "\n",
    "Example for Mistral:\n",
    "\n",
    "```python\n",
    "MISTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": MistralAttention,\n",
    "    \"flash_attention_2\": MistralFlashAttention2,\n",
    "    \"sdpa\": MistralSdpaAttention,\n",
    "}\n",
    "\n",
    "class MistralDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: int):\n",
    "        self.self_attn = MISTRAL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
    "        \n",
    "# https://github.com/huggingface/transformers/blob/3cefac1d974db5e2825a0cb2b842883a628be7a0/src/transformers/models/mistral/modeling_mistral.py#L321\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "- FlashAttention-2 does not support computing attention scores with padding tokens: this leads to a significant slowdown for batched generations with padding tokens\n",
    "- you must manually pad/unpad the attention scores for batched inference when the sequence contains padding tokens\n",
    "- to overcome this, you should use FlashAttention-2 without padding tokens in the sequence during training (by packing a dataset or concatenating sequences until reaching the maximum sequence length)\n",
    "\n",
    "List of supported models as of 12/30/2023:\n",
    "- mbart\n",
    "- bark\n",
    "- distilbert\n",
    "- falcon\n",
    "- gpt_bigcode\n",
    "- gpt_neo / gpt_neox\n",
    "- llama / \n",
    "- llava / vipllava\n",
    "- mistral / mixtral\n",
    "- opt\n",
    "- phi\n",
    "- whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28803c9-a2f6-4276-9656-ed6d1ee49ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea64ba65-c999-4400-a1b7-75e70353e003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T08:54:30.159591Z",
     "iopub.status.busy": "2023-12-30T08:54:30.158771Z",
     "iopub.status.idle": "2023-12-30T08:54:30.199669Z",
     "shell.execute_reply": "2023-12-30T08:54:30.199206Z",
     "shell.execute_reply.started": "2023-12-30T08:54:30.159562Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flash_attn\n",
    "flash_attn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c2f2f-504a-48e9-9824-5f23d4b0681d",
   "metadata": {},
   "source": [
    "## Load models to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe34e8-0575-4dcb-b6fc-08ec248f2bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd5846e-2f6b-4f5e-91a5-2265958c0e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:09:22.073059Z",
     "iopub.status.busy": "2023-12-30T10:09:22.072612Z",
     "iopub.status.idle": "2023-12-30T10:09:22.080743Z",
     "shell.execute_reply": "2023-12-30T10:09:22.079767Z",
     "shell.execute_reply.started": "2023-12-30T10:09:22.073028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d56ea8-56ec-41f0-a5e6-a6395288b859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:52:37.782700Z",
     "iopub.status.busy": "2023-12-30T10:52:37.781810Z",
     "iopub.status.idle": "2023-12-30T10:52:37.790523Z",
     "shell.execute_reply": "2023-12-30T10:52:37.789974Z",
     "shell.execute_reply.started": "2023-12-30T10:52:37.782665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # 15.78 GB https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # 21.00 GB https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # 15.00 GB https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # 17.07 GB https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # 17.33 GB https://huggingface.co/01-ai/Yi-34B    \n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # 22.18 GB https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90043bca-8dad-43cd-8b20-e3aa800a2972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:52:38.471417Z",
     "iopub.status.busy": "2023-12-30T10:52:38.470557Z",
     "iopub.status.idle": "2023-12-30T10:52:38.477830Z",
     "shell.execute_reply": "2023-12-30T10:52:38.476649Z",
     "shell.execute_reply.started": "2023-12-30T10:52:38.471377Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter_ns\n",
    "\n",
    "time_unit_Âµs = 1000\n",
    "time_unit_ms = 1000*1000\n",
    "time_unit_s = 1000*1000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35979ae8-9b3d-4e1e-9f29-a52083e6792b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:53:14.502839Z",
     "iopub.status.busy": "2023-12-30T10:53:14.502360Z",
     "iopub.status.idle": "2023-12-30T10:53:14.510053Z",
     "shell.execute_reply": "2023-12-30T10:53:14.509267Z",
     "shell.execute_reply.started": "2023-12-30T10:53:14.502809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "memory_unit_mb = 1024*1024\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_gpu_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit_mb):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit_mb):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit_mb):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit_mb):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used (total): {(used_memory/memory_unit_mb):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Used (model): {((used_memory-overhead_memory)/memory_unit_mb):8,.1f} MB - {int((used_memory-overhead_memory)/total_memory*100):3} %\")\n",
    "    print(f\"Max used    : {(max_used_memory/memory_unit_mb):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f03afda9-b997-4037-86a0-4ee62a23409b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:07:27.702676Z",
     "iopub.status.busy": "2023-12-30T11:07:27.701641Z",
     "iopub.status.idle": "2023-12-30T11:07:27.708943Z",
     "shell.execute_reply": "2023-12-30T11:07:27.708059Z",
     "shell.execute_reply.started": "2023-12-30T11:07:27.702636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "def get_model_path_and_size_on_disk(model_name):    \n",
    "    model_config_file = cached_file(model_name, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(model_config_file)\n",
    "    \n",
    "    total_size = 0\n",
    "    for entry in os.listdir(model_directory):\n",
    "        full_entry_path = os.path.join(model_directory, entry)\n",
    "        if os.path.isfile(full_entry_path):\n",
    "            file_size = os.path.getsize(full_entry_path)\n",
    "            if file_size > 1024*1024:\n",
    "                print(f\"- {entry} = {file_size/memory_unit_mb:.1f} MB\")\n",
    "            total_size += file_size\n",
    "    return model_directory,total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "287b7524-8dfc-4247-aeb5-1f06d558aab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:19:31.364941Z",
     "iopub.status.busy": "2023-12-30T11:19:31.363347Z",
     "iopub.status.idle": "2023-12-30T11:19:31.372256Z",
     "shell.execute_reply": "2023-12-30T11:19:31.371380Z",
     "shell.execute_reply.started": "2023-12-30T11:19:31.364879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'openlm-research/open_llama_3b_v2')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "model_name = list(models.values())[idx]\n",
    "idx,model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9dcfb5b9-08d2-4642-bdcd-bc17910a49db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:19:32.734272Z",
     "iopub.status.busy": "2023-12-30T11:19:32.733194Z",
     "iopub.status.idle": "2023-12-30T11:19:32.744589Z",
     "shell.execute_reply": "2023-12-30T11:19:32.744107Z",
     "shell.execute_reply.started": "2023-12-30T11:19:32.734223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files size on disk:\n",
      "- pytorch_model.bin = 6535.6 MB\n",
      "Total size = 6536.1 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Files size on disk:\")\n",
    "path,size = get_model_path_and_size_on_disk(model_name)\n",
    "print(f\"Total size = {size/memory_unit_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a46adbd9-695f-4870-a101-a1ba727745be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:19:35.883897Z",
     "iopub.status.busy": "2023-12-30T11:19:35.883045Z",
     "iopub.status.idle": "2023-12-30T11:19:35.889582Z",
     "shell.execute_reply": "2023-12-30T11:19:35.888491Z",
     "shell.execute_reply.started": "2023-12-30T11:19:35.883860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(path, \"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4161707-12b1-43b8-8835-2d6f1343a8ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:19:38.744918Z",
     "iopub.status.busy": "2023-12-30T11:19:38.743907Z",
     "iopub.status.idle": "2023-12-30T11:20:13.061424Z",
     "shell.execute_reply": "2023-12-30T11:20:13.060451Z",
     "shell.execute_reply.started": "2023-12-30T11:19:38.744878Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time (cold) = 34.310 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "with open(filename, mode='rb') as file:\n",
    "    fileContent = file.read()\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time (cold) = {(duration_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50c51d27-6e06-49b9-b394-d1d7c15f4b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:20:23.091059Z",
     "iopub.status.busy": "2023-12-30T11:20:23.090783Z",
     "iopub.status.idle": "2023-12-30T11:20:31.047766Z",
     "shell.execute_reply": "2023-12-30T11:20:31.046991Z",
     "shell.execute_reply.started": "2023-12-30T11:20:23.091047Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- disk load time (hot) = 7.952 s\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "with open(filename, mode='rb') as file: # b is important -> binary\n",
    "    fileContent = file.read()\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- disk load time (hot) = {(duration_ns/time_unit_s):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91677afd-7a26-49dc-9718-b408593dded2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:21:33.315514Z",
     "iopub.status.busy": "2023-12-30T11:21:33.314897Z",
     "iopub.status.idle": "2023-12-30T11:21:46.978100Z",
     "shell.execute_reply": "2023-12-30T11:21:46.976974Z",
     "shell.execute_reply.started": "2023-12-30T11:21:33.315498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s Â± 357 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit 10\n",
    "with open(filename, mode='rb') as file: # b is important -> binary\n",
    "    fileContent = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a56bba8-ed66-4d1a-9d9a-7e82bb47b369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:30:13.102777Z",
     "iopub.status.busy": "2023-12-30T11:30:13.102026Z",
     "iopub.status.idle": "2023-12-30T11:30:39.707240Z",
     "shell.execute_reply": "2023-12-30T11:30:39.706169Z",
     "shell.execute_reply.started": "2023-12-30T11:30:13.102746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temptensor = torch.rand(int(6536/2),1024,1024,dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5db4efa0-b2d1-4543-878c-c040d9817dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:32:50.991939Z",
     "iopub.status.busy": "2023-12-30T11:32:50.990901Z",
     "iopub.status.idle": "2023-12-30T11:32:52.019556Z",
     "shell.execute_reply": "2023-12-30T11:32:52.019094Z",
     "shell.execute_reply.started": "2023-12-30T11:32:50.991910Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved : 11,026.0 MB -  44 %\n",
      "Free     : 11,982.0 MB -  48 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,903.1 MB -  15 %\n",
      "Max used    : 10,917.1 MB -  44 %\n",
      "\n",
      "- gpu transfer time (cold) = 0.984 s\n",
      "\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,696.5 MB -   6 %\n",
      "Reserved : 17,562.0 MB -  71 %\n",
      "Free     :  5,305.0 MB -  21 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,762.1 MB -  15 %\n",
      "Max used    : 11,994.6 MB -  48 %\n"
     ]
    }
   ],
   "source": [
    "display_gpu_memory()\n",
    "print()\n",
    "\n",
    "before_time_ns = perf_counter_ns()\n",
    "temptensor.to('cuda')\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- gpu transfer time (cold) = {(duration_ns/time_unit_s):.3f} s\")\n",
    "\n",
    "print()\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2103d808-f5e4-4f70-ab46-4af06baba7e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:34:49.581466Z",
     "iopub.status.busy": "2023-12-30T11:34:49.580560Z",
     "iopub.status.idle": "2023-12-30T11:34:50.249826Z",
     "shell.execute_reply": "2023-12-30T11:34:50.249393Z",
     "shell.execute_reply.started": "2023-12-30T11:34:49.581421Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- gpu transfer time (hot) = 0.663 s\n",
      "\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,696.5 MB -   6 %\n",
      "Reserved : 17,562.0 MB -  71 %\n",
      "Free     :  5,305.0 MB -  21 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,762.1 MB -  15 %\n",
      "Max used    : 11,994.6 MB -  48 %\n"
     ]
    }
   ],
   "source": [
    "before_time_ns = perf_counter_ns()\n",
    "temptensor.to('cuda')\n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- gpu transfer time (hot) = {(duration_ns/time_unit_s):.3f} s\")\n",
    "\n",
    "print()\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35abe513-8c25-44a4-80c8-d07c810599ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T11:00:43.429770Z",
     "iopub.status.busy": "2023-12-30T11:00:43.429423Z",
     "iopub.status.idle": "2023-12-30T11:00:48.204801Z",
     "shell.execute_reply": "2023-12-30T11:00:48.204257Z",
     "shell.execute_reply.started": "2023-12-30T11:00:43.429754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading togethercomputer/RedPajama-INCITE-Base-3B-v1 from disk to GPU:\n",
      "- load time = 4.758 s\n",
      "- GPU memory = 3903.1 MB\n",
      "\n",
      "GPU memory after\n",
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,555.5 MB -   6 %\n",
      "Reserved : 11,026.0 MB -  44 %\n",
      "Free     : 11,982.0 MB -  48 %\n",
      "------------------------------\n",
      "Used (total):  5,458.6 MB -  22 %\n",
      "Used (model):  3,903.1 MB -  15 %\n",
      "Max used    : 10,917.1 MB -  44 %\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(f\"Loading {model_name} from disk to GPU:\")\n",
    "\n",
    "before_time_ns = perf_counter_ns()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\") \n",
    "duration_ns = perf_counter_ns() - before_time_ns\n",
    "print(f\"- load time = {(duration_ns/time_unit_s):.3f} s\")\n",
    "free_memory = torch.cuda.mem_get_info()[0]\n",
    "reserved_memory = torch.cuda.memory_reserved(0)\n",
    "overhead_memory = total_memory - free_memory - reserved_memory\n",
    "used_memory = torch.cuda.memory_allocated(0)    \n",
    "model_used_memory = used_memory - overhead_memory\n",
    "print(f\"- GPU memory = {((used_memory-overhead_memory)/memory_unit_mb):.1f} MB\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"GPU memory after\")\n",
    "display_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758907cb-c13a-42a5-9625-be57666c10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
