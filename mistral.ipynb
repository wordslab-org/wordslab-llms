{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77732e1a-2917-4691-b4d3-15fcc12d5985",
   "metadata": {},
   "source": [
    "# Mistral.ai LLM\n",
    "\n",
    "## 7B v0.1 release - 27 sept 2023\n",
    "\n",
    "Mission statement: https://mistral.ai/news/about-mistral-ai/\n",
    "\n",
    "-  Mistral 7B, our first 7B-parameter model, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. \n",
    "\n",
    "- Mistral 7B is only a first step toward building the frontier models on our roadmap. Yet, it can be used to solve many tasks: summarisation, structuration and question answering to name a few.\n",
    "\n",
    "- Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.\n",
    "\n",
    "- We‚Äôre committing to release the strongest open models in parallel to developing our commercial offering. \n",
    "\n",
    "- We will propose optimised proprietary models for on-premise/virtual private cloud deployment. These models will be distributed as white-box solutions, making both weights and code sources available. We are actively working on hosted solutions and dedicated deployment for enterprises.\n",
    "\n",
    "- We‚Äôre already training much larger models, and are shifting toward novel architectures. Stay tuned for further releases this fall.\n",
    "\n",
    "Model announcement: https://mistral.ai/news/announcing-mistral-7b/\n",
    "\n",
    "- Mistral 7B is a 7.3B parameter model that:\n",
    "  - Outperforms Llama 2 13B on all benchmarks\n",
    "  - Outperforms Llama 1 34B on many benchmarks\n",
    "  - Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n",
    "  - Uses Grouped-query attention (GQA) for faster inference\n",
    "  - Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost\n",
    "\n",
    "- We‚Äôre releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.\n",
    "\n",
    "- Mistral 7B is easy to fine-tune on any task. As a demonstration, we‚Äôre providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.\n",
    "\n",
    "- Mistral 7B uses a sliding window attention (SWA) mechanism ([Child et al.](https://arxiv.org/pdf/1904.10509.pdf), [Beltagy et al.](https://arxiv.org/pdf/2004.05150v2.pdf)), in which each layer attends to the previous 4,096 hidden states.\n",
    "\n",
    "- In practice, changes made to FlashAttention and xFormers yield a 2x speed improvement for sequence length of 16k with a window of 4k\n",
    "\n",
    "- A fixed attention span means we can limit our cache to a size of sliding_window tokens, using rotating buffers (read more in our reference implementation repo). This saves half of the cache memory for inference on sequence length of 8192, without impacting model quality.\n",
    "\n",
    "- To show the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on HuggingFace. No tricks, no proprietary data. The resulting model, Mistral 7B Instruct, outperforms all 7B models on MT-Bench, and is comparable to 13B chat models.\n",
    "\n",
    "- Huggingface org: https://huggingface.co/mistralai\n",
    "\n",
    "- Weights: https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar\n",
    "\n",
    "- Reference implementation: https://github.com/mistralai/mistral-src\n",
    "\n",
    "- Cloud deployment: https://docs.mistral.ai/cloud-deployment/skypilot\n",
    "\n",
    "Deploying the model: https://docs.mistral.ai/\n",
    "\n",
    "- This documentation details the deployment bundle that allows to quickly spin a completion API on any major cloud provider with NVIDIA GPUs.\n",
    "\n",
    "- A Docker image bundling vLLM, a fast Python inference server, with everything required to run our model is provided.\n",
    "\n",
    "- To run the image, you need a cloud virtual machine with at least 24GB of vRAM for good throughput and float16 weights. Other inference stacks can lower these requirements to 16GB vRAM.\n",
    "\n",
    "Interacting with the model: https://docs.mistral.ai/usage/how-to-use\n",
    "\n",
    "- Once you have deployed an the model with vLLM on a GPU instance, you can query it using the OpenAI-compatible REST API. This API is described on the API specification, but you can use any library implementing OpenAI API.\n",
    "\n",
    "Github repository: https://github.com/mistralai/mistral-src\n",
    "\n",
    "Discord channel: https://discord.com/invite/mistralai\n",
    "\n",
    "- The changes have been merged into the main branch, but are yet to be released on PyPI. To get the latest version, run: pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "- 0.0.22 xformers contains rhe slising window patch\n",
    "\n",
    "- Similarly to llama 2, there are two different formats used to save these models. The huggingface link you posted is for the variant that is meant to be used with the transformers library, whereas the mistral-src repo expects a slightly different format.\n",
    "\n",
    "- git clone the repo,  setup your env by installing the requirements (xFormers needs to be 0.0.22 which may or may not still be pushing the wheels out), then python -m one_file_ref /path/to/model  where the path is to the model folder available by direct download or torrent.\n",
    "\n",
    "- Please note, this repo won't work with the models downloadable from huggingface (different rope implem leading to switcharoos in the qkv proj). Our rope implem is closer to the llama2 one. Difference is  [cos... cos,sin...sin] vs [cos, sin, cos,sin...]\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Hey, thanks ! We didn't use more french for this one, but there's definitely a good chunk of french in our tokens so we're geared for more european language goodness in the future üòâ\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Not trained on 8T tokens no. but at the time we trained this tokenizer we had cleaned up to 8T tokens.\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî We're currently quite busy with training the future models and handling this release. Papers is definitely something we have in mind when we'll have time though.\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Sorry, we won't give any details on training. We'll be as open as possible on the models we release and some choices we made, but our training recipes we'll keep for ourselves in the short term üòâ\n",
    "\n",
    "- On my (german) micro benchmark, the (qlora finetuned) 7bn model reaches Llama2 70b quality ü§©. Will release a first finetuned German Model soon.\n",
    "\n",
    "- We train with a technique called sliding windows, where each layer attends to 4k tokens in the past, allowing to broaden the context by stacking more layers. Nice pictures here. It surely helps up to 16k\n",
    "\n",
    "Huggingface model card: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "\n",
    "- The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.\n",
    "\n",
    "- In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [\\INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n",
    "\n",
    "- \"MistralForCausalLM\"\n",
    "  - \"hidden_size\": 4096\n",
    "  - \"max_position_embeddings\": 32768\n",
    "  - num_hidden_layers\": 32,\n",
    "  - \"sliding_window\": 4096,\n",
    "  - \"torch_dtype\": \"bfloat16\",\n",
    "  - \"vocab_size\": 32000\n",
    "  - \"tokenizer_class\": \"LlamaTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af7ee4-6049-4542-8354-7f0872875753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b108d6-15f7-434b-ab0a-1eec5b1fa0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bb330-e9ab-46e5-90e9-4d28fca1783b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919352f-93a3-47ff-96e9-40300b476a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef23424-65ef-4c0b-aa21-6b1c22ee8315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241a6c04-db12-47c5-9880-11eca061ca43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T19:57:22.153153Z",
     "iopub.status.busy": "2023-10-04T19:57:22.153050Z",
     "iopub.status.idle": "2023-10-04T19:57:38.896723Z",
     "shell.execute_reply": "2023-10-04T19:57:38.895753Z",
     "shell.execute_reply.started": "2023-10-04T19:57:22.153145Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b6827f3b5a4d15b0c7a2f767706eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig, GenerationConfig\n",
    "\n",
    "quantization_config_8bits = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quantization_config_4bits = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\", device_map=\"auto\", quantization_config=quantization_config_4bits)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eaeb85-828f-4718-9a7e-20c6ab71ff11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Orcas were not known to be drawn to mistral energy, but they were seen recently \",\n",
    "    return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=256, use_cache=True, do_sample=True,\n",
    "    temperature=0.2, top_p=0.95)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcad8cf-43f9-42a7-bbb1-d9b2784dac61",
   "metadata": {},
   "source": [
    "**16 bits => 4 min 15 sec**\n",
    "\n",
    "Orcas were not known to be drawn to mistral energy, but they were seen recently 100 miles off the coast of Brazil, in the South Atlantic Ocean, where they were attracted to a large underwater gas leak.\n",
    "\n",
    "The gas leak was discovered by a team of researchers from the University of S√£o Paulo, who were studying the behavior of the marine mammals in the area.\n",
    "\n",
    "The researchers found that the orcas were attracted to the gas leak, which was releasing methane, a potent greenhouse gas. The gas was leaking from a natural gas well that had been drilled in the area.\n",
    "\n",
    "The orcas were seen swimming in circles around the leak, seemingly fascinated by the bubbles of gas rising from the ocean floor. The researchers believe that the orcas were attracted to the gas because of its unique smell, which is different from the smell of regular seawater.\n",
    "\n",
    "The gas leak was discovered by accident when the researchers were using a remotely operated vehicle (ROV) to study the orcas' behavior. The ROV accidentally bumped into the gas well, causing the gas to start leaking.\n",
    "\n",
    "The researchers immediately notified the authorities, who took steps to secure the well and stop the leak. In the meantime, the orcas swam away from the area, likely due"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bbab0-ebb8-438b-8f11-b936b2f4edde",
   "metadata": {},
   "source": [
    "**8 bits => 46 sec**\n",
    "\n",
    "Orcas were not known to be drawn to mistral energy, but they were seen recently 100 miles off the coast of Portugal, in the Atlantic Ocean, where a mistral energy platform was located.\n",
    "\n",
    "The orcas, also known as killer whales, were seen swimming around the platform, which is used for drilling oil and gas. The sighting was captured on video by a drone, which showed the orcas interacting with the platform.\n",
    "\n",
    "Mistral Energy, a Portuguese company, has been operating in the area for several years. The company has been exploring for oil and gas in the region, and the presence of the orcas near the platform has raised concerns about the potential impact on the marine ecosystem.\n",
    "\n",
    "Orcas are known to be highly intelligent and social animals, and their presence near the platform could be seen as a sign that the animals are curious about the structure. However, it is also possible that the orcas were attracted by the noise and vibrations created by the drilling activities.\n",
    "\n",
    "The sighting has sparked a debate about the potential impact of offshore drilling on marine life. Critics argue that such activities can disrupt the natural habitat of marine animals, leading to a decline in their populations. They also point out that oil spills and other accidents can have devastating effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f5830-b99b-49a1-a4f0-d47c38e5eafc",
   "metadata": {},
   "source": [
    "**4 bits => 5 sec**\n",
    "\n",
    "Orcas were not known to be drawn to mistral energy, but they were seen recently 200 meters from the platform of the Mistral Energy terminal in the Bay of Biscay, France.\n",
    "<|im_end|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724d872-defc-4bc1-adce-924f8cfc0b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys_prompt = \"Tu es Eddy, un conseiller bancaire fran√ßais. Tu √©coutes attentivement les besoins de tes clients, puis tu leur donne le meilleur conseil pour g√©rer leur argent en expliquant pas √† pas ton raisonnement.\"\n",
    "prompt = \"Je souhaite pr√©parer la transmission de mon capital √† mes petits enfants, que dois-je faire ?\"\n",
    "\n",
    "prefix = \"<|im_start|>\"\n",
    "suffix = \"<|im_end|>\\n\"\n",
    "sys_format = prefix + \"system\\n\" + sys_prompt + suffix\n",
    "user_format = prefix + \"user\\n\" + prompt + suffix\n",
    "assistant_format = prefix + \"assistant\\n\"\n",
    "input_text = sys_format + user_format + assistant_format\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=2000, temperature=1.1, top_p=0.95, repetition_penalty=1.0,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078275ed-c597-413b-b7d6-628a970afb9a",
   "metadata": {},
   "source": [
    "**8 bits => 1 min 26 sec**\n",
    "\n",
    "Cher client, merci de m'avoir confi√© votre question. Il est important de bien organiser la transmission de votre capital aux futurs g√©n√©rations de mani√®re √† assurer leur confort et la p√©rennit√© de votre patrimoine. Voici quelques conseils pour vous aider dans ce processus :\n",
    "\n",
    "1. √âlaborer un plan de succession : Avant de faire d√©pend de vos souhaits et des besoins de vos enfants, √©tablissez un plan de succession qui indique comment les biens et les propri√©t√©s devront √™tre r√©partis.\n",
    "\n",
    "2. Constituer un testament : Un testament est un acte notari√© qui permet d'exprimer vos volont√©s en cas de d√©c√®s, en pr√©cisant comment vos biens devront √™tre partag√©s entre vos ayants cause et votre prog√©niture.\n",
    "\n",
    "3. Cr√©er un fonds patrimonial : Pour prot√©ger et favoriser la croissance de votre capital, envisagez de le investir dans un fonds patrimonial, une assurance-vie ou une autre solution adapt√©e √† vos attentes.\n",
    "\n",
    "4. Informer vos enfants : Communiquez avec vos enfants sur la pr√©paration de la transmission du capital, leur expliquant le plan que vous pr√©parez et les attentes. Cela facilitera la mise en place des diff√©rentes √©tapes et les aidera √† prendre en main leurs responsabilit√©s financi√®res.\n",
    "\n",
    "5. B√©n√©ficier d'un conseil financier : Prenez le temps de parler avec un conseiller financier, afin qu'il puisse vous aider √† √©valuer vos besoins et votre situation, et proposer des solutions adapt√©es √† votre situation et √† vos attentes.\n",
    "\n",
    "En somme, la pr√©paration de la transmission de votre capital aux petits enfants n√©cessite un plan de succession clair, un testament √©crit, un investissement s√©curis√© et une communication franche avec vos prog√©nitures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfd908-3f51-4431-95f0-bc98a3e98285",
   "metadata": {},
   "source": [
    "**4 bits => 21 sec**\n",
    "\n",
    "Bien que je ne suis pas autoris√© √† donner de conseils financiers individuels, je vais t'aider √† prendre des d√©cisions en me basant sur une situation hypoth√©tique.\n",
    "\n",
    "Pour commencer, je te recommande d'avoir un plan de succession bien structur√© pour assurer la continuit√© de ta fortune vers tes petits-enfants. Voici quelques √©tapes importantes √† suivre :\n",
    "\n",
    "1. √âvalue le montant de ta succession : Toutes les valeurs que tu souhaites transmettre doivent √™tre √©valu√©es. Cela inclut vos avoirs liquides, tes actifs et ta participation aux entreprises.\n",
    "\n",
    "2. √âvalue les besoins financiers et les objectifs de tes petits-enfants : Il est important de conna√Ætre les besoins et les objectifs financiers de tes petits-enfants. Ceci aidera √† d√©terminer quel besoin est prioritaire et comment ta succession pourrait les soutenir.\n",
    "\n",
    "3. Pr√©pare un plan de succession : Bas√© sur l'√©valuation des besoins et des objectifs financiers de tes petits-enfants, pr√©pare un plan de succession qui met en ≈ìuvre tes intentions. Ce plan devrait inclure des trusts, des contrats de fondation et des testaments pour tes biens.\n",
    "\n",
    "4. Obtains une assistance l√©gale : Vous devriez consulter un avocat de la famille ou un notaire pour aider √† pr√©parer ces documents l√©gaux et veiller √† ce que tes souhaits soient respect√©s.\n",
    "\n",
    "5. Revisite r√©guli√®rement le plan : Il est important de relire et mettre √† jour r√©guli√®rement ton plan de succession, surtout lors de la naissance de petits-enfants suppl√©mentaires, lorsque tes objectifs financiers √©voluent ou lorsque tes circonstances change.\n",
    "\n",
    "En suivant ces √©tapes, tu peu vas faire preuve d'une bonne planification financi√®re et te pr√©parer √† la transmission de ta fortune vers tes petits-enfants. Cependant, je te demande de consulter un professionnel du domaine pour t'aider √† mettre en ≈ìuvre ces suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e567037-5980-420e-b354-a471d06c4058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
