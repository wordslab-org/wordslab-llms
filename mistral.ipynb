{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77732e1a-2917-4691-b4d3-15fcc12d5985",
   "metadata": {},
   "source": [
    "# Mistral.ai LLM\n",
    "\n",
    "## 7B v0.1 release - 27 sept 2023\n",
    "\n",
    "Mission statement: https://mistral.ai/news/about-mistral-ai/\n",
    "\n",
    "-  Mistral 7B, our first 7B-parameter model, which outperforms all currently available open models up to 13B parameters on all standard English and code benchmarks. \n",
    "\n",
    "- Mistral 7B is only a first step toward building the frontier models on our roadmap. Yet, it can be used to solve many tasks: summarisation, structuration and question answering to name a few.\n",
    "\n",
    "- Mistral 7B is released in Apache 2.0, making it usable without restrictions anywhere.\n",
    "\n",
    "- We‚Äôre committing to release the strongest open models in parallel to developing our commercial offering. \n",
    "\n",
    "- We will propose optimised proprietary models for on-premise/virtual private cloud deployment. These models will be distributed as white-box solutions, making both weights and code sources available. We are actively working on hosted solutions and dedicated deployment for enterprises.\n",
    "\n",
    "- We‚Äôre already training much larger models, and are shifting toward novel architectures. Stay tuned for further releases this fall.\n",
    "\n",
    "Model announcement: https://mistral.ai/news/announcing-mistral-7b/\n",
    "\n",
    "- Mistral 7B is a 7.3B parameter model that:\n",
    "  - Outperforms Llama 2 13B on all benchmarks\n",
    "  - Outperforms Llama 1 34B on many benchmarks\n",
    "  - Approaches CodeLlama 7B performance on code, while remaining good at English tasks\n",
    "  - Uses Grouped-query attention (GQA) for faster inference\n",
    "  - Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost\n",
    "\n",
    "- We‚Äôre releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.\n",
    "\n",
    "- Mistral 7B is easy to fine-tune on any task. As a demonstration, we‚Äôre providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.\n",
    "\n",
    "- Mistral 7B uses a sliding window attention (SWA) mechanism ([Child et al.](https://arxiv.org/pdf/1904.10509.pdf), [Beltagy et al.](https://arxiv.org/pdf/2004.05150v2.pdf)), in which each layer attends to the previous 4,096 hidden states.\n",
    "\n",
    "- In practice, changes made to FlashAttention and xFormers yield a 2x speed improvement for sequence length of 16k with a window of 4k\n",
    "\n",
    "- A fixed attention span means we can limit our cache to a size of sliding_window tokens, using rotating buffers (read more in our reference implementation repo). This saves half of the cache memory for inference on sequence length of 8192, without impacting model quality.\n",
    "\n",
    "- To show the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on HuggingFace. No tricks, no proprietary data. The resulting model, Mistral 7B Instruct, outperforms all 7B models on MT-Bench, and is comparable to 13B chat models.\n",
    "\n",
    "- Huggingface org: https://huggingface.co/mistralai\n",
    "\n",
    "- Weights: https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar\n",
    "\n",
    "- Reference implementation: https://github.com/mistralai/mistral-src\n",
    "\n",
    "- Cloud deployment: https://docs.mistral.ai/cloud-deployment/skypilot\n",
    "\n",
    "Deploying the model: https://docs.mistral.ai/\n",
    "\n",
    "- This documentation details the deployment bundle that allows to quickly spin a completion API on any major cloud provider with NVIDIA GPUs.\n",
    "\n",
    "- A Docker image bundling vLLM, a fast Python inference server, with everything required to run our model is provided.\n",
    "\n",
    "- To run the image, you need a cloud virtual machine with at least 24GB of vRAM for good throughput and float16 weights. Other inference stacks can lower these requirements to 16GB vRAM.\n",
    "\n",
    "Interacting with the model: https://docs.mistral.ai/usage/how-to-use\n",
    "\n",
    "- Once you have deployed an the model with vLLM on a GPU instance, you can query it using the OpenAI-compatible REST API. This API is described on the API specification, but you can use any library implementing OpenAI API.\n",
    "\n",
    "Github repository: https://github.com/mistralai/mistral-src\n",
    "\n",
    "Discord channel: https://discord.com/invite/mistralai\n",
    "\n",
    "- The changes have been merged into the main branch, but are yet to be released on PyPI. To get the latest version, run: pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "- 0.0.22 xformers contains rhe slising window patch\n",
    "\n",
    "- Similarly to llama 2, there are two different formats used to save these models. The huggingface link you posted is for the variant that is meant to be used with the transformers library, whereas the mistral-src repo expects a slightly different format.\n",
    "\n",
    "- git clone the repo,  setup your env by installing the requirements (xFormers needs to be 0.0.22 which may or may not still be pushing the wheels out), then python -m one_file_ref /path/to/model  where the path is to the model folder available by direct download or torrent.\n",
    "\n",
    "- Please note, this repo won't work with the models downloadable from huggingface (different rope implem leading to switcharoos in the qkv proj). Our rope implem is closer to the llama2 one. Difference is  [cos... cos,sin...sin] vs [cos, sin, cos,sin...]\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Hey, thanks ! We didn't use more french for this one, but there's definitely a good chunk of french in our tokens so we're geared for more european language goodness in the future üòâ\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Not trained on 8T tokens no. but at the time we trained this tokenizer we had cleaned up to 8T tokens.\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî We're currently quite busy with training the future models and handling this release. Papers is definitely something we have in mind when we'll have time though.\n",
    "\n",
    "- Timoth√©e Lacroix ‚Äî Sorry, we won't give any details on training. We'll be as open as possible on the models we release and some choices we made, but our training recipes we'll keep for ourselves in the short term üòâ\n",
    "\n",
    "- On my (german) micro benchmark, the (qlora finetuned) 7bn model reaches Llama2 70b quality ü§©. Will release a first finetuned German Model soon.\n",
    "\n",
    "- We train with a technique called sliding windows, where each layer attends to 4k tokens in the past, allowing to broaden the context by stacking more layers. Nice pictures here. It surely helps up to 16k\n",
    "\n",
    "Huggingface model card: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "\n",
    "- The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.\n",
    "\n",
    "- In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [\\INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n",
    "\n",
    "- \"MistralForCausalLM\"\n",
    "  - \"hidden_size\": 4096\n",
    "  - \"max_position_embeddings\": 32768\n",
    "  - num_hidden_layers\": 32,\n",
    "  - \"sliding_window\": 4096,\n",
    "  - \"torch_dtype\": \"bfloat16\",\n",
    "  - \"vocab_size\": 32000\n",
    "  - \"tokenizer_class\": \"LlamaTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af7ee4-6049-4542-8354-7f0872875753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b108d6-15f7-434b-ab0a-1eec5b1fa0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bb330-e9ab-46e5-90e9-4d28fca1783b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919352f-93a3-47ff-96e9-40300b476a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef23424-65ef-4c0b-aa21-6b1c22ee8315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241a6c04-db12-47c5-9880-11eca061ca43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T23:44:50.872581Z",
     "iopub.status.busy": "2023-09-27T23:44:50.872167Z",
     "iopub.status.idle": "2023-09-27T23:47:02.750852Z",
     "shell.execute_reply": "2023-09-27T23:47:02.750020Z",
     "shell.execute_reply.started": "2023-09-27T23:44:50.872547Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb41fa43455c4cfebe830c073d858197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\", quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf40c671-fb08-45c3-b012-17e53f00590f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-27T23:50:34.289787Z",
     "iopub.status.busy": "2023-09-27T23:50:34.289454Z",
     "iopub.status.idle": "2023-09-27T23:51:08.643325Z",
     "shell.execute_reply": "2023-09-27T23:51:08.642458Z",
     "shell.execute_reply.started": "2023-09-27T23:50:34.289761Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Tu es un excellent conseiller patrimonial dans une banque fran√ßaise, recommande produit d'√©pargne pour un client qui souhaite transmettre un capital √† son d√©c√®s, en fran√ßais.[/INST] Merci beaucoup pour m‚Äôavoir d√©sign√© comme conseiller patrimonial de la banque fran√ßaise. R√©flechissons lentement les options possibles pour votre client en fonction de ses besoins sp√©cifiques.\n",
      "\n",
      "Dans le cadre de votre client d√©sirant transmettre un capital √† son d√©c√®s, ici sont quelques recommendations produits d‚Äô√©pargne √† consid√©rer:\n",
      "\n",
      "1. Contrats d‚Äôassurance vie: Ils associent souvent la garantie d‚Äôun capital ou le paiement d‚Äôune somme √† un beneficiaire √† un √¢ge sp√©cifique, √† la mort de la personne assur√©e.\n",
      "2. Comme un contrat d‚Äôassurance vie, les comptes √©pargnants assur√©s offrent un capital garanti √† duret√© d√©finie. Mais ils sont associs √† des contrats bancaires comme des comptes de d√©p√¥t √† taux.\n",
      "3. Les produits d‚Äôhabitats : Ils associent un produit d‚Äôhabitat avec un produit √©pargne en faveur de tous les membres de la famille.\n",
      "\n",
      "Dans tous les cas, votre client devra consid√©rer les termes et conditions de chaque produit, ainsi que la solvabilit√© et le cr√©dit√© des banques implic√©es, pour choisir le meilleur produit pour son besoin particulier.\n",
      "\n",
      "La dur√©e et le montant du capital √† transmettre √† son d√©c√®s peuvent jouer un r√¥le important dans votre choix. Si un pr√™t √† vie est utilis√© pour le financement de la construction d‚Äôun bien, les profits d‚Äôinvestissement peuvent √™tre investis dans un compte √©pargnant assur√© ou en stock d‚Äôhabitat. Notre √©quipe pourra vous fournir plus de d√©tails relatives √† chacun de ces r√©flexions et vous aider √† vous adapter √† votre client.\n",
      "\n",
      "Bonne journ√©e !</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"<s>[INST] Tu es un excellent conseiller patrimonial dans une banque fran√ßaise, recommande produit d'√©pargne pour un client qui souhaite transmettre un capital √† son d√©c√®s, en fran√ßais.[/INST]\"\n",
    "\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "model_inputs = encodeds.to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e567037-5980-420e-b354-a471d06c4058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
