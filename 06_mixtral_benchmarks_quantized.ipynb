{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b02285-f7ac-4153-a5e5-486baa6fdd78",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B-v0.1 benchmarks - with AQLM and HQQ quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb98fe1-b103-492f-94f6-fc1ac7997ac0",
   "metadata": {},
   "source": [
    "Vaibhav (VB) Srivastav @reach_vb\n",
    "\n",
    "Run Mixtral 8x7B w/ ~13 GB VRAM 🤯\n",
    "\n",
    "https://x.com/reach_vb/status/1758237703580111058?t=bD7p-kc7O9TbGttgj4Y0Fw&s=09\n",
    "\n",
    "On a free colab too, powered by Transformers & AQLM!\n",
    "\n",
    "AQLM is a new SOTA method for low-bitwidth LLM quantization, targeted to the “extreme” 2-3bit / parameter range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907384e6-5899-4278-89a2-ce962aaf5cb3",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de9eb03-6374-4f1f-b443-a801441c570a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:13:45.849488Z",
     "iopub.status.busy": "2024-03-10T21:13:45.848880Z",
     "iopub.status.idle": "2024-03-10T21:14:34.452311Z",
     "shell.execute_reply": "2024-03-10T21:14:34.450201Z",
     "shell.execute_reply.started": "2024-03-10T21:13:45.849449Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aqlm[gpu]\n",
      "  Using cached aqlm-1.1.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: transformers>=4.38.0 in ./.venv/lib/python3.10/site-packages (from aqlm[gpu]) (4.38.2)\n",
      "Collecting torch>=2.2.0\n",
      "  Using cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Requirement already satisfied: accelerate>=0.27.0 in ./.venv/lib/python3.10/site-packages (from aqlm[gpu]) (0.27.2)\n",
      "Collecting triton>=2.1\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Requirement already satisfied: ninja in ./.venv/lib/python3.10/site-packages (from aqlm[gpu]) (1.11.1.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.10/site-packages (from accelerate>=0.27.0->aqlm[gpu]) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.27.0->aqlm[gpu]) (0.4.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.27.0->aqlm[gpu]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate>=0.27.0->aqlm[gpu]) (21.3)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=2.2.0->aqlm[gpu]) (3.0.3)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=2.2.0->aqlm[gpu]) (1.9)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch>=2.2.0->aqlm[gpu]) (3.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch>=2.2.0->aqlm[gpu]) (2.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch>=2.2.0->aqlm[gpu]) (2024.2.0)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.0->aqlm[gpu]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers>=4.38.0->aqlm[gpu]) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers>=4.38.0->aqlm[gpu]) (4.66.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers>=4.38.0->aqlm[gpu]) (2023.12.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.38.0->aqlm[gpu]) (2020.6.20)\n",
      "Installing collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, aqlm\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Not uninstalling typing-extensions at /usr/local/lib/python3.10/dist-packages, outside environment /workspace/mixtral-aqlm/.venv\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Not uninstalling torch at /usr/lib/python3/dist-packages, outside environment /workspace/mixtral-aqlm/.venv\n",
      "    Can't uninstall 'torch'. No files were found to uninstall.\n",
      "Successfully installed aqlm-1.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 triton-2.2.0 typing-extensions-4.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade aqlm[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7edf6-96ea-4409-8f5f-f87025c74c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip uninstall -y flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443f4f3-41a2-4142-985d-80819ec295cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e6b93-5efc-471e-97aa-33c7125c875a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate # git+https://github.com/huggingface/accelerate.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df554c0-65c7-4a1f-81bf-02f379e8f488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers # git+https://github.com/huggingface/transformers.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79f424-090f-4171-ab14-736e21f7b269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8a77c7-0b6c-459b-b768-448c3ef51edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:14:54.932792Z",
     "iopub.status.busy": "2024-03-10T21:14:54.932678Z",
     "iopub.status.idle": "2024-03-10T21:14:54.943426Z",
     "shell.execute_reply": "2024-03-10T21:14:54.943036Z",
     "shell.execute_reply.started": "2024-03-10T21:14:54.932782Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19e2c0c-94f6-4c78-9105-12605d49f874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:14:56.283750Z",
     "iopub.status.busy": "2024-03-10T21:14:56.283316Z",
     "iopub.status.idle": "2024-03-10T21:14:56.287732Z",
     "shell.execute_reply": "2024-03-10T21:14:56.287143Z",
     "shell.execute_reply.started": "2024-03-10T21:14:56.283733Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"aqlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd092ed3-da28-43d6-b728-4586894af0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:14:57.444528Z",
     "iopub.status.busy": "2024-03-10T21:14:57.444095Z",
     "iopub.status.idle": "2024-03-10T21:14:57.452476Z",
     "shell.execute_reply": "2024-03-10T21:14:57.452012Z",
     "shell.execute_reply.started": "2024-03-10T21:14:57.444501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"hqq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd29a92-4d5e-47fc-9069-eb23a5d5dbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.metadata.version(\"flash_attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6484fb21-f68d-4e74-ab15-b2d386109544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:15:01.384847Z",
     "iopub.status.busy": "2024-03-10T21:15:01.384377Z",
     "iopub.status.idle": "2024-03-10T21:15:01.394079Z",
     "shell.execute_reply": "2024-03-10T21:15:01.393531Z",
     "shell.execute_reply.started": "2024-03-10T21:15:01.384818Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3f933f-7c94-4734-a1de-4bcc14542ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:15:01.611548Z",
     "iopub.status.busy": "2024-03-10T21:15:01.610104Z",
     "iopub.status.idle": "2024-03-10T21:15:01.619129Z",
     "shell.execute_reply": "2024-03-10T21:15:01.618433Z",
     "shell.execute_reply.started": "2024-03-10T21:15:01.611505Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.27.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"accelerate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1f48a3-6957-469c-afbd-0d657af90c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:15:01.799491Z",
     "iopub.status.busy": "2024-03-10T21:15:01.798663Z",
     "iopub.status.idle": "2024-03-10T21:15:01.806671Z",
     "shell.execute_reply": "2024-03-10T21:15:01.805926Z",
     "shell.execute_reply.started": "2024-03-10T21:15:01.799460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bbed-3a80-4b29-8aa4-03c635906b2f",
   "metadata": {},
   "source": [
    "## BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a2806-f49b-4cbc-bb13-346b50053131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "model_name = \"BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72528f97-2ed4-4587-8b76-e525f8d13ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T09:11:56.048267Z",
     "iopub.status.busy": "2024-02-17T09:11:56.047816Z",
     "iopub.status.idle": "2024-02-17T09:11:56.051465Z",
     "shell.execute_reply": "2024-02-17T09:11:56.051071Z",
     "shell.execute_reply.started": "2024-02-17T09:11:56.048253Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def display_local_cache(model_name):\n",
    "    print(f\"Model {model_name} downloaded in local cache:\")\n",
    "    path,size = get_model_path_and_size_on_disk(model_name)\n",
    "    print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"--> stored in directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1d8c8e-b58b-4ea8-bfa5-8d1ecf342d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T09:11:56.052140Z",
     "iopub.status.busy": "2024-02-17T09:11:56.052025Z",
     "iopub.status.idle": "2024-02-17T09:11:56.072013Z",
     "shell.execute_reply": "2024-02-17T09:11:56.071599Z",
     "shell.execute_reply.started": "2024-02-17T09:11:56.052131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch downloaded in local cache:\n",
      "--> model files size   : 12.20 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--BlackSamorez--Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch/snapshots\n"
     ]
    }
   ],
   "source": [
    "display_local_cache(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957db3-fc66-4557-b037-990ba4624d47",
   "metadata": {},
   "source": [
    "## mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82296328-d470-4205-bed9-059b1e5d81a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:15:43.046395Z",
     "iopub.status.busy": "2024-03-10T21:15:43.045328Z",
     "iopub.status.idle": "2024-03-10T21:15:43.804781Z",
     "shell.execute_reply": "2024-03-10T21:15:43.802633Z",
     "shell.execute_reply.started": "2024-03-10T21:15:43.046339Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hqq'...\n",
      "remote: Enumerating objects: 458, done.\u001b[K\n",
      "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
      "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
      "remote: Total 458 (delta 160), reused 201 (delta 109), pack-reused 185\u001b[K\n",
      "Receiving objects: 100% (458/458), 157.49 KiB | 6.30 MiB/s, done.\n",
      "Resolving deltas: 100% (242/242), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mobiusml/hqq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a1f14-150a-4372-afda-39cf9e589af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!source .venv/bin/activate && cd hqq/hqq/kernels && python setup_cuda.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad3b5b2-1afe-4741-9ac1-453b1e664f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:22:22.716053Z",
     "iopub.status.busy": "2024-03-10T21:22:22.715666Z",
     "iopub.status.idle": "2024-03-10T21:25:13.845763Z",
     "shell.execute_reply": "2024-03-10T21:25:13.844367Z",
     "shell.execute_reply.started": "2024-03-10T21:22:22.716033Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/mixtral-aqlm/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package available. Set backend to HQQBackend.ATEN for faster inference and HQQBackend.ATEN_BACKPROP for faster training!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f7f40da0344000a3b0269b63a263ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 17.20it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 293.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a083de8-2dc6-473c-a6f0-a148a4fe2ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:25:13.852300Z",
     "iopub.status.busy": "2024-03-10T21:25:13.851299Z",
     "iopub.status.idle": "2024-03-10T21:25:13.862849Z",
     "shell.execute_reply": "2024-03-10T21:25:13.862397Z",
     "shell.execute_reply.started": "2024-03-10T21:25:13.852285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Optional: set backend/compile\n",
    "#You will need to install CUDA kernels apriori\n",
    "from hqq.core.quantize import *\n",
    "HQQLinear.set_backend(HQQBackend.ATEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0687b95a-fd1d-4e4b-9f9b-82bf501395d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:09:53.889369Z",
     "iopub.status.busy": "2024-03-10T21:09:53.888850Z",
     "iopub.status.idle": "2024-03-10T21:09:53.894512Z",
     "shell.execute_reply": "2024-03-10T21:09:53.894043Z",
     "shell.execute_reply.started": "2024-03-10T21:09:53.889357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ERROR\n",
    "#import torch\n",
    "#model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc05f5d-eee6-4e86-af01-0effec91b843",
   "metadata": {},
   "source": [
    "## Cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808aa5f5-9252-441c-bb30-f7ef39fd2ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:20:48.101289Z",
     "iopub.status.busy": "2024-03-10T13:20:48.100819Z",
     "iopub.status.idle": "2024-03-10T13:20:48.138510Z",
     "shell.execute_reply": "2024-03-10T13:20:48.138012Z",
     "shell.execute_reply.started": "2024-03-10T13:20:48.101276Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,592.1 MB -   6 %\n",
      "Reserved : 21,794.0 MB -  88 %\n",
      "Free     :  1,177.4 MB -   4 %\n",
      "------------------------------\n",
      "Used     : 20,513.8 MB -  83 %\n",
      "Max used : 20,513.8 MB -  83 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "\n",
    "# https://zdevito.github.io/2022/08/16/memory-snapshots.html\n",
    "# https://zdevito.github.io/2022/12/09/memory-traces.html\n",
    "\n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29605eb0-6672-46b5-b5ff-2fa331c0d0bc",
   "metadata": {},
   "source": [
    "## Chat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f1ddaf-958f-42d1-8a5f-e4d1ad9b8b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:25:13.863923Z",
     "iopub.status.busy": "2024-03-10T21:25:13.863616Z",
     "iopub.status.idle": "2024-03-10T21:25:13.876727Z",
     "shell.execute_reply": "2024-03-10T21:25:13.876240Z",
     "shell.execute_reply.started": "2024-03-10T21:25:13.863913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers \n",
    "from threading import Thread\n",
    "\n",
    "def chat_processor(chat, max_new_tokens=100, do_sample=True):\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_params = dict(\n",
    "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=0.90,\n",
    "        top_k=50,\n",
    "        temperature= 0.6,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_params)\n",
    "    t.start()\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c3a33f-3a4d-4a36-80db-800e93b567b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:34:30.919825Z",
     "iopub.status.busy": "2024-03-10T21:34:30.918877Z",
     "iopub.status.idle": "2024-03-10T21:36:26.737602Z",
     "shell.execute_reply": "2024-03-10T21:36:26.737128Z",
     "shell.execute_reply.started": "2024-03-10T21:34:30.919786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoutez, noble voyageur, l'histoire héroïque et épique de la mission du CIC Banque Privée. Comme Ulysse dans son périple vers Ithaque, ce guerrier des temps modernes entreprend un long voyage à travers les mers agitées de la finance pour accompagner ses clients vers leur destination ultime : la prospérité et la sécurité financière.\n",
      "\n",
      "Au début de cette odyssée moderne se trouve le conseiller en gestion de patrimoine, un guide sage et expérimenté qui aide les clients à naviguer les eaux troubles de la planification financière. Il est armé de sa connaissance approfondie des marchés financiers et de sa capacité à comprendre les besoins spécifiques de chaque client. Avec une écoute attentive et un dévouement sans faille, il offre des solutions sur mesure pour atteindre leurs objectifs financiers.\n",
      "\n",
      "Dans sa quête pour offrir le meilleur service possible, le CIC Banque Privée doit faire face à de nombreux défis, tels que les tempêtes économiques imprévues ou les monstres fiscaux qui menacent les fortunes duresment gagnées par ses clients. Mais grâce à sa force collective et à son expertise inégalée, il arrive toujours à vaincre ces obstacles et à continuer d'avancer vers sa destination finale.\n",
      "\n",
      "Le CIC Banque Privée met également un point d'honneur à respecter les valeurs traditionnelles d'honnêteté, d'intégrité et de transparence. Tout comme Ulysse refusait de céder aux sirènes qui cherchaient à le distraire de sa quête, le CIC Banque Privée reste fidèle à sa mission originelle, qui consiste à fournir des services bancaires haut de gamme axés sur les intérêts de ses clients.\n",
      "\n",
      "Enfin, tout comme les dieux grecs accordaient leur protection bienveillante aux héros antiques, le CIC Banque Privée peut compter sur le soutien indéfectible de ses partenaires stratégiques et de ses alliés commerciaux. Grâce à ces collaborations solides, elle continue de renforcer sa position de leader dans le domaine de la banque privée et de repousser les limites de l'innovation financière.\n",
      "\n",
      "Voilà donc l'incroyable histoire du CIC Banque Privée, un héros moderne qui poursuit fièrement sa mission au nom de la prospérité et de la sécurité financière de tous ses clients."
     ]
    }
   ],
   "source": [
    "outputs = chat_processor(\"présente-moi les missions du CIC banque privée comme un récit homérique\", max_new_tokens=1000, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e544ced-0664-47be-a91e-08d205a85386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T21:36:48.917750Z",
     "iopub.status.busy": "2024-03-10T21:36:48.917161Z",
     "iopub.status.idle": "2024-03-10T21:36:48.922610Z",
     "shell.execute_reply": "2024-03-10T21:36:48.922036Z",
     "shell.execute_reply.started": "2024-03-10T21:36:48.917717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.034782608695652"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([token for token in outputs if token!='']) / 115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75dcaf-ec0d-4c54-9793-5abf76fa4e1b",
   "metadata": {},
   "source": [
    "Generation speed: 3 tokens / sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e851-39ff-4906-8d22-4545839673c3",
   "metadata": {},
   "source": [
    "## Perplexity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95e5814-6469-45f0-993d-e1f968ead777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:05.878424Z",
     "iopub.status.busy": "2024-03-10T13:21:05.877898Z",
     "iopub.status.idle": "2024-03-10T13:21:05.886350Z",
     "shell.execute_reply": "2024-03-10T13:21:05.885852Z",
     "shell.execute_reply.started": "2024-03-10T13:21:05.878388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42192ae5-9501-4dd5-bcad-84cb0d7ecb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:07.010563Z",
     "iopub.status.busy": "2024-03-10T13:21:07.009658Z",
     "iopub.status.idle": "2024-03-10T13:21:20.170455Z",
     "shell.execute_reply": "2024-03-10T13:21:20.169978Z",
     "shell.execute_reply.started": "2024-03-10T13:21:07.010512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9aa0d7bcc401881c6c2178df7e2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1df967461d041099166ec9b800b28a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b9e8fa8ea84f13afa14055ee0d19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeef87c5fa2e43cface70e9c991a86a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7130c48f868423f8109e383155ed3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f243ea5edf2b433fb91c6f5a86d8b3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ad2ae07ca5477d9eface5d8a1ce8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424c1055f18445528ed456710d8176ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25351b2b38047f397b6bdf9e988f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "dataset_fr = load_dataset(dataset_name_fr, token=myhftoken)\n",
    "\n",
    "#dataset_name_en = \"frenchtext/bank-en-2401\"\n",
    "#dataset_en = load_dataset(dataset_name_en, token=myhftoken)\n",
    "\n",
    "#dataset_name_de = \"frenchtext/bank-de-2401\"\n",
    "#dataset_de = load_dataset(dataset_name_de, token=myhftoken)\n",
    "\n",
    "#dataset_name_es = \"frenchtext/bank-es-2401\"\n",
    "#dataset_es = load_dataset(dataset_name_es, token=myhftoken)\n",
    "\n",
    "dataset_name = dataset_name_fr\n",
    "split = \"valid\"\n",
    "dataset = dataset_fr[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa44df5-7753-435f-8dbd-58e809e68cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.171791Z",
     "iopub.status.busy": "2024-03-10T13:21:20.171238Z",
     "iopub.status.idle": "2024-03-10T13:21:20.178118Z",
     "shell.execute_reply": "2024-03-10T13:21:20.177638Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.171776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>15)\n",
    "    sorted_dataset = filtered_dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]\n",
    "\n",
    "def get_encoding_offsets(encoding):\n",
    "    start_index = encoding.offsets[0][0]\n",
    "    end_index = encoding.offsets[-1][1]\n",
    "    if end_index==0: end_index = -1\n",
    "    return (start_index, end_index)\n",
    "\n",
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "    encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5786d0e2-8ae8-498b-aa65-cc6ac45b14ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.178861Z",
     "iopub.status.busy": "2024-03-10T13:21:20.178644Z",
     "iopub.status.idle": "2024-03-10T13:21:20.184486Z",
     "shell.execute_reply": "2024-03-10T13:21:20.184088Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.178851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        if hasattr(tokenizer,\"vocab\"):\n",
    "            self.vocab_size = len(tokenizer.vocab)\n",
    "        else:\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Number of tokens predicted, ignoring padding tokens\n",
    "        predicted_tokens_count = labels_to_ignore.sum(dim=1)\n",
    "        \n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = (1/predicted_tokens_count)*self.perplexity_loss(logits, labels_for_crossentropy).sum(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/predicted_tokens_count)*torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return predicted_tokens_count, batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities\n",
    "\n",
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f31dcf-0270-47eb-8ba1-3eee78471991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.185751Z",
     "iopub.status.busy": "2024-03-10T13:21:20.185471Z",
     "iopub.status.idle": "2024-03-10T13:21:20.265877Z",
     "shell.execute_reply": "2024-03-10T13:21:20.265438Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.185741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\n",
      "- model vocabulary: 32000\n",
      "- model sequence length: 8192\n",
      "- model torch dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Optimize perf on RTX 4090\n",
    "tokenizer.model_max_length = 8192\n",
    "    \n",
    "print(f\"Computing perplexity on dataset {dataset_name}:{split} for {model_name}\")\n",
    "print(f\"- model vocabulary: {len(tokenizer.vocab)}\")\n",
    "print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")\n",
    "print(f\"- model torch dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674d25a8-751c-4ccd-a919-db43a63efad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.266871Z",
     "iopub.status.busy": "2024-03-10T13:21:20.266434Z",
     "iopub.status.idle": "2024-03-10T13:21:30.677891Z",
     "shell.execute_reply": "2024-03-10T13:21:30.677372Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.266821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (335203 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 5,765,378 tokens\n",
      "... 12,887,127 tokens\n",
      "... 15,169,531 tokens\n",
      "Done: 15,453,930 tokens\n",
      "CPU times: user 39.8 s, sys: 12.3 s, total: 52.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f6fe1c-979a-4aa3-a2cc-5309f2d58da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:30.678700Z",
     "iopub.status.busy": "2024-03-10T13:21:30.678471Z",
     "iopub.status.idle": "2024-03-10T13:21:30.681434Z",
     "shell.execute_reply": "2024-03-10T13:21:30.681007Z",
     "shell.execute_reply.started": "2024-03-10T13:21:30.678689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dataset examples: 8522\n",
      "- batch_size=1, stride=256\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "stride = 256\n",
    "\n",
    "print(f\"- dataset examples: {len(dataset)}\")\n",
    "print(f\"- batch_size={batch_size}, stride={stride}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5a22f5-cc18-4528-9937-f4762a727112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:32.173754Z",
     "iopub.status.busy": "2024-03-10T13:21:32.172735Z",
     "iopub.status.idle": "2024-03-10T13:21:38.918360Z",
     "shell.execute_reply": "2024-03-10T13:21:38.917884Z",
     "shell.execute_reply.started": "2024-03-10T13:21:32.173703Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "output = model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17774703-32a0-45d8-a0b6-6933996c51d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9732d-0b66-4a96-97c7-eb24c88a53aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(pred_tokens_count, ppl_losses, unigram_losses):        \n",
    "    pt_pred_tokens_count = torch.Tensor(pred_tokens_count)\n",
    "    total_pred_tokens_count = pt_pred_tokens_count.sum().item()\n",
    "    \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp((pt_ppl_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "    pplu = math.exp((pt_pplu_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "pred_tokens_count = [] \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n",
    "        ppl_losses.extend(batch_ppl_losses.tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.tolist(), batch_pplu.tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    #if idx%10 == 0:\n",
    "    print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "    display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ee209-3c5b-4810-bb76-12423892332e",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=1, stride=256\n",
    "- 15,453,930 tokens in 10.4 s\n",
    "- perplexity = 3.102\n",
    "- unigram-normalized perplexity = 4.041 (x1000)\n",
    "\n",
    "Max used : 23,865.6 MB -  97 %\n",
    "Wall time: 5h 39min 3s\n",
    "\n",
    "COMPARISON with unquantized version: uppl 3.967 batch size 6 wall time 1h30min\n",
    "- unigram-normalized perplexity: +1.86 % only !\n",
    "- wall time: 3,76x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31267e9-e308-44ba-91e3-3964a9296b80",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 9,243,621 tokens in 14 sec\n",
    "- perplexity = 4.690\n",
    "- unigram-normalized perplexity = 6.273 (x1000)\n",
    "\n",
    "2 h 22 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142a66-45c7-4ead-b730-d976fa2823d1",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=4, stride=256- 13,622,486 tokens in 14 sec\n",
    "\n",
    "- perplexity = 3.- 5\n",
    "> unigram-normalized perplexity = 4.098 (x100 0)\n",
    "\n",
    "3h 17min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938af06d-d784-42bf-9326-0814e04a581b",
   "metadata": {},
   "source": [
    "## Generation speed test with VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56612268-87a4-4438-9347-0f2d1e26b7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install vllm==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aba769e-947c-4345-8741-a16e73b071d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:24:59.053606Z",
     "iopub.status.busy": "2024-03-10T20:24:59.053505Z",
     "iopub.status.idle": "2024-03-10T20:24:59.061523Z",
     "shell.execute_reply": "2024-03-10T20:24:59.061118Z",
     "shell.execute_reply.started": "2024-03-10T20:24:59.053595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff0380c-ac8a-4ea3-aac7-b52309329f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:24:59.760656Z",
     "iopub.status.busy": "2024-03-10T20:24:59.759988Z",
     "iopub.status.idle": "2024-03-10T20:24:59.768992Z",
     "shell.execute_reply": "2024-03-10T20:24:59.768596Z",
     "shell.execute_reply.started": "2024-03-10T20:24:59.760624Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1dafdb-5896-452c-924a-286042874d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:25:00.441658Z",
     "iopub.status.busy": "2024-03-10T20:25:00.441179Z",
     "iopub.status.idle": "2024-03-10T20:25:00.451212Z",
     "shell.execute_reply": "2024-03-10T20:25:00.450409Z",
     "shell.execute_reply.started": "2024-03-10T20:25:00.441627Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"hqq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e273289b-3b22-4b8f-8305-9edc497fb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T20:25:01.599371Z",
     "iopub.status.busy": "2024-03-10T20:25:01.597890Z",
     "iopub.status.idle": "2024-03-10T20:25:04.138078Z",
     "shell.execute_reply": "2024-03-10T20:25:04.137237Z",
     "shell.execute_reply.started": "2024-03-10T20:25:01.599311Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/mixtral-aqlm/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mLangchain not installed. You can install it via \"pip install langchain\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cebcdca42b14df7b40e051f808777c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Model architecture MixtralForCausalLM not supported yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11351/3368129205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHQQLLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_quantized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mHQQLinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHQQBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mATEN_BACKPROP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/mixtral-aqlm/.venv/lib/python3.10/site-packages/hqq/engine/vllm.py\u001b[0m in \u001b[0;36mfrom_quantized\u001b[0;34m(cls, save_dir_or_hub, compute_dtype, cache_dir, tensor_parallel_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mconfig\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0march_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arch_key_from_save_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_arch_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;31m#Trick to initialize the tokenizer and a dummy model inside a VLLM instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/mixtral-aqlm/.venv/lib/python3.10/site-packages/hqq/engine/base.py\u001b[0m in \u001b[0;36m_check_arch_support\u001b[0;34m(cls, arg)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_check_arch_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HQQ_REGISTRY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model architecture \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0march\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not supported yet.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Model architecture MixtralForCausalLM not supported yet."
     ]
    }
   ],
   "source": [
    "from hqq.engine.vllm import HQQLLM\n",
    "\n",
    "model_name = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ'\n",
    "model = HQQLLM.from_quantized(model_name)\n",
    "\n",
    "HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb4601-dd16-4f73-a532-937c5a323e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixtral-aqlm",
   "language": "python",
   "name": "mixtral-aqlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
