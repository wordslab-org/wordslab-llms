{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b02285-f7ac-4153-a5e5-486baa6fdd78",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B-v0.1 benchmarks - with AQLM and HQQ quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb98fe1-b103-492f-94f6-fc1ac7997ac0",
   "metadata": {},
   "source": [
    "Vaibhav (VB) Srivastav @reach_vb\n",
    "\n",
    "Run Mixtral 8x7B w/ ~13 GB VRAM ü§Ø\n",
    "\n",
    "https://x.com/reach_vb/status/1758237703580111058?t=bD7p-kc7O9TbGttgj4Y0Fw&s=09\n",
    "\n",
    "On a free colab too, powered by Transformers & AQLM!\n",
    "\n",
    "AQLM is a new SOTA method for low-bitwidth LLM quantization, targeted to the ‚Äúextreme‚Äù 2-3bit / parameter range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907384e6-5899-4278-89a2-ce962aaf5cb3",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9eb03-6374-4f1f-b443-a801441c570a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade aqlm[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443f4f3-41a2-4142-985d-80819ec295cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7edf6-96ea-4409-8f5f-f87025c74c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip uninstall -y flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e6b93-5efc-471e-97aa-33c7125c875a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade git+https://github.com/huggingface/accelerate.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df554c0-65c7-4a1f-81bf-02f379e8f488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade git+https://github.com/huggingface/transformers.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79f424-090f-4171-ab14-736e21f7b269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8a77c7-0b6c-459b-b768-448c3ef51edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:19.497785Z",
     "iopub.status.busy": "2024-03-10T12:35:19.496232Z",
     "iopub.status.idle": "2024-03-10T12:35:19.511156Z",
     "shell.execute_reply": "2024-03-10T12:35:19.510718Z",
     "shell.execute_reply.started": "2024-03-10T12:35:19.497725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "importlib.metadata.version(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19e2c0c-94f6-4c78-9105-12605d49f874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:19.921355Z",
     "iopub.status.busy": "2024-03-10T12:35:19.920404Z",
     "iopub.status.idle": "2024-03-10T12:35:19.927798Z",
     "shell.execute_reply": "2024-03-10T12:35:19.927170Z",
     "shell.execute_reply.started": "2024-03-10T12:35:19.921319Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"aqlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd092ed3-da28-43d6-b728-4586894af0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:20.538966Z",
     "iopub.status.busy": "2024-03-10T12:35:20.538153Z",
     "iopub.status.idle": "2024-03-10T12:35:20.545664Z",
     "shell.execute_reply": "2024-03-10T12:35:20.545269Z",
     "shell.execute_reply.started": "2024-03-10T12:35:20.538933Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"hqq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd29a92-4d5e-47fc-9069-eb23a5d5dbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.metadata.version(\"flash_attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6484fb21-f68d-4e74-ab15-b2d386109544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:28.169731Z",
     "iopub.status.busy": "2024-03-10T12:35:28.169132Z",
     "iopub.status.idle": "2024-03-10T12:35:28.176704Z",
     "shell.execute_reply": "2024-03-10T12:35:28.176209Z",
     "shell.execute_reply.started": "2024-03-10T12:35:28.169710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.39.0.dev0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3f933f-7c94-4734-a1de-4bcc14542ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:28.363348Z",
     "iopub.status.busy": "2024-03-10T12:35:28.362891Z",
     "iopub.status.idle": "2024-03-10T12:35:28.374528Z",
     "shell.execute_reply": "2024-03-10T12:35:28.373408Z",
     "shell.execute_reply.started": "2024-03-10T12:35:28.363317Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.28.0.dev0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"accelerate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1f48a3-6957-469c-afbd-0d657af90c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T12:35:28.497477Z",
     "iopub.status.busy": "2024-03-10T12:35:28.496710Z",
     "iopub.status.idle": "2024-03-10T12:35:28.505191Z",
     "shell.execute_reply": "2024-03-10T12:35:28.504784Z",
     "shell.execute_reply.started": "2024-03-10T12:35:28.497437Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.metadata.version(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bbed-3a80-4b29-8aa4-03c635906b2f",
   "metadata": {},
   "source": [
    "## BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a2806-f49b-4cbc-bb13-346b50053131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "model_name = \"BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72528f97-2ed4-4587-8b76-e525f8d13ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T09:11:56.048267Z",
     "iopub.status.busy": "2024-02-17T09:11:56.047816Z",
     "iopub.status.idle": "2024-02-17T09:11:56.051465Z",
     "shell.execute_reply": "2024-02-17T09:11:56.051071Z",
     "shell.execute_reply.started": "2024-02-17T09:11:56.048253Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.hub import cached_file\n",
    "\n",
    "memory_unit_gb = 1024*1024*1024\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def get_model_path_and_size_on_disk(pretrained_model_id):    \n",
    "    model_config_file = cached_file(pretrained_model_id, \"config.json\", local_files_only=True)\n",
    "    model_directory = os.path.dirname(os.path.dirname(model_config_file))    \n",
    "    total_size = get_directory_size(model_directory)\n",
    "    return model_directory,total_size\n",
    "\n",
    "def display_local_cache(model_name):\n",
    "    print(f\"Model {model_name} downloaded in local cache:\")\n",
    "    path,size = get_model_path_and_size_on_disk(model_name)\n",
    "    print(f\"--> model files size   : {(size/memory_unit_gb):.2f} GB\")\n",
    "    print(f\"--> stored in directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1d8c8e-b58b-4ea8-bfa5-8d1ecf342d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T09:11:56.052140Z",
     "iopub.status.busy": "2024-02-17T09:11:56.052025Z",
     "iopub.status.idle": "2024-02-17T09:11:56.072013Z",
     "shell.execute_reply": "2024-02-17T09:11:56.071599Z",
     "shell.execute_reply.started": "2024-02-17T09:11:56.052131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch downloaded in local cache:\n",
      "--> model files size   : 12.20 GB\n",
      "--> stored in directory: /models/huggingface/transformers/models--BlackSamorez--Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch/snapshots\n"
     ]
    }
   ],
   "source": [
    "display_local_cache(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957db3-fc66-4557-b037-990ba4624d47",
   "metadata": {},
   "source": [
    "## mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82296328-d470-4205-bed9-059b1e5d81a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/mobiusml/hqq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d00b7-84b9-4533-9751-25ba153d6c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a1f14-150a-4372-afda-39cf9e589af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!source .venv/bin/activate && cd hqq/hqq/kernels && python setup_cuda.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad3b5b2-1afe-4741-9ac1-453b1e664f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:18:16.373296Z",
     "iopub.status.busy": "2024-03-10T13:18:16.372618Z",
     "iopub.status.idle": "2024-03-10T13:20:43.345365Z",
     "shell.execute_reply": "2024-03-10T13:20:43.343701Z",
     "shell.execute_reply.started": "2024-03-10T13:18:16.373278Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/mixtral-aqlm/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mhqq_aten package available. Set backend to HQQBackend.ATEN for faster inference and HQQBackend.ATEN_BACKPROP for faster training!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca9706cbb7b424a80ef9a5ee8e9ba35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:03<00:00,  8.98it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 183.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = HQQModelForCausalLM.from_quantized(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a083de8-2dc6-473c-a6f0-a148a4fe2ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:20:43.351005Z",
     "iopub.status.busy": "2024-03-10T13:20:43.350102Z",
     "iopub.status.idle": "2024-03-10T13:20:43.359551Z",
     "shell.execute_reply": "2024-03-10T13:20:43.359028Z",
     "shell.execute_reply.started": "2024-03-10T13:20:43.350992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Optional: set backend/compile\n",
    "#You will need to install CUDA kernels apriori\n",
    "from hqq.core.quantize import *\n",
    "HQQLinear.set_backend(HQQBackend.ATEN)\n",
    "#HQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc05f5d-eee6-4e86-af01-0effec91b843",
   "metadata": {},
   "source": [
    "## Cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808aa5f5-9252-441c-bb30-f7ef39fd2ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:20:48.101289Z",
     "iopub.status.busy": "2024-03-10T13:20:48.100819Z",
     "iopub.status.idle": "2024-03-10T13:20:48.138510Z",
     "shell.execute_reply": "2024-03-10T13:20:48.138012Z",
     "shell.execute_reply.started": "2024-03-10T13:20:48.101276Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Total    : 24,563.5 MB\n",
      "------------------------------\n",
      "Overhead :  1,592.1 MB -   6 %\n",
      "Reserved : 21,794.0 MB -  88 %\n",
      "Free     :  1,177.4 MB -   4 %\n",
      "------------------------------\n",
      "Used     : 20,513.8 MB -  83 %\n",
      "Max used : 20,513.8 MB -  83 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "\n",
    "memory_unit = 1024*1024\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "\n",
    "def display_memory():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Total    : {(total_memory/memory_unit):8,.1f} MB\")\n",
    "    print(\"------------------------------\")\n",
    "    free_memory = torch.cuda.mem_get_info()[0]\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    used_memory = torch.cuda.memory_allocated(0)    \n",
    "    max_used_memory = torch.cuda.max_memory_allocated(0)\n",
    "    overhead_memory = total_memory - free_memory - reserved_memory\n",
    "    print(f\"Overhead : {(overhead_memory/memory_unit):8,.1f} MB - {int(overhead_memory/total_memory*100):3} %\")\n",
    "    print(f\"Reserved : {(reserved_memory/memory_unit):8,.1f} MB - {int(reserved_memory/total_memory*100):3} %\")\n",
    "    print(f\"Free     : {(free_memory/memory_unit):8,.1f} MB - {int(free_memory/total_memory*100):3} %\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Used     : {(used_memory/memory_unit):8,.1f} MB - {int(used_memory/total_memory*100):3} %\")\n",
    "    print(f\"Max used : {(max_used_memory/memory_unit):8,.1f} MB - {int(max_used_memory/total_memory*100):3} %\")\n",
    "    \n",
    "def display_memory_summary():\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "def release_cached_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def reset_peak_memory_stats():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def record_memory_history(enabled):\n",
    "    torch.cuda.memory._record_memory_history(enabled=enabled)\n",
    "    \n",
    "def dump_memory_snapshot():\n",
    "    filename_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"memory_snapshot_{filename_datetime}.pickle\"\n",
    "    s = torch.cuda.memory._snapshot(0)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(s, f)\n",
    "    print(f\"Dumped memory snapshot to file: {filename}\")\n",
    "\n",
    "# https://zdevito.github.io/2022/08/16/memory-snapshots.html\n",
    "# https://zdevito.github.io/2022/12/09/memory-traces.html\n",
    "\n",
    "def display_memory_snapshot():\n",
    "    url = \"https://pytorch.org/memory_viz\"\n",
    "    return HTML(f\"Call dump_memory_snapshot(), <a href='{url}' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file\")\n",
    "    \n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29605eb0-6672-46b5-b5ff-2fa331c0d0bc",
   "metadata": {},
   "source": [
    "## Chat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f1ddaf-958f-42d1-8a5f-e4d1ad9b8b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:05:43.018572Z",
     "iopub.status.busy": "2024-03-10T13:05:43.018117Z",
     "iopub.status.idle": "2024-03-10T13:05:43.022355Z",
     "shell.execute_reply": "2024-03-10T13:05:43.021846Z",
     "shell.execute_reply.started": "2024-03-10T13:05:43.018556Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers \n",
    "from threading import Thread\n",
    "\n",
    "def chat_processor(chat, max_new_tokens=100, do_sample=True):\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_params = dict(\n",
    "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=0.90,\n",
    "        top_k=50,\n",
    "        temperature= 0.6,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_params)\n",
    "    t.start()\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c3a33f-3a4d-4a36-80db-800e93b567b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:08:24.359217Z",
     "iopub.status.busy": "2024-03-10T13:08:24.358716Z",
     "iopub.status.idle": "2024-03-10T13:11:16.511317Z",
     "shell.execute_reply": "2024-03-10T13:11:16.510867Z",
     "shell.execute_reply.started": "2024-03-10T13:08:24.359183Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher client,\n",
      "\n",
      "Je suis ravi de vous pr√©senter le Plan Epargne Logement (PEL), un produit d'√©pargne r√©glement√© qui offre des avantages consid√©rables en termes de rendement et de fiscalit√©. En ouvrant un PEL chez nous, vous aurez l‚Äôopportunit√© de constituer progressivement un capital tout en b√©n√©ficiant d‚Äôune r√©mun√©ration attractive et garantie par l‚Äô√âtat. De plus, ce placement peut √™tre utilis√© comme une solution de financement pour votre futur projet immobilier.\n",
      "\n",
      "Le taux du PEL est actuellement fix√© √† 1% brut par an*, soit un taux net apr√®s d√©duction fiscale et pr√©l√®vements sociaux attractif comparativement aux autres placements sans risque disponibles sur le march√© actuellement. Ce taux est garanti pendant toute la dur√©e de vie de votre plan, procurant ainsi une stabilit√© dans vos revenus compl√©mentaires.\n",
      "\n",
      "En souscrivant un PEL, vous profitez √©galement d‚Äôavantages fiscaux non n√©gligeables : les int√©r√™ts per√ßus sont exon√©r√©s d‚Äôimp√¥t sur le revenu durant les douze premi√®res ann√©es, seuls les pr√©l√®vements sociaux restent applicables**. Cela repr√©sente donc une √©conomie substantielle pour vous, notamment si vous √™tes soumis √† une tranche marginale d'imposition √©lev√©e.\n",
      "\n",
      "Autre atout majeur du PEL, il permet de b√©n√©ficier d‚Äôun pr√™t √©pargne logement √† un taux pr√©f√©rentiel lorsque vous d√©cidez d‚Äôutiliser ces fonds pour financer un bien immobilier neuf ou ancien, qu‚Äôil s‚Äôagisse d‚Äôune r√©sidence principale, secondaire ou locative. Le montant empruntable sera calcul√© selon le montant total des versements effectu√©s sur le PEL, avec une prime d‚Äô√âtat pouvant aller jusqu‚Äô√† 1 525 ‚Ç¨ suppl√©mentaires offerte sous certaines conditions***.\n",
      "\n",
      "Par ailleurs, gr√¢ce √† sa liquidit√© limit√©e, le PEL incite √† l‚Äô√©pargne r√©guli√®re et responsable, favorisant ainsi la constitution d‚Äôun patrimoine solide sur le long terme. Vous avez cependant la possibilit√© de proc√©der √† deux retraits anticip√©s au cours de la vie de votre PEL, sans que celui-ci ne soit cl√¥tur√©****. Ces caract√©ristiques font du PEL un outil id√©al pour concr√©tiser vos objectifs d‚Äô√©pargne et contribuent √† s√©curiser votre avenir financier.\n",
      "\n",
      "Pour conclure, je tiens √† souligner que notre √©tablissement propose diff√©rentes options de gestion adapt√©es √† vos besoins sp√©cifiques, telles que des versements programm√©s ou encore des alertes SMS / e-mail pour suivre l‚Äô√©volution de votre √©pargne. Nos experts sont √©galement √† disposition pour vous accompagner dans la compr√©hension des m√©canismes li√©s au PEL et optimiser son utilisation en fonction de votre situation personnelle.\n",
      "\n",
      "N‚Äôh√©sitez pas √† me contacter pour discuter davantage des avantages du PEL et comment il peut correspondre √† vos attentes et objectifs financiers. Je reste convaincu que cette solution d‚Äô√©pargne constituera un choix judicieux pour assurer votre s√©curit√© financi√®re future.\n",
      "\n",
      "Bien cordialement,\n",
      "[Votre nom]\n",
      "Conseiller bancaire\n",
      "\n",
      "* Taux brut annonc√© au 01/08/2021 susceptible de modifications ult√©rieures ** Selon la loi en vigueur au moment o√π cet article a √©t√© r√©dig√© *** Consulter les conditions d'obtention de la prime d'√âtat aupr√®s de nos conseillers bancaires **** Dans les limites et conditions pr√©vues contractuellement"
     ]
    }
   ],
   "source": [
    "outputs = chat_processor(\"Tu es un conseiller bancaire, r√©dige un argumentaire pour vendre un PEL au client\", max_new_tokens=1000, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e851-39ff-4906-8d22-4545839673c3",
   "metadata": {},
   "source": [
    "## Perplexity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95e5814-6469-45f0-993d-e1f968ead777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:05.878424Z",
     "iopub.status.busy": "2024-03-10T13:21:05.877898Z",
     "iopub.status.idle": "2024-03-10T13:21:05.886350Z",
     "shell.execute_reply": "2024-03-10T13:21:05.885852Z",
     "shell.execute_reply.started": "2024-03-10T13:21:05.878388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42192ae5-9501-4dd5-bcad-84cb0d7ecb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:07.010563Z",
     "iopub.status.busy": "2024-03-10T13:21:07.009658Z",
     "iopub.status.idle": "2024-03-10T13:21:20.170455Z",
     "shell.execute_reply": "2024-03-10T13:21:20.169978Z",
     "shell.execute_reply.started": "2024-03-10T13:21:07.010512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9aa0d7bcc401881c6c2178df7e2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1df967461d041099166ec9b800b28a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b9e8fa8ea84f13afa14055ee0d19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeef87c5fa2e43cface70e9c991a86a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7130c48f868423f8109e383155ed3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f243ea5edf2b433fb91c6f5a86d8b3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ad2ae07ca5477d9eface5d8a1ce8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424c1055f18445528ed456710d8176ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25351b2b38047f397b6bdf9e988f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "dataset_fr = load_dataset(dataset_name_fr, token=myhftoken)\n",
    "\n",
    "#dataset_name_en = \"frenchtext/bank-en-2401\"\n",
    "#dataset_en = load_dataset(dataset_name_en, token=myhftoken)\n",
    "\n",
    "#dataset_name_de = \"frenchtext/bank-de-2401\"\n",
    "#dataset_de = load_dataset(dataset_name_de, token=myhftoken)\n",
    "\n",
    "#dataset_name_es = \"frenchtext/bank-es-2401\"\n",
    "#dataset_es = load_dataset(dataset_name_es, token=myhftoken)\n",
    "\n",
    "dataset_name = dataset_name_fr\n",
    "split = \"valid\"\n",
    "dataset = dataset_fr[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa44df5-7753-435f-8dbd-58e809e68cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.171791Z",
     "iopub.status.busy": "2024-03-10T13:21:20.171238Z",
     "iopub.status.idle": "2024-03-10T13:21:20.178118Z",
     "shell.execute_reply": "2024-03-10T13:21:20.177638Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.171776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>15)\n",
    "    sorted_dataset = filtered_dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]\n",
    "\n",
    "def get_encoding_offsets(encoding):\n",
    "    start_index = encoding.offsets[0][0]\n",
    "    end_index = encoding.offsets[-1][1]\n",
    "    if end_index==0: end_index = -1\n",
    "    return (start_index, end_index)\n",
    "\n",
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "    encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5786d0e2-8ae8-498b-aa65-cc6ac45b14ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.178861Z",
     "iopub.status.busy": "2024-03-10T13:21:20.178644Z",
     "iopub.status.idle": "2024-03-10T13:21:20.184486Z",
     "shell.execute_reply": "2024-03-10T13:21:20.184088Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.178851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        if hasattr(tokenizer,\"vocab\"):\n",
    "            self.vocab_size = len(tokenizer.vocab)\n",
    "        else:\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Number of tokens predicted, ignoring padding tokens\n",
    "        predicted_tokens_count = labels_to_ignore.sum(dim=1)\n",
    "        \n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = (1/predicted_tokens_count)*self.perplexity_loss(logits, labels_for_crossentropy).sum(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/predicted_tokens_count)*torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return predicted_tokens_count, batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities\n",
    "\n",
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f31dcf-0270-47eb-8ba1-3eee78471991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.185751Z",
     "iopub.status.busy": "2024-03-10T13:21:20.185471Z",
     "iopub.status.idle": "2024-03-10T13:21:20.265877Z",
     "shell.execute_reply": "2024-03-10T13:21:20.265438Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.185741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ\n",
      "- model vocabulary: 32000\n",
      "- model sequence length: 8192\n",
      "- model torch dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Optimize perf on RTX 4090\n",
    "tokenizer.model_max_length = 8192\n",
    "    \n",
    "print(f\"Computing perplexity on dataset {dataset_name}:{split} for {model_name}\")\n",
    "print(f\"- model vocabulary: {len(tokenizer.vocab)}\")\n",
    "print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")\n",
    "print(f\"- model torch dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674d25a8-751c-4ccd-a919-db43a63efad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:20.266871Z",
     "iopub.status.busy": "2024-03-10T13:21:20.266434Z",
     "iopub.status.idle": "2024-03-10T13:21:30.677891Z",
     "shell.execute_reply": "2024-03-10T13:21:30.677372Z",
     "shell.execute_reply.started": "2024-03-10T13:21:20.266821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (335203 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 5,765,378 tokens\n",
      "... 12,887,127 tokens\n",
      "... 15,169,531 tokens\n",
      "Done: 15,453,930 tokens\n",
      "CPU times: user 39.8 s, sys: 12.3 s, total: 52.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f6fe1c-979a-4aa3-a2cc-5309f2d58da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:30.678700Z",
     "iopub.status.busy": "2024-03-10T13:21:30.678471Z",
     "iopub.status.idle": "2024-03-10T13:21:30.681434Z",
     "shell.execute_reply": "2024-03-10T13:21:30.681007Z",
     "shell.execute_reply.started": "2024-03-10T13:21:30.678689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dataset examples: 8522\n",
      "- batch_size=1, stride=256\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "stride = 256\n",
    "\n",
    "print(f\"- dataset examples: {len(dataset)}\")\n",
    "print(f\"- batch_size={batch_size}, stride={stride}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5a22f5-cc18-4528-9937-f4762a727112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:32.173754Z",
     "iopub.status.busy": "2024-03-10T13:21:32.172735Z",
     "iopub.status.idle": "2024-03-10T13:21:38.918360Z",
     "shell.execute_reply": "2024-03-10T13:21:38.917884Z",
     "shell.execute_reply.started": "2024-03-10T13:21:32.173703Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "output = model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17774703-32a0-45d8-a0b6-6933996c51d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fda21287-811c-4e42-a03d-0ca65fa319dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T13:21:46.837550Z",
     "iopub.status.busy": "2024-03-10T13:21:46.837222Z"
    },
    "tags": []
   },
   "source": [
    "%%time\n",
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(pred_tokens_count, ppl_losses, unigram_losses):        \n",
    "    pt_pred_tokens_count = torch.Tensor(pred_tokens_count)\n",
    "    total_pred_tokens_count = pt_pred_tokens_count.sum().item()\n",
    "    \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp((pt_ppl_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "    pplu = math.exp((pt_pplu_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "pred_tokens_count = [] \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n",
    "        ppl_losses.extend(batch_ppl_losses.tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.tolist(), batch_pplu.tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    #if idx%10 == 0:\n",
    "    print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "    display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "display_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31267e9-e308-44ba-91e3-3964a9296b80",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 9,243,621 tokens in 14 sec\n",
    "- perplexity = 4.690\n",
    "- unigram-normalized perplexity = 6.273 (x1000)\n",
    "\n",
    "2 h 22 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1142a66-45c7-4ead-b730-d976fa2823d1",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=4, stride=256- 13,622,486 tokens in 14 sec\n",
    "\n",
    "- perplexity = 3.- 5\n",
    "> unigram-normalized perplexity = 4.098 (x100 0)\n",
    "\n",
    "3h 17min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e57a0-38d4-4301-9596-6b54845d6228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixtral-aqlm",
   "language": "python",
   "name": "mixtral-aqlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
