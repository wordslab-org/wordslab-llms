{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10092cc2-d4c6-410e-977b-35e35524fb01",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401dd4f-6aa7-449c-9b18-c980d7229369",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load a french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd212d-0242-4b30-bb2c-12c992acecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7bf0bc-b27e-48d1-bb30-f5329b0add63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T08:29:50.671396Z",
     "iopub.status.busy": "2023-12-16T08:29:50.671223Z",
     "iopub.status.idle": "2023-12-16T08:29:51.115729Z",
     "shell.execute_reply": "2023-12-16T08:29:51.115304Z",
     "shell.execute_reply.started": "2023-12-16T08:29:50.671386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"frenchtext/banque-fr-2311\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943b889-1c7f-415c-8723-c6a6210809d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff2050-e6d5-4b84-bc47-1e43b7c359fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T20:15:41.875407Z",
     "iopub.status.busy": "2023-11-17T20:15:41.875120Z",
     "iopub.status.idle": "2023-11-17T20:15:41.878400Z",
     "shell.execute_reply": "2023-11-17T20:15:41.878028Z",
     "shell.execute_reply.started": "2023-11-17T20:15:41.875396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Les nouvelles normes européennes sur le paiement pourraient affecter l'e-commerce\\r\\n\\r\\nicone ecommerce\\r\\n\\r\\nLes nouvelles règles européennes sur la sécurisation des paiements en ligne entreront en vigueur à partir du 14 septembre 2019. Elles ont notamment été pensées pour limiter les fraudes dans le domaine. Les banques et les acteurs du secteur interpellent toutefois les autorités sur les perturbations pouvant être induites par le déploiement de cette nouvelle norme.\\r\\n\\r\\nLes plateformes d'e-commerce sont généralement débordées en fin d’année, notamment avec Thanksgiving, le Black Friday et les achats de Noël. Cette période s’annonce encore plus compliquée pour 2019.\\r\\n\\r\\nEn effet, les nouvelles normes de sécurité pour le paiement en ligne seront appliquées en Europe à compter de mi-septembre. Elles concerneront notamment les banques, les fournisseurs de services de paiement et les e-commerçants.\\r\\n\\r\\nAu regard de cette échéance, les protagonistes de ces différents secteurs s’inquiètent. La fédération des banques européennes a ainsi adressé une lettre à la Commission et à l'Autorité bancaire européenne (ABE) pour les alerter sur les perturbations que pourrait provoquer ce calendrier. Les deux institutions, de leur côté, excluent d’emblée tout report.\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nBoursoBank\\r\\n\\r\\n100 €\\r\\n\\r\\nBoursoBank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nHello Bank\\r\\n\\r\\nHello Bank\\r\\n\\r\\n180 €\\r\\n\\r\\nHello Bank\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nFortuneo\\r\\n\\r\\nFortuneo\\r\\n\\r\\n80 €\\r\\n\\r\\nFortuneo\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOfferts\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nMonabanq\\r\\n\\r\\nMonabanq\\r\\n\\r\\njusqu'à 240 €\\r\\n\\r\\nMonabanq\\r\\n\\r\\nJe découvre l’offre\\r\\n\\r\\nOffre du moment\\r\\n\\r\\nNotre sélection des promos\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Une réforme inévitable\\r\\n\\r\\nAfin d’endiguer les fraudes sur Internet, les nouvelles normes de sécurité en matière de paiements en ligne exigent la confirmation de l’identité de l’utilisateur à travers deux outils d'identification forte avant de valider le paiement.\\r\\n\\r\\nÀ l’approche de l’échéance, les e-commerçants dénoncent un manque de préparation en la matière. Ils craignent ainsi que de nombreuses transactions se retrouvent refusées dès l’entrée en vigueur de la nouvelle norme. D’autre part, les acteurs du secteur s’inquiètent de l'impact de l’abandon du système permettant au client de valider ses achats en ligne à l’aide d'un code reçu par SMS.\\r\\n\\r\\nEn effet, l’Autorité bancaire européenne considérait que ce dispositif n'était pas assez sécurisé. De ce fait, elle a décidé que ce système de validation devra être abandonné de manière progressive. Il faudra donc privilégier d'autres types d’outils pour améliorer la sécurité de la transaction. Face aux inquiétudes des acteurs du secteur, l’ABE envisage de se pencher sur la question et de prendre des mesures adaptées, si nécessaire.\\r\\n\\r\\nAu regard de ces nombreux problèmes, les opérateurs économiques touchant de près ou de loin au paiement demandent le report de l’application des nouvelles règles européenne.\\r\\n\\r\\nEn France, en revanche, les professionnels dans le domaine tendent à tenir des propos plus nuancés et plaident en faveur de sa mise en œuvre effective. C’est ainsi qu’une source rapportée par un quotidien financier souligne :\\r\\n\\r\\nUn report n'est pas souhaitable dans la mesure où il récompenserait les mauvais élèves.\\r\\n\\r\\nToutefois, il faudrait privilégier un déploiement progressif de ces nouvelles normes. En d’autres termes, les régulateurs devraient effectuer des contrôles proportionnés au fur et à mesure. La Fédération bancaire française partage également cette approche. Elle souhaiterait ainsi que ces normes soient appliquées de manière plus pragmatique et mesurée.\\r\\n\\r\\nJe compare les offres bancaires\\r\\n\\r\\n## Des perturbations pénalisant l'e-commerce\\r\\n\\r\\nAu-delà de la question de la sécurisation du moyen de paiement , les acteurs du secteur s’inquiètent surtout du calendrier établi par l’ABE. Ils reconnaissent en effet la nécessité d’améliorer le niveau de sécurité en la matière. Toutefois, les activités sur les plateformes d'e-commerce risquent d’être gravement perturbées par la mise en œuvre de ces nouvelles normes.\\r\\n\\r\\nDans sa lettre adressée à la Commission européenne au président de l'ABE, la fédération bancaire européenne souligne la gravité de la situation :\\r\\n\\r\\nIl y a un risque sérieux de perturbation d'e-commerce à partir du 14 septembre, nous craignons des effets néfastes de l'application de ces règles : 30 % des achats par carte sur des sites d'e-commerce risquent d'être refusés.\\r\\n\\r\\nPourtant, il s’agit d’une période particulièrement déterminante pour leurs résultats annuels. La fédération estime par ailleurs que ces perturbations risquent d’être dommageables pour tous les acteurs du paiement :\\r\\n\\r\\nCela pourrait engendrer des pertes de revenus massives pour les marchands et de nombreuses plaintes de clients des banques.\\r\\n\\r\\nLes banques européennes ne sont pas les seules à s’inquiéter par rapport à la mise en œuvre de cette nouvelle norme. La Fédération du e-commerce en France (la Fevad) ainsi que Mercatel (think tank consacré aux commerçants) attirent également l’attention des responsables sur la maturité de ce nouveau dispositif de sécurité.\\r\\n\\r\\nSelon le responsable des moyens de paiement à la Fevad, Bertrand Pineau :\\r\\n\\r\\nPlusieurs acteurs se disent prêts, mais en réalité les systèmes n'ont pas été testés grandeur nature.\\r\\n\\r\\nBertrand Pineau.\\r\\n\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"valid\"][0][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358c12-d431-4fe0-8a6b-30cdb2983d34",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Get popular tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db3dab-568e-4c94-9753-75de7bd31a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c21256-7f62-404a-820e-cca8043f89be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c591263-b3a9-47e7-abf2-2aaaa5613bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2d915a-b910-48cb-b308-8ab5ac9eb65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:53:40.369711Z",
     "iopub.status.busy": "2023-12-16T07:53:40.369287Z",
     "iopub.status.idle": "2023-12-16T07:53:40.381296Z",
     "shell.execute_reply": "2023-12-16T07:53:40.380948Z",
     "shell.execute_reply.started": "2023-12-16T07:53:40.369683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    #\"rmkv_world_1b5\" : \"BlinkDL/rwkv-5-world\",\n",
    "\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\",\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\",\n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\n",
    "    \"open_llama_3b\" : \"openlm-research/open_llama_3b_v2\",\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\",      \n",
    "    \n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\",\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\",\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\",\n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\",\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\",\n",
    "    \"open_llama_7b\" : \"openlm-research/open_llama_7b_v2\",\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\",\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\",\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\",\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\",\n",
    "    \n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\",\n",
    "    \n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\",\n",
    "    \n",
    "    \"mixtral_8x7B\" : \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\",\n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\",\n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbc2a24-55f1-4547-8a21-daaff40271d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:55:07.680138Z",
     "iopub.status.busy": "2023-12-16T07:55:07.679266Z",
     "iopub.status.idle": "2023-12-16T07:55:07.695674Z",
     "shell.execute_reply": "2023-12-16T07:55:07.694944Z",
     "shell.execute_reply.started": "2023-12-16T07:55:07.680093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "135024e9-d16d-499b-acb7-aca76d8e75cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T11:35:56.994402Z",
     "iopub.status.busy": "2023-12-16T11:35:56.993977Z",
     "iopub.status.idle": "2023-12-16T11:35:57.501587Z",
     "shell.execute_reply": "2023-12-16T11:35:57.500979Z",
     "shell.execute_reply.started": "2023-12-16T11:35:56.994374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( \"Qwen/Qwen-7B\", trust_remote_code=True, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd763063-fcea-4e43-9027-2e0094f764fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T11:33:04.257682Z",
     "iopub.status.busy": "2023-12-16T11:33:04.255680Z",
     "iopub.status.idle": "2023-12-16T11:33:10.880320Z",
     "shell.execute_reply": "2023-12-16T11:33:10.879884Z",
     "shell.execute_reply.started": "2023-12-16T11:33:04.257644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Loading phi2_3b tokenizer\n",
      "4837434508465586422\n",
      "{'type': <class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50257, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading btlm_3b tokenizer\n",
      "8272226667653940756\n",
      "{'type': <class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50257, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': '', 'end_of_word_suffix': '', 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_3b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading open_llama_3b tokenizer\n",
      "-2792885599119712379\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading stablelm_3b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_6b tokenizer\n",
      "-2020660444933683167\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}]}}\n",
      "------------------------\n",
      "Loading mistral_7b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading falcon_7b tokenizer\n",
      "-5253717437475191100\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading redpajama_7b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading llama2_7b_32k tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 32768, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading open_llama_7b tokenizer\n",
      "-2792885599119712379\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 2048, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_7b_8k tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading qwen_7b tokenizer\n",
      "-5546107980889120355\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-7B.ffe04dd57f85293043ba999a2c0daa788d6182e9.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 32768, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading llama2_7b tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading bloomz_7b tokenizer\n",
      "-1357707804698952073\n",
      "{'type': <class 'transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 250680, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Split', 'pattern': {'Regex': ' ?[^(\\\\s|[.,!?…。，、।۔،])]+'}, 'behavior': 'Isolated', 'invert': False}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': False, 'use_regex': False}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': False}}\n",
      "------------------------\n",
      "Loading decilm_7b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading solar_10b tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading llama2_13b tokenizer\n",
      "8051807531637384684\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading qwen_14b tokenizer\n",
      "-5546107980889120355\n",
      "{'type': <class 'transformers_modules.Qwen.Qwen-14B.c4051215126d906ac22bb67fe5edb39a921cd831.tokenization_qwen.QWenTokenizer'>, 'vocab_size': 151851, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True}\n",
      "------------------------\n",
      "Loading mixtral_8x7B tokenizer\n",
      "-7863300483653561688\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 32000, 'model_max_length': 1000000000000000019884624838656, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Prepend', 'prepend': '▁'}, {'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'SpecialToken': {'id': '<s>', 'type_id': 0}}, {'Sequence': {'id': 'A', 'type_id': 0}}, {'SpecialToken': {'id': '<s>', 'type_id': 1}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {'<s>': {'id': '<s>', 'ids': [1], 'tokens': ['<s>']}}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}, {'type': 'Strip', 'content': ' ', 'start': 1, 'stop': 0}]}}\n",
      "------------------------\n",
      "Loading mpt_30b tokenizer\n",
      "-8337468301703944686\n",
      "{'type': <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 50254, 'model_max_length': 8192, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': {'type': 'NFC'}, 'pre_tokenizer': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}}\n",
      "------------------------\n",
      "Loading yi_34b tokenizer\n",
      "-2020660444933683167\n",
      "{'type': <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 64000, 'model_max_length': 4096, 'padding_side': 'left', 'truncation_side': 'right', 'clean_up_tokenization_spaces': False, 'truncation': None, 'padding': None, 'normalizer': {'type': 'Sequence', 'normalizers': [{'type': 'Replace', 'pattern': {'String': ' '}, 'content': '▁'}]}, 'pre_tokenizer': None, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': True, 'byte_fallback': True}, 'post_processor': {'type': 'TemplateProcessing', 'single': [{'Sequence': {'id': 'A', 'type_id': 0}}], 'pair': [{'Sequence': {'id': 'A', 'type_id': 0}}, {'Sequence': {'id': 'B', 'type_id': 1}}], 'special_tokens': {}}, 'decoder': {'type': 'Sequence', 'decoders': [{'type': 'Replace', 'pattern': {'String': '▁'}, 'content': ' '}, {'type': 'ByteFallback'}, {'type': 'Fuse'}]}}\n",
      "------------------------\n",
      "Loading falcon_40b tokenizer\n",
      "-5253717437475191100\n",
      "{'type': <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>, 'backend_type': <class 'tokenizers.models.BPE'>, 'vocab_size': 65024, 'model_max_length': 2048, 'padding_side': 'right', 'truncation_side': 'right', 'clean_up_tokenization_spaces': True, 'truncation': None, 'padding': None, 'normalizer': None, 'pre_tokenizer': {'type': 'Sequence', 'pretokenizers': [{'type': 'Punctuation', 'behavior': 'Contiguous'}, {'type': 'ByteLevel', 'add_prefix_space': False, 'trim_offsets': True, 'use_regex': True}, {'type': 'Digits', 'individual_digits': False}, {'type': 'Split', 'pattern': {'Regex': '[0-9][0-9][0-9]'}, 'behavior': 'Isolated', 'invert': False}]}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': None, 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False}, 'post_processor': None, 'decoder': {'type': 'ByteLevel', 'add_prefix_space': True, 'trim_offsets': True, 'use_regex': True}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "models_tokenizers = {}\n",
    "models_tokenizers_config = {}\n",
    "models_tokenizers_specialtokens = {}\n",
    "for model in models.keys():\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Loading {model} tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model], trust_remote_code=True, token=myhftoken)\n",
    "    models_tokenizers[model] = tokenizer\n",
    "    if model[:5]==\"qwen_\":\n",
    "        print(hash(tuple(sorted(tokenizer.mergeable_ranks.items()))))\n",
    "    else:\n",
    "        print(hash(tuple(sorted(tokenizer.vocab.items()))))\n",
    "    config = {}\n",
    "    specialtokens = {}\n",
    "    config[\"type\"] = type(tokenizer)\n",
    "    if model[:5]==\"qwen_\":\n",
    "         type(tokenizer.tokenizer)\n",
    "    else:\n",
    "        config[\"backend_type\"] = type(tokenizer.backend_tokenizer.model)\n",
    "    config[\"vocab_size\"] = tokenizer.vocab_size\n",
    "    config[\"model_max_length\"] = tokenizer.model_max_length\n",
    "    if hasattr(tokenizer, \"special_tokens\"): specialtokens[\"special_tokens\"] = tokenizer.special_tokens\n",
    "    config[\"padding_side\"] = tokenizer.padding_side\n",
    "    config[\"truncation_side\"] = tokenizer.truncation_side\n",
    "    config[\"clean_up_tokenization_spaces\"] = tokenizer.clean_up_tokenization_spaces\n",
    "    if tokenizer.is_fast:\n",
    "        backend_config = json.loads(tokenizer.backend_tokenizer.to_str())\n",
    "        if \"vocab\" in backend_config[\"model\"]: del backend_config[\"model\"][\"vocab\"]\n",
    "        if \"merges\" in backend_config[\"model\"]: del backend_config[\"model\"][\"merges\"]\n",
    "        config['truncation'] = backend_config['truncation']\n",
    "        config['padding'] = backend_config['padding']\n",
    "        specialtokens['added_tokens'] = backend_config['added_tokens']\n",
    "        config['normalizer'] = backend_config['normalizer']\n",
    "        config['pre_tokenizer'] = backend_config['pre_tokenizer']\n",
    "        config['model'] = backend_config['model']\n",
    "        config['post_processor'] = backend_config['post_processor']\n",
    "        config['decoder'] = backend_config['decoder']\n",
    "    elif model[:3]==\"yi_\":\n",
    "        config['model'] = type(tokenizer.sp_model)\n",
    "    models_tokenizers_config[model] = config\n",
    "    models_tokenizers_specialtokens[model] = specialtokens\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed42e6-bb71-4405-8b7e-387aae91ea95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test tokenizers on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b1b0c81-b8f7-4a90-9434-3423dc6082af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:51.176339Z",
     "iopub.status.busy": "2023-11-14T23:50:51.175279Z",
     "iopub.status.idle": "2023-11-14T23:50:51.182731Z",
     "shell.execute_reply": "2023-11-14T23:50:51.181353Z",
     "shell.execute_reply.started": "2023-11-14T23:50:51.176260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = models_tokenizers[\"falcon_7b\"]\n",
    "\n",
    "def tokenization(example):\n",
    "    return tokenizer(example[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32895d76-19d5-422f-a9be-f17eae238161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:50:55.132715Z",
     "iopub.status.busy": "2023-11-14T23:50:55.132208Z",
     "iopub.status.idle": "2023-11-14T23:52:09.960044Z",
     "shell.execute_reply": "2023-11-14T23:52:09.959330Z",
     "shell.execute_reply.started": "2023-11-14T23:50:55.132684Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275e37987704a969a8ed07e86d6f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0892f943-b39e-49d0-b063-197c7d8082f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:53:35.976222Z",
     "iopub.status.busy": "2023-11-14T23:53:35.975219Z",
     "iopub.status.idle": "2023-11-14T23:54:25.291763Z",
     "shell.execute_reply": "2023-11-14T23:54:25.290942Z",
     "shell.execute_reply.started": "2023-11-14T23:53:35.976195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67061556, 131722778)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "tokens = 0\n",
    "\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "    \n",
    "words, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ab6b3e-fdf6-4311-b56b-76ca906e745b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T23:55:12.515567Z",
     "iopub.status.busy": "2023-11-14T23:55:12.515295Z",
     "iopub.status.idle": "2023-11-14T23:55:12.521367Z",
     "shell.execute_reply": "2023-11-14T23:55:12.520181Z",
     "shell.execute_reply.started": "2023-11-14T23:55:12.515549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65024, 1.9642070040844266)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokens/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb9f5d7f-7daa-458c-a495-028997402c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T08:30:08.487681Z",
     "iopub.status.busy": "2023-12-16T08:30:08.486702Z",
     "iopub.status.idle": "2023-12-16T10:54:36.991466Z",
     "shell.execute_reply": "2023-12-16T10:54:36.987356Z",
     "shell.execute_reply.started": "2023-12-16T08:30:08.487645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d664e99727466da664ce7c3e18f8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5886aff7371249988ade4ac4d96d7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec246a5dd914ab691af0f14465f3fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991b0cb843f042b3aded8dcf77be7fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728a5339fd5d4079ad522afdb5b23f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caba6e3932f349a7840ddfdf173f43b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb175c6f72c4553a8a522cc2db3b5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3094 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "phi2_3b: 50257 vocab => 2.329939242686227 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e803e05e141242fb80252a7e527ce7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10506 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "btlm_3b: 50257 vocab => 2.340823750048388 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4a51cac60f4585aef9463c51647f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfdfd0b114743a3b8d012338333315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_3b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d8b16db5544ab7bc2fb37674fede03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "stablelm_3b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748aaf52c5224fd6ad77cce0b881678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_6b: 64000 vocab => 2.4612843758054166 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed020d9d85f47bea9ed5d5c3d06628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mistral_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412dec40f92441b487e88f9560b71f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfcb7c161794325957528188e6baf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_7b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab62bce6b9e4574b13e83bdb1f1575b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2765 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "redpajama_7b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc74ce5295e4cde942621de75c5e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (127882 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b_32k: 32000 vocab => 2.1736836675844504 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09932ab593c4908b284452569f4f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3326 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "open_llama_7b: 32000 vocab => 2.4012566752850173 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5f209297bd450ba20cbe7c91ac5c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_7b_8k: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229fc3fadf414d56bf6353d56c5aeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112350 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_7b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79148d520c647f69aa89de137bb1987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_7b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a929b0e788243cc90adc335834a2511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "bloomz_7b: 250680 vocab => 1.4450713759161806 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322654c675d4a6daa6afedfd158f893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "decilm_7b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c1ffa70804c30979c637eb0ec7635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "solar_10b: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e754d26fd004b8287ec1bca76af34a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "llama2_13b: 32000 vocab => 2.1749545745702648 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b8ca6756b408b9467aa25c8eebcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10460 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "qwen_14b: 151851 vocab => 1.8874039844825552 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f514c4fa334454ca2160174c0ff2064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mixtral_8x7B: 32000 vocab => 2.234656410298622 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4433a090f3d74c62a02406a94ac050f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9937 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "mpt_30b: 50254 vocab => 2.0548674265774567 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b16778005a04127ab7b6d29d71fcf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4374 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "yi_34b: 64000 vocab => 2.4612843758054166 tokens per word\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0917cd54e4242499a7b5d89c7bf178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3038 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "falcon_40b: 65024 vocab => 1.9642070040844266 tokens per word\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, split=\"train+valid+test\")\n",
    "    \n",
    "for model in models:\n",
    "    tokenizer = models_tokenizers[model]\n",
    "\n",
    "    def tokenization(example):\n",
    "        return tokenizer(example[\"Text\"])\n",
    "    \n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "    \n",
    "    words = 0\n",
    "    tokens = 0\n",
    "    for example in dataset:\n",
    "        words += example['Words']\n",
    "        tokens += len(example['input_ids'])\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    print(f\"{model}: {tokenizer.vocab_size} vocab => {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174d63-9a29-4252-90a2-fdf65e1c5a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train a tokenizer on french dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc72d5-6af8-4078-aab2-70ce7d236ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:25:30.973096Z",
     "iopub.status.busy": "2023-11-15T23:25:30.972785Z",
     "iopub.status.idle": "2023-11-15T23:25:30.979241Z",
     "shell.execute_reply": "2023-11-15T23:25:30.978503Z",
     "shell.execute_reply.started": "2023-11-15T23:25:30.973054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf92390a-f47b-4ab0-bfe9-3ade2cf3e149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:57.148174Z",
     "iopub.status.busy": "2023-11-15T23:43:57.147587Z",
     "iopub.status.idle": "2023-11-15T23:43:57.211376Z",
     "shell.execute_reply": "2023-11-15T23:43:57.210630Z",
     "shell.execute_reply.started": "2023-11-15T23:43:57.148152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic byte-level BPE\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.normalizer = None\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=100,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01aa1382-693d-4265-81f2-a75d9823bdfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:43:59.033471Z",
     "iopub.status.busy": "2023-11-15T23:43:59.033097Z",
     "iopub.status.idle": "2023-11-15T23:45:43.658469Z",
     "shell.execute_reply": "2023-11-15T23:45:43.657815Z",
     "shell.execute_reply.started": "2023-11-15T23:43:59.033452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea634fa-ecba-4106-ad3f-197cddab8997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(examples):\n",
    "    return {'input_ids': [enc.ids for enc in tokenizer.encode_batch(examples[\"Text\"])]}\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "words = 0\n",
    "tokens = 0\n",
    "for example in dataset:\n",
    "    words += example['Words']\n",
    "    tokens += len(example['input_ids'])\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6ac1b16-e763-4e4b-bc3a-bfabb0a555d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T23:59:11.397725Z",
     "iopub.status.busy": "2023-11-15T23:59:11.397465Z",
     "iopub.status.idle": "2023-11-15T23:59:11.401196Z",
     "shell.execute_reply": "2023-11-15T23:59:11.400527Z",
     "shell.execute_reply.started": "2023-11-15T23:59:11.397708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom: 1.4874704070391687 tokens per word\n"
     ]
    }
   ],
   "source": [
    "print(f\"custom: {tokens/words} tokens per word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58d92-3cc5-42ba-9c7f-4111bbac7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab().keys() if len(token)>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb8e9db-3bca-4b02-ac21-70655cb6a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:10:30.899895Z",
     "iopub.status.busy": "2023-11-16T00:10:30.899588Z",
     "iopub.status.idle": "2023-11-16T00:11:46.582009Z",
     "shell.execute_reply": "2023-11-16T00:11:46.581204Z",
     "shell.execute_reply.started": "2023-11-16T00:10:30.899879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counts = Counter()\n",
    "\n",
    "for example in dataset:\n",
    "    tokens_counts.update(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9041cd-4fc8-46c4-b22d-6cb24564d02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:12:36.105416Z",
     "iopub.status.busy": "2023-11-16T00:12:36.104897Z",
     "iopub.status.idle": "2023-11-16T00:12:36.109632Z",
     "shell.execute_reply": "2023-11-16T00:12:36.108831Z",
     "shell.execute_reply.started": "2023-11-16T00:12:36.105394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31877"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e91fdec-c35f-40cf-a132-66309a78d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:16:53.630152Z",
     "iopub.status.busy": "2023-11-16T00:16:53.629915Z",
     "iopub.status.idle": "2023-11-16T00:16:53.639242Z",
     "shell.execute_reply": "2023-11-16T00:16:53.638574Z",
     "shell.execute_reply.started": "2023-11-16T00:16:53.630135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_counts) - len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fba6c57-111f-4faa-b326-0539de4c5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T00:17:29.281534Z",
     "iopub.status.busy": "2023-11-16T00:17:29.281298Z",
     "iopub.status.idle": "2023-11-16T00:17:29.291246Z",
     "shell.execute_reply": "2023-11-16T00:17:29.290381Z",
     "shell.execute_reply.started": "2023-11-16T00:17:29.281519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([{tokenizer.decode([token]):tokens_counts[token]} for token in tokens_counts.keys() if tokens_counts[token]<100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ce51-8c0c-4c17-8250-afb2f5d8ebbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Test model perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2ef1-30a2-433a-ae85-925dd7f903b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d211ba3-af65-431a-9c5b-8e5e15f9cbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:34:12.402566Z",
     "iopub.status.busy": "2023-12-01T23:34:12.402085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     577  30,026  31,785 MB   1.82% \n",
      "GPU:   1,344  23,219  24,564 MB   5.47% \n",
      "\n",
      "\n",
      "Loading model mistralai/Mistral-7B-v0.1 in local cache ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482c325420f4a199b71ca5ac96e66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(cl_enable=False):\n",
    "    model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "    ModelForCausalLMBenchmark.download_in_local_cache(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015924f-8c33-4fcc-adfa-6414574c536a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1596c51f-885d-49bd-9a5c-b3eb46fa9424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:22:28.756839Z",
     "iopub.status.busy": "2023-12-02T02:22:28.756296Z",
     "iopub.status.idle": "2023-12-02T02:22:29.072065Z",
     "shell.execute_reply": "2023-12-02T02:22:29.071569Z",
     "shell.execute_reply.started": "2023-12-02T02:22:28.756802Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 4090 (24564 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:     551  29,953  31,785 MB   1.74% \n",
      "GPU:     996  23,567  24,564 MB   4.06% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = IPyExperimentsPytorch(cl_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aba89c-5812-42b0-8a7c-5596ab162f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:32:04.300537Z",
     "iopub.status.busy": "2023-12-02T02:32:04.299222Z",
     "iopub.status.idle": "2023-12-02T02:32:04.362187Z",
     "shell.execute_reply": "2023-12-02T02:32:04.361654Z",
     "shell.execute_reply.started": "2023-12-02T02:32:04.300497Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_memory_history(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8390e6ef-d7b5-44dc-8f4e-0fe3e653c0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:32:06.102311Z",
     "iopub.status.busy": "2023-12-02T02:32:06.101200Z",
     "iopub.status.idle": "2023-12-02T02:33:01.449434Z",
     "shell.execute_reply": "2023-12-02T02:33:01.448758Z",
     "shell.execute_reply.started": "2023-12-02T02:32:06.102272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31cacce670449659f79104cfecc1184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files: 13.98 GB on disk\n",
      "(cache path: /models/huggingface/transformers/models--mistralai--Mistral-7B-v0.1/snapshots/5e9c98b96d071dce59368012254c55b0ec6f8658)\n",
      "\n",
      "Tokenizer load time : 301.49 ms\n",
      "Tokenizer CPU memory: 42.26 MB\n",
      "\n",
      "Model load time : 55007.52 ms\n",
      "Model CPU memory: 1.87 GB\n",
      "Model GPU memory: 13.99 GB\n",
      "Max   GPU memory: 13.99 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_benchmark = ModelForCausalLMBenchmark(model_id)\n",
    "model_benchmark.trace_load_from_cache(device_map=\"auto\", torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffbb07e-8651-46c8-ac7e-1bd0dbbc3558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:33:05.208756Z",
     "iopub.status.busy": "2023-12-02T02:33:05.208360Z",
     "iopub.status.idle": "2023-12-02T02:33:05.213484Z",
     "shell.execute_reply": "2023-12-02T02:33:05.212951Z",
     "shell.execute_reply.started": "2023-12-02T02:33:05.208734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231202_023305.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46631b-fa20-4334-8f02-5c92a7bf4060",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb05fc9f-ce25-49b8-b300-7c499f3bd7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T23:37:12.632791Z",
     "iopub.status.busy": "2023-12-01T23:37:12.631277Z",
     "iopub.status.idle": "2023-12-01T23:37:12.669446Z",
     "shell.execute_reply": "2023-12-01T23:37:12.668853Z",
     "shell.execute_reply.started": "2023-12-01T23:37:12.632688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "MistralForCausalLM\n",
      "> submodules\n",
      "- model: MistralModel\n",
      "- lm_head: Linear\n",
      "  ---------------------\n",
      "  model#MistralModel\n",
      "  > submodules\n",
      "  - embed_tokens: Embedding\n",
      "  - layers: ModuleList\n",
      "  - norm: MistralRMSNorm\n",
      "    ---------------------\n",
      "    embed_tokens#Embedding\n",
      "    > parameters\n",
      "    - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "    ---------------------\n",
      "    layers#ModuleList\n",
      "    > submodules\n",
      "    - 0..31: 32X MistralDecoderLayer\n",
      "      ---------------------\n",
      "      0..31#MistralDecoderLayer\n",
      "      > submodules\n",
      "      - self_attn: MistralAttention\n",
      "      - mlp: MistralMLP\n",
      "      - input_layernorm: MistralRMSNorm\n",
      "      - post_attention_layernorm: MistralRMSNorm\n",
      "        ---------------------\n",
      "        self_attn#MistralAttention\n",
      "        > submodules\n",
      "        - q_proj: Linear\n",
      "        - k_proj: Linear\n",
      "        - v_proj: Linear\n",
      "        - o_proj: Linear\n",
      "        - rotary_emb: MistralRotaryEmbedding\n",
      "          ---------------------\n",
      "          q_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          k_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          v_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [1024, 4096] (8.0 MB)\n",
      "          ---------------------\n",
      "          o_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 4096] (32.0 MB)\n",
      "          ---------------------\n",
      "          rotary_emb#MistralRotaryEmbedding\n",
      "          > buffers\n",
      "          - inv_freq: float32 [64] (0.0 MB)\n",
      "          - cos_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "          - sin_cached: bfloat16 [32768, 128] (8.0 MB)\n",
      "        ---------------------\n",
      "        mlp#MistralMLP\n",
      "        > submodules\n",
      "        - gate_proj: Linear\n",
      "        - up_proj: Linear\n",
      "        - down_proj: Linear\n",
      "        - act_fn: SiLUActivation\n",
      "          ---------------------\n",
      "          gate_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          up_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [14336, 4096] (112.0 MB)\n",
      "          ---------------------\n",
      "          down_proj#Linear\n",
      "          > parameters\n",
      "          - weight: bfloat16 [4096, 14336] (112.0 MB)\n",
      "          ---------------------\n",
      "          act_fn#SiLUActivation\n",
      "        ---------------------\n",
      "        input_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "        ---------------------\n",
      "        post_attention_layernorm#MistralRMSNorm\n",
      "        > parameters\n",
      "        - weight: bfloat16 [4096] (0.0 MB)\n",
      "    ---------------------\n",
      "    norm#MistralRMSNorm\n",
      "    > parameters\n",
      "    - weight: bfloat16 [4096] (0.0 MB)\n",
      "  ---------------------\n",
      "  lm_head#Linear\n",
      "  > parameters\n",
      "  - weight: bfloat16 [32000, 4096] (250.0 MB)\n",
      "\n",
      "\n",
      "---------------------\n",
      "MistralForCausalLM.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        labels: Optional[torch.LongTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "        r\"\"\"\n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "\n",
      "        Returns:\n",
      "\n",
      "        Example:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, MistralForCausalLM\n",
      "\n",
      "        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
      "\n",
      "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
      "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
      "\n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
      "        ```\"\"\"\n",
      "\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "        outputs = self.model(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_values=past_key_values,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        hidden_states = outputs[0]\n",
      "        logits = self.lm_head(hidden_states)\n",
      "        logits = logits.float()\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # Shift so that tokens < n predict n\n",
      "            shift_logits = logits[..., :-1, :].contiguous()\n",
      "            shift_labels = labels[..., 1:].contiguous()\n",
      "            # Flatten the tokens\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
      "            shift_labels = shift_labels.view(-1)\n",
      "            # Enable model parallelism\n",
      "            shift_labels = shift_labels.to(shift_logits.device)\n",
      "            loss = loss_fct(shift_logits, shift_labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return (loss,) + output if loss is not None else output\n",
      "\n",
      "        return CausalLMOutputWithPast(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            past_key_values=outputs.past_key_values,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralModel.forward()\n",
      "---------------------\n",
      "    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        # retrieve input_ids and inputs_embeds\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            batch_size, seq_length = input_ids.shape\n",
      "        elif inputs_embeds is not None:\n",
      "            batch_size, seq_length, _ = inputs_embeds.shape\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
      "\n",
      "        seq_length_with_past = seq_length\n",
      "        past_key_values_length = 0\n",
      "\n",
      "        if past_key_values is not None:\n",
      "            past_key_values_length = past_key_values[0][0].shape[2]\n",
      "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
      "\n",
      "        if position_ids is None:\n",
      "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "            position_ids = torch.arange(\n",
      "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
      "            )\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
      "        else:\n",
      "            position_ids = position_ids.view(-1, seq_length).long()\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if (\n",
      "            attention_mask is not None\n",
      "            and hasattr(self.config, \"_flash_attn_2_enabled\")\n",
      "            and self.config._flash_attn_2_enabled\n",
      "            and past_key_values is not None\n",
      "        ):\n",
      "            is_padding_right = attention_mask[:, -1].sum().item() != batch_size\n",
      "            if is_padding_right:\n",
      "                raise ValueError(\n",
      "                    \"You are attempting to perform batched generation with padding_side='right'\"\n",
      "                    \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n",
      "                    \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n",
      "                )\n",
      "\n",
      "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
      "            # 2d mask is passed through the layers\n",
      "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "        else:\n",
      "            # 4d mask is passed through the layers\n",
      "            attention_mask = _prepare_4d_causal_attention_mask(\n",
      "                attention_mask,\n",
      "                (batch_size, seq_length),\n",
      "                inputs_embeds,\n",
      "                past_key_values_length,\n",
      "                sliding_window=self.config.sliding_window,\n",
      "            )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "        next_decoder_cache = () if use_cache else None\n",
      "\n",
      "        for idx, decoder_layer in enumerate(self.layers):\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    attention_mask,\n",
      "                    position_ids,\n",
      "                    past_key_value,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=attention_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_value,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if use_cache:\n",
      "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        next_cache = next_decoder_cache if use_cache else None\n",
      "        if not return_dict:\n",
      "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
      "        return BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=next_cache,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "Embedding.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input, self.weight, self.padding_idx, self.max_norm,\n",
      "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\n",
      "---------------------\n",
      "MistralDecoderLayer.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
      "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
      "                `(batch, sequence_length)` where padding elements are indicated by 0.\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more detail.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
      "                (see `past_key_values`).\n",
      "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
      "        \"\"\"\n",
      "\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        if use_cache:\n",
      "            outputs += (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "---------------------\n",
      "MistralAttention.forward()\n",
      "---------------------\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        **kwargs,\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        if \"padding_mask\" in kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n",
      "            )\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        query_states = self.q_proj(hidden_states)\n",
      "        key_states = self.k_proj(hidden_states)\n",
      "        value_states = self.v_proj(hidden_states)\n",
      "\n",
      "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
      "\n",
      "        kv_seq_len = key_states.shape[-2]\n",
      "        if past_key_value is not None:\n",
      "            kv_seq_len += past_key_value[0].shape[-2]\n",
      "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # reuse k, v, self_attention\n",
      "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
      "\n",
      "        past_key_value = (key_states, value_states) if use_cache else None\n",
      "\n",
      "        # repeat k/v heads if n_kv_heads < n_heads\n",
      "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
      "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
      "\n",
      "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "\n",
      "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "            raise ValueError(\n",
      "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
      "                f\" {attn_weights.size()}\"\n",
      "            )\n",
      "\n",
      "        if attention_mask is not None:\n",
      "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "                raise ValueError(\n",
      "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
      "                )\n",
      "\n",
      "            attn_weights = attn_weights + attention_mask\n",
      "\n",
      "        # upcast attention to fp32\n",
      "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "        attn_output = torch.matmul(attn_weights, value_states)\n",
      "\n",
      "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "            raise ValueError(\n",
      "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
      "                f\" {attn_output.size()}\"\n",
      "            )\n",
      "\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "\n",
      "        if not output_attentions:\n",
      "            attn_weights = None\n",
      "\n",
      "        return attn_output, attn_weights, past_key_value\n",
      "\n",
      "---------------------\n",
      "Linear.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "---------------------\n",
      "MistralRotaryEmbedding.forward()\n",
      "---------------------\n",
      "    def forward(self, x, seq_len=None):\n",
      "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
      "        if seq_len > self.max_seq_len_cached:\n",
      "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
      "\n",
      "        return (\n",
      "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
      "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
      "        )\n",
      "\n",
      "---------------------\n",
      "MistralMLP.forward()\n",
      "---------------------\n",
      "    def forward(self, x):\n",
      "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "\n",
      "---------------------\n",
      "SiLUActivation.forward()\n",
      "---------------------\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return nn.functional.silu(input)\n",
      "\n",
      "---------------------\n",
      "MistralRMSNorm.forward()\n",
      "---------------------\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_modules(model_benchmark.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715f476-0c3a-4547-a4be-da5de9c28db8",
   "metadata": {},
   "source": [
    "#### Perf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533074ac-3463-4399-bc05-7a826a489d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:24:26.563700Z",
     "iopub.status.busy": "2023-12-02T02:24:26.563385Z",
     "iopub.status.idle": "2023-12-02T02:24:26.569114Z",
     "shell.execute_reply": "2023-12-02T02:24:26.568523Z",
     "shell.execute_reply.started": "2023-12-02T02:24:26.563667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15020351488, 15020351488)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_cached_memory()\n",
    "get_used_and_max_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0e7ccc-6181-402a-b1c9-50a570cf1d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:24:28.513196Z",
     "iopub.status.busy": "2023-12-02T02:24:28.512688Z",
     "iopub.status.idle": "2023-12-02T02:25:14.219512Z",
     "shell.execute_reply": "2023-12-02T02:25:14.218800Z",
     "shell.execute_reply.started": "2023-12-02T02:24:28.513177Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill test for batch size 2 and sequence length 4096:\n",
      "model.embed_tokens;True;;516.3;;int64[2, 4096];0.1;0.1;64.1;64.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.input_layernorm;True;;487.2;;bfloat16[2, 4096, 4096];64.0;128.1;384.1;192.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.q_proj;True;;381.7;;bfloat16[2, 4096, 4096];64.0;192.1;256.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.k_proj;True;;251.0;;bfloat16[2, 4096, 4096];64.0;256.1;272.1;272.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.v_proj;True;;148.7;;bfloat16[2, 4096, 4096];64.0;272.1;288.1;288.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.rotary_emb;True;;236.7;;bfloat16[2, 8, 4096, 128];16.0;288.1;288.1;288.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn.o_proj;True;;168.4;;bfloat16[2, 4096, 4096];64.0;2496.1;2560.1;2560.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.self_attn;False;;2977.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;2560.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.post_attention_layernorm;True;;746.9;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.gate_proj;True;;189.4;;bfloat16[2, 4096, 4096];64.0;256.1;480.1;480.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.act_fn;True;;267.5;;bfloat16[2, 4096, 14336];224.0;480.1;704.1;704.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.up_proj;True;;135.3;;bfloat16[2, 4096, 4096];64.0;480.1;704.1;704.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp.down_proj;True;;139.2;;bfloat16[2, 4096, 14336];224.0;480.1;544.1;544.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0.mlp;False;;1332.0;;bfloat16[2, 4096, 4096];64.0;256.1;544.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.0;False;;6459.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;128.1;544.1;192.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.input_layernorm;True;;290.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.q_proj;True;;135.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.k_proj;True;;125.3;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.v_proj;True;;129.2;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.rotary_emb;True;;123.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn.o_proj;True;;163.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.self_attn;False;;1939.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.post_attention_layernorm;True;;228.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.gate_proj;True;;141.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.act_fn;True;;114.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.up_proj;True;;120.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp.down_proj;True;;129.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1.mlp;False;;1147.0;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.1;False;;4336.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.input_layernorm;True;;247.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.q_proj;True;;159.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.k_proj;True;;187.3;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.v_proj;True;;193.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.rotary_emb;True;;121.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn.o_proj;True;;168.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.self_attn;False;;2892.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.post_attention_layernorm;True;;342.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.gate_proj;True;;545.0;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.act_fn;True;;133.8;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.up_proj;True;;149.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp.down_proj;True;;176.6;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2.mlp;False;;1543.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.2;False;;5589.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.input_layernorm;True;;855.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.q_proj;True;;141.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.k_proj;True;;117.1;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.v_proj;True;;107.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.rotary_emb;True;;113.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn.o_proj;True;;301.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.self_attn;False;;1840.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.post_attention_layernorm;True;;201.3;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.gate_proj;True;;142.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.act_fn;True;;104.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.up_proj;True;;114.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp.down_proj;True;;116.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3.mlp;False;;1098.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.3;False;;4611.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.input_layernorm;True;;191.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.q_proj;True;;143.1;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.k_proj;True;;269.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.v_proj;True;;107.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.rotary_emb;True;;100.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn.o_proj;True;;150.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.self_attn;False;;1755.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.post_attention_layernorm;True;;201.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.gate_proj;True;;147.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.act_fn;True;;118.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.up_proj;True;;113.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp.down_proj;True;;114.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4.mlp;False;;1118.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.4;False;;3761.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.input_layernorm;True;;201.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.q_proj;True;;116.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.k_proj;True;;108.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.v_proj;True;;106.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.rotary_emb;True;;139.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn.o_proj;True;;145.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.self_attn;False;;1749.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.post_attention_layernorm;True;;199.6;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.gate_proj;True;;118.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.act_fn;True;;105.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.up_proj;True;;270.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp.down_proj;True;;114.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5.mlp;False;;1079.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.5;False;;3877.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.input_layernorm;True;;232.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.q_proj;True;;128.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.k_proj;True;;114.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.v_proj;True;;107.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.rotary_emb;True;;103.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn.o_proj;True;;136.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.self_attn;False;;1700.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.post_attention_layernorm;True;;216.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.gate_proj;True;;121.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.act_fn;True;;103.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.up_proj;True;;111.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp.down_proj;True;;130.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6.mlp;False;;939.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.6;False;;3776.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.input_layernorm;True;;206.3;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.q_proj;True;;118.6;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.k_proj;True;;108.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.v_proj;True;;107.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.rotary_emb;True;;284.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn.o_proj;True;;155.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.self_attn;False;;1760.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.post_attention_layernorm;True;;210.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.gate_proj;True;;122.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.act_fn;True;;108.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.up_proj;True;;114.2;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp.down_proj;True;;114.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7.mlp;False;;1073.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.7;False;;3765.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.input_layernorm;True;;195.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.q_proj;True;;278.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.k_proj;True;;110.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.v_proj;True;;121.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.rotary_emb;True;;100.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn.o_proj;True;;146.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.self_attn;False;;1759.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.post_attention_layernorm;True;;353.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.gate_proj;True;;118.0;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.act_fn;True;;103.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.up_proj;True;;123.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp.down_proj;True;;117.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8.mlp;False;;932.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.8;False;;3901.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.input_layernorm;True;;196.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.q_proj;True;;115.9;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.k_proj;True;;109.2;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.v_proj;True;;106.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.rotary_emb;True;;102.2;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn.o_proj;True;;151.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.self_attn;False;;1684.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.post_attention_layernorm;True;;201.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.gate_proj;True;;122.4;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.act_fn;True;;250.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.up_proj;True;;124.1;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp.down_proj;True;;124.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9.mlp;False;;1098.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.9;False;;3666.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.input_layernorm;True;;222.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.q_proj;True;;129.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.k_proj;True;;127.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.v_proj;True;;113.4;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.rotary_emb;True;;119.1;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn.o_proj;True;;147.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.self_attn;False;;1791.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.post_attention_layernorm;True;;216.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.gate_proj;True;;122.4;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.act_fn;True;;103.5;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.up_proj;True;;112.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp.down_proj;True;;257.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10.mlp;False;;1064.3;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.10;False;;3985.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.input_layernorm;True;;197.8;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.q_proj;True;;143.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.k_proj;True;;121.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.v_proj;True;;118.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.rotary_emb;True;;109.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn.o_proj;True;;133.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.self_attn;False;;1775.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.post_attention_layernorm;True;;198.8;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.gate_proj;True;;317.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.act_fn;True;;109.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.up_proj;True;;114.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp.down_proj;True;;111.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11.mlp;False;;1127.3;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.11;False;;3831.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.input_layernorm;True;;362.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.q_proj;True;;122.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.k_proj;True;;107.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.v_proj;True;;121.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.rotary_emb;True;;102.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn.o_proj;True;;315.7;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.self_attn;False;;1763.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.post_attention_layernorm;True;;202.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.gate_proj;True;;119.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.act_fn;True;;105.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.up_proj;True;;114.2;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp.down_proj;True;;118.9;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12.mlp;False;;1071.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.12;False;;3928.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.input_layernorm;True;;202.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.q_proj;True;;124.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.k_proj;True;;285.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.v_proj;True;;111.4;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.rotary_emb;True;;102.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn.o_proj;True;;147.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.self_attn;False;;1690.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.post_attention_layernorm;True;;193.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.gate_proj;True;;135.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.act_fn;True;;106.9;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.up_proj;True;;117.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp.down_proj;True;;113.4;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13.mlp;False;;1115.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.13;False;;3743.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.input_layernorm;True;;199.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.q_proj;True;;120.8;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.k_proj;True;;108.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.v_proj;True;;107.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.rotary_emb;True;;119.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn.o_proj;True;;146.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.self_attn;False;;1702.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.post_attention_layernorm;True;;183.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.gate_proj;True;;117.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.act_fn;True;;106.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.up_proj;True;;260.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp.down_proj;True;;124.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14.mlp;False;;1099.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.14;False;;3856.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.input_layernorm;True;;183.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.q_proj;True;;127.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.k_proj;True;;114.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.v_proj;True;;107.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.rotary_emb;True;;127.9;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn.o_proj;True;;151.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.self_attn;False;;1702.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.post_attention_layernorm;True;;195.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.gate_proj;True;;123.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.act_fn;True;;106.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.up_proj;True;;115.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp.down_proj;True;;116.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15.mlp;False;;936.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.15;False;;3687.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.input_layernorm;True;;190.0;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.q_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.k_proj;True;;110.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.v_proj;True;;111.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.rotary_emb;True;;289.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn.o_proj;True;;135.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.self_attn;False;;1705.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.post_attention_layernorm;True;;181.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.gate_proj;True;;114.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.act_fn;True;;102.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.up_proj;True;;119.5;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp.down_proj;True;;110.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16.mlp;False;;1062.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.16;False;;3643.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.input_layernorm;True;;188.3;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.q_proj;True;;280.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.k_proj;True;;110.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.v_proj;True;;107.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.rotary_emb;True;;110.9;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn.o_proj;True;;146.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.self_attn;False;;1682.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.post_attention_layernorm;True;;388.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.gate_proj;True;;128.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.act_fn;True;;273.6;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.up_proj;True;;152.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp.down_proj;True;;131.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17.mlp;False;;1213.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.17;False;;4201.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.input_layernorm;True;;286.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.q_proj;True;;139.1;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.k_proj;True;;118.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.v_proj;True;;116.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.rotary_emb;True;;104.5;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn.o_proj;True;;149.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.self_attn;False;;1756.8;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.post_attention_layernorm;True;;190.8;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.gate_proj;True;;118.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.act_fn;True;;323.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.up_proj;True;;125.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp.down_proj;True;;113.3;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18.mlp;False;;1174.4;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.18;False;;3959.6;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.input_layernorm;True;;193.0;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.q_proj;True;;758.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.k_proj;True;;114.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.v_proj;True;;110.6;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.rotary_emb;True;;103.0;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn.o_proj;True;;146.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.self_attn;False;;2513.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.post_attention_layernorm;True;;201.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.gate_proj;True;;134.6;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.act_fn;True;;105.8;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.up_proj;True;;117.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp.down_proj;True;;276.7;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19.mlp;False;;1142.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.19;False;;4555.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.input_layernorm;True;;199.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.q_proj;True;;118.3;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.k_proj;True;;116.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.v_proj;True;;115.9;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.rotary_emb;True;;102.0;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn.o_proj;True;;272.4;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.self_attn;False;;745463.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.post_attention_layernorm;True;;2680.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.gate_proj;True;;352.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.act_fn;True;;126.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.up_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp.down_proj;True;;124.3;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20.mlp;False;;1250.9;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.20;False;;750152.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.input_layernorm;True;;2330.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.q_proj;True;;4847.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.k_proj;True;;7406.5;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.v_proj;True;;3620.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.rotary_emb;True;;188.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn.o_proj;True;;593.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.self_attn;False;;821322.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.post_attention_layernorm;True;;2357.6;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.gate_proj;True;;186.9;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.act_fn;True;;166.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.up_proj;True;;259.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp.down_proj;True;;138.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21.mlp;False;;1675.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.21;False;;828301.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.input_layernorm;True;;1866.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.q_proj;True;;4823.5;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.k_proj;True;;7366.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.v_proj;True;;3651.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.rotary_emb;True;;149.2;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn.o_proj;True;;267.2;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.self_attn;False;;821560.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.post_attention_layernorm;True;;2673.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.gate_proj;True;;139.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.act_fn;True;;120.9;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.up_proj;True;;118.8;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp.down_proj;True;;120.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22.mlp;False;;1169.7;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.22;False;;827862.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.input_layernorm;True;;2298.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.q_proj;True;;4416.8;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.k_proj;True;;7390.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.v_proj;True;;3673.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.rotary_emb;True;;138.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn.o_proj;True;;282.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.self_attn;False;;820502.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.post_attention_layernorm;True;;2693.0;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.gate_proj;True;;130.2;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.act_fn;True;;132.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.up_proj;True;;280.5;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp.down_proj;True;;117.5;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23.mlp;False;;1177.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.23;False;;827607.0;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.input_layernorm;True;;2441.5;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.q_proj;True;;4791.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.k_proj;True;;7093.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.v_proj;True;;3680.0;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.rotary_emb;True;;111.8;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn.o_proj;True;;275.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.self_attn;False;;821008.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.post_attention_layernorm;True;;2748.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.gate_proj;True;;290.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.act_fn;True;;171.0;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.up_proj;True;;133.9;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp.down_proj;True;;126.1;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24.mlp;False;;1358.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.24;False;;828507.4;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.input_layernorm;True;;1768.1;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.q_proj;True;;4690.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.k_proj;True;;7408.8;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.v_proj;True;;3647.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.rotary_emb;True;;290.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn.o_proj;True;;270.0;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.self_attn;False;;821681.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.post_attention_layernorm;True;;2709.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.gate_proj;True;;137.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.act_fn;True;;136.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.up_proj;True;;120.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp.down_proj;True;;115.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25.mlp;False;;1224.4;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.25;False;;827969.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.input_layernorm;True;;2366.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.q_proj;True;;4847.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.k_proj;True;;7357.7;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.v_proj;True;;3676.2;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.rotary_emb;True;;112.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn.o_proj;True;;263.3;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.self_attn;False;;823341.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.post_attention_layernorm;True;;2711.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.gate_proj;True;;130.3;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.act_fn;True;;113.3;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.up_proj;True;;118.4;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp.down_proj;True;;120.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26.mlp;False;;983.8;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.26;False;;830203.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.input_layernorm;True;;2320.6;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.q_proj;True;;4621.9;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.k_proj;True;;7428.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.v_proj;True;;3607.8;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.rotary_emb;True;;116.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn.o_proj;True;;258.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.self_attn;False;;821668.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.post_attention_layernorm;True;;2709.5;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.gate_proj;True;;152.5;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.act_fn;True;;303.4;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.up_proj;True;;125.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp.down_proj;True;;120.6;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27.mlp;False;;1296.1;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.27;False;;828694.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.input_layernorm;True;;2255.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.q_proj;True;;4698.4;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.k_proj;True;;7383.1;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.v_proj;True;;3681.5;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.rotary_emb;True;;165.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn.o_proj;True;;320.6;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.self_attn;False;;820609.7;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.post_attention_layernorm;True;;2272.2;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.gate_proj;True;;155.1;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.act_fn;True;;125.0;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.up_proj;True;;123.6;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp.down_proj;True;;462.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28.mlp;False;;1465.6;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.28;False;;827377.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.input_layernorm;True;;2059.4;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.q_proj;True;;4847.0;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.k_proj;True;;7392.0;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.v_proj;True;;3422.3;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.rotary_emb;True;;127.3;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn.o_proj;True;;255.8;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.self_attn;False;;820669.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.post_attention_layernorm;True;;2712.9;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.gate_proj;True;;324.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.act_fn;True;;112.2;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.up_proj;True;;112.3;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp.down_proj;True;;113.8;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29.mlp;False;;1142.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.29;False;;827138.1;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.input_layernorm;True;;2453.7;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.q_proj;True;;4843.2;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.k_proj;True;;7377.6;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.v_proj;True;;3694.1;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.rotary_emb;True;;141.4;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn.o_proj;True;;561.5;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.self_attn;False;;822634.9;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.post_attention_layernorm;True;;2594.1;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.gate_proj;True;;194.7;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.act_fn;True;;147.1;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.up_proj;True;;213.0;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp.down_proj;True;;159.2;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30.mlp;False;;1584.5;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.30;False;;829909.2;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.input_layernorm;True;;1737.7;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.q_proj;True;;4837.7;;bfloat16[2, 4096, 4096];64.0;256.1;320.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.k_proj;True;;7332.9;;bfloat16[2, 4096, 4096];64.0;320.1;336.1;336.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.v_proj;True;;3682.7;;bfloat16[2, 4096, 4096];64.0;336.1;352.1;352.1;bfloat16[2, 4096, 1024];16.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.rotary_emb;True;;117.6;;bfloat16[2, 8, 4096, 128];16.0;352.1;352.1;352.1;bfloat16[4096, 128]bfloat16[4096, 128];2.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn.o_proj;True;;306.1;;bfloat16[2, 4096, 4096];64.0;2560.1;2624.1;2624.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.self_attn;False;;822358.3;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;256.1;2624.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.post_attention_layernorm;True;;2644.4;;bfloat16[2, 4096, 4096];64.0;256.1;512.1;320.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.gate_proj;True;;162.8;;bfloat16[2, 4096, 4096];64.0;320.1;544.1;544.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.act_fn;True;;125.7;;bfloat16[2, 4096, 14336];224.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.up_proj;True;;123.7;;bfloat16[2, 4096, 4096];64.0;544.1;768.1;768.1;bfloat16[2, 4096, 14336];224.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp.down_proj;True;;174.0;;bfloat16[2, 4096, 14336];224.0;544.1;608.1;608.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31.mlp;False;;1560.2;;bfloat16[2, 4096, 4096];64.0;320.1;608.1;384.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.layers.31;False;;828913.5;;bfloat16[2, 4096, 4096]bfloat16[2, 1, 4096, 4096]int64[1, 4096];128.0;192.1;608.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model.norm;True;;2098.2;;bfloat16[2, 4096, 4096];64.0;192.1;448.1;256.1;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "model;False;;9953890.9;;int64[2, 4096]float32[2, 4096];0.1;0.0;448.1;64.0;bfloat16[2, 4096, 4096];64.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "lm_head;True;;620.5;;bfloat16[2, 4096, 4096];64.0;64.0;564.0;564.0;bfloat16[2, 4096, 32000];500.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n",
      "<model>;False;;9975285.6;;int64[2, 4096]float32[2, 4096];0.1;0.0;1564.0;1000.0;float32[2, 4096, 32000];1000.0;;0.0;;;-15332.7;-15332.7;-15332.7;0.0;;0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1588: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_benchmark.trace_prefill(2, 4096)\n",
    "finally:\n",
    "    del model_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6c2ca2-2608-4c5e-b47c-ac5020cb72e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:27:10.396037Z",
     "iopub.status.busy": "2023-12-02T02:27:10.394176Z",
     "iopub.status.idle": "2023-12-02T02:27:10.425785Z",
     "shell.execute_reply": "2023-12-02T02:27:10.425269Z",
     "shell.execute_reply.started": "2023-12-02T02:27:10.395966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped memory snapshot to file: memory_snapshot_20231202_022710.pickle\n"
     ]
    }
   ],
   "source": [
    "dump_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d09ad0-f801-4e1b-b2f1-89896d9c5b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T02:27:41.071205Z",
     "iopub.status.busy": "2023-12-02T02:27:41.070266Z",
     "iopub.status.idle": "2023-12-02T02:27:41.081821Z",
     "shell.execute_reply": "2023-12-02T02:27:41.081050Z",
     "shell.execute_reply.started": "2023-12-02T02:27:41.071169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Call dump_memory_snapshot(), <a href='https://pytorch.org/memory_viz' target='_blank'>click here to open Pytorch memory viz</a>, then drag and drop the snapshot file"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9c0212-735d-4fe8-8757-c75c22fdf5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T00:25:29.632043Z",
     "iopub.status.busy": "2023-12-02T00:25:29.631533Z",
     "iopub.status.idle": "2023-12-02T00:25:30.156694Z",
     "shell.execute_reply": "2023-12-02T00:25:30.156119Z",
     "shell.execute_reply.started": "2023-12-02T00:25:29.632027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPyExperimentsPytorch: Finishing\n",
      "\n",
      "*** Experiment finished in 00:24:30 (elapsed wallclock time)\n",
      "\n",
      "*** Newly defined local variables:\n",
      "Deleted: model_id\n",
      "\n",
      "*** Circular ref objects gc collected during the experiment:\n",
      "cleared 1049 objects (only temporary leakage)\n",
      "\n",
      "*** Experiment memory:\n",
      "RAM: Consumed       Reclaimed\n",
      "CPU:    2,002        0 MB (  0.00%)\n",
      "GPU:     -162        0 MB ( -0.00%)\n",
      "\n",
      "*** Current state:\n",
      "RAM:    Used    Free   Total       Util\n",
      "CPU:   2,571  27,941  31,785 MB   8.09% \n",
      "GPU:     910  23,653  24,564 MB   3.71% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc22d5-b937-4a3f-b940-a24db29be5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
