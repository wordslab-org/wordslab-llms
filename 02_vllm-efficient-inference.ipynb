{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d1ebcd-3a20-4e59-9779-cf97a3cbb826",
   "metadata": {},
   "source": [
    "# vLLM efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de3b0d5-2eb0-4c86-8c46-8cdda39d8fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:50:00.377376Z",
     "iopub.status.busy": "2025-11-11T08:50:00.377227Z",
     "iopub.status.idle": "2025-11-11T08:50:00.380157Z",
     "shell.execute_reply": "2025-11-11T08:50:00.379635Z",
     "shell.execute_reply.started": "2025-11-11T08:50:00.377366Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29e1ae1-625f-4bb7-aee7-1c0b53d895ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:50:08.131225Z",
     "iopub.status.busy": "2025-11-11T08:50:08.131065Z",
     "iopub.status.idle": "2025-11-11T08:50:08.150038Z",
     "shell.execute_reply": "2025-11-11T08:50:08.149671Z",
     "shell.execute_reply.started": "2025-11-11T08:50:08.131215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version('vllm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcf936-d728-4b8d-87bc-2f8e86384157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:51:48.834121Z",
     "iopub.status.busy": "2025-11-11T08:51:48.833842Z",
     "iopub.status.idle": "2025-11-11T08:51:48.837642Z",
     "shell.execute_reply": "2025-11-11T08:51:48.836643Z",
     "shell.execute_reply.started": "2025-11-11T08:51:48.834102Z"
    }
   },
   "source": [
    "## vLLM 0.11.0 documentation\n",
    "\n",
    "https://docs.vllm.ai/en/stable\n",
    "\n",
    "### Python API\n",
    "\n",
    "Quick start\n",
    "\n",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/#offline-batched-inference\n",
    "\n",
    "Examples\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/\n",
    "\n",
    "User guide\n",
    "\n",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/generative_models/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/pooling_models/\n",
    "\n",
    "API reference\n",
    "\n",
    "https://docs.vllm.ai/en/latest/api/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/api/vllm/#vllm.LLM\n",
    "\n",
    "Config arguments you can pass\n",
    "\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ModelConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.CacheConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.LoadConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ParallelConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.SchedulerConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.DeviceConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.SpeculativeConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.LoRAConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.MultiModalConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.PoolerConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.StructuredOutputsConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.ObservabilityConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.KVTransferConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.CompilationConfig\n",
    "- https://docs.vllm.ai/en/latest/api/vllm/config/#vllm.config.VllmConfig\n",
    "\n",
    "Supported models\n",
    "\n",
    "https://docs.vllm.ai/en/latest/models/supported_models/\n",
    "\n",
    "https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/models\n",
    "\n",
    "### OpenAI-Compatible RESTful API server\n",
    "\n",
    "Quick start\n",
    "\n",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/#openai-compatible-server\n",
    "\n",
    "Examples\n",
    "\n",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/\n",
    "\n",
    "User guide\n",
    "\n",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/\n",
    "\n",
    "Configuration\n",
    "\n",
    "https://docs.vllm.ai/en/latest/configuration/\n",
    "\n",
    "Syntax reference\n",
    "\n",
    "https://docs.vllm.ai/en/latest/cli/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/cli/serve/\n",
    "\n",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107979b6-a56e-4dcf-abdb-c8f8fdf7889d",
   "metadata": {},
   "source": [
    "## Streaming a response in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e439988c-3c0d-451d-b855-2f935015624b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:38:52.844766Z",
     "iopub.status.busy": "2025-11-17T22:38:52.844594Z",
     "iopub.status.idle": "2025-11-17T22:38:56.113445Z",
     "shell.execute_reply": "2025-11-17T22:38:56.113006Z",
     "shell.execute_reply.started": "2025-11-17T22:38:52.844756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 23:38:55 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import RequestOutputKind\n",
    "from vllm.v1.engine.async_llm import AsyncLLM\n",
    " \n",
    "def start_vllm_engine(model: str, **kwargs):\n",
    "    engine_args = AsyncEngineArgs(\n",
    "        model=model,\n",
    "        enforce_eager=True,  # Faster startup for examples\n",
    "        **kwargs\n",
    "    )\n",
    "    engine = AsyncLLM.from_engine_args(engine_args)\n",
    "    return engine\n",
    "\n",
    "def stop_vllm_engine(engine: AsyncLLM):\n",
    "    engine.shutdown()\n",
    "\n",
    "async def stream_vllm_response(engine: AsyncLLM, prompt: str, request_id = \"default\") -> None:\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=4096,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        seed=42,  # For reproducible results\n",
    "        output_kind=RequestOutputKind.DELTA,  # Get only new tokens each iteration\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Stream tokens from AsyncLLM\n",
    "        async for output in engine.generate(\n",
    "            request_id=request_id, prompt=prompt, sampling_params=sampling_params\n",
    "        ):            \n",
    "            # Process each completion in the output\n",
    "            for completion in output.outputs:\n",
    "                # In DELTA mode, we get only new tokens generated since last iteration\n",
    "                new_text = completion.text\n",
    "                if new_text:\n",
    "                    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "            # Check if generation is finished\n",
    "            if output.finished:\n",
    "                print(\"\\n✅ Generation complete!\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during streaming: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4513fc7-6acc-4ab4-97c7-98dd4a2df898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:38:56.117997Z",
     "iopub.status.busy": "2025-11-17T22:38:56.117905Z",
     "iopub.status.idle": "2025-11-17T22:39:08.377201Z",
     "shell.execute_reply": "2025-11-17T22:39:08.376605Z",
     "shell.execute_reply.started": "2025-11-17T22:38:56.117989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 23:38:56 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 23:38:56 [model.py:1510] Using max model len 32768\n",
      "INFO 11-17 23:38:57 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 11-17 23:38:57 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:58 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:58 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-4B-Thinking-2507-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Thinking-2507-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-4B-Thinking-2507-FP8, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:38:58 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:59 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:38:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:59 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-4B-Thinking-2507-FP8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:59 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:38:59 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:00 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:00 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d0031aad2046409425c60500099e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:05 [default_loader.py:267] Loading weights took 4.75 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:05 [gpu_model_runner.py:2653] Model loading took 4.2299 GiB and 5.521851 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:39:05 [fp8_utils.py:576] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/python/archive-v0/anz9liZrnnyqrcULZj6es/vllm/model_executor/layers/quantization/utils/configs/N=6144,K=2560,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:39:06 [fp8_utils.py:576] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/python/archive-v0/anz9liZrnnyqrcULZj6es/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=4096,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:39:06 [fp8_utils.py:576] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/python/archive-v0/anz9liZrnnyqrcULZj6es/vllm/model_executor/layers/quantization/utils/configs/N=19456,K=2560,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:39:06 [fp8_utils.py:576] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/python/archive-v0/anz9liZrnnyqrcULZj6es/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=9728,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:07 [gpu_worker.py:298] Available KV cache memory: 16.62 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:07 [kv_cache_utils.py:1087] GPU KV cache size: 121,040 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 3.69x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m WARNING 11-17 23:39:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:07 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.18 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11386)\u001b[0;0m INFO 11-17 23:39:08 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 11-17 23:39:08 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 7565\n"
     ]
    }
   ],
   "source": [
    "engine = start_vllm_engine(model=\"Qwen/Qwen3-4B-Thinking-2507-FP8\", max_model_len=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb70542-18a2-4e95-816d-4f88263f3d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:40:08.982266Z",
     "iopub.status.busy": "2025-11-17T22:40:08.982043Z",
     "iopub.status.idle": "2025-11-17T22:40:18.296365Z",
     "shell.execute_reply": "2025-11-17T22:40:18.295981Z",
     "shell.execute_reply.started": "2025-11-17T22:40:08.982249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In your explanation, include at most two sentences about the relationship between attention and language processing.\n",
      "\n",
      "Okay, the user wants me to explain how transformers use attention for language processing, with a specific constraint: I can only include two sentences about the relationship between attention and language processing. \n",
      "\n",
      "Hmm, this seems like someone studying NLP or machine learning who needs a concise yet precise explanation. They're probably preparing for an exam or writing a report where brevity matters. I should avoid jargon overload while staying technically accurate.\n",
      "\n",
      "First, I recall that transformers' core innovation is self-attention. Each token gets a vector that weighs all other tokens' relevance through attention scores. The key is that this allows modeling long-range dependencies without RNNs' sequential limitations. \n",
      "\n",
      "For the two-sentence requirement, I'll focus on: (1) how attention computes weighted relationships between tokens, and (2) why this matters for language understanding. Must be tight - no extra fluff. \n",
      "\n",
      "*checks draft* First sentence covers mechanism: \"Transformers process language using self-attention mechanisms...\" Good. Second sentence links to language processing: \"...enabling the model to weigh the relevance of different words in context...\" Yes, that directly answers the relationship part without drifting into architecture details. \n",
      "\n",
      "User didn't specify depth, so I'll skip explaining query/key/value or positional encoding since they only want two sentences on attention-language ties. Smart constraint - they're testing if I can prioritize exactly what was asked. \n",
      "\n",
      "*double-checks* Yep, both sentences explicitly connect attention to language processing as requested. No markdown, clean and academic tone. Done.\n",
      "</thinking>\n",
      "Transformers process language using self-attention mechanisms that compute weighted relationships between every pair of tokens in a sequence, allowing the model to understand context by emphasizing relevant parts of the input. This attention mechanism enables the model to weigh the relevance of different words in context, which is crucial for accurately capturing dependencies and semantic meaning in language processing.\n",
      "✅ Generation complete!\n"
     ]
    }
   ],
   "source": [
    "await stream_vllm_response(engine, \"Explain how transformers use attention to process language.\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea893dac-0a58-422c-9793-8d1492d9fc99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:40:31.810910Z",
     "iopub.status.busy": "2025-11-17T22:40:31.810753Z",
     "iopub.status.idle": "2025-11-17T22:40:32.042239Z",
     "shell.execute_reply": "2025-11-17T22:40:32.041683Z",
     "shell.execute_reply.started": "2025-11-17T22:40:31.810901Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_vllm_engine(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9fe5-3201-4430-953c-7d13f8fc8c40",
   "metadata": {},
   "source": [
    "## Efficient batch inference - compute model perplexity on 4 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df62e2-ccfb-4d74-b893-a21d6bb0d7e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:46:50.041726Z",
     "iopub.status.busy": "2025-11-17T22:46:50.041330Z",
     "iopub.status.idle": "2025-11-17T22:46:50.045935Z",
     "shell.execute_reply": "2025-11-17T22:46:50.045054Z",
     "shell.execute_reply.started": "2025-11-17T22:46:50.041698Z"
    }
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d90661-9a3a-43bf-868e-92c3f17b3717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:48:33.551397Z",
     "iopub.status.busy": "2025-11-17T22:48:33.551067Z",
     "iopub.status.idle": "2025-11-17T22:48:33.683363Z",
     "shell.execute_reply": "2025-11-17T22:48:33.681584Z",
     "shell.execute_reply.started": "2025-11-17T22:48:33.551384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/jupyterlab/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m280 packages\u001b[0m \u001b[2min 0.59ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m180 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a314a42e-d1d7-46a4-baa0-a7e991eedffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:26.947301Z",
     "iopub.status.busy": "2025-11-17T23:20:26.947144Z",
     "iopub.status.idle": "2025-11-17T23:20:29.657926Z",
     "shell.execute_reply": "2025-11-17T23:20:29.657567Z",
     "shell.execute_reply.started": "2025-11-17T23:20:26.947290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10a8734e6814e999295ae1d14d4c71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60e1a2ff72f452abacf210a81679246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b518891efd8417cadc4f1365c40eccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2499df784f4d3ab09d8c759236f2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56420bce0faf4bbba7305a58e662829c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a14f73622c4023a311142a462cc079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train+valid+test\"\n",
    "\n",
    "dataset_en_name = \"frenchtext/bank-en-2401\"\n",
    "dataset_en = load_dataset(dataset_en_name , split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c60551-4017-408b-8bfc-78ceb412e01d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:29.658543Z",
     "iopub.status.busy": "2025-11-17T23:20:29.658385Z",
     "iopub.status.idle": "2025-11-17T23:20:30.713235Z",
     "shell.execute_reply": "2025-11-17T23:20:30.712909Z",
     "shell.execute_reply.started": "2025-11-17T23:20:29.658533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8962c8a1a5b4fb592835b818eac130b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd81b2d02f4a2b86c118ee92b11aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1705a1fc16ea44678eea1b57c0f13c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf087713ef54627b404b9e9d0358766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc6d3c6992043b4ac955ba00555833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106b5eff7b4f4fa08b741825b84274cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_fr_name = \"frenchtext/banque-fr-2311\"\n",
    "dataset_fr = load_dataset(dataset_fr_name, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfa1148-a41c-4d84-930d-9e2c697fc474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:30.713714Z",
     "iopub.status.busy": "2025-11-17T23:20:30.713615Z",
     "iopub.status.idle": "2025-11-17T23:20:31.692062Z",
     "shell.execute_reply": "2025-11-17T23:20:31.691699Z",
     "shell.execute_reply.started": "2025-11-17T23:20:30.713705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83bdb7f36e940d8a2d4c9788e00ff8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13db3aa593914c2d8bed51a04743d709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c5919c93f248169a66063e6679cacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb197a7ee90d45d89af5f77490e14b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b19d33a237040a2819226bb7ba4cb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5473e689ffc4cfaa3eb1d5053efaf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_de_name = \"frenchtext/bank-de-2401\"\n",
    "dataset_de = load_dataset(dataset_de_name, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c808c0d-c4b7-4eba-ae05-a16c6dd3bc3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:31.692897Z",
     "iopub.status.busy": "2025-11-17T23:20:31.692796Z",
     "iopub.status.idle": "2025-11-17T23:20:32.726600Z",
     "shell.execute_reply": "2025-11-17T23:20:32.726194Z",
     "shell.execute_reply.started": "2025-11-17T23:20:31.692888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f453d9d857d44a08d7279a651692fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb2444c22a43bfb56ebf07e337718b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243668bc48584858a135127eb7575d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba2b9b6490c4473acacd733a8497488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16a8d0d8c764130b6563c970cde4ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746cc3b35ea7476bbf95d71198cae4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_es_name = \"frenchtext/bank-es-2401\"\n",
    "dataset_es = load_dataset(dataset_es_name, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903c0bf-b234-4405-a9d1-260bd31db05f",
   "metadata": {},
   "source": [
    "### Batching and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350bfac6-1d71-4213-8b3d-47c2e71d5241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.727188Z",
     "iopub.status.busy": "2025-11-17T23:20:32.727080Z",
     "iopub.status.idle": "2025-11-17T23:20:32.729665Z",
     "shell.execute_reply": "2025-11-17T23:20:32.729264Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.727179Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>15)\n",
    "    sorted_dataset = filtered_dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06d0179-109e-4b0c-8557-ddba5cad3cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.730198Z",
     "iopub.status.busy": "2025-11-17T23:20:32.730086Z",
     "iopub.status.idle": "2025-11-17T23:20:32.733491Z",
     "shell.execute_reply": "2025-11-17T23:20:32.733137Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.730190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encoding_offsets(encoding):\n",
    "    start_token_idx = 0\n",
    "    while encoding.special_tokens_mask[start_token_idx]==1: start_token_idx+=1\n",
    "    start_index = encoding.offsets[start_token_idx][0]\n",
    "    end_token_idx = len(encoding.offsets)-1\n",
    "    while encoding.special_tokens_mask[end_token_idx]==1: end_token_idx-=1\n",
    "    end_index = encoding.offsets[end_token_idx][1]\n",
    "    return (start_index,end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0de6a3e-5b06-415c-9895-2b1f4b035bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.733973Z",
     "iopub.status.busy": "2025-11-17T23:20:32.733849Z",
     "iopub.status.idle": "2025-11-17T23:20:32.739481Z",
     "shell.execute_reply": "2025-11-17T23:20:32.739145Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.733965Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    \n",
    "    # SPECIAL CASE: tiktoken tokenizer does not implement truncation=True, return_overflowing_tokens=True, and encodings offsets\n",
    "    # => we must implement it manually on top of Huggingface tokenizers\n",
    "    if hasattr(tokenizer,\"tokenizer\") and tokenizer.tokenizer.__class__.__module__.startswith(\"tiktoken\"):\n",
    "        encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", \n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "        \n",
    "        input_tensor = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "       \n",
    "        batch_size = input_tensor.size(0)\n",
    "        encodings_length = input_tensor.size(1)\n",
    "        texts_length = torch.tensor([len(text) for text in dataset_batch[\"Text\"]])\n",
    "        \n",
    "        max_length = tokenizer.model_max_length \n",
    "        \n",
    "        if encodings_length > max_length:\n",
    "        \n",
    "            unfolded_tensor, overflow_to_sample_mapping = truncate_tensor_with_overflow(input_tensor, padding_value=tokenizer.pad_token_id, max_length=max_length, stride=stride)\n",
    "            unfolded_mask, _ = truncate_tensor_with_overflow(attention_mask, padding_value=0, max_length=max_length, stride=stride)\n",
    "\n",
    "            encodings['input_ids'] = unfolded_tensor\n",
    "            encodings['attention_mask'] = unfolded_mask\n",
    "            encodings['overflow_to_sample_mapping'] = overflow_to_sample_mapping\n",
    "            \n",
    "            offset = max_length - stride\n",
    "            overflow_lines = 1 + math.ceil((encodings_length - max_length)/offset)\n",
    "            last_line_padding = overflow_lines*offset + stride - encodings_length\n",
    "            \n",
    "            tokens_per_sample = attention_mask.sum(1).tolist()\n",
    "            start_indexes = []\n",
    "            end_indexes = []\n",
    "            for sample_tokens in tokens_per_sample:\n",
    "                start_indexes.append(torch.clamp(torch.arange(0,overflow_lines*offset,offset), max=sample_tokens)/sample_tokens)\n",
    "                end_indexes.append(torch.clamp(torch.arange(max_length,encodings_length+last_line_padding+1,offset), max=sample_tokens)/sample_tokens)                \n",
    "            overflow_to_sample_offset = torch.stack((torch.concat(start_indexes),torch.concat(end_indexes)))\n",
    "\n",
    "            texts_length_multiplier = torch.repeat_interleave(texts_length, overflow_lines).unsqueeze(0)\n",
    "            otso = (overflow_to_sample_offset*texts_length_multiplier).int()\n",
    "            encodings['overflow_to_sample_offset'] = [(otso[0,i].item(),otso[1,i].item()) for i in range(otso.size(1))]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            encodings['overflow_to_sample_mapping'] = torch.zeros(batch_size, dtype=torch.int32)\n",
    "            encodings['overflow_to_sample_offset'] = [(0,texts_length[i].item()) for i in range(batch_size)]\n",
    "    \n",
    "    # GENERAL CASE: just rely on Huggingface tokenizers for truncation\n",
    "    else:\n",
    "        encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                          padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                          # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                          # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                          pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "        encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1fdd0e-a8d4-4367-bda8-dc133941222b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.740017Z",
     "iopub.status.busy": "2025-11-17T23:20:32.739911Z",
     "iopub.status.idle": "2025-11-17T23:20:32.743163Z",
     "shell.execute_reply": "2025-11-17T23:20:32.742791Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.740009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def truncate_tensor_with_overflow(input_tensor, padding_value, max_length=2048, stride=256):\n",
    "    batch_length = input_tensor.size(0)\n",
    "    encoding_length = input_tensor.size(1)\n",
    "\n",
    "    offset = max_length - stride\n",
    "    overflow_lines = 1 + math.ceil((encoding_length - max_length)/offset)\n",
    "    last_line_padding = overflow_lines*offset + stride - encoding_length\n",
    "\n",
    "    padded_tensor = F.pad(input_tensor, (0,last_line_padding), \"constant\", padding_value)\n",
    "    unfolded_tensor = padded_tensor.unfold(1, max_length, offset).reshape(-1, max_length)\n",
    "\n",
    "    overflow_to_sample_mapping = torch.arange(batch_length).repeat_interleave(overflow_lines)\n",
    " \n",
    "    return unfolded_tensor, overflow_to_sample_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e341e9e0-f6de-4d8e-9bd8-92fbee615353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.743585Z",
     "iopub.status.busy": "2025-11-17T23:20:32.743495Z",
     "iopub.status.idle": "2025-11-17T23:20:32.747052Z",
     "shell.execute_reply": "2025-11-17T23:20:32.746677Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.743578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = encodings['input_ids'].size(0)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9ad19-81fc-4a9c-8b2a-76baa02b4bb8",
   "metadata": {},
   "source": [
    "## Unigram-normalized perplexity\n",
    "\n",
    "https://arxiv.org/pdf/2011.13220.pdf\n",
    "\n",
    "Unigram-Normalized Perplexity as a Language Model Performance Measure with Different Vocabulary Sizes\n",
    "\n",
    "*Jihyeon Roh, Sang-Hoon Oh, Soo-Young Lee*\n",
    "\n",
    "Although Perplexity is a widely used performance metric for language models, the values are highly dependent upon the number of words in the corpus and is useful to compare performance of the same corpus only.\n",
    "\n",
    "Perplexity may not be suitable for comparing LMs using different vocabularies because a larger vocabulary size tends to result in lower word probabilities and thus a higher Perplexity.\n",
    "\n",
    "In this paper, we propose a new metric that can be used to evaluate language model performance with different vocabulary sizes. \n",
    "\n",
    "The proposed unigram-normalized Perplexity actually presents the performance improvement of the language models from that of simple unigram model, and is robust on the vocabulary size.\n",
    "\n",
    "To overcome the limitations of the perplexity, we adopt the basic idea of normalizing the word probability with respect to a quantity containing the vocabulary size. \n",
    "\n",
    "We apply a unigram probability that is calculated from the word occurrence as a normalization factor for the perplexity. The unigram probability from the unigram LM is computed as Count(vk) / Count(all words), where Count(vk) is the number of occurrences of word vk in the corpus.\n",
    "\n",
    "Our proposed metric is obtained by normalizing the perplexity with this unigram probability.\n",
    "\n",
    "The proposed “Perplexity normalized with unigram” (PPLu) is defined as\n",
    "PPLu = (Product for all words in sequence of : P(word | language model) / P(word | unigram))^1/length of sequence \n",
    "\n",
    "This metric shows the likelihood improvement of a context-dependent LM from unigram LM without the context information, and enables us to evaluate the effectiveness of an LM.\n",
    "\n",
    "PPLu contains a unigram probability term, which allows PPLu to evaluate LMs more accurately than PPL does. Specifically, even if an LM fails to capture word relationships, it may achieve a good PPL by simply assigning high probabilities to words that frequently appear (e.g., unknown tokens). This case can be corrected with our PPLu, which considers the word frequencies via unigram probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf98db-f408-44d1-ad6d-83593f6a362f",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "``` \n",
    "log(PPLu) = 1/length of sequence * Sum for all words in sequence( log(P(word | language model)) - log(P(word | unigram)))\n",
    "          = Log(PPL) - 1/length of sequence * Sum for all words in sequence( log(P(word | unigram) )\n",
    "```\n",
    "\n",
    "**Perplexity = 1 / geometric mean of model token probabilities****1 / geometric mean of model token probabilities**\n",
    "\n",
    "pt_ppl_losses = [ -ln(prob_model) ]\n",
    "\n",
    "pt_unigram_losses = [ -ln(prob_unigram) ]\n",
    "\n",
    "avg_ppl_losses = pt_ppl_losses.sum() / tokens_count \n",
    "\n",
    "avg_unigram_losses = pt_unigram_losses.sum() / tokens_count \n",
    "\n",
    "ppl = math.exp( avg_ppl_losses ) \n",
    "\n",
    "ppl_unigram = math.exp( avg_unigram_losses )\n",
    "\n",
    "**Unigram-normalized perplexity = (1 / geometric mean of model token probabilities) / (1 / geometric mean of unigram token probabilities)**\n",
    "\n",
    "pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "avg_pplu_losses = pt_pplu_losses.sum() / tokens_count\n",
    "\n",
    "pplu = math.exp( avg_pplu_losses )\n",
    "\n",
    "= math.exp( avg_ppl_losses - avg_unigram_losses ) \n",
    "\n",
    "= math.exp( avg_ppl_losses)/math.exp( avg_unigram_losses ) \n",
    "\n",
    "= ppl / ppl_unigram\n",
    "\n",
    "ppl_unigram = ppl / pplu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67009982-4016-4f00-8ce1-6f7dab141f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:21:02.449743Z",
     "iopub.status.busy": "2025-11-17T23:21:02.449570Z",
     "iopub.status.idle": "2025-11-17T23:21:02.453858Z",
     "shell.execute_reply": "2025-11-17T23:21:02.453427Z",
     "shell.execute_reply.started": "2025-11-17T23:21:02.449733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        if hasattr(tokenizer,\"vocab\"):\n",
    "            self.vocab_size = len(tokenizer.vocab)\n",
    "        else:\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Number of tokens predicted, ignoring padding tokens\n",
    "        predicted_tokens_count_r = labels_to_ignore.sum(dim=1)\n",
    "        # ... make sure we don't divide by 0 below ...\n",
    "        predicted_tokens_count = predicted_tokens_count_r.clamp(min=1)\n",
    "        \n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = (1/predicted_tokens_count)*self.perplexity_loss(logits, labels_for_crossentropy).sum(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/predicted_tokens_count)*torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return predicted_tokens_count_r, batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab3548a1-fe99-48a7-adff-1aaab73dd00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:21:03.303155Z",
     "iopub.status.busy": "2025-11-17T23:21:03.302979Z",
     "iopub.status.idle": "2025-11-17T23:21:03.306283Z",
     "shell.execute_reply": "2025-11-17T23:21:03.305826Z",
     "shell.execute_reply.started": "2025-11-17T23:21:03.303144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0fa4f-8941-42f0-b319-4ca5ba54c63d",
   "metadata": {},
   "source": [
    "### Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9969ddec-81c7-46bd-8fc5-852141b92596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.756237Z",
     "iopub.status.busy": "2025-11-17T23:20:32.756139Z",
     "iopub.status.idle": "2025-11-17T23:20:32.759740Z",
     "shell.execute_reply": "2025-11-17T23:20:32.759381Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.756228Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b7a076f-1914-4126-aef6-d0bfcff8460a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:32.760190Z",
     "iopub.status.busy": "2025-11-17T23:20:32.760089Z",
     "iopub.status.idle": "2025-11-17T23:20:35.870897Z",
     "shell.execute_reply": "2025-11-17T23:20:35.870325Z",
     "shell.execute_reply.started": "2025-11-17T23:20:32.760182Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# load the tokenizer and the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86864687-7714-4e2f-bf18-587393720254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:35.871686Z",
     "iopub.status.busy": "2025-11-17T23:20:35.871465Z",
     "iopub.status.idle": "2025-11-17T23:20:35.873675Z",
     "shell.execute_reply": "2025-11-17T23:20:35.873363Z",
     "shell.execute_reply.started": "2025-11-17T23:20:35.871676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Memory limit of RTX 4090\n",
    "\n",
    "if tokenizer.model_max_length>8192:\n",
    "    tokenizer.model_max_length = 8192\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "stride = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7c7857-792d-4b39-963a-47fbb0375799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:20:35.874112Z",
     "iopub.status.busy": "2025-11-17T23:20:35.874023Z",
     "iopub.status.idle": "2025-11-17T23:20:35.889725Z",
     "shell.execute_reply": "2025-11-17T23:20:35.889320Z",
     "shell.execute_reply.started": "2025-11-17T23:20:35.874104Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = dataset_en_name\n",
    "dataset = dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ed3ab4-3801-4a88-843f-e66b1aa5f3fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:21:09.841840Z",
     "iopub.status.busy": "2025-11-17T23:21:09.841388Z",
     "iopub.status.idle": "2025-11-17T23:22:12.557251Z",
     "shell.execute_reply": "2025-11-17T23:22:12.556561Z",
     "shell.execute_reply.started": "2025-11-17T23:21:09.841811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (944733 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 36,865,308 tokens\n",
      "... 71,881,304 tokens\n",
      "... 78,657,869 tokens\n",
      "... 82,617,954 tokens\n",
      "... 85,343,413 tokens\n",
      "... 87,286,150 tokens\n",
      "... 88,545,120 tokens\n",
      "... 89,249,148 tokens\n",
      "Done: 89,404,417 tokens\n"
     ]
    }
   ],
   "source": [
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88f2214a-1650-45db-b23b-a24e7e223abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T23:22:35.851043Z",
     "iopub.status.busy": "2025-11-17T23:22:35.850777Z",
     "iopub.status.idle": "2025-11-17T23:22:50.512126Z",
     "shell.execute_reply": "2025-11-17T23:22:50.511269Z",
     "shell.execute_reply.started": "2025-11-17T23:22:35.851028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.54 GiB. GPU 0 has a total capacity of 23.99 GiB of which 456.21 MiB is free. Process 2273 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 19.67 GiB is allocated by PyTorch, and 290.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m attention_mask = encodings_batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(model.device)\n\u001b[32m     27\u001b[39m outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=\u001b[38;5;28;01mFalse\u001b[39;00m, output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m, output_hidden_states=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = \u001b[43mpplu_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n\u001b[32m     32\u001b[39m ppl_losses.extend(batch_ppl_losses.tolist())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mPPLu.__call__\u001b[39m\u001b[34m(self, input_ids, attention_mask, output_logits)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, output_logits):\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Next-token prediction: shift prediction scores and input ids by one\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     logits = \u001b[43moutput_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     labels = input_ids[:, \u001b[32m1\u001b[39m:].contiguous()\n\u001b[32m     36\u001b[39m     labels_to_ignore = attention_mask[:, \u001b[32m1\u001b[39m:]\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 18.54 GiB. GPU 0 has a total capacity of 23.99 GiB of which 456.21 MiB is free. Process 2273 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 19.67 GiB is allocated by PyTorch, and 290.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(pred_tokens_count, ppl_losses, unigram_losses):        \n",
    "    pt_pred_tokens_count = torch.Tensor(pred_tokens_count)\n",
    "    total_pred_tokens_count = pt_pred_tokens_count.sum().item()\n",
    "    \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp((pt_ppl_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "    pplu = math.exp((pt_pplu_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "pred_tokens_count = [] \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):       \n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n",
    "        ppl_losses.extend(batch_ppl_losses.tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.tolist(), batch_pplu.tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "        display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cada7ef-e804-475a-aebd-cfecf5168711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
