{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1282ee-ce66-4665-9049-057431d59f0f",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a15aa-978c-460a-9f6c-0c401910ed74",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1721f2-2876-44e4-871e-8eb8439c68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565e32d-be17-4a3e-9b06-fcd8f9a0976a",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5dbc53-38e1-4483-aef8-25310952f772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:48.161738Z",
     "iopub.status.busy": "2024-01-06T20:56:48.161257Z",
     "iopub.status.idle": "2024-01-06T20:56:48.169661Z",
     "shell.execute_reply": "2024-01-06T20:56:48.169109Z",
     "shell.execute_reply.started": "2024-01-06T20:56:48.161706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {   \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # 15.78 GB https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # 21.00 GB https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # 15.00 GB https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # 17.07 GB https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # 17.33 GB https://huggingface.co/01-ai/Yi-34B    \n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # 22.18 GB https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f6691-1719-4cb7-99ea-f8fb7e67dd51",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae3299f-b520-4278-919b-2cdc661e6020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:49.050997Z",
     "iopub.status.busy": "2024-01-06T20:56:49.050100Z",
     "iopub.status.idle": "2024-01-06T20:56:49.425199Z",
     "shell.execute_reply": "2024-01-06T20:56:49.424695Z",
     "shell.execute_reply.started": "2024-01-06T20:56:49.050963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7714b7e-ed3d-47e9-b02e-dff03299cf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:49.426229Z",
     "iopub.status.busy": "2024-01-06T20:56:49.425983Z",
     "iopub.status.idle": "2024-01-06T20:56:59.305929Z",
     "shell.execute_reply": "2024-01-06T20:56:59.305417Z",
     "shell.execute_reply.started": "2024-01-06T20:56:49.426214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a20387f60741eda836c97ddf0afb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a054ebe9e92c4e0381545439c00be67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3ea9d2ecfa4957b8a3a49c8ff7bb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a500262d640a4f66b6826f7fc002b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfa7a05a80f418da5118942c3a13f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d33cc5a7024a85993f034b98f16015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "dataset_fr = load_dataset(dataset_name_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df161a8-456f-4eaa-b99f-13c3b90d2c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.306811Z",
     "iopub.status.busy": "2024-01-06T20:56:59.306521Z",
     "iopub.status.idle": "2024-01-06T20:56:59.309864Z",
     "shell.execute_reply": "2024-01-06T20:56:59.309489Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.306795Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 68166\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 8522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 8541\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b9ed0-02c3-43ea-8647-40db7517ff27",
   "metadata": {},
   "source": [
    "## Batching and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1603b5-31cf-4dc9-94ce-27b20091787d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.310751Z",
     "iopub.status.busy": "2024-01-06T20:56:59.310627Z",
     "iopub.status.idle": "2024-01-06T20:56:59.325537Z",
     "shell.execute_reply": "2024-01-06T20:56:59.325150Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.310743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = dataset_name_fr\n",
    "split = \"valid\"\n",
    "dataset = dataset_fr[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef174d1d-8294-485e-91fd-bd9889fbcba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.326079Z",
     "iopub.status.busy": "2024-01-06T20:56:59.325969Z",
     "iopub.status.idle": "2024-01-06T20:56:59.332783Z",
     "shell.execute_reply": "2024-01-06T20:56:59.332364Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.326071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>10)\n",
    "    sorted_dataset = dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8fd7f9-aeec-451e-80b2-3c40360203a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.333613Z",
     "iopub.status.busy": "2024-01-06T20:56:59.333377Z",
     "iopub.status.idle": "2024-01-06T20:56:59.335941Z",
     "shell.execute_reply": "2024-01-06T20:56:59.335537Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.333599Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encoding_offsets(encoding):\n",
    "    start_index = encoding.offsets[0][0]\n",
    "    end_index = encoding.offsets[-1][1]\n",
    "    if end_index==0: end_index = -1\n",
    "    return (start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f631c448-19a0-456c-abef-19aeaad60cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.336620Z",
     "iopub.status.busy": "2024-01-06T20:56:59.336393Z",
     "iopub.status.idle": "2024-01-06T20:56:59.339437Z",
     "shell.execute_reply": "2024-01-06T20:56:59.339055Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.336609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "    encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8b9cf9-24e0-4800-81cd-c010e8b7d68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.340207Z",
     "iopub.status.busy": "2024-01-06T20:56:59.339948Z",
     "iopub.status.idle": "2024-01-06T20:56:59.342647Z",
     "shell.execute_reply": "2024-01-06T20:56:59.342258Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.340195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c715b1d-5b91-4d7e-99f5-609573d93b19",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b7de50-6539-4ece-bf0d-c8569de15786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.343978Z",
     "iopub.status.busy": "2024-01-06T20:56:59.343757Z",
     "iopub.status.idle": "2024-01-06T20:56:59.346248Z",
     "shell.execute_reply": "2024-01-06T20:56:59.345814Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.343968Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6acc7c5-e652-4511-81a4-e830d2d9ed4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:56:59.346834Z",
     "iopub.status.busy": "2024-01-06T20:56:59.346719Z",
     "iopub.status.idle": "2024-01-06T20:57:05.635804Z",
     "shell.execute_reply": "2024-01-06T20:57:05.635336Z",
     "shell.execute_reply.started": "2024-01-06T20:56:59.346826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for tiiuae/falcon-7b\n",
      "- dataset examples: 8522\n",
      "- batch_size= 8, stride=256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ed52430a084df985753a928347bdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6507ef11965f480885ca1182dc84546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model torch dtype: torch.bfloat16\n",
      "- model vocabulary: 65024\n",
      "- model sequence length: 2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "batch_size = 8\n",
    "stride = 256\n",
    "\n",
    "model_id = list(models)[6]\n",
    "model_name = models[model_id]\n",
    "print(f\"Computing perplexity on dataset {dataset_name}:{split} for {model_name}\")\n",
    "print(f\"- dataset examples: {len(dataset)}\")\n",
    "print(f\"- batch_size= {batch_size}, stride={stride}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)#, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")#, trust_remote_code=True, token=myhftoken) \n",
    "print(f\"- model torch dtype: {model.dtype}\")\n",
    "print(f\"- model vocabulary: {len(tokenizer.vocab)}\")\n",
    "print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8db231-5141-42b9-9ece-0605d3c49cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T16:30:08.958039Z",
     "iopub.status.busy": "2024-01-06T16:30:08.957126Z",
     "iopub.status.idle": "2024-01-06T16:30:08.962505Z",
     "shell.execute_reply": "2024-01-06T16:30:08.961520Z",
     "shell.execute_reply.started": "2024-01-06T16:30:08.958005Z"
    },
    "tags": []
   },
   "source": [
    "## Compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042d937f-b4ed-4699-8279-eabf72836579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T16:20:50.657651Z",
     "iopub.status.busy": "2024-01-06T16:20:50.657427Z",
     "iopub.status.idle": "2024-01-06T16:20:50.659972Z",
     "shell.execute_reply": "2024-01-06T16:20:50.659599Z",
     "shell.execute_reply.started": "2024-01-06T16:20:50.657641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_perplexity.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, perplexity, uri, span):\n",
    "        self.file.write(f\"{perplexity},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef201e01-e6ed-4a05-8754-804849a1cbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = PerplexityLogger(dataset_name, split, model_name)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "losses = []    \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        # compute perplexity\n",
    "        # we are doing next-token prediction; shift prediction µscores and input ids by one\n",
    "        shift_logits = outputs.logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = inputs[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "        # CrossEntropyLoss: ignore_index=-100\n",
    "        labels = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_losses = loss_fct(shift_logits, labels).mean(1)\n",
    "        losses.extend(batch_losses)\n",
    "        batch_perplexities = torch.exp(batch_losses).tolist()\n",
    "\n",
    "    for perplexity,uri,span in zip(batch_perplexities, encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(perplexity, uri, span)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        perplexity = torch.exp(torch.stack(losses).mean().float()).item()\n",
    "        print(f\"{(idx+1)*batch_size} encodings processed -> perplexity = {perplexity}\")\n",
    "\n",
    "perplexity = torch.exp(torch.stack(losses).mean().float()).item()\n",
    "print(f\"-> perplexity = {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d26af8-8e20-4f02-a182-e7ad75b9f6ab",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311 for togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
    "- dataset examples: 68166\n",
    "- batch_size= 16, stride=256\n",
    "- model torch dtype: torch.float16\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- perplexity = 5.301388263702393 (train)\n",
    "- perplexity = 5.480365753173828 (valid) [+3,4%]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91025c3f-96a1-4b07-8c86-1350a178eb51",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311 for openlm-research/open_llama_3b_v2\n",
    "- dataset examples: 68166\n",
    "- batch_size= 12, stride=256\n",
    "- model torch dtype: torch.float16\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- perplexity = 4.064583778381348 (train)\n",
    "- perplexity = 3.9680004119873047 (valid) [-2,3%]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55cdc7-cffc-4eb3-a3d7-ae588a4a8654",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311 for togethercomputer/RedPajama-INCITE-7B-Base\n",
    "- dataset examples: 68166\n",
    "- batch_size= 8, stride=256\n",
    "- model torch dtype: torch.float16\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- perplexity = 4.955935478210449"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e22d95-6fd3-4c62-bad7-8495c93243ea",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mistralai/Mistral-7B-v0.1\n",
    "- dataset examples: 8522\n",
    "- batch_size= 4, stride=256\n",
    "- model torch dtype: torch.bfloat16\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- perplexity = 3.9531056880950928 (valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f419adc-4c00-4a8f-9e72-b8d04150306a",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for 01-ai/Yi-6B\n",
    "- dataset examples: 8522\n",
    "- batch_size= 4, stride=256\n",
    "- model torch dtype: torch.bfloat16\n",
    "- model vocabulary: 64000\n",
    "- model sequence length: 4096\n",
    "- perplexity = 3.990814685821533 (valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855274a-1c94-4f60-bd80-be9418814eb2",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for tiiuae/falcon-7b\n",
    "- dataset examples: 8522\n",
    "- batch_size= 8, stride=256\n",
    "- model torch dtype: torch.bfloat16\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 2048\n",
    "- perplexity = 3.8035600185394287 (valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956a9c2-62f2-4699-98f4-8f7a6306009a",
   "metadata": {},
   "source": [
    "## Unigram-normalized perplexity\n",
    "\n",
    "https://arxiv.org/pdf/2011.13220.pdf\n",
    "\n",
    "Unigram-Normalized Perplexity as a Language Model Performance Measure with Different Vocabulary Sizes\n",
    "\n",
    "*Jihyeon Roh, Sang-Hoon Oh, Soo-Young Lee*\n",
    "\n",
    "Although Perplexity is a widely used performance metric for language models, the values are highly dependent upon the number of words in the corpus and is useful to compare performance of the same corpus only.\n",
    "\n",
    "Perplexity may not be suitable for comparing LMs using different vocabularies because a larger vocabulary size tends to result in lower word probabilities and thus a higher Perplexity.\n",
    "\n",
    "In this paper, we propose a new metric that can be used to evaluate language model performance with different vocabulary sizes. \n",
    "\n",
    "The proposed unigram-normalized Perplexity actually presents the performance improvement of the language models from that of simple unigram model, and is robust on the vocabulary size.\n",
    "\n",
    "To overcome the limitations of the perplexity, we adopt the basic idea of normalizing the word probability with respect to a quantity containing the vocabulary size. \n",
    "\n",
    "We apply a unigram probability that is calculated from the word occurrence as a normalization factor for the perplexity. The unigram probability from the unigram LM is computed as Count(vk) / Count(all words), where Count(vk) is the number of occurrences of word vk in the corpus.\n",
    "\n",
    "Our proposed metric is obtained by normalizing the perplexity with this unigram probability.\n",
    "\n",
    "The proposed “Perplexity normalized with unigram” (PPLu) is defined as\n",
    "PPLu = (Product for all words in sequence of : P(word | language model) / P(word | unigram))^1/length of sequence \n",
    "\n",
    "This metric shows the likelihood improvement of a context-dependent LM from unigram LM without the context information, and enables us to evaluate the effectiveness of an LM.\n",
    "\n",
    "PPLu contains a unigram probability term, which allows PPLu to evaluate LMs more accurately than PPL does. Specifically, even if an LM fails to capture word relationships, it may achieve a good PPL by simply assigning high probabilities to words that frequently appear (e.g., unknown tokens). This case can be corrected with our PPLu, which considers the word frequencies via unigram probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068465a-c0e0-4ea3-b8b6-cad49777b131",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "``` \n",
    "log(PPLu) = 1/length of sequence * Sum for all words in sequence( log(P(word | language model)) - log(P(word | unigram)))\n",
    "          = Log(PPL) - 1/length of sequence * Sum for all words in sequence( log(P(word | unigram) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67cdb98-d947-4575-a0c5-5d598d3251b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:57:05.636636Z",
     "iopub.status.busy": "2024-01-06T20:57:05.636366Z",
     "iopub.status.idle": "2024-01-06T20:57:05.641137Z",
     "shell.execute_reply": "2024-01-06T20:57:05.640719Z",
     "shell.execute_reply.started": "2024-01-06T20:57:05.636621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        self.vocab_size = len(tokenizer.vocab)\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = self.perplexity_loss(logits, labels_for_crossentropy).mean(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/labels_to_ignore.sum(dim=1)) * torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb7752c-3cbf-4131-bdf3-94d618448dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:57:05.641710Z",
     "iopub.status.busy": "2024-01-06T20:57:05.641593Z",
     "iopub.status.idle": "2024-01-06T20:57:05.645094Z",
     "shell.execute_reply": "2024-01-06T20:57:05.644676Z",
     "shell.execute_reply.started": "2024-01-06T20:57:05.641698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99701fc-91a6-4a23-90f9-61b55ff86fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T20:57:05.646058Z",
     "iopub.status.busy": "2024-01-06T20:57:05.645944Z",
     "iopub.status.idle": "2024-01-06T20:57:14.101372Z",
     "shell.execute_reply": "2024-01-06T20:57:14.100529Z",
     "shell.execute_reply.started": "2024-01-06T20:57:05.646051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (288564 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 5,135,027 tokens\n",
      "... 11,359,101 tokens\n",
      "... 13,372,596 tokens\n",
      "Done: 13,630,532 tokens\n"
     ]
    }
   ],
   "source": [
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce28aee-520e-4eba-9c74-57fb5531a71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-06T21:00:32.448001Z",
     "iopub.status.busy": "2024-01-06T21:00:32.446405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 encodings processed\n",
      "-> perplexity = 4.932\n",
      "-> unigram-normalized perplexity = 4.835 (x1000)\n",
      "88 encodings processed\n",
      "-> perplexity = 5.453\n",
      "-> unigram-normalized perplexity = 7.048 (x1000)\n",
      "168 encodings processed\n",
      "-> perplexity = 5.344\n",
      "-> unigram-normalized perplexity = 6.879 (x1000)\n",
      "248 encodings processed\n",
      "-> perplexity = 4.872\n",
      "-> unigram-normalized perplexity = 5.779 (x1000)\n",
      "328 encodings processed\n",
      "-> perplexity = 4.591\n",
      "-> unigram-normalized perplexity = 5.339 (x1000)\n",
      "408 encodings processed\n",
      "-> perplexity = 4.326\n",
      "-> unigram-normalized perplexity = 4.965 (x1000)\n",
      "488 encodings processed\n",
      "-> perplexity = 4.266\n",
      "-> unigram-normalized perplexity = 4.795 (x1000)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(ppl_losses, unigram_losses):        \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp(pt_ppl_losses.mean().item())\n",
    "    pplu = math.exp(pt_pplu_losses.mean().item())\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        ppl_losses.extend(batch_ppl_losses.to('cpu').tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.to('cpu').tolist(), batch_pplu.to('cpu').tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "        display_perplexities(ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(ppl_losses, unigram_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746294a8-9947-43d1-a114-511d626e531f",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for tiiuae/falcon-7b\n",
    "- dataset examples: 8522\n",
    "- batch_size= 8, stride=256\n",
    "- model torch dtype: torch.bfloat16\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f1f1c-0f6c-4961-b964-765fac82b20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
