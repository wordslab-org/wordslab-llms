{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1282ee-ce66-4665-9049-057431d59f0f",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a15aa-978c-460a-9f6c-0c401910ed74",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1721f2-2876-44e4-871e-8eb8439c68fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f295b8-2f93-486a-ad96-990d1768e238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:34:53.846907Z",
     "iopub.status.busy": "2024-01-09T09:34:53.846494Z",
     "iopub.status.idle": "2024-01-09T09:34:55.888300Z",
     "shell.execute_reply": "2024-01-09T09:34:55.887802Z",
     "shell.execute_reply.started": "2024-01-09T09:34:53.846883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flash_attn\n",
    "flash_attn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5beab14-f32f-4b1c-8df8-d624a0c4e61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers optimum auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565e32d-be17-4a3e-9b06-fcd8f9a0976a",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5dbc53-38e1-4483-aef8-25310952f772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:07.655537Z",
     "iopub.status.busy": "2024-01-28T12:27:07.655194Z",
     "iopub.status.idle": "2024-01-28T12:27:07.661497Z",
     "shell.execute_reply": "2024-01-28T12:27:07.661012Z",
     "shell.execute_reply.started": "2024-01-28T12:27:07.655521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " models = { \n",
    "    \"tinyllama_1b\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\", # 4.10 GB\n",
    "    \"stablelm2_1b\" : \"stabilityai/stablelm-2-1_6b\", # 3.06 GB\n",
    "     \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"mpt_30b\" : \"mosaicml/mpt-30b\", # 55.80 GB \n",
    "    \"codellama_34b\" : \"codellama/CodeLlama-34b-hf\", # 62.86 GB \n",
    "    \"yi_34b\" : \"01-ai/Yi-34B\", # 64.06 GB    \n",
    "     \n",
    "    \"falcon_40b\" : \"tiiuae/falcon-40b\", # 77.93 GB\n",
    "    \"alfred_40b\": \"lightonai/alfred-40b-1023\", # 77.93 GB\n",
    "    \"mixtral_8x7B\" : \"mistralai/Mixtral-8x7B-v0.1\" # 86.99 GB\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f6691-1719-4cb7-99ea-f8fb7e67dd51",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b7de50-6539-4ece-bf0d-c8569de15786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:09.465780Z",
     "iopub.status.busy": "2024-01-28T12:27:09.465010Z",
     "iopub.status.idle": "2024-01-28T12:27:09.474839Z",
     "shell.execute_reply": "2024-01-28T12:27:09.474441Z",
     "shell.execute_reply.started": "2024-01-28T12:27:09.465749Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/hftoken\", 'r') as file:\n",
    "    myhftoken = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae3299f-b520-4278-919b-2cdc661e6020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:10.451671Z",
     "iopub.status.busy": "2024-01-28T12:27:10.451006Z",
     "iopub.status.idle": "2024-01-28T12:27:10.891440Z",
     "shell.execute_reply": "2024-01-28T12:27:10.891001Z",
     "shell.execute_reply.started": "2024-01-28T12:27:10.451643Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7714b7e-ed3d-47e9-b02e-dff03299cf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:14.488413Z",
     "iopub.status.busy": "2024-01-28T12:27:14.487702Z",
     "iopub.status.idle": "2024-01-28T12:27:28.106458Z",
     "shell.execute_reply": "2024-01-28T12:27:28.106102Z",
     "shell.execute_reply.started": "2024-01-28T12:27:14.488374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf4639afa424aceb35b386a8cb3a960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c4ee58776f487898b94b3447a47fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c0e1d637974a3e9167602b07865e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43a8a54b90b44be8a6cec8e148584ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b64b33e916745c0a5bcdfa729762706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a895fb7fe2045028e9f4c937bf7532b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "# dataset_fr = load_dataset(dataset_name_fr, token=myhftoken)\n",
    "\n",
    "dataset_name_en = \"frenchtext/bank-en-2401\"\n",
    "dataset_en = load_dataset(dataset_name_en, token=myhftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df161a8-456f-4eaa-b99f-13c3b90d2c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.109653Z",
     "iopub.status.busy": "2024-01-28T12:27:28.108651Z",
     "iopub.status.idle": "2024-01-28T12:27:28.113443Z",
     "shell.execute_reply": "2024-01-28T12:27:28.113116Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.109637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Uri', 'ExtractedFromPDF', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars'],\n",
       "        num_rows: 20451\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['Uri', 'ExtractedFromPDF', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars'],\n",
       "        num_rows: 2555\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Uri', 'ExtractedFromPDF', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars'],\n",
       "        num_rows: 2579\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b9ed0-02c3-43ea-8647-40db7517ff27",
   "metadata": {},
   "source": [
    "## Batching and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1603b5-31cf-4dc9-94ce-27b20091787d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.115939Z",
     "iopub.status.busy": "2024-01-28T12:27:28.115041Z",
     "iopub.status.idle": "2024-01-28T12:27:28.122891Z",
     "shell.execute_reply": "2024-01-28T12:27:28.122478Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.115924Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = dataset_name_en\n",
    "split = \"valid\"\n",
    "dataset = dataset_en[split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef174d1d-8294-485e-91fd-bd9889fbcba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.126136Z",
     "iopub.status.busy": "2024-01-28T12:27:28.125081Z",
     "iopub.status.idle": "2024-01-28T12:27:28.130760Z",
     "shell.execute_reply": "2024-01-28T12:27:28.130425Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.126121Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>15)\n",
    "    sorted_dataset = filtered_dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8fd7f9-aeec-451e-80b2-3c40360203a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.133434Z",
     "iopub.status.busy": "2024-01-28T12:27:28.132324Z",
     "iopub.status.idle": "2024-01-28T12:27:28.137069Z",
     "shell.execute_reply": "2024-01-28T12:27:28.136688Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.133396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encoding_offsets(encoding):\n",
    "    start_token_idx = 0\n",
    "    while encoding.special_tokens_mask[start_token_idx]==1: start_token_idx+=1\n",
    "    start_index = encoding.offsets[start_token_idx][0]\n",
    "    end_token_idx = len(encoding.offsets)-1\n",
    "    while encoding.special_tokens_mask[end_token_idx]==1: end_token_idx-=1\n",
    "    end_index = encoding.offsets[end_token_idx][1]\n",
    "    return (start_index,end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f631c448-19a0-456c-abef-19aeaad60cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.141374Z",
     "iopub.status.busy": "2024-01-28T12:27:28.139817Z",
     "iopub.status.idle": "2024-01-28T12:27:28.154791Z",
     "shell.execute_reply": "2024-01-28T12:27:28.154102Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.141354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    \n",
    "    # SPECIAL CASE: tiktoken tokenizer does not implement truncation=True, return_overflowing_tokens=True, and encodings offsets\n",
    "    # => we must implement it manually on top of Huggingface tokenizers\n",
    "    if hasattr(tokenizer,\"tokenizer\") and tokenizer.tokenizer.__class__.__module__.startswith(\"tiktoken\"):\n",
    "        encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", \n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "        \n",
    "        input_tensor = encodings['input_ids']\n",
    "        attention_mask = encodings['attention_mask']\n",
    "       \n",
    "        batch_size = input_tensor.size(0)\n",
    "        encodings_length = input_tensor.size(1)\n",
    "        texts_length = torch.tensor([len(text) for text in dataset_batch[\"Text\"]])\n",
    "        \n",
    "        max_length = tokenizer.model_max_length \n",
    "        \n",
    "        if encodings_length > max_length:\n",
    "        \n",
    "            unfolded_tensor, overflow_to_sample_mapping = truncate_tensor_with_overflow(input_tensor, padding_value=tokenizer.pad_token_id, max_length=max_length, stride=stride)\n",
    "            unfolded_mask, _ = truncate_tensor_with_overflow(attention_mask, padding_value=0, max_length=max_length, stride=stride)\n",
    "\n",
    "            encodings['input_ids'] = unfolded_tensor\n",
    "            encodings['attention_mask'] = unfolded_mask\n",
    "            encodings['overflow_to_sample_mapping'] = overflow_to_sample_mapping\n",
    "            \n",
    "            offset = max_length - stride\n",
    "            overflow_lines = 1 + math.ceil((encodings_length - max_length)/offset)\n",
    "            last_line_padding = encodings_length % offset\n",
    "            \n",
    "            tokens_per_sample = attention_mask.sum(1).tolist()\n",
    "            start_indexes = []\n",
    "            end_indexes = []\n",
    "            for sample_tokens in tokens_per_sample:                \n",
    "                start_indexes.append(torch.clamp(torch.arange(0,overflow_lines*offset,offset), max=sample_tokens)/sample_tokens)\n",
    "                end_indexes.append(torch.clamp(torch.arange(max_length,encodings_length+last_line_padding+1,offset), max=sample_tokens)/sample_tokens)\n",
    "            overflow_to_sample_offset = torch.stack((torch.concat(start_indexes),torch.concat(end_indexes)))\n",
    "\n",
    "            texts_length_multiplier = torch.repeat_interleave(texts_length, overflow_lines).unsqueeze(0)\n",
    "            otso = (overflow_to_sample_offset*texts_length_multiplier).int()\n",
    "            encodings['overflow_to_sample_offset'] = [(otso[0,i].item(),otso[1,i].item()) for i in range(otso.size(1))]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            encodings['overflow_to_sample_mapping'] = torch.zeros(batch_size, dtype=torch.int32)\n",
    "            encodings['overflow_to_sample_offset'] = [(0,texts_length[i].item()) for i in range(batch_size)]\n",
    "    \n",
    "    # GENERAL CASE: just rely on Huggingface tokenizers for truncation\n",
    "    else:\n",
    "        encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                          padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                          # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                          # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                          pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "        encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ccabeb-3196-4877-a1a1-ec0582927125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.155317Z",
     "iopub.status.busy": "2024-01-28T12:27:28.155200Z",
     "iopub.status.idle": "2024-01-28T12:27:28.910174Z",
     "shell.execute_reply": "2024-01-28T12:27:28.909697Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.155297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def truncate_tensor_with_overflow(input_tensor, padding_value, max_length=2048, stride=256):\n",
    "    batch_length = input_tensor.size(0)\n",
    "    encoding_length = input_tensor.size(1)\n",
    "\n",
    "    offset = max_length - stride\n",
    "    overflow_lines = 1 + math.ceil((encoding_length - max_length)/offset)\n",
    "    last_line_padding = encoding_length % offset\n",
    "\n",
    "    padded_tensor = F.pad(input_tensor, (0,last_line_padding), \"constant\", padding_value)\n",
    "    unfolded_tensor = padded_tensor.unfold(1, max_length, offset).reshape(-1, max_length)\n",
    "\n",
    "    overflow_to_sample_mapping = torch.arange(batch_length).repeat_interleave(overflow_lines)\n",
    "\n",
    "    return unfolded_tensor, overflow_to_sample_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8b9cf9-24e0-4800-81cd-c010e8b7d68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:28.911606Z",
     "iopub.status.busy": "2024-01-28T12:27:28.910907Z",
     "iopub.status.idle": "2024-01-28T12:27:28.914261Z",
     "shell.execute_reply": "2024-01-28T12:27:28.913764Z",
     "shell.execute_reply.started": "2024-01-28T12:27:28.911590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e942b3f2-793d-4e3c-b0e7-c45a95467c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:29.177305Z",
     "iopub.status.busy": "2024-01-28T12:27:29.176275Z",
     "iopub.status.idle": "2024-01-28T12:27:29.182044Z",
     "shell.execute_reply": "2024-01-28T12:27:29.181073Z",
     "shell.execute_reply.started": "2024-01-28T12:27:29.177267Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "#batches_iter = get_dataset_batches(dataset, 2)\n",
    "#for i in range(10): next(batches_iter)\n",
    "#dataset_batch=next(batches_iter)\n",
    "\n",
    "#encodings = encode_dataset_batch(tokenizer, dataset_batch, stride=256)\n",
    "#encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c715b1d-5b91-4d7e-99f5-609573d93b19",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6acc7c5-e652-4511-81a4-e830d2d9ed4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:33.809896Z",
     "iopub.status.busy": "2024-01-28T12:27:33.809051Z",
     "iopub.status.idle": "2024-01-28T12:27:44.852377Z",
     "shell.execute_reply": "2024-01-28T12:27:44.851790Z",
     "shell.execute_reply.started": "2024-01-28T12:27:33.809845Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/bank-en-2401:valid for stabilityai/stablelm-2-1_6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stablelm-2-1_6b:\n",
      "- tokenization_arcade100k.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stablelm-2-1_6b:\n",
      "- configuration_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a07e77053436cb1916e6b5637c4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_stablelm_epoch.py:   0%|          | 0.00/38.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stablelm-2-1_6b:\n",
      "- modeling_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = list(models)[1]\n",
    "model_name = models[model_id]\n",
    "print(f\"Computing perplexity on dataset {dataset_name}:{split} for {model_name}\")\n",
    "\n",
    "if model_id==\"stablelm2_1b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "elif model_id==\"stablelm_3b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=myhftoken)\n",
    "elif model_id==\"qwen_7b\":\n",
    "    # https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md#special-tokens\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cpad_token = '<|endoftext|>')\n",
    "elif model_id==\"yi_34b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if model_id==\"tinyllama_1b\":\n",
    "    # torch_dtype=\"auto\" loads the model in fp32, which is not compatible with flash attention\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"stablelm2_1b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "elif model_id==\"btlm_3b\":\n",
    "    # no flash attention support as of 01/07/2024, using device_map triggers a fatal error\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, torch_dtype=\"auto\", attn_implementation=\"eager\", trust_remote_code=True).to('cuda')\n",
    "    # max context length supported without flahs attention on a RTX 4090\n",
    "    tokenizer.model_max_length = 4096\n",
    "elif model_id==\"stablelm_3b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True, token=myhftoken)\n",
    "elif model_id==\"phi2_3b\" or model_id==\"qwen_7b\":\n",
    "    # no flash attention support for phi2 as of 01/07/2024\n",
    "    # for qwen: latest version of flash_attn installed, but module dropout_layer_norm not found\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"eager\", trust_remote_code=True)\n",
    "elif model_id==\"bloomz_7b\" or model_id==\"mpt_7b\":\n",
    "    # no flash attention support as of 01/08/2024\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"eager\")\n",
    "elif model_id==\"decilm_7b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "elif model_id==\"openllama1_13b\":\n",
    "    # Chunking error during model conversion to safetensors\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"llama2_13b\" or model_id==\"solar_10b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, load_in_8bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"mpt_30b\":\n",
    "    # no flash attention support as of 01/18/2024\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=False, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "elif model_id==\"codellama_34b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"yi_34b\": \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"falcon_40b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=False, load_in_4bit=True, device_map=0, torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "elif model_id==\"alfred_40b\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=False, trust_remote_code=True, load_in_4bit=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True, device_map=\"auto\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0449d9d-448f-4ff8-97b4-fa6b0f7353b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:53.150437Z",
     "iopub.status.busy": "2024-01-28T12:27:53.149333Z",
     "iopub.status.idle": "2024-01-28T12:27:53.157388Z",
     "shell.execute_reply": "2024-01-28T12:27:53.156398Z",
     "shell.execute_reply.started": "2024-01-28T12:27:53.150388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- free memory after load: 21420.82 MB\n"
     ]
    }
   ],
   "source": [
    "free_mem_mb = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0))/1024/1024\n",
    "print(f\"- free memory after load: {free_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d33a90-874f-49cb-b430-69453b7a8668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:27:57.846818Z",
     "iopub.status.busy": "2024-01-28T12:27:57.845840Z",
     "iopub.status.idle": "2024-01-28T12:27:57.855867Z",
     "shell.execute_reply": "2024-01-28T12:27:57.855005Z",
     "shell.execute_reply.started": "2024-01-28T12:27:57.846775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model vocabulary: 100289\n",
      "- model sequence length: 4096\n",
      "- model torch dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if model_id==\"bloomz_7b\":\n",
    "    tokenizer.model_max_length = model.config.seq_length\n",
    "elif model_id==\"mpt_7b\" or model_id==\"mpt_30b\":\n",
    "    pass\n",
    "else:\n",
    "    # IMPORTANT fix: https://github.com/huggingface/transformers/issues/16186\n",
    "    tokenizer.model_max_length = int(min(tokenizer.model_max_length, model.config.max_position_embeddings))\n",
    "\n",
    "if model_id==\"stablelm2_1b\" or model_id==\"qwen_7b\" or model_id==\"yi_34b\":\n",
    "    print(f\"- model vocabulary: {tokenizer.vocab_size}\")\n",
    "else:\n",
    "    print(f\"- model vocabulary: {len(tokenizer.vocab)}\")\n",
    "\n",
    "# Memory limit of RTX 4090\n",
    "if tokenizer.model_max_length>8192:\n",
    "    tokenizer.model_max_length = 8192\n",
    "elif model_id==\"decilm_7b\" or model_id==\"codellama_34b\":\n",
    "    tokenizer.model_max_length = 4096\n",
    "elif model_id==\"mpt_30b\":\n",
    "    tokenizer.model_max_length = 2048\n",
    "print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")\n",
    "\n",
    "print(f\"- model torch dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956a9c2-62f2-4699-98f4-8f7a6306009a",
   "metadata": {},
   "source": [
    "## Unigram-normalized perplexity\n",
    "\n",
    "https://arxiv.org/pdf/2011.13220.pdf\n",
    "\n",
    "Unigram-Normalized Perplexity as a Language Model Performance Measure with Different Vocabulary Sizes\n",
    "\n",
    "*Jihyeon Roh, Sang-Hoon Oh, Soo-Young Lee*\n",
    "\n",
    "Although Perplexity is a widely used performance metric for language models, the values are highly dependent upon the number of words in the corpus and is useful to compare performance of the same corpus only.\n",
    "\n",
    "Perplexity may not be suitable for comparing LMs using different vocabularies because a larger vocabulary size tends to result in lower word probabilities and thus a higher Perplexity.\n",
    "\n",
    "In this paper, we propose a new metric that can be used to evaluate language model performance with different vocabulary sizes. \n",
    "\n",
    "The proposed unigram-normalized Perplexity actually presents the performance improvement of the language models from that of simple unigram model, and is robust on the vocabulary size.\n",
    "\n",
    "To overcome the limitations of the perplexity, we adopt the basic idea of normalizing the word probability with respect to a quantity containing the vocabulary size. \n",
    "\n",
    "We apply a unigram probability that is calculated from the word occurrence as a normalization factor for the perplexity. The unigram probability from the unigram LM is computed as Count(vk) / Count(all words), where Count(vk) is the number of occurrences of word vk in the corpus.\n",
    "\n",
    "Our proposed metric is obtained by normalizing the perplexity with this unigram probability.\n",
    "\n",
    "The proposed “Perplexity normalized with unigram” (PPLu) is defined as\n",
    "PPLu = (Product for all words in sequence of : P(word | language model) / P(word | unigram))^1/length of sequence \n",
    "\n",
    "This metric shows the likelihood improvement of a context-dependent LM from unigram LM without the context information, and enables us to evaluate the effectiveness of an LM.\n",
    "\n",
    "PPLu contains a unigram probability term, which allows PPLu to evaluate LMs more accurately than PPL does. Specifically, even if an LM fails to capture word relationships, it may achieve a good PPL by simply assigning high probabilities to words that frequently appear (e.g., unknown tokens). This case can be corrected with our PPLu, which considers the word frequencies via unigram probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068465a-c0e0-4ea3-b8b6-cad49777b131",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "``` \n",
    "log(PPLu) = 1/length of sequence * Sum for all words in sequence( log(P(word | language model)) - log(P(word | unigram)))\n",
    "          = Log(PPL) - 1/length of sequence * Sum for all words in sequence( log(P(word | unigram) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67cdb98-d947-4575-a0c5-5d598d3251b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:28:01.775199Z",
     "iopub.status.busy": "2024-01-28T12:28:01.774710Z",
     "iopub.status.idle": "2024-01-28T12:28:01.787533Z",
     "shell.execute_reply": "2024-01-28T12:28:01.787026Z",
     "shell.execute_reply.started": "2024-01-28T12:28:01.775167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PPLu():\n",
    "    \n",
    "    def __init__(self, dataset_iterator, tokenizer, device):\n",
    "        if hasattr(tokenizer,\"vocab\"):\n",
    "            self.vocab_size = len(tokenizer.vocab)\n",
    "        else:\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        dataset_token_id_counts = torch.zeros(self.vocab_size+1, dtype=torch.int64)\n",
    "        dataset_tokens_count = 0\n",
    "        \n",
    "        for idx,dataset_batch in enumerate(dataset_iterator):\n",
    "            encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "            \n",
    "            # Padding tokens should be ignored: count them as token_id=vocabulary_size\n",
    "            token_ids = encodings.input_ids*encodings.attention_mask + self.vocab_size*(1-encodings.attention_mask)\n",
    "            \n",
    "            token_id_counts = torch.bincount(token_ids.view(-1), minlength=self.vocab_size+1)\n",
    "            tokens_count = encodings.attention_mask.sum()\n",
    "\n",
    "            dataset_token_id_counts += token_id_counts\n",
    "            dataset_tokens_count += tokens_count\n",
    "            if idx%100==9: print(f\"... {dataset_tokens_count:,} tokens\")\n",
    "        \n",
    "        # Then discard the tokens count for token_id=vocabulary_size\n",
    "        self.token_id_probs =  (dataset_token_id_counts[:-1] / dataset_tokens_count).unsqueeze(1).to(device)\n",
    "        self.perplexity_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "        print(f\"Done: {dataset_tokens_count:,} tokens\")\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, output_logits):\n",
    "        # Next-token prediction: shift prediction scores and input ids by one\n",
    "        logits = output_logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        labels_to_ignore = attention_mask[:, 1:]\n",
    "\n",
    "        # Number of tokens predicted, ignoring padding tokens\n",
    "        predicted_tokens_count = labels_to_ignore.sum(dim=1)\n",
    "        \n",
    "        # Cross entropy loss (ignore_index=-100)\n",
    "        labels_for_crossentropy = labels*labels_to_ignore -100*(1-labels_to_ignore)\n",
    "        batch_perplexity_losses = (1/predicted_tokens_count)*self.perplexity_loss(logits, labels_for_crossentropy).sum(1)\n",
    "        \n",
    "        # Unigram probability loss\n",
    "        labels_probs = F.embedding(labels, self.token_id_probs).squeeze()\n",
    "        # prob = 1 for padding tokens => log prob = 0, ignored in the sum below\n",
    "        labels_probs = labels_probs*labels_to_ignore + (1-labels_to_ignore) \n",
    "        batch_unigram_losses = -(1/predicted_tokens_count)*torch.log(labels_probs).sum(dim=1)\n",
    "        \n",
    "        # Unigram-nomralized perplexities\n",
    "        perplexities = torch.exp(batch_perplexity_losses)\n",
    "        unigram_normalized_perplexities = torch.exp(batch_perplexity_losses - batch_unigram_losses)\n",
    "        \n",
    "        return predicted_tokens_count, batch_perplexity_losses, batch_unigram_losses, perplexities, unigram_normalized_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb7752c-3cbf-4131-bdf3-94d618448dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:28:02.991199Z",
     "iopub.status.busy": "2024-01-28T12:28:02.989978Z",
     "iopub.status.idle": "2024-01-28T12:28:02.998564Z",
     "shell.execute_reply": "2024-01-28T12:28:02.997159Z",
     "shell.execute_reply.started": "2024-01-28T12:28:02.991139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizedPerplexityLogger:\n",
    "    def __init__(self, dataset_name, split, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{split}_{model_name.replace('/','_')}_pplu.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, ppl, pplu, uri, span):\n",
    "        self.file.write(f\"{ppl},{pplu},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99701fc-91a6-4a23-90f9-61b55ff86fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:28:04.172647Z",
     "iopub.status.busy": "2024-01-28T12:28:04.171767Z",
     "iopub.status.idle": "2024-01-28T12:28:12.473672Z",
     "shell.execute_reply": "2024-01-28T12:28:12.472951Z",
     "shell.execute_reply.started": "2024-01-28T12:28:04.172605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514827 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 6,676,337 tokens\n",
      "Done: 8,448,894 tokens\n"
     ]
    }
   ],
   "source": [
    "pplu_loss = PPLu(get_dataset_batches(dataset), tokenizer, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195039f2-1b85-400c-ae90-9d7b889fb46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:28:12.475235Z",
     "iopub.status.busy": "2024-01-28T12:28:12.474795Z",
     "iopub.status.idle": "2024-01-28T12:28:12.480676Z",
     "shell.execute_reply": "2024-01-28T12:28:12.479892Z",
     "shell.execute_reply.started": "2024-01-28T12:28:12.475215Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dataset examples: 2555\n",
      "- batch_size=32, stride=256\n"
     ]
    }
   ],
   "source": [
    "if model_id==\"stablelm2_1b\":\n",
    "    batch_size = 32\n",
    "elif model_id==\"tinyllama_1b\" or model_id==\"redpajama_3b\" or model_id==\"openllama2_3b\":\n",
    "    batch_size = 16\n",
    "elif model_id==\"redpajama_7b\" :\n",
    "    batch_size = 8\n",
    "elif model_id==\"stablelm_3b\" or model_id==\"phi2_3b\" or model_id==\"falcon_7b\" or model_id==\"mpt_7b\"or model_id==\"openllama1_13b\":\n",
    "    batch_size = 6\n",
    "elif model_id==\"btlm_3b\" or model_id==\"llama2_7b\":\n",
    "    batch_size = 4\n",
    "elif model_id==\"yi_6b\" or model_id==\"llama2_13b\":\n",
    "    batch_size = 3\n",
    "elif model_id==\"bloomz_7b\" or model_id==\"llama2_7b_32k\" or model_id==\"mistral_7b\" or model_id==\"qwen_7b\" or model_id==\"decilm_7b\" or model_id==\"solar_10b\":\n",
    "    batch_size = 2\n",
    "elif model_id==\"mpt_30b\" or model_id==\"codellama_34b\" or model_id==\"yi_34b\":\n",
    "    batch_size = 1\n",
    "stride = 256\n",
    "\n",
    "print(f\"- dataset examples: {len(dataset)}\")\n",
    "print(f\"- batch_size={batch_size}, stride={stride}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66f3a6a1-522f-4cfb-874e-b42d51d14f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:28:21.773443Z",
     "iopub.status.busy": "2024-01-28T12:28:21.772483Z",
     "iopub.status.idle": "2024-01-28T12:28:25.732194Z",
     "shell.execute_reply": "2024-01-28T12:28:25.731404Z",
     "shell.execute_reply.started": "2024-01-28T12:28:21.773403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4480] at entry 0 and [4448] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5912/737804013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mppl_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0munigram_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencodings_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encodings_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# predict next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5912/448396831.py\u001b[0m in \u001b[0;36mget_encodings_batches\u001b[0;34m(tokenizer, dataset, batch_size, stride)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_encodings_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_dataset_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mencodings_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5912/1671694065.py\u001b[0m in \u001b[0;36mencode_dataset_batch\u001b[0;34m(tokenizer, dataset_batch, stride)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mstart_indexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverflow_lines\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mend_indexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencodings_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlast_line_padding\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msample_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moverflow_to_sample_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtexts_length_multiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverflow_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4480] at entry 0 and [4448] at entry 1"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logger = NormalizedPerplexityLogger(dataset_name, split, model_name)\n",
    "\n",
    "def display_perplexities(pred_tokens_count, ppl_losses, unigram_losses):        \n",
    "    pt_pred_tokens_count = torch.Tensor(pred_tokens_count)\n",
    "    total_pred_tokens_count = pt_pred_tokens_count.sum().item()\n",
    "    \n",
    "    pt_ppl_losses = torch.Tensor(ppl_losses)\n",
    "    pt_unigram_losses = torch.Tensor(unigram_losses)    \n",
    "    pt_pplu_losses = pt_ppl_losses - pt_unigram_losses\n",
    "\n",
    "    ppl = math.exp((pt_ppl_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "    pplu = math.exp((pt_pplu_losses*pt_pred_tokens_count).sum().item() / total_pred_tokens_count)\n",
    "\n",
    "    print(f\"-> perplexity = {ppl:.3f}\")\n",
    "    print(f\"-> unigram-normalized perplexity = {pplu*1000:.3f} (x1000)\")\n",
    "    \n",
    "pred_tokens_count = [] \n",
    "ppl_losses = []   \n",
    "unigram_losses = [] \n",
    "for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "    with torch.no_grad():\n",
    "        # predict next token\n",
    "        inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "        outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "        batch_pred_tokens_count, batch_ppl_losses, batch_unigram_losses, batch_ppl, batch_pplu = pplu_loss(inputs, attention_mask, outputs.logits)\n",
    "        \n",
    "        pred_tokens_count.extend(batch_pred_tokens_count.tolist())\n",
    "        ppl_losses.extend(batch_ppl_losses.tolist())\n",
    "        unigram_losses.extend(batch_unigram_losses.tolist())\n",
    "\n",
    "    for ppl,pplu,uri,span in zip(batch_ppl.tolist(), batch_pplu.tolist(), encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "        logger.log_batch(ppl, pplu, uri, span)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(f\"{(idx+1)*batch_size} encodings processed\")\n",
    "        display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)\n",
    "\n",
    "print(f\"FINAL RESULT: {(idx+1)*batch_size} encodings processed\")\n",
    "display_perplexities(pred_tokens_count, ppl_losses, unigram_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed41699-2587-40e8-9580-b1d7f7726e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c96d9db-7ec5-4517-bf3a-5b1a3236f61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T06:10:31.686710Z",
     "iopub.status.busy": "2024-01-22T06:10:31.685888Z",
     "iopub.status.idle": "2024-01-22T06:10:31.726034Z",
     "shell.execute_reply": "2024-01-22T06:10:31.725155Z",
     "shell.execute_reply.started": "2024-01-22T06:10:31.686630Z"
    }
   },
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for codellama/CodeLlama-34b-hf\n",
    "- model vocabulary: 32004\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=1, stride=256\n",
    "- 10,523,047 tokens in 5 min 3 sec\n",
    "- perplexity = 4.312\n",
    "- unigram-normalized perplexity = 6.577 (x1000)\n",
    "\n",
    "2 h 40 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bbbfa-3d24-412c-9328-91e51b55ebe6",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for 01-ai/Yi-34B\n",
    "- free memory after load: 4716.67 MB\n",
    "- model vocabulary: 64000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=1, stride=256\n",
    "- 9,943,316 tokens in 4 min 47 sec\n",
    "- perplexity = 4.054\n",
    "- unigram-normalized perplexity = 6.919 (x1000)\n",
    "\n",
    "2 h 46 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd8119-f1e1-4184-a5fd-8f129726b55a",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=4, stride=256\n",
    "- 13,622,486 tokens in 14 sec\n",
    "- perplexity = 3.815\n",
    "- unigram-normalized perplexity = 4.098 (x1000)\n",
    "\n",
    "3 h 17min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f679f8-b7e7-45f5-9342-df29fb6c6147",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for lightonai/alfred-40b-1023\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 9,243,621 tokens in 14 sec\n",
    "- perplexity = 4.690\n",
    "- unigram-normalized perplexity = 6.273 (x1000)\n",
    "\n",
    "2 h 22 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccbada-f673-4295-b595-cd64252083c0",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for meta-llama/Llama-2-13b-hf\n",
    "- **load_in_4bit=True**\n",
    "- free memory after load: 17001.15 MB\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- 10,523,047 tokens in 5 min 4 sec\n",
    "- dataset examples: 2555\n",
    "- batch_size=3, stride=256\n",
    "- perplexity = 3.988\n",
    "- unigram-normalized perplexity = 6.109 (x1000)\n",
    "\n",
    "2h 16 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb237e-b700-40a1-bef1-329520ee375c",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for mosaicml/mpt-30b\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=1, stride=256\n",
    "- 8,530,699 tokens in 7 sec\n",
    "- perplexity = 5.918\n",
    "- unigram-normalized perplexity = 5.243 (x1000)\n",
    "\n",
    "2 h 16 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44962bd-b0a9-42e6-a7c0-5d1ef03307eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T08:05:34.323167Z",
     "iopub.status.busy": "2024-01-20T08:05:34.322413Z",
     "iopub.status.idle": "2024-01-20T08:05:34.347225Z",
     "shell.execute_reply": "2024-01-20T08:05:34.346601Z",
     "shell.execute_reply.started": "2024-01-20T08:05:34.323139Z"
    },
    "tags": []
   },
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for tiiuae/falcon-40b\n",
    "- free memory after load: 1643.57 MB\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "\n",
    "==> Impossible on RTX 4090, need to run on the RTX A6000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0f853-bc0c-4a85-ba97-90b22ed27be5",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for 01-ai/Yi-34B\n",
    "- model vocabulary: 64000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=1, stride=256\n",
    "- 16,978,029 tokens in 44 sec\n",
    "- perplexity = 3.422\n",
    "- unigram-normalized perplexity = 5.688 (x1000)\n",
    "\n",
    "5 h 44 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b5655-7a3a-4013-b860-d3f6779bed97",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for codellama/CodeLlama-34b-hf\n",
    "- model vocabulary: 32004\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=1, stride=256\n",
    "- 15,042,809 tokens in 57 sec\n",
    "- perplexity = 4.032\n",
    "- unigram-normalized perplexity = 5.028 (x1000)\n",
    "\n",
    "3 h 40 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255195e-557d-4269-afe5-4d5e30ff5c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-19T05:07:00.684418Z",
     "iopub.status.busy": "2024-01-19T05:07:00.672192Z",
     "iopub.status.idle": "2024-01-19T05:07:00.740543Z",
     "shell.execute_reply": "2024-01-19T05:07:00.738879Z",
     "shell.execute_reply.started": "2024-01-19T05:07:00.684351Z"
    },
    "tags": []
   },
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mosaicml/mpt-30b\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=1, stride=256\n",
    "- 14,248,418 tokens in 8 sec\n",
    "- perplexity = 4.986\n",
    "- unigram-normalized perplexity = 4.632 (x1000)\n",
    "\n",
    "4 h 12 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b316f57-4051-4664-961c-abe73c99d944",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size= 16, stride=256\n",
    "- perplexity = 6.196\n",
    "- unigram-normalized perplexity = 7.739 (x1000)\n",
    "\n",
    "7 min 52 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456fcf4-8980-4e3d-9abb-b2b1c1dff484",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
    "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=16, stride=256\n",
    "- perplexity = 6.197\n",
    "- unigram-normalized perplexity = 5.757 (x1000)\n",
    "\n",
    "13 min 52 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d365e29-bdc0-4285-837d-20d67e6c4bc5",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for cerebras/btlm-3b-8k-base\n",
    "- model vocabulary: 50257\n",
    "- model sequence length: 4096 (8192 supported but too big for the RTX 4090)\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=4, stride=256\n",
    "- perplexity = 7.333\n",
    "- unigram-normalized perplexity = 9.866 (x1000)\n",
    "\n",
    "42 min 56 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4590712-2598-49c9-99af-8c76c4ae0cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T20:02:22.391069Z",
     "iopub.status.busy": "2024-01-07T20:02:22.390681Z",
     "iopub.status.idle": "2024-01-07T20:02:22.395292Z",
     "shell.execute_reply": "2024-01-07T20:02:22.394686Z",
     "shell.execute_reply.started": "2024-01-07T20:02:22.391045Z"
    }
   },
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for openlm-research/open_llama_3b_v2\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=16, stride=256\n",
    "- 16,584,523 tokens in 36 sec\n",
    "- perplexity = 4.762\n",
    "- unigram-normalized perplexity = 7.148 (x1000)\n",
    "\n",
    "25 min 00 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1bc57-488d-46b4-9e01-b5eb7998e5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T21:11:05.305862Z",
     "iopub.status.busy": "2024-01-07T21:11:05.305016Z",
     "iopub.status.idle": "2024-01-07T21:11:05.311978Z",
     "shell.execute_reply": "2024-01-07T21:11:05.311191Z",
     "shell.execute_reply.started": "2024-01-07T21:11:05.305831Z"
    },
    "tags": []
   },
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for stabilityai/stablelm-3b-4e1t\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256\n",
    "- 14,248,418 tokens in 16 sec\n",
    "- perplexity = 4.950\n",
    "- unigram-normalized perplexity = 4.590 (x1000)\n",
    "\n",
    "14 min 36 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f035fde-9aeb-49bc-b876-b1517c9e1a27",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for microsoft/phi-2\n",
    "- model vocabulary: 50295\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256\n",
    "- 16,122,587 tokens in 18 sec\n",
    "- perplexity = 8.083\n",
    "- unigram-normalized perplexity = 10.807 (x1000)\n",
    "\n",
    "45 min 16 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c71da-420b-4435-af7f-babf8cd174ec",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for bigscience/bloomz-7b1-mt\n",
    "- model vocabulary: 250680\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 10,041,864 tokens in 6 sec\n",
    "- perplexity = 10.038\n",
    "- unigram-normalized perplexity = 4.591 (x1000)\n",
    "\n",
    "58 min 52 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da4f3a-b535-4b3d-af2f-b2ff5a37108e",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for tiiuae/falcon-7b\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256\n",
    "- 13,622,432 tokens in 11 sec\n",
    "- perplexity = 4.335\n",
    "- unigram-normalized perplexity = 4.660 (x1000)\n",
    "\n",
    "29 min 43 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0386a35-8d44-4689-ad60-7ad001f95926",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for togethercomputer/RedPajama-INCITE-7B-Base\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=8, stride=256\n",
    "- 14,248,418 tokens in 13 sec\n",
    "- perplexity = 5.512\n",
    "- unigram-normalized perplexity = 5.120 (x1000)\n",
    "\n",
    "29 min 31 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2cfba5-9eb0-468f-927d-29bb3263114d",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mosaicml/mpt-7b\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256\n",
    "- 14,248,418 tokens in 8 sec\n",
    "- perplexity = 5.581\n",
    "- unigram-normalized perplexity = 5.184 (x1000)\n",
    "\n",
    "51 min 52 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b35a0-d222-479e-95a5-bf467f0e7223",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for meta-llama/Llama-2-7b-hf\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=4, stride=256\n",
    "- 15,042,809 tokens in 47 sec\n",
    "- perplexity = 4.236\n",
    "- unigram-normalized perplexity = 5.289 (x1000)\n",
    "\n",
    "36 min 24 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30017acf-3d86-4f8a-9d26-ba7c82d22cda",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for togethercomputer/LLaMA-2-7B-32K\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192 (32768 supported but too large)\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 15,034,641 tokens in 43 sec\n",
    "- perplexity = 4.409\n",
    "- unigram-normalized perplexity = 5.504 (x1000)\n",
    "\n",
    "36 min 29 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bf439-054b-4f60-b07e-3c73512f5898",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mistralai/Mistral-7B-v0.1\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192 (32768 supported but too large)\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 15,453,930 tokens in 39 sec\n",
    "- perplexity = 3.803\n",
    "- unigram-normalized perplexity = 4.955 (x1000)\n",
    "\n",
    "40 min 27 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a73809-10a8-47f7-8f4a-ec7b74d20836",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for Qwen/Qwen-7B\n",
    "- model vocabulary: 151851\n",
    "- model sequence length: 32768\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 13,057,768 tokens in 11 sec\n",
    "\n",
    "ERROR - Could not resolve the error in tiktoken fast tokenizer:\n",
    "Unable to create tensor returning overflowing tokens of different lengths. Please see if a fast version of this tokenizer is available to have this feature available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81299f2-38eb-4d61-a0c8-8ba70054b876",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for 01-ai/Yi-6B\n",
    "- model vocabulary: 64000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=3, stride=256\n",
    "- 16,978,029 tokens in 38 sec\n",
    "- perplexity = 4.108\n",
    "- unigram-normalized perplexity = 6.828 (x1000)\n",
    "\n",
    "37 min 2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dd796-04e1-47a3-b41b-d426b8a3e24c",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for Deci/DeciLM-7B\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 15,453,930 tokens in 34 sec\n",
    "- perplexity = 5.827\n",
    "- unigram-normalized perplexity = 7.595 (x1000)\n",
    "\n",
    "46 min 25 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac06b0a-dbaa-434e-b67c-ce3c9e29e52a",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for openlm-research/open_llama_13b\n",
    "- **load_in_8bit=True**\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256\n",
    "- 16,739,789 tokens in 35 sec\n",
    "- perplexity = 4.236\n",
    "- unigram-normalized perplexity = 6.540 (x1000)\n",
    "\n",
    "1 h 4 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f07d8-d1f3-4b39-b718-c8d4f9caa198",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for meta-llama/Llama-2-13b-hf\n",
    "- **load_in_8bit=True**\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=3, stride=256\n",
    "- 15,042,809 tokens in 35 sec\n",
    "- perplexity = 3.923\n",
    "- unigram-normalized perplexity = 4.898 (x1000)\n",
    "\n",
    "59 min 19 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6b877-a952-4073-b64c-4f7f64fa8d2b",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for upstage/SOLAR-10.7B-v1.0\n",
    "- **load_in_8bit=True**\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 8522\n",
    "- batch_size=2, stride=256\n",
    "- 15,453,930 tokens in 36 sec\n",
    "- perplexity = 4.056\n",
    "- unigram-normalized perplexity = 5.286 (x1000)\n",
    "\n",
    "1h 29 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0d2e8-38e2-4f6e-9ee4-5b882b331422",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=16, stride=256\n",
    "- 10,523,047 tokens in 4 min 37 sec\n",
    "- perplexity = 5.699\n",
    "- unigram-normalized perplexity = 8.742 (x1000)\n",
    "\n",
    "9 min 28 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375c038-a45a-4d61-a1c9-8ba2d735833e",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=16, stride=256\n",
    "- 8,530,699 tokens in 9 sec\n",
    "- perplexity = 7.313\n",
    "- unigram-normalized perplexity = 6.479 (x1000)\n",
    "\n",
    "8 min 11 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785e774-d41d-48f9-a613-34fcbec1362e",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for cerebras/btlm-3b-8k-base\n",
    "- model vocabulary: 50257\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 9,179,425 tokens in 8 sec\n",
    "- perplexity = 12.594\n",
    "- unigram-normalized perplexity = 15.200 (x1000)\n",
    "\n",
    "29 min 16 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06664176-056a-445b-8128-9781705cbbac",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for openlm-research/open_llama_3b_v2\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=16, stride=256\n",
    "- 9,911,960 tokens in 5 min 40 sec\n",
    "- perplexity = 5.761\n",
    "- unigram-normalized perplexity = 8.875 (x1000)\n",
    "\n",
    "20 min 38 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e0231-cce3-4457-ab2c-5c4c6638d6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T05:50:30.063288Z",
     "iopub.status.busy": "2024-01-12T05:50:30.062340Z",
     "iopub.status.idle": "2024-01-12T05:50:30.068792Z",
     "shell.execute_reply": "2024-01-12T05:50:30.067956Z",
     "shell.execute_reply.started": "2024-01-12T05:50:30.063250Z"
    }
   },
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for stabilityai/stablelm-3b-4e1t\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=6, stride=256\n",
    "- 8,530,699 tokens in 15 sec\n",
    "- perplexity = 5.947\n",
    "- unigram-normalized perplexity = 5.249 (x1000)\n",
    "\n",
    "8 min 50 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea69096-f749-4930-a39d-4a8facb66a46",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for microsoft/phi-2\n",
    "- model vocabulary: 50295\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=6, stride=256\n",
    "- 9,135,636 tokens in 18 sec\n",
    "- perplexity = 8.090\n",
    "- unigram-normalized perplexity = 9.720 (x1000)\n",
    "\n",
    "24 min 55 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc38e9-0f5a-4432-b2a0-bc47653fa3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T22:47:14.566649Z",
     "iopub.status.busy": "2024-01-12T22:47:14.566144Z",
     "iopub.status.idle": "2024-01-12T22:47:14.572894Z",
     "shell.execute_reply": "2024-01-12T22:47:14.572410Z",
     "shell.execute_reply.started": "2024-01-12T22:47:14.566615Z"
    }
   },
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for bigscience/bloomz-7b1-mt\n",
    "- model vocabulary: 250680\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=2, stride=256\n",
    "- 8,014,168 tokens in 23 sec\n",
    "- perplexity = 13.149\n",
    "- unigram-normalized perplexity = 8.554 (x1000)\n",
    "\n",
    "44 min 6 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025a277-833a-4bd7-a34e-920d30a67211",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for tiiuae/falcon-7b\n",
    "- model vocabulary: 65024\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=6, stride=256\n",
    "- 9,243,615 tokens in 8 sec\n",
    "- perplexity = 5.713\n",
    "- unigram-normalized perplexity = 7.689 (x1000)\n",
    "\n",
    "19 min 27 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfab4fc-62c6-447b-890f-374b6ccc6a5c",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for togethercomputer/RedPajama-INCITE-7B-Base\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=8, stride=256\n",
    "- 8,530,699 tokens in 7 sec\n",
    "- perplexity = 6.799\n",
    "- unigram-normalized perplexity = 6.024 (x1000)\n",
    "\n",
    "17 min 18 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13f1f7-4205-4f01-a9cb-5d81318f2e25",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for mosaicml/mpt-7b\n",
    "- model vocabulary: 50277\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=6, stride=256\n",
    "- 8,530,699 tokens in 7 sec\n",
    "- perplexity = 6.455\n",
    "- unigram-normalized perplexity = 5.719 (x1000)\n",
    "\n",
    "31 min 50 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77e2be-b1b7-4016-bd4a-babf98fbea0a",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for meta-llama/Llama-2-7b-hf\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=4, stride=256\n",
    "- 10,523,047 tokens in 4 min 38 sec\n",
    "- perplexity = 4.123\n",
    "- unigram-normalized perplexity = 6.273 (x1000)\n",
    "\n",
    "26 min 38 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa117d4-0e3a-4a0a-bbf8-854d57d65e87",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for togethercomputer/LLaMA-2-7B-32K\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=2, stride=256\n",
    "- 10,520,590 tokens in 4 min 39 sec\n",
    "- perplexity = 4.059\n",
    "- unigram-normalized perplexity = 6.195 (x1000)\n",
    "\n",
    "27 min 58 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ba341-4258-444a-a208-6b30fdf0ef68",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for mistralai/Mistral-7B-v0.1\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=2, stride=256\n",
    "- 10,159,135 tokens in 4 min 38 sec\n",
    "- perplexity = 3.927\n",
    "- unigram-normalized perplexity = 5.991 (x1000)\n",
    "\n",
    "29 min 7 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3f7f3-bed4-4831-b7db-f5186f006173",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for 01-ai/Yi-6B\n",
    "- model vocabulary: 64000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=3, stride=256\n",
    "- 9,943,316 tokens in 4 min 39 sec\n",
    "- dataset examples: 2555\n",
    "- batch_size=3, stride=256\n",
    "- perplexity = 4.640\n",
    "- unigram-normalized perplexity = 7.918 (x1000)\n",
    "\n",
    "27 min 20 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff228f-9f1d-4598-89a9-932d5436453f",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for Deci/DeciLM-7B\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 2555\n",
    "- batch_size=2, stride=256\n",
    "- 10,159,135 tokens in 4 min 45 sec\n",
    "- perplexity = 5.106\n",
    "- unigram-normalized perplexity = 7.814 (x1000)\n",
    "\n",
    "35 min 44 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6f132-93d7-481a-a691-b8960559e1f6",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for openlm-research/open_llama_13b\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 2048\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=6, stride=256\n",
    "- 9,907,182 tokens in 4 min 38 sec\n",
    "- perplexity = 5.011\n",
    "- unigram-normalized perplexity = 7.895 (x1000)\n",
    "\n",
    "41 min 7 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28581052-8949-4b00-8167-4b714d69f603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T17:14:09.324802Z",
     "iopub.status.busy": "2024-01-13T17:14:09.323927Z",
     "iopub.status.idle": "2024-01-13T17:14:09.331905Z",
     "shell.execute_reply": "2024-01-13T17:14:09.331127Z",
     "shell.execute_reply.started": "2024-01-13T17:14:09.324763Z"
    },
    "tags": []
   },
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for meta-llama/Llama-2-13b-hf\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=3, stride=256\n",
    "- 10,523,047 tokens in 4 min 47 sec\n",
    "- perplexity = 3.883\n",
    "- unigram-normalized perplexity = 5.948 (x1000)\n",
    "\n",
    "43 min 47 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62936a28-ab08-4edb-a4fd-c7f62eabf90f",
   "metadata": {},
   "source": [
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for upstage/SOLAR-10.7B-v1.0\n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 4096\n",
    "- model torch dtype: torch.float16\n",
    "- dataset examples: 2555\n",
    "- batch_size=2, stride=256\n",
    "- 10,159,135 tokens in 4 min 40 sec\n",
    "- perplexity = 4.251\n",
    "- unigram-normalized perplexity = 6.504 (x1000)\n",
    "\n",
    "55 min 52 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c8a50-371f-4d2d-9286-b5ffa38408f8",
   "metadata": {},
   "source": [
    "[RTX A6000 Ada 48GB]\n",
    "\n",
    "Computing perplexity on dataset frenchtext/banque-fr-2311:valid for mistralai/Mixtral-8x7B-v0.1\n",
    "- model files size   : 86.99 GB\n",
    "- **load_in_4bit** => VRAM used: 25,978.5 MB \n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256 => max VRAM used: 43,988.5 MB\n",
    "- 15,169,531 tokens in 25 sec\n",
    "- perplexity = 3.045\n",
    "- unigram-normalized perplexity = 3.967 (x1000)\n",
    "\n",
    "(time not measured)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc0ddc-86e5-49e4-8c37-dbb888dea2eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-14T08:28:59.223324Z"
    },
    "tags": []
   },
   "source": [
    "[RTX A6000 Ada 48GB]\n",
    "\n",
    "Computing perplexity on dataset frenchtext/bank-en-2401:valid for mistralai/Mixtral-8x7B-v0.1\n",
    "- model files size   : 86.99 GB\n",
    "- **load_in_4bit** => VRAM used: 25,978.5 MB \n",
    "- model vocabulary: 32000\n",
    "- model sequence length: 8192\n",
    "- model torch dtype: torch.bfloat16\n",
    "- dataset examples: 8522\n",
    "- batch_size=6, stride=256 => max VRAM used: 43,988.5 MB\n",
    "- perplexity = 3.672\n",
    "- unigram-normalized perplexity = 5.603 (x1000)\n",
    "\n",
    "56 min 44 sec\n",
    "\n",
    "Max VRAM used : 43,988.5 MB -  90 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb228c-3d9c-43bc-8fe1-e00ea9ea52c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
