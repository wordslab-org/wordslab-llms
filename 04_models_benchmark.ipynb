{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1282ee-ce66-4665-9049-057431d59f0f",
   "metadata": {},
   "source": [
    "# Most popular open weights LLMs - December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a15aa-978c-460a-9f6c-0c401910ed74",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1721f2-2876-44e4-871e-8eb8439c68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565e32d-be17-4a3e-9b06-fcd8f9a0976a",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5dbc53-38e1-4483-aef8-25310952f772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:26.791499Z",
     "iopub.status.busy": "2023-12-28T07:49:26.791395Z",
     "iopub.status.idle": "2023-12-28T07:49:26.796354Z",
     "shell.execute_reply": "2023-12-28T07:49:26.795920Z",
     "shell.execute_reply.started": "2023-12-28T07:49:26.791489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    \"redpajama_3b\" : \"togethercomputer/RedPajama-INCITE-Base-3B-v1\", # 5.30 GB\n",
    "    \"btlm_3b\" : \"cerebras/btlm-3b-8k-base\", #  4.93 GB\n",
    "    \"openllama2_3b\" : \"openlm-research/open_llama_3b_v2\", #  6.38 GB\n",
    "    \"stablelm_3b\" : \"stabilityai/stablelm-3b-4e1t\", # 5.21 GB\n",
    "    \"phi2_3b\" : \"microsoft/phi-2\", # 5.18 GB\n",
    "\n",
    "    \"bloomz_7b\" : \"bigscience/bloomz-7b1-mt\", # 13.18 GB\n",
    "    \"falcon_7b\" : \"tiiuae/falcon-7b\", # 13.45 GB       \n",
    "    \"redpajama_7b\" : \"togethercomputer/RedPajama-INCITE-7B-Base\", # 12.90 GB\n",
    "    \"mpt_7b\" : \"mosaicml/mpt-7b\", # 12.39 GB\n",
    "    \"mpt_7b_8k\" : \"mosaicml/mpt-7b-8k\", # 12.39 GB\n",
    "    \"openllama2_7b\" : \"openlm-research/open_llama_7b_v2\", # 12.55 GB\n",
    "    \"llama2_7b\" : \"meta-llama/Llama-2-7b-hf\", # 12.55 GB\n",
    "    \"llama2_7b_32k\" : \"togethercomputer/LLaMA-2-7B-32K\", # 12.55 GB\n",
    "    \"mistral_7b\" : \"mistralai/Mistral-7B-v0.1\", # 13.49 GB\n",
    "    \"qwen_7b\" : \"Qwen/Qwen-7B\", # 14.38 GB\n",
    "    \"yi_6b\" : \"01-ai/Yi-6B\", # 11.29 GB\n",
    "    \"decilm_7b\" : \"Deci/DeciLM-7B\", # 13.12 GB\n",
    "    \n",
    "    \"openllama1_13b\" : \"openlm-research/open_llama_13b\", # 24.24 GB\n",
    "    \"llama2_13b\" : \"meta-llama/Llama-2-13b-hf\", # 24.25 GB\n",
    "    \"qwen_14b\" : \"Qwen/Qwen-14B\", # 26.39 GB\n",
    "    \"solar_10b\" : \"upstage/SOLAR-10.7B-v1.0\", # 19.99 GB\n",
    "    \n",
    "    \"llama1_33b\" : \"TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\", # 15.78 GB https://huggingface.co/alexl83/LLaMA-33B-HF\n",
    "    \"falcon_40b\" : \"TheBloke/falcon-40b-instruct-GPTQ\", # 21.00 GB https://huggingface.co/tiiuae/falcon-40b\n",
    "    \"mpt_30b\" : \"abhinavkulkarni/mosaicml-mpt-30b-instruct-w4-g128-awq\", # 15.00 GB https://huggingface.co/mosaicml/mpt-30b\n",
    "    \"codellama_34b\" : \"TheBloke/CodeLlama-34B-Instruct-GPTQ\", # 17.07 GB https://huggingface.co/codellama/CodeLlama-34b-hf\n",
    "    \"yi_34b\" : \"TheBloke/Yi-34B-GPTQ\", # 17.33 GB https://huggingface.co/01-ai/Yi-34B    \n",
    "    \"mixtral_8x7B\" : \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # 22.18 GB https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f6691-1719-4cb7-99ea-f8fb7e67dd51",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae3299f-b520-4278-919b-2cdc661e6020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:26.797509Z",
     "iopub.status.busy": "2023-12-28T07:49:26.797181Z",
     "iopub.status.idle": "2023-12-28T07:49:27.218595Z",
     "shell.execute_reply": "2023-12-28T07:49:27.218074Z",
     "shell.execute_reply.started": "2023-12-28T07:49:26.797498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7714b7e-ed3d-47e9-b02e-dff03299cf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:27.219501Z",
     "iopub.status.busy": "2023-12-28T07:49:27.219189Z",
     "iopub.status.idle": "2023-12-28T07:49:38.492600Z",
     "shell.execute_reply": "2023-12-28T07:49:38.492108Z",
     "shell.execute_reply.started": "2023-12-28T07:49:27.219489Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1975b72fc7a4f5c8659afeba6c9c106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3002dca56a9247dbad4786cfc0eb66dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a258c9f2514be8a6628dab63decaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d9b7008bda4feea3ba4debc09b3d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f026f05174e447aaf64d2950be338b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b7e4164a746fe96cc83dd2db5eeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name_fr = \"frenchtext/banque-fr-2311\"\n",
    "dataset_fr = load_dataset(dataset_name_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df161a8-456f-4eaa-b99f-13c3b90d2c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.493263Z",
     "iopub.status.busy": "2023-12-28T07:49:38.493139Z",
     "iopub.status.idle": "2023-12-28T07:49:38.496445Z",
     "shell.execute_reply": "2023-12-28T07:49:38.496000Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.493253Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 68166\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 8522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Uri', 'Timestamp', 'Lang', 'Title', 'Text', 'Words', 'AvgWordsLength', 'Chars', 'LetterChars', 'NumberChars', 'OtherChars', 'Website', 'PDF'],\n",
       "        num_rows: 8541\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b9ed0-02c3-43ea-8647-40db7517ff27",
   "metadata": {},
   "source": [
    "## Batching and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1603b5-31cf-4dc9-94ce-27b20091787d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.497102Z",
     "iopub.status.busy": "2023-12-28T07:49:38.496959Z",
     "iopub.status.idle": "2023-12-28T07:49:38.517384Z",
     "shell.execute_reply": "2023-12-28T07:49:38.516907Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.497092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = dataset_name_fr\n",
    "dataset = dataset_fr[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef174d1d-8294-485e-91fd-bd9889fbcba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.518125Z",
     "iopub.status.busy": "2023-12-28T07:49:38.517879Z",
     "iopub.status.idle": "2023-12-28T07:49:38.522196Z",
     "shell.execute_reply": "2023-12-28T07:49:38.521763Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.518111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_batches(dataset, batch_size=32):\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"Words\"]>10)\n",
    "    sorted_dataset = dataset.sort(\"Words\",reverse=True)\n",
    "    \n",
    "    dataset_length = len(sorted_dataset)\n",
    "    for start_idx in range(0, dataset_length, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_length)\n",
    "        yield sorted_dataset[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8fd7f9-aeec-451e-80b2-3c40360203a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.523552Z",
     "iopub.status.busy": "2023-12-28T07:49:38.523191Z",
     "iopub.status.idle": "2023-12-28T07:49:38.526032Z",
     "shell.execute_reply": "2023-12-28T07:49:38.525550Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.523538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encoding_offsets(encoding):\n",
    "    start_index = encoding.offsets[0][0]\n",
    "    end_index = encoding.offsets[-1][1]\n",
    "    if end_index==0: end_index = -1\n",
    "    return (start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f631c448-19a0-456c-abef-19aeaad60cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.526673Z",
     "iopub.status.busy": "2023-12-28T07:49:38.526512Z",
     "iopub.status.idle": "2023-12-28T07:49:38.529635Z",
     "shell.execute_reply": "2023-12-28T07:49:38.529201Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.526664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_batch(tokenizer, dataset_batch, stride=256):\n",
    "    encodings = tokenizer(text = dataset_batch[\"Text\"], add_special_tokens=True, \n",
    "                      padding=\"longest\", truncation=True, return_overflowing_tokens=True, stride=stride,\n",
    "                      # 2020: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#tensor-core-shape\n",
    "                      # However now in 2023, this is less and less true, newer drivers and cuda versions are smarter about this and will be able to use tensorcores even without this aligned padding\n",
    "                      pad_to_multiple_of=16, return_tensors=\"pt\")\n",
    "\n",
    "    encodings[\"overflow_to_sample_uri\"] = list(map(lambda sample_id: dataset_batch[\"Uri\"][sample_id.item()], encodings[\"overflow_to_sample_mapping\"]))\n",
    "    encodings[\"overflow_to_sample_offset\"] = list(map(get_encoding_offsets, encodings.encodings))\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8b9cf9-24e0-4800-81cd-c010e8b7d68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.530293Z",
     "iopub.status.busy": "2023-12-28T07:49:38.530163Z",
     "iopub.status.idle": "2023-12-28T07:49:38.533520Z",
     "shell.execute_reply": "2023-12-28T07:49:38.532921Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.530285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encodings_batches(tokenizer, dataset, batch_size=32, stride=256):\n",
    "    for dataset_batch in get_dataset_batches(dataset, batch_size):\n",
    "        encodings = encode_dataset_batch(tokenizer, dataset_batch, stride)\n",
    "        \n",
    "        encodings_length = len(encodings.encodings)\n",
    "        for start_idx in range(0, encodings_length, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, encodings_length)\n",
    "            yield {key: encodings[key][start_idx:end_idx] for key in encodings.data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c715b1d-5b91-4d7e-99f5-609573d93b19",
   "metadata": {},
   "source": [
    "## Compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042d937f-b4ed-4699-8279-eabf72836579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.536261Z",
     "iopub.status.busy": "2023-12-28T07:49:38.535760Z",
     "iopub.status.idle": "2023-12-28T07:49:38.538589Z",
     "shell.execute_reply": "2023-12-28T07:49:38.538127Z",
     "shell.execute_reply.started": "2023-12-28T07:49:38.536248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerplexityLogger:\n",
    "    def __init__(self, dataset_name, model_name):\n",
    "        self.filename = f\"{dataset_name.replace('/','_')}_{model_name.replace('/','_')}_perplexity.csv\"\n",
    "        self.file = open(self.filename, 'w')\n",
    "        \n",
    "    def log_batch(self, perplexity, uri, span):\n",
    "        self.file.write(f\"{perplexity},{uri},{span}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6acc7c5-e652-4511-81a4-e830d2d9ed4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T07:49:38.539309Z",
     "iopub.status.busy": "2023-12-28T07:49:38.539097Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wordslab-llms/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity on dataset frenchtext/banque-fr-2311 for togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "- dataset examples: 68166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model sequence length: 2048\n",
      "4 encodings processed -> perplexity = 7.730084419250488\n",
      "44 encodings processed -> perplexity = 6.754348278045654\n",
      "84 encodings processed -> perplexity = 6.998272895812988\n",
      "124 encodings processed -> perplexity = 6.814899921417236\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "batch_size = 4\n",
    "stride = 256\n",
    "\n",
    "for model_id in models:\n",
    "    model_name = models[model_id]\n",
    "    print(f\"Computing perplexity on dataset {dataset_name} for {model_name}\")\n",
    "    print(f\"- dataset examples: {len(dataset)}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\") \n",
    "    print(f\"- model sequence length: {int(tokenizer.model_max_length)}\")\n",
    "    \n",
    "    logger = PerplexityLogger(dataset_name, model_name)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    losses = []    \n",
    "    for idx,encodings_batch in enumerate(get_encodings_batches(tokenizer, dataset, batch_size=batch_size, stride=stride)):\n",
    "        with torch.no_grad():\n",
    "            # predict next token\n",
    "            inputs = encodings_batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = encodings_batch[\"attention_mask\"].to(model.device)\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask, use_cache=False, output_attentions=False, output_hidden_states=False)\n",
    "            \n",
    "            # compute perplexity\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shift_logits = outputs.logits[:, :-1, :].permute(0, 2, 1).contiguous()\n",
    "            labels = inputs[:, 1:].contiguous()\n",
    "            batch_losses = loss_fct(shift_logits, labels).mean(1)\n",
    "            losses.extend(batch_losses)\n",
    "            batch_perplexities = torch.exp(batch_losses).tolist()\n",
    "                    \n",
    "        for uri,span,perplexity in zip(batch_perplexities, encodings_batch[\"overflow_to_sample_uri\"], encodings_batch[\"overflow_to_sample_offset\"]):\n",
    "            logger.log_batch(uri, span, perplexity)\n",
    "    \n",
    "        if idx%10 == 0:\n",
    "            perplexity = torch.exp(torch.stack(losses).mean().float()).item()\n",
    "            print(f\"{(idx+1)*batch_size} encodings processed -> perplexity = {perplexity}\")\n",
    "        \n",
    "    perplexity = torch.exp(torch.stack(losses).mean().float()).item()\n",
    "    print(f\"-> perplexity = {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05092f7-8c51-4c4e-bad9-e29c7ca4adcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-llms",
   "language": "python",
   "name": "wordslab-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
